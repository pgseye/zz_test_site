{
  "hash": "49c3141938f521435139bc396e0623d8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"ROC Curves in Clinical Decision-Making: A Gentle Introduction\"\ndate: 2026-04-24\ncategories: [concept, code]\nimage: \"images/roc.png\"\ndescription: \"What they are and how they're used.\"\n---\n\n\n## Introduction\n\nWhen clinicians or researchers develop a diagnostic test - say, a blood biomarker for detecting a disease - they’re really asking:\n\n> \"How well does this test distinguish sick from healthy?\"\n\nIn being able to answer that question, we ultimately care about two things:\n\n-   **Sensitivity** — the ability to correctly identify the sick (true positives).\n\n-   **Specificity** — the ability to correctly identify the healthy (true negatives).\n\nBut diagnostic tests often give a continuous value (e.g., concentration of a biomarker). So we have to choose a threshold: values above are positive and values below are negative. So where should we set this cut-off?\n\nEnter the receiver-operator characteristic (ROC) curve. This is a tool that evaluates how **sensitivity and specificity trade off as we vary a test threshold of interest**. In addition, the area under the ROC curve gives an idea about the benefit of using the test in question - the greater the area the more accurate the test. In today's post we'll talk about ROC curves and how they're used to aid in clinical-decision making. At first blush the ROC curve can appear quite confusing and difficult to interpret, but I'm hoping by the end of this post I'll have demystified it enough that you can comfortably employ one in your next diagnostic study.\n\n------------------------------------------------------------------------\n\n## A Bit of History: From Radar to Medicine\n\nInterestingly, ROC curves were first developed during World War II by electrical and radar engineers in the UK. They were trying to distinguish real aircraft returns from noise and clutter on radar screens. The question was essentially the same: given a signal, how do we choose a threshold to decide \"target present\" vs \"target absent\" while balancing false alarms and misses? After the war, this framework migrated into signal detection theory and then into statistics and also to multiple areas within medicine. The ROC curve has also been applied in other research fields including biometrics, forecasting of natural hazards, meteorology, model performance assessment, and is increasingly used in machine learning and data mining research. As you can tell, it has wide utility in decision making, but it's ultimately based on a relatively simple idea.\n\n------------------------------------------------------------------------\n\n## Key Concepts\n\n### The Confusion Matrix (2×2 Table)\n\nAt the heart of ROC analysis is the confusion matrix, a simple 2×2 table that compares the results of a diagnostic test against the true disease status. For any chosen threshold that classifies a continuous test result as positive or negative, each individual falls into one of four categories. True positives (TP) are patients who truly have the disease and whose test result is positive, while true negatives (TN) are disease-free individuals correctly identified as negative by the test. Errors arise in two ways: false positives (FP) occur when healthy individuals are incorrectly labelled as having the disease, and false negatives (FN) occur when diseased individuals are missed by the test.\n\nThese four quantities form the basis of nearly all diagnostic performance measures. In particular, sensitivity (also called the true positive rate) is the proportion of diseased individuals correctly detected by the test. Specificity is the proportion of non-diseased individuals correctly classified. ROC curves are constructed by repeatedly recalculating these quantities as the decision threshold moves, tracing out how sensitivity increases at the cost of decreasing specificity, **for every possible test threshold**.\n\n|                   | **Disease Present** | **Disease Absent**  |\n|-------------------|---------------------|---------------------|\n| **Test Positive** | True Positive (TP)  | False Positive (FP) |\n| **Test Negative** | False Negative (FN) | True Negative (TN)  |\n\n<br>\n\nSo from this simple table, we calculate:\n\n-   Sensitivity (True Positive Rate) = TP / (TP + FN)\n\n-   Specificity = TN / (TN + FP)\n\n-   False Positive Rate = 1 – Specificity = FP / (FP + TN)\n\n### ROC Curve\n\nTo construct a ROC curve it's then a relatively simple case of plotting Sensitivity (True Positive Rate) vs False Positive Rate (FPR) at every possible threshold. A perfect test would go straight up the y-axis to `1.0` then straight across with a resulting Area Under Curve (AUC) = `1.0`. In contrast, a test no better than chance would follow the diagonal with an AUC = `0.5`. In practice, most clinical tests fall somewhere in between.\n\nLet's illustrate this in practice. I will use the `pROC` package in `R` for the analyses in this post, but there are other ROC creation packages available in `R`, and your mileage will vary depending on what you want. I find `pROC` fairly simple but if you are after something with a little more flexibility than I would encourage you to also explore the `cutpointR` package - I know that it offers more options in the calculation of optimal cutpoints. I have not personally used the `ROCR` package but this is another one that is available for these type of analyses.\n\n## A Simulated Example\n\nTo help you to better understand what a ROC curve is doing in calculations under the hood, I have simulated some data (`50` observations) containing a continuous biomarker (`test_values`) and a binary `disease` status. Let's take a look at the first few rows of the simulated data:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pROC)\nlibrary(kableExtra)\nlibrary(tidyverse)\n\nset.seed(123)\n\n# Simulate data\nn <- 50\n\n# Disease status: 0 = healthy, 1 = diseased\ndisease <- rbinom(n, size = 1, prob = 0.4)\n\n# Continuous test: diseased group has higher values\ntest_values <- rnorm(n, mean = 0 + 2*disease, sd = 1)\n\nsim_data <- data.frame(disease, test_values) |> \n  arrange(desc(test_values))\n\n# Peek at data\nhead(sim_data) |> \n    kable(align = \"c\", digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n| disease | test_values |\n|:-------:|:-----------:|\n|    1    |    3.52     |\n|    1    |    3.25     |\n|    1    |    3.21     |\n|    1    |    2.90     |\n|    1    |    2.84     |\n|    1    |    2.78     |\n\n\n:::\n:::\n\n\nNow, let's plot these data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sim_data, aes(x = factor(disease), y = test_values)) +\n  geom_point(aes(color = factor(disease))) +\n  theme_bw(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=80%}\n:::\n:::\n\n\nWhat can you see from this plot? Well, the obvious thing we can appreciate is that if you have this fictitious disease, the biomarker test value tends to be higher compared to if you don't. Ahhh, you might think - that is a useful characteristic to have if you want the test to be able to discriminate between those with and those without the disease. And you'd be right of course. But you might also be able to appreciate that because there is overlap in test values across disease states, no single test value is going to be `100%` accurate.\n\nFor the sake of the illustration let's set an initial test value cut-off (threshold) at `0`, like so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sim_data, aes(x = factor(disease), y = test_values)) +\n  geom_point(aes(color = factor(disease))) +\n  geom_hline(color = \"red\", yintercept = 0) +\n  theme_bw(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=80%}\n:::\n:::\n\n\nIn interpreting this and the following plot, keep in mind the following:\n\n> We are diagnosing everyone with a test value ABOVE the red line - regardless of their actual status - as having the disease.\n\n> We are calling everyone with a test value BELOW the red line - regardless of their actual status - as being healthy.\n\nWith a threshold set at `0` we will correctly identify *all* of those with the disease, so our true positive rate (TPR) will be `100%`, but you'll note that we will also misclassify about half of those without the disease - our false positive rate (FPR) will be about `50%`. Not good enough you might think - we don't want to diagnose a bunch of people as sick when they're not, because then we might subject them to unnecessary treatment. Instead, let's make the cut-off for our biomarker higher and see if that helps.\n\nSo now we raise the cut-off to `1`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sim_data, aes(x = factor(disease), y = test_values)) +\n  geom_point(aes(color = factor(disease))) +\n  geom_hline(color = \"red\", yintercept = 1) +\n  theme_bw(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=80%}\n:::\n:::\n\n\nWe no longer correctly identify all those with the disease (TPR = `85%`), but we also don't incorrectly misdiagnose as many healthy individuals either (FPR = `13%`). Only clinical judgement can ultimately decide whether this is a more acceptable decision threshold but certainly it better balances misclassification error.\n\nThe reason I am showing you these plots is that they form the basis of a ROC curve. If you created such a plot for every test value (threshold) in the dataset, calculating the TPR and FPR in each case (and collating each as a paired set of coordinates), then this is all the information you would need to plot the resulting ROC curve.\n\n### Plotting a ROC Curve by First Principles\n\nLet's do just that now. I am going to take you through this process step by step to illustrate (and demystify) how a ROC curve is constructed. To make my life a little easier I will use the `coords()` function in the `pROC` package to calculate the TPR and FPR at every test value. I will also list the test value itself, as well as disease status an the cumulative number of TP's and FP's - these are helpful to understand how the rates are generated. To aid interpretation biomarker test values are first ranked in order from highest to lowest. The resulting data are as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute ROC object\nroc_obj <- roc(sim_data$disease, sim_data$test_values)\n# Calculate threshold data\ncoord_data <-  coords(roc_obj, ret = c(\"threshold\", \"tp\", \"fp\", \"tpr\", \"fpr\"))\n# Merge in simulated data\ncoord_data <-  coord_data |> \n  arrange(desc(threshold)) |> \n  slice(-1)\ncoord_data <-  cbind(coord_data, sim_data) |> \n  mutate(id = row_number()) |> \n  select(id, test_values, disease, tp, fp, tpr, fpr)\n# Print\ncoord_data |> \n      kable(align = \"c\", digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n| id | test_values | disease | tp | fp | tpr  | fpr  |\n|:--:|:-----------:|:-------:|:--:|:--:|:----:|:----:|\n| 1  |    3.52     |    1    | 1  | 0  | 0.05 | 0.00 |\n| 2  |    3.25     |    1    | 2  | 0  | 0.10 | 0.00 |\n| 3  |    3.21     |    1    | 3  | 0  | 0.15 | 0.00 |\n| 4  |    2.90     |    1    | 4  | 0  | 0.20 | 0.00 |\n| 5  |    2.84     |    1    | 5  | 0  | 0.25 | 0.00 |\n| 6  |    2.78     |    1    | 6  | 0  | 0.30 | 0.00 |\n| 7  |    2.69     |    1    | 7  | 0  | 0.35 | 0.00 |\n| 8  |    2.58     |    1    | 8  | 0  | 0.40 | 0.00 |\n| 9  |    2.25     |    1    | 9  | 0  | 0.45 | 0.00 |\n| 10 |    2.17     |    0    | 9  | 1  | 0.45 | 0.03 |\n| 11 |    2.12     |    1    | 10 | 1  | 0.50 | 0.03 |\n| 12 |    2.05     |    0    | 10 | 2  | 0.50 | 0.07 |\n| 13 |    1.94     |    1    | 11 | 2  | 0.55 | 0.07 |\n| 14 |    1.92     |    1    | 12 | 2  | 0.60 | 0.07 |\n| 15 |    1.60     |    1    | 13 | 2  | 0.65 | 0.07 |\n| 16 |    1.53     |    1    | 14 | 2  | 0.70 | 0.07 |\n| 17 |    1.50     |    1    | 15 | 2  | 0.75 | 0.07 |\n| 18 |    1.37     |    0    | 15 | 3  | 0.75 | 0.10 |\n| 19 |    1.31     |    1    | 16 | 3  | 0.80 | 0.10 |\n| 20 |    1.31     |    1    | 17 | 3  | 0.85 | 0.10 |\n| 21 |    1.01     |    0    | 17 | 4  | 0.85 | 0.13 |\n| 22 |    0.92     |    0    | 17 | 5  | 0.85 | 0.17 |\n| 23 |    0.88     |    0    | 17 | 6  | 0.85 | 0.20 |\n| 24 |    0.88     |    1    | 18 | 6  | 0.90 | 0.20 |\n| 25 |    0.86     |    1    | 19 | 6  | 0.95 | 0.20 |\n| 26 |    0.82     |    0    | 19 | 7  | 0.95 | 0.23 |\n| 27 |    0.55     |    0    | 19 | 8  | 0.95 | 0.27 |\n| 28 |    0.45     |    1    | 20 | 8  | 1.00 | 0.27 |\n| 29 |    0.45     |    0    | 20 | 9  | 1.00 | 0.30 |\n| 30 |    0.43     |    0    | 20 | 10 | 1.00 | 0.33 |\n| 31 |    0.38     |    0    | 20 | 11 | 1.00 | 0.37 |\n| 32 |    0.30     |    0    | 20 | 12 | 1.00 | 0.40 |\n| 33 |    0.22     |    0    | 20 | 13 | 1.00 | 0.43 |\n| 34 |    0.15     |    0    | 20 | 14 | 1.00 | 0.47 |\n| 35 |    0.05     |    0    | 20 | 15 | 1.00 | 0.50 |\n| 36 |    -0.03    |    0    | 20 | 16 | 1.00 | 0.53 |\n| 37 |    -0.04    |    0    | 20 | 17 | 1.00 | 0.57 |\n| 38 |    -0.21    |    0    | 20 | 18 | 1.00 | 0.60 |\n| 39 |    -0.23    |    0    | 20 | 19 | 1.00 | 0.63 |\n| 40 |    -0.30    |    0    | 20 | 20 | 1.00 | 0.67 |\n| 41 |    -0.31    |    0    | 20 | 21 | 1.00 | 0.70 |\n| 42 |    -0.33    |    0    | 20 | 22 | 1.00 | 0.73 |\n| 43 |    -0.38    |    0    | 20 | 23 | 1.00 | 0.77 |\n| 44 |    -0.49    |    0    | 20 | 24 | 1.00 | 0.80 |\n| 45 |    -0.71    |    0    | 20 | 25 | 1.00 | 0.83 |\n| 46 |    -1.02    |    0    | 20 | 26 | 1.00 | 0.87 |\n| 47 |    -1.07    |    0    | 20 | 27 | 1.00 | 0.90 |\n| 48 |    -1.27    |    0    | 20 | 28 | 1.00 | 0.93 |\n| 49 |    -1.69    |    0    | 20 | 29 | 1.00 | 0.97 |\n| 50 |    -2.31    |    0    | 20 | 30 | 1.00 | 1.00 |\n\n\n:::\n:::\n\n\nThe way to interpret this table is as follows. Each row contains the data for an \"independent\" set of calculations assuming the `test_value` in that row were the threshold value decided on for the data. The TP and FP numbers are cumulative counts of individuals who are classified as having the disease regardless of whether they actually do (TP) or don't (FP). In other words, the `test_value` in that row **indicates the biomarker value whereby ANY test measure equal to OR greater than that, will result in a positive test classification**. Let's pick a few rows out and perform the calculations manually. But before we do that, let's quickly establish the denominators we need for these calculations - the total numbers of individuals with and without the disease. We can do that with a simple `table(sim_data$disease)` which tells us that there are `30` healthy individuals and `20` diseased individuals in this fictitious dataset.\n\nLet's look at the first row of data (`id` = `1`). This contains the highest recorded biomarker level and there is one individual with this value. This individual also happens to have the disease, so they are considered a TP. The TPR is then calculated as `1/20 = 0.05` and the FPR is calculated as `0/30 = 0`. This gives us the first point in our ROC curve so let's plot that:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoord_data |> \n  slice(1) |> \n  ggplot(aes(fpr, tpr)) +\n  geom_point(size = 3) +\n  xlab(\"FPR (1 - Specificity)\") + ylab(\"TPR (Sensitivity)\") +\n  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  theme_bw(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=80%}\n:::\n:::\n\n\nNow let's consider the second row of data (`id` = `2`). This individual also happens to have the disease, so they are considered a TP. Our cumulate TP count increases to `2`, but our FP count remains unchanged. The TPR is now calculated as `2/20 = 0.1` and the FPR remains as `0/30 = 0`. This gives us the second point in our ROC curve so we can now plot both:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoord_data |> \n  slice(1:2) |> \n  ggplot(aes(fpr, tpr)) +\n  geom_point(size = 3) +\n  xlab(\"FPR (1 - Specificity)\") + ylab(\"TPR (Sensitivity)\") +\n  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  theme_bw(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=80%}\n:::\n:::\n\n\nNow let's skip ahead to `id` = `10` because I hope you're starting to get a feel for things. This represents our first healthy individual. But because the biomarker value of `2.17` measured for this individual is the new threshold at play, they would be (mis)classified as having the disease. So their count contributes to the cumulative FP and FPR. Thus, the TPR is now calculated as `9/20 = 0.45` and the FPR as `1/30 = 0.03`. This gives us the 10th point in our ROC curve, so let's plot these and all preceding points.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoord_data |> \n  slice(1:10) |>  \n  ggplot(aes(fpr, tpr)) +\n  geom_point(size = 3) +\n  xlab(\"FPR (1 - Specificity)\") + ylab(\"TPR (Sensitivity)\") +\n  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  theme_bw(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=80%}\n:::\n:::\n\n\nWe can do the same for the first `20` observations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoord_data |> \n  slice(1:20) |> \n  ggplot(aes(fpr, tpr)) +\n  geom_point(size = 3) +\n  xlab(\"FPR (1 - Specificity)\") + ylab(\"TPR (Sensitivity)\") +\n  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  theme_bw(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=80%}\n:::\n:::\n\n\nand all observations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoord_data |> \n  ggplot(aes(fpr, tpr)) +\n  geom_point(size = 3) +\n  xlab(\"FPR (1 - Specificity)\") + ylab(\"TPR (Sensitivity)\") +\n  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  theme_bw(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=80%}\n:::\n:::\n\n\nSo that's how you construct a ROC curve manually, but of course there's no need to do this in practice. There are plenty of functions available to do this automatically for you. \n\n### Plotting a ROC Curve the Easy Way\n\nComputing the ROC curve using the `pROC` package, is then really quite simple. The first way I will show you is with the out-of-the-box roc plot function that comes with `pROC`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute ROC object\nroc_obj <- roc(sim_data$disease, sim_data$test_values)\n\n# Basic ROC Plot\nplot(roc_obj)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=1152}\n:::\n:::\n\n\nAs you can see it's a functional, but no frills plot. We can embellish this, however, by including the 95% C.I.s and some annotation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extended ROC Plot  (base R)\nroc_obj <- plot.roc(sim_data$disease, sim_data$test_values,\n                     main = \"Confidence intervals\", percent = TRUE,\n                     ci = TRUE, # compute AUC (of AUC by default)\n                     print.auc = TRUE) # print the AUC (will contain the CI)\nci_obj <- ci.se(roc_obj, # CI of sensitivity\n               specificities = seq(0, 100, 5)) # over a select set of specificities\nplot(ci_obj, type = \"shape\", col = \"#1c61b6AA\") # plot as a blue shape\nplot(ci(roc_obj, of = \"thresholds\", thresholds = \"best\")) # add one threshold\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=1152}\n:::\n:::\n\n\nThere is a second plotting option that leverages `ggplot()` functionality and while a little bit more coding effort is required, is considerably more flexible. That can generate something like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute ROC object\nroc_obj <- roc(sim_data$disease, sim_data$test_values)\n\n# Calculate CI for sensitivity across specificities\nci_obj <- ci.se(roc_obj, \n                specificities = seq(0, 1, 0.05))\n\n# Extract CI data for plotting - CONVERT TO 1-SPECIFICITY\nci_data <- data.frame(\n  fpr = 1 - as.numeric(rownames(ci_obj)),  # 1 - specificity\n  lower = ci_obj[, 1],\n  upper = ci_obj[, 3]\n)\n\n# Calculate AUC CI\nauc_ci <- ci.auc(roc_obj)\n\n# Create label\nauc_label <- paste0(\"AUC = \", round(auc(roc_obj), 3),\n                    \"\\n95% CI: \", round(auc_ci[1], 3), \n                    \" - \", round(auc_ci[3], 3))\n\n# Find optimal threshold (Youden's index)\nbest_coords <- coords(roc_obj, \"best\", ret = c(\"threshold\", \"sensitivity\", \"specificity\"))\n\n# Plot\nggroc(roc_obj, legacy.axes = TRUE) +\n  geom_ribbon(data = ci_data, \n              aes(x = fpr, ymin = lower, ymax = upper),  # Use fpr (1-specificity)\n              fill = \"#1c61b6AA\", alpha = 0.3, inherit.aes = FALSE) +\n  annotate(\"point\", x = 1 - best_coords$specificity, y = best_coords$sensitivity,  # Convert here too\n           size = 3, color = \"red\") +\n  annotate(\"text\", x = 0.75, y = 0.5, label = auc_label, hjust = 0) +\n  annotate(\"text\", x = 1 - best_coords$specificity + 0.12,  # Adjust position\n           y = best_coords$sensitivity - 0.06,\n           label = paste(\"Threshold =\", round(best_coords$threshold, 2))) +\n  xlab(\"FPR (1 - Specificity)\") + ylab(\"TPR (Sensitivity)\") +\n  theme_bw(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=1152}\n:::\n:::\n\n\nThat looks even nicer to my eye. In this plot I have also included and annotated the optimal threshold(s) based on the [Youden index](https://en.wikipedia.org/wiki/Youden%27s_J_statistic) - a popular statistic for evaluating ROC performance. In this case there are two (equivalent) optimum thresholds at each of biomarker values `0.84` and `1.16`. Remember when I said earlier that a test value of `1` might count for a sensible threshold...\n\n### Other Bits and Pieces\n\nI mentioned the Youden Index above. If this is something you're after it's simple to extract this as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get optimal cutpoint based on youden Index\ncoords(roc_obj, x = \"best\", best.method = \"youden\") |> \n      kable(align = \"c\", digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n| threshold | specificity | sensitivity |\n|:---------:|:-----------:|:-----------:|\n|   0.84    |     0.8     |    0.95     |\n|   1.16    |     0.9     |    0.85     |\n\n\n:::\n:::\n\n\nAnd likewise, for the AUC:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get AUC\nauc(roc_obj)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nArea under the curve: 0.9383\n```\n\n\n:::\n:::\n\n\n## Wrap-Up\n\nI hope I've shown you how ROC curves are a foundational tool in clinical decision analytics. But as importantly I hope I've given you some insight into how they are created. Understanding the calculations underlying their visualisation will place you in better stead for interpreting a ROC curve in the broader clinical context of your patient's care.\n\nUntil next month...",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}