{
  "hash": "c709d5f9dc28dca223d49da2bd126f82",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Everything is a Linear Model\"\ndate: 2024-04-19\ncategories: [analysis, concept, code, modelling]\ndescription: \"The t-test and linear model with one grouping variable are two sides of the same coin.\"\n---\n\n\nI want to share with you a secret - maybe you already know it. It took me a while into my statistical learnings to realise this and since then I've seen people write about it (see [here](https://lindeloev.github.io/tests-as-linear/) and [here](https://danielroelfs.com/blog/everything-is-a-linear-model/) for examples). But the basic idea is that many of the common statistical tests that we use (e.g. t-test, ANOVA, etc) are really nothing more than variations on the general linear model that we're all accustomed to:\n\n$$ y = ax + b $$\n\nThe former are specific-use tests, whereas the latter is an 'umbrella' model that can be broadly adapted to accomplish each of the same tasks - perhaps there's something to be said for learning just one set of syntax. Let me illustrate this to you with one example using the two-sample t-test. We'll use the `genderweight` dataset from the `datarium` package in R which consists of the bodyweights of 40 subjects (20 males, 20 females). We're interested in working out whether there is a gender difference. A look at the data shows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\ndata(\"genderweight\", package = \"datarium\")\nhead(genderweight, 10)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|id |group |   weight|\n|:--|:-----|--------:|\n|1  |F     | 61.58587|\n|2  |F     | 64.55486|\n|3  |F     | 66.16888|\n|4  |F     | 59.30860|\n|5  |F     | 64.85825|\n|6  |F     | 65.01211|\n|7  |F     | 62.85052|\n|8  |F     | 62.90674|\n|9  |F     | 62.87110|\n|10 |F     | 62.21992|\n\n</div>\n:::\n:::\n\n\n# Plot the Data\n\nIt's always helpful to first plot the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(genderweight, aes(x = group, y = weight)) +\n  geom_jitter(size = 3, width = 0.05) +\n  scale_y_continuous(limits = c(50, 100), breaks = seq(50, 100, by = 10)) +\n  stat_summary(fun = mean, \n               geom = \"errorbar\", \n               aes(ymax = after_stat(y), ymin = after_stat(y)), \n               width = 0.25) +\n  theme_bw(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-1.png){width=1152}\n:::\n:::\n\n\n# Two-Sample t-Test\n\nNow, we can run our standard t-test as follows (by default, computing the Welch version of the test which does not assume the same variances in each group). In words, we are asking to test the difference in weight by group (i.e. males vs females).\n\n`t.test(weight ~ group, data = genderweight)`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(weight ~ group, data = genderweight)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  weight by group\nt = -20.791, df = 26.872, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n95 percent confidence interval:\n -24.53135 -20.12353\nsample estimates:\nmean in group F mean in group M \n       63.49867        85.82612 \n```\n\n\n:::\n:::\n\n\nThis output tells us that the mean weight in females and males is `63.5 kg` and `85.8 kg`, respectively. Furthermore, the 95% C.I. for the difference (note that is does not give us the actual difference) in those two weights is `-24.5, -20.1` and as the interval does not contain `0` this is statistically significant (as also reflected in the p-value).\n\n# Linear Model\n\nNow, the equivalent linear model (i.e. linear regression) in `R` is simply:\n\n`summary(lm(weight ~ group, data = genderweight))`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(weight ~ group, data = genderweight))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = weight ~ group, data = genderweight)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8163 -1.3647 -0.4869  1.3980  9.2365 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  63.4987     0.7593   83.62   <2e-16 ***\ngroupM       22.3274     1.0739   20.79   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.396 on 38 degrees of freedom\nMultiple R-squared:  0.9192,\tAdjusted R-squared:  0.9171 \nF-statistic: 432.3 on 1 and 38 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nThe output is slightly different but the information contained is almost the same. `(Intercept)` represents the mean weight in the reference category of the `group` variable (in this case females). `groupM` represents the difference in means between females and males (`22.3 kg`). Note that the 95% C.I.'s aren't presented as part of this standard output, but we can obtain that information easily enough with:\n\n`confint(lm(weight ~ group, data = genderweight))`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(lm(weight ~ group, data = genderweight))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               2.5 %   97.5 %\n(Intercept) 61.96145 65.03589\ngroupM      20.15349 24.50140\n```\n\n\n:::\n:::\n\n\nNote the slight difference in the 95% C.I.'s to that obtained from the t-test. The general linear model, by assumption, assumes homogeneity of variances among the two groups. \n\nFinally, if you would prefer to know the actual mean values of each group as well, it's possible to amend the `lm` call slightly by removing the intercept term. This gives:\n\n`summary(lm(weight ~ group - 1, data = genderweight))`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(weight ~ group - 1, data = genderweight))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = weight ~ group - 1, data = genderweight)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8163 -1.3647 -0.4869  1.3980  9.2365 \n\nCoefficients:\n       Estimate Std. Error t value Pr(>|t|)    \ngroupF  63.4987     0.7593   83.62   <2e-16 ***\ngroupM  85.8261     0.7593  113.03   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.396 on 38 degrees of freedom\nMultiple R-squared:  0.9981,\tAdjusted R-squared:  0.998 \nF-statistic:  9884 on 2 and 38 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nThe two-sample t-test is just one example of a special case of the general linear model. The first link I provided above contains a neat pdf describing many other special cases and I would encourage you to have a look at these. While you might still use these specific tests in your day to day work, it is nonetheless helpful to broaden your statistical knowledge in the realisation that the general linear model is fundamental to all of these.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}