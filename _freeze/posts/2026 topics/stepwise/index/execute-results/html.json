{
  "hash": "ba747e60eb0b0ca0a017cf7685ab28fe",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"The Perils of Stepwise Regression - And what to do instead\"\ndate: 2026-01-30\ncategories: [code, concept]\nimage: \"elastic.png\"\ndescription: \"Data-driven variable selection does not represent best practice in applied statistics.\"\n---\n\n\n\n\n## Introduction\n\nAs a clinician/researcher you might not be aware that data-driven variable selection methods are widely regarded within the statistical community as the the equivalent of Facebook's 'Crazy Uncle' - an individual who is misinformed, opinionated and someone we just can't easily shed from our lives.\n\nSo what's the parallel I'm trying to draw, you might ask?\n\nWell, despite decades of criticism in the statistical literature, stepwise regression (as a form of data-driven variable selection) continues to appear in academic papers, regulatory submissions, and industry analyses, when it is universally recognised as plain wrong. Despite that, stepwise regression remains one of the most commonly taught and used model-building techniques in applied statistics. Indeed, I was taught these methods during my Masters of Biostatistics 10 years ago. The appeal is obvious: when faced with many candidate predictors and limited sample size, stepwise procedures promise an automated, objective way to \"let the data decide\" which variables matter.\n\n::: callout-note\nWhen I refer to \"data-driven\" or \"stepwise\" I am meaning any decision process leading to the addition or removal of candidate variables from a regression model that is based on data-derived statistics such as significance, R-squared, Akaike Information Criteria (AIC), etc...\n:::\n\nSo then, what's the problem?\n\nWell, there are many. The most notable I will summarise from page 68 of Frank Harrell's modern statistical classic - [*Regression Modelling Strategies*](https://hbiostat.org/rmsc/):\n\n1.  The R-squared or even adjusted R-squared values of the end model are biased high.\n\n2.  The F and Chi-square test statistics of the final model do not have the claimed distribution.\n\n3.  The standard errors of coefficient estimates are biased low and confidence intervals for effects and predictions are falsely narrow.\n\n4.  The p values are too small (there are severe multiple comparison problems in addition to problems 2. and 3.) and do not have the proper meaning, and it is difficult to correct for this.\n\n5.  The regression coefficients are biased high in absolute value and need shrinkage but this is rarely done.\n\n6.  Variable selection is made arbitrary by collinearity.\n\n7.  It allows us to not think about the problem.\n\nThe net effect of the above is that we may end up making claims, assertions or clinical decisions based on incorrect evidence.\n\nIn this post I will attempt to make the case that stepwise regression is not just suboptimal, but seriously flawed when used for inference, explanation, or decision-making. The problems are not subtle or academic; they directly undermine the validity, reproducibility, and interpretability of results. I will outline why stepwise regression fails, why these failures are amplified in applied research (particularly clinical and epidemiological settings), and what modern, defensible alternatives look like in practice.\n\nThe goal is not to shame analysts who use stepwise regression — I too am guilty of the multitude of sins I am about to discuss — but to provide a clear roadmap towards best practice.\n\n------------------------------------------------------------------------\n\n## Why Stepwise Regression Persists\n\nLet's get the elephant in the room out of the way. If there are so many issues with stepwise regression why is it still so commonly used in the analysis of clinical data? Well, I don't think there is a single answer to this question, but rather several potential factors that probably interplay in maintaining its veneer of contemporary methodological relevance:\n\n-   Historical inertia: it has been taught for decades and appears in older textbooks.\n\n-   Fear of change: researchers may worry that journal editors/reviewers do not appreciate new approaches.\n\n-   Cognitive appeal: produces a single, seemingly parsimonious model.\n\n-   Ease of implementation: many statistical packages make stepwise selection easy and prominent.\n\n-   Misplaced objectivity: automated procedures appear neutral, even when they enforce arbitrary thresholds on our data.\n\nThese justifications, when considered on an individual basis, become hard to defend. But before we look at what we can do instead, let's first delve a little deeper into the specific problems associated with stepwise regression.\n\n------------------------------------------------------------------------\n\n## What is Stepwise Regression?\n\nMost of you already know what I'm talking about, but let's recap the basics for those who don't.\n\nStepwise regression refers to a family of automated variable selection procedures applied to regression models. The most common variants are:\n\n-   Forward selection: start with no predictors, then add variables one at a time based on some criterion (often the smallest p-value or largest improvement in AIC).\n\n-   Backward elimination: start with all candidate predictors, then remove variables that fail to meet a significance threshold.\n\n-   Bidirectional (stepwise) selection: alternate between adding and removing variables at each step.\n\nThese procedures are typically driven by hypothesis tests (e.g. p \\< 0.05) or information criteria (AIC, BIC), and they terminate once no further steps improve the chosen criterion. The end result is a single model containing a data-driven subset of the original pool of available candidate predictor variables.\n\n------------------------------------------------------------------------\n\n## The Core Statistical Problems\n\nAlthough I mentioned these at the outset of this post (based on Harrell's text), let's flesh some of them out in more detail now.\n\n### Invalid Inference After Selection\n\nPerhaps the most fundamental problem is that standard inferential quantities are wrong after stepwise selection. When using data-driven variable selection:\n\n> P-values, confidence intervals, and standard errors are computed as if the model had been **specified in advance**.\n\nBut in reality, the data were used at least twice: once to select variables, and again to estimate their effects.\n\nThis reuse of data leads to:\n\n-   underestimated standard errors,\n\n-   overly narrow confidence intervals,\n\n-   and inflated statistical significance.\n\nYou may not be aware that statistical inference methods using p values, standard errors and confidence intervals were designed to be **valid when applied once to a pre-specified model**, not iteratively reapplied to a model where chance partly informs the decision at each step. This invalidates the nominal properties of a statistical test, giving us as researchers, a false sense of certainty.\n\n### Multiple Testing in Disguise\n\nStepwise regression performs a large number of implicit hypothesis tests while pretending to conduct only a few. Each step involves testing multiple candidate variables, but no adjustment is made for multiplicity.\n\nViewed correctly, stepwise selection is a form of uncorrected multiple testing with a stopping rule. The resulting family-wise Type I error rate can be dramatically higher than the nominal level, especially when predictors are correlated.\n\nThis means that \"statistically significant\" predictors selected by stepwise methods are often artifacts of chance rather than real signals.\n\n### Overfitting and Optimism\n\nStepwise procedures are designed to optimise in-sample fit. As a result, they tend to overfit, especially when:\n\n-   the number of predictors is large relative to the sample size,\n\n-   predictors are correlated,\n\n-   or the true signal-to-noise ratio is modest.\n\nOverfitted models appear impressive in the training data but perform poorly on new data. This optimism is rarely quantified or acknowledged when stepwise regression is used.\n\n### Model Instability\n\nOne of the most damaging properties of stepwise regression is instability. Small perturbations in the data - e.g. removing a few observations, changing a significance threshold, or using a different random split - can produce entirely different selected models.\n\nThis instability arises because stepwise selection operates near decision boundaries. When predictors are correlated or effects are weak, tiny changes can flip inclusion decisions.\n\nAn unstable model cannot be trusted for interpretation, explanation, or clinical decision-making.\n\n### Biased Coefficient Estimates\n\nEven when a predictor truly has an effect, stepwise regression tends to overestimate its magnitude if it is selected. This phenomenon - sometimes called the \"winner’s curse\" - occurs because variables are selected given they look large in the sample. The result is that reported effect sizes tend to be exaggerated, with subsequent studies often failing to replicate the findings.\n\n------------------------------------------------------------------------\n\n## Why These Problems Matter in Applied Research\n\nIn some purely predictive settings, poor inference may be tolerable. In most applied research, however, regression models are used to:\n\n-   draw causal conclusions,\n\n-   inform clinical or policy decisions,\n\n-   support regulatory submissions,\n\n-   or generate scientific knowledge.\n\nIn these contexts, stepwise regression is especially problematic.\n\n### Clinical and Epidemiological Studies\n\nIn medical research, variable inclusion is often interpreted causally - even when analysts disclaim causal intent. A covariate that survives stepwise selection may be described as a \"risk factor\" or \"independent predictor\", while excluded variables are implicitly treated as unimportant.\n\nBut this is deeply misleading. Stepwise regression does not distinguish between confounders, mediators, and colliders, and it frequently excludes clinically important variables simply because their effects are imprecisely estimated.\n\n### Regulatory and Reporting Contexts\n\nIn regulatory settings, such as clinical trial reporting, transparency and reproducibility are paramount. Stepwise regression undermines both, because:\n\n-   the analysis path is data-driven and difficult to justify prospectively,\n\n-   results may not reproduce across datasets or populations,\n\n-   and inferential quantities lack a clear interpretation.\n\nFor these reasons, many regulatory guidelines explicitly discourage data-driven variable selection.\n\n------------------------------------------------------------------------\n\n## What to Do Instead\n\nOk, we've spent a fair amount of time talking about the problem. Let's now discuss what we can actually do about it. The good news is that we are not short of alternatives. In fact, modern approaches are often simpler, more transparent, and more defensible.\n\nI am going to discuss these approaches in the context of \"model intent\" - i.e. what is the purpose for your regression model? This ultimately boils down to one of two options - **explanation** vs **prediction**. Model explanation and model prediction answer fundamentally different questions. Explanatory models aim to estimate and interpret the effect of variables, often with a causal lens, which makes pre-specification, subject-matter knowledge, and valid inference essential. An example research question that such a model could answer might be:\n\n> \"What is the effect of DMT x on the risk of disease relapse, after adjusting for age, sex, smoking status, and baseline disease severity?\"\n\ni.e. we are interested primarily in the explanatory 'effect' of a new DMT on disease relapse.\n\nPredictive models, by contrast, are judged by how well they perform on new data, not by whether their coefficients are interpretable or causally meaningful - in other words, we don't really care about individual variable 'effects' but rather just the overall predictive power of the model. Here, the commensurate research question takes a different form:\n\n> \"Using age, sex, smoking status, current therapy and disease severity data at baseline, what is an individual's 5-year risk of disease relapse?\"\n\ni.e. we want to predict relapse risk but don't really care about any individual pedictor.\n\n### Explanation\n\n#### Pre-Specification and Subject-Matter Knowledge\n\nWhen we are interested in model explanation, the most robust approach is also the least glamorous: decide in advance (i.e. prior to seeing the data) which variables belong in the model, and make that our **one and only model**.\n\nHere, variable inclusion should be guided by:\n\n-   scientific understanding,\n\n-   causal reasoning,\n\n-   and study design.\n\nConsideration should be given to sythesising these justifications in a supporting causal diagram (DAG).\n\nNote that if a variable is a known confounder, it should be included regardless of its p-value. In contrast, if it is irrelevant to the scientific question, it should not be tested for inclusion.\n\nThis approach prioritises validity over convenience. We are very interested in understanding the characteristics of each individual predictor variable (i.e. is it a potential confounder, collider, mediator, etc in the exposure -\\> outcome relationship), and the interplay between all such variables. Again, this is where a causal diagram, even if we are not planning an explicit causal analysis, can be very helpful. But this is where the hard work ends in explanatory modelling. From a coding perspective, things are easier than ever - we just run whatever model we have decided on (and nothing more), comfortable in the fact that our p-values and confidence intervals are correct.\n\n#### Transparent Sensitivity Analyses\n\nIf, for whatever reason, pre-specification is not possible and variable selection cannot be avoided, resultant models should be treated as exploratory and accompanied by:\n\n-   sensitivity analyses,\n\n-   stability assessments,\n\n-   and clear disclaimers about inferential limitations.\n\nThis is still inferior to pre-specification, but far better than unqualified stepwise regression.\n\n### Prediction\n\n#### Full Models with Shrinkage\n\nIn contrast, when we are interested in model prediction our goals change. We are simply after raw predictive performance and it matters less about the individual variables that comprise the model or how they were chosen. There is actually a fairly nice solution to the problem of variable selection in the context of prediction modelling, and that is \"regularisation\" or \"penalised regression\" methods. These methods acknowledge uncertainty more honestly than stepwise procedures.\n\nRegularisation is central to prediction because it controls overfitting by shrinking model coefficients, trading a small amount of bias for a large reduction in variance and thus improving performance on new data. **Ridge regression** shrinks all coefficients toward zero and is particularly effective when many predictors have small, correlated effects. **Lasso** applies stronger shrinkage that can set some coefficients exactly to zero, producing sparse models that are easier to deploy but can be unstable when predictors are highly correlated. **Elastic net** combines ridge and lasso penalties, often giving the best predictive performance in practice by encouraging sparsity while retaining groups of correlated predictors.\n\nKeep reading to the end of this section to see an example of how penalised regression with Elastic net is used.\n\n#### Cross-Validation and External Validation\n\nBecause we are ultimately interested in predictive power, once we have established a prediction model we should evaluate its performance. There are multiple ways to do this (see [here](https://www.bmj.com/content/384/bmj-2023-074819) for a good primer), with the main techniques being:\n\n- cross-validation,\n\n- bootstrap resampling,\n\n- or, ideally, external validation datasets.\n\nThe general idea in each case is that we \"train\" our model on a dataset and then \"test\" its predictive power on new data. The reason for this is that it is not uncommon when we create a model from a single dataset that the model usually fits the peculiarities of that dataset much better than one it hasn't seen before (i.e. it overfits). Thus, predictive performance will often be less on a \"test\" compared to a \"train\" dataset. In cross-validation and bootstrapping we are using sleight of hand to create \"train\" and \"test\" data from the only dataset we have - these are referred to as internal validation methods. While external validation is better because we can train the model on all of our data and then test on a completely unseen dataset, it is harder to achieve in practice for obvious reasons.\n\nWhatever the method of model performance evaluation, the focus now changes from p-values and confidence intervals  to predictive accuracy and generalisability.\n\n#### Model Averaging\n\nSometimes we may end up with more than one candidate prediction model that have similar predictive power but are based on different modelling strategies (e.g. lasso vs random forest vs GAM). Model averaging is an approach that accounts for uncertainty about model choice by combining predictions or estimates from multiple plausible prediction models rather than relying on a single selected model. Instead of treating one model as “the truth,” it weights each model according to a measure of support - such as predictive performance, information criteria, or posterior probability - and averages across them. This can improve predictive accuracy and reduce overconfidence, particularly when several models perform similarly well. Model averaging is most natural when there is genuine uncertainty among a discrete set of competing models, and it contrasts with single-model selection approaches that ignore this uncertainty.\n\n#### Elastic Net Example\n\nAlright, let's put some of what we've learned into practice. Let's simulate some data and run both a stepwise regression and an elastic net, comparing the performance of each. For the simulation I will create a dataset with `500` observations and `20` predictors. I'll make the regression coefficients for the first `5` predictors non-zero (representing real effects) and set the coefficients for the last `15` to zero (representing noise). I'll also specify that all predictors are moderately correlated with each other (r = `0.4`). With this data-generating process we can then meaningfully compare how stepwise regression and elastic net behave when only a few predictors truly matter (and hopefully find that the latter approach performs better).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\nn <- 500\np <- 20\n\nSigma <- matrix(0.4, p, p)\ndiag(Sigma) <- 1\n\nlibrary(MASS)\nX <- mvrnorm(n, mu = rep(0, p), Sigma = Sigma)\n\nbeta <- c(2, 2, 2, -1.5, -1.5, rep(0, p - 5))\ny <- X %*% beta + rnorm(n, sd = 2)\n\ndat <- data.frame(y, X)\ncolnames(dat) <- c(\"y\", paste0(\"X\", 1:p))\n```\n:::\n\n\nNow let's create train and test datasets from the simulated data. To do this we'll randomly choose `70%` of the observations for the train dataset and allocate the remaining `30%` to the test dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(456)\nidx <- sample(seq_len(n), size = 0.7 * n)\n\ntrain <- dat[idx, ]\ntest  <- dat[-idx, ]\n```\n:::\n\n\nWe will now perform a classic stepwise regression using `step()` to automate the process. `step()` fits multiple models moving back and forth between a model with no predictors (intercept only model) and one with all `20` predictors (full model). At each stage it computes the AIC for each candidate model, moves to the model with the lowest AIC and then stops when no further addition or deletion of variables improves the AIC.\n\nWe can see that the first five predictors are correctly selected, but the algorithm also selects four noise predictors, two of which are deemed statistically significant.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_mod <- lm(y ~ ., data = train)\nnull_mod <- lm(y ~ 1, data = train)\n  \nstep_mod <- step(\n  null_mod,\n  scope = list(lower = null_mod, upper = full_mod),\n  direction = \"both\",\n  trace = FALSE\n)\n\nsummary(step_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ X1 + X3 + X5 + X2 + X4 + X15 + X10 + X14 + X18, \n    data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6759 -1.2016  0.0696  1.2556  5.7055 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.07313    0.10297   0.710   0.4781    \nX1           2.08653    0.11979  17.418   <2e-16 ***\nX3           2.12815    0.13199  16.123   <2e-16 ***\nX5          -1.61482    0.12589 -12.827   <2e-16 ***\nX2           2.02325    0.12951  15.622   <2e-16 ***\nX4          -1.58574    0.12656 -12.530   <2e-16 ***\nX15          0.30294    0.13056   2.320   0.0209 *  \nX10         -0.32018    0.12951  -2.472   0.0139 *  \nX14          0.22121    0.13102   1.688   0.0922 .  \nX18         -0.20767    0.13261  -1.566   0.1183    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.907 on 340 degrees of freedom\nMultiple R-squared:  0.7858,\tAdjusted R-squared:  0.7802 \nF-statistic: 138.6 on 9 and 340 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nLet's check the performance by using the model to predict on the test data, then calculating a common regression performance measure - the root mean square error (RMSE). The lower the RMSE, the better.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_pred_step <- predict(step_mod, newdata = test)\nrmse_step <- sqrt(mean((test$y - y_pred_step)^2))\nrmse_step\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.018798\n```\n\n\n:::\n:::\n\n\nNow let's compare this to an elastic net, using the `glmnet` package, which fits penalised regression models (ridge, lasso, elastic net). We can break this code block down into a series of steps as follows:\n\n- `glmnet` fits many elastic net models, each with:\n\n    - the same predictors,\n\n    - the same alpha = 0.5 (mix of ridge and lasso),\n\n    - different values of lambda (penalty strength).\n\n- It then performs `10`-fold cross-validation:\n\n    - the training data are split into 10 folds,\n\n    - each fold is held out in turn,\n\n    - prediction error is estimated for each lambda.\n\n- And finally, chooses the optimal penalty:\n\n    - lambda.min → lowest cross-validated error,\n\n    - lambda.1se → largest λ within 1 SE of the minimum (simpler model).\n\nThis step answers:\n\"How much regularisation gives the best out-of-sample performance?\"\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n\n# glmnet requires predictors as a numeric matrix, not a data frame.\nX_train <- as.matrix(train[, -1]) # contains all predictors but remove the outcome variable (y)\ny_train <- train$y                # response\nX_test  <- as.matrix(test[, -1])\ny_test  <- test$y\n\ncv_enet <- cv.glmnet(\n  X_train, y_train,\n  alpha = 0.5,                    # elastic net\n  nfolds = 10\n)\n\n# Extract the coefficients from the single elastic net model corresponding to the optimal λ.\ncoef(cv_enet, s = \"lambda.min\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n              lambda.min\n(Intercept)  0.066848837\nX1           2.012777264\nX2           1.921988809\nX3           2.043667440\nX4          -1.469708904\nX5          -1.503814250\nX6           0.010503422\nX7          -0.111528143\nX8          -0.065850506\nX9          -0.080697600\nX10         -0.232749640\nX11          0.010956657\nX12          .          \nX13          0.120277174\nX14          0.166942224\nX15          0.237722152\nX16          0.030739719\nX17          0.058957699\nX18         -0.115478884\nX19         -0.046143403\nX20          0.004132131\n```\n\n\n:::\n:::\n\n\nNote that even when the true data-generating process is sparse, elastic net does not necessarily recover the true set of predictors, particularly when predictors are correlated. This is not a failure: the elastic net objective is to minimise prediction error, not to identify the true model. At the penalty that optimises cross-validated performance, elastic net may retain many small coefficients because correlated noise variables still carry predictive information. In other words, if allowing small non-zero coefficients on \"noise\" predictors improves prediction even slightly, elastic net will do that.\n\nNow let's compare the prediction error from both modelling approaches.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_pred_enet <- predict(cv_enet, X_test, s = \"lambda.min\")\nrmse_enet <- sqrt(mean((y_test - y_pred_enet)^2))\nrmse_enet\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.999773\n```\n\n\n:::\n\n```{.r .cell-code}\nc(\n  Stepwise_RMSE   = rmse_step,\n  ElasticNet_RMSE = rmse_enet\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Stepwise_RMSE ElasticNet_RMSE \n       2.018798        1.999773 \n```\n\n\n:::\n:::\n\n\nWe can see that it's actually pretty similar. But that doesn't mean that this exercise has been a waste of time. Even when stepwise regression and elastic net achieve similar RMSE on a given test set, elastic net is preferable because it achieves this performance through explicit regularisation rather than discrete variable selection. \n\nPenalised models are:\n\n- more stable to sampling variation, \n\n- handle correlated predictors more gracefully, \n\n- and control overfitting directly. \n\nThe apparent equivalence in point performance masks important differences in reliability and robustness, which become evident under resampling or repeated evaluation. Penalised regression does not eliminate sampling variability, but it responds to it smoothly. Unlike stepwise regression, which makes brittle include–exclude decisions, elastic net adjusts coefficients continuously, allowing predictive performance to remain stable even when individual coefficients fluctuate.\n\nAll of this is to say that if we ran these models many times across different datasets, penalised regression will still give us a relatively stable model and predictive performance - stepwise methods won't.\n\n## A Pragmatic Takeaway\n\nStepwise regression persists because it is easy, familiar, and superficially tidy. But its apparent simplicity masks deep statistical flaws that undermine inference, reproducibility, and scientific credibility.\n\nModern alternatives - pre-specification for explanatory modelling; and shrinkage, validation ± model-averaging for prediction modelling - are not only more principled, they are often easier to explain and defend.\n\nThe question is no longer whether stepwise regression is flawed. The question is why we continue to use it when better tools are readily available. \n\nWell that's pretty much it for this post folks. Hopefully you can start to incorporate some of these techniques into your next regression modelling endeavour. See you next month.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}