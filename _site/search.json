[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Test Site",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nEasily view your data by a grouping variable\n\n\n\n\n\n\n\ncode\n\n\n\n\nUse by() to view your data by a grouping variable.\n\n\n\n\n\n\nFeb 23, 2024\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nImmortal time bias - “The fallacy that never dies” (Part 2)\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nmodelling\n\n\nsurvival\n\n\nvisualisation\n\n\n\n\nLet’s investigate immortal time bias with a coded example.\n\n\n\n\n\n\nFeb 16, 2024\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nImmortal time bias - “The fallacy that never dies” (Part 1)\n\n\n\n\n\n\n\nconcept\n\n\nsurvival\n\n\n\n\nLearn why this misclassification of time in a survival analysis can seriously bias your results.\n\n\n\n\n\n\nFeb 9, 2024\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nPut your ggplot on steroids\n\n\n\n\n\n\n\nvisualisation\n\n\ncode\n\n\n\n\nPlotly adds some interactivity and can help clarify your data.\n\n\n\n\n\n\nFeb 2, 2024\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nA Very Merry Christmas\n\n\n\n\n\n\n\nfun\n\n\ncode\n\n\nvisualisation\n\n\n\n\nHo, Ho, Ho\n\n\n\n\n\n\nDec 8, 2023\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nIt pays to think like a Bayesian\n\n\n\n\n\n\n\npuzzle\n\n\nprobability\n\n\nBayesian\n\n\n\n\nBayesian reasoning is more in line with how we process chance in everyday life.\n\n\n\n\n\n\nDec 1, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nInteractions (effect modifiers) are important - don’t ignore them\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nmodelling\n\n\nlogistic\n\n\n\n\nKeep an open mind to interactions in your next model.\n\n\n\n\n\n\nNov 24, 2023\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\nStats Tips - Welcome\n\n\n\n\n\n\n\nnews\n\n\n\n\nWelcome to what I hope can become a useful stats resource.\n\n\n\n\n\n\nNov 22, 2023\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/001_24Nov2023/index.html",
    "href": "posts/001_24Nov2023/index.html",
    "title": "Interactions (effect modifiers) are important - don’t ignore them",
    "section": "",
    "text": "Code\nlibrary(knitr)\nlibrary(quarto)\nlibrary(emmeans)\nlibrary(flextable)\nlibrary(ggplot2)\nsuppressPackageStartupMessages(library(gtsummary))\nopts_chunk$set(echo = T,\n               cache = F,\n               prompt = F,\n               tidy = F,\n               message = F,\n               warning = F)"
  },
  {
    "objectID": "posts/001_24Nov2023/index.html#the-question",
    "href": "posts/001_24Nov2023/index.html#the-question",
    "title": "Interactions (effect modifiers) are important - don’t ignore them",
    "section": "1 The Question",
    "text": "1 The Question\n\nRecall that the question this week was to choose between:\nA) Age is a confounder in the relationship between sex and hospitalisation from car crash.\nB) Age is an effect modifier in the relationship between sex and hospitalisation from car crash.\nusing the data supplied below."
  },
  {
    "objectID": "posts/001_24Nov2023/index.html#the-answer",
    "href": "posts/001_24Nov2023/index.html#the-answer",
    "title": "Interactions (effect modifiers) are important - don’t ignore them",
    "section": "2 The Answer",
    "text": "2 The Answer\n\nThe answer is B. Younger male drivers tend to be more stupid and injure themselves more seriously than their female counterparts. Upon reaching a suitable age of maturity their risk of hospitalisation reduces to about the same (if we assume the 95% CI for the 8% reduction includes 0) as that for females.\nWhen a third variable plays a role in the association between an exposure and an outcome it may act as a confounder OR effect modifier (AND sometimes both).\nA simple confounder (e.g. age) will show the same exposure -&gt; outcome association across all of its categories (e.g. same risk ratio in &lt; 40 yrs and ≥ 40 yrs)\nAn effect modifier will show different magnitudes of association across its categories (e.g. the risk ratio will differ in those &lt; 40 yrs and ≥ 40 yrs).\nStratification is the simplest form of exploring and adjusting for confounding/interaction effects (used before we had all this computing power).\nSubgroups of data are created for each category of confounder/effect modifier and estimates of interest (mean differences, risk ratios, etc) calculated in each.\nThese can then be combined in a weighted manner to give an overall (adjusted) estimate if NO effect modification is present.\nThis is equivalent to including the third variable as a covariate in our regression model (we now use models rather than stratification methods).\nSimple inclusion in the regression model (using + in R) FORCES the exposure -&gt; outcome association to be the same across all categories of the effect modifier even if in reality it’s not.\n+ assumes confounding ONLY and NO effect modification.\nA problem arises, however, when the third variable is more an effect modifier, rather than confounder.\nIf we suspect effect modification is present, we need to include this third variable in the model as an interaction term (using * in R)\nThis will allow the exposure -&gt; outcome association to differ across categories of the effect modifier.\n* assumes effect modification is present.\nThis is a more flexible model specification, than simply ‘adjusting’ for a variable.\nInterpretation is a little more involved (always happy to help with this) but the point is it’s important not to blindly assume a third variable can only ever be a confounder.\nIf effect modification is present, you need to know about it.\nIt is simple to test for effect modification in R, Stata, etc. Include the interaction term and then drop it if not clinically/statistically significant at some level.\nI have included some R output below showing the equivalence of stratification and modelling approaches to interaction effects.\n\nBefore we get to that - a simple set of guidelines for how to think about crude vs stratified associations:\n\nEquivalence of model-derived crude estimate\nRecall that the aggregated data that the crude estimate is calculated from is:\n\n\nCode\ndat_agg &lt;- data.frame(sex = c(\"Male\", \"Female\"),\n                      hospitalised = as.numeric(c(1330, 798)), \n                      not_hospitalised = as.numeric(c(7018,6400)))\ndat_agg\n\n\n\n\n\n\nsex\nhospitalised\nnot_hospitalised\n\n\n\n\nMale\n1330\n7018\n\n\nFemale\n798\n6400\n\n\n\n\n\n\nTo estimate this model in R we essentially run a logistic regression but instead of outputting an odds ratio, we calculate a risk ratio by specifying a log rather than the default logit link. We will also use the aggregate model specification, as we don’t have the individual-level data.\nThe model-derived crude risk ratio for sex = 1.44 (95% CI 1.32, 1.56; p &lt; 0.001). This is very close to the estimate that we initially calculated manually from the 2 x 2 table (1.45).\n\n\nCode\nmod_crude &lt;- glm(cbind(hospitalised, not_hospitalised) ~ sex, data = dat_agg, family = binomial(link = \"log\"))\ntbl_regression(mod_crude, exp = T)\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      RR1\n      95% CI1\n      p-value\n    \n  \n  \n    sex\n\n\n\n        Female\n—\n—\n\n        Male\n1.44\n1.32, 1.56\n&lt;0.001\n  \n  \n  \n    \n      1 RR = Relative Risk, CI = Confidence Interval"
  },
  {
    "objectID": "posts/001_24Nov2023/index.html#lets-introduce-age-40-vs-40-as-a-third-variable",
    "href": "posts/001_24Nov2023/index.html#lets-introduce-age-40-vs-40-as-a-third-variable",
    "title": "Interactions (effect modifiers) are important - don’t ignore them",
    "section": "3 Let’s introduce age (<40 vs ≥ 40) as a third variable",
    "text": "3 Let’s introduce age (&lt;40 vs ≥ 40) as a third variable\n\n\nCode\ndat_disagg &lt;- data.frame(sex = c(\"Male\", \"Female\", \"Male\", \"Female\"),\n                         age = c(\"&lt; 40\", \"&lt; 40\", \"≥ 40\", \"≥ 40\"),\n                         hospitalised = as.numeric(c(966, 460, 364, 348)), \n                         not_hospitalised = as.numeric(c(3146, 3000, 3872, 3400)))\ndat_disagg\n\n\n\n\n\n\nsex\nage\nhospitalised\nnot_hospitalised\n\n\n\n\nMale\n&lt; 40\n966\n3146\n\n\nFemale\n&lt; 40\n460\n3000\n\n\nMale\n≥ 40\n364\n3872\n\n\nFemale\n≥ 40\n348\n3400\n\n\n\n\n\n\n\n3.1 Age as a confounder\nFor now, let’s just assume age is a confounder in the association between sex and hospitalisation risk. The model formulation in R is then:\nmod_adj &lt;- glm(cbind(hospitalised, not_hospitalised) ~ sex + age, data = dat_agg, family = binomial(link = \"log\"))\nand the risk ratios we get are:\n\n\nCode\nmod_confound &lt;- glm(cbind(hospitalised, not_hospitalised) ~ sex + age, data = dat_disagg, family = binomial(link = \"log\"))\ntbl_regression(mod_confound, exp = T)\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      RR1\n      95% CI1\n      p-value\n    \n  \n  \n    sex\n\n\n\n        Female\n—\n—\n\n        Male\n1.43\n1.32, 1.55\n&lt;0.001\n    age\n\n\n\n        &lt; 40\n—\n—\n\n        ≥ 40\n0.47\n0.43, 0.51\n&lt;0.001\n  \n  \n  \n    \n      1 RR = Relative Risk, CI = Confidence Interval\n    \n  \n\n\n\n\nSo, what we are seeing here is that the magnitude of association between sex and hospitalisation risk is averaged (in a weighted way) over both categories of age to produce one effect estimate sex = 1.43 (95% CI 1.32, 1.55; p &lt; 0.001). This just so happens to be almost the same as the crude estimate when you ignore age altogether.\nThe effect for age in this model is such that whatever your sex, there is about a 53% reduction in the risk of hospitalisation if you are over 40 vs under 40. Note that in the stratification approach, you aren’t able to calculate an effect for age because you are stratifying by it (essentially treating it as a nuisance variable).\n\n\n3.2 Age as an effect modifier\nNow, let’s correctly model age as an effect modifier in the association between sex and hospitalisation risk. The model formulation in R is then (note the * operator):\nmod_adj &lt;- glm(cbind(hospitalised, not_hospitalised) ~ sex * age, data = dat_agg, family = binomial(link = \"log\"))\nand the risk ratios we get are:\n\n\nCode\nmod_interact &lt;- glm(cbind(hospitalised, not_hospitalised) ~ sex * age, data = dat_disagg, family = binomial(link = \"log\"))\ntbl_regression(mod_interact, exp = T)\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      RR1\n      95% CI1\n      p-value\n    \n  \n  \n    sex\n\n\n\n        Female\n—\n—\n\n        Male\n1.77\n1.60, 1.96\n&lt;0.001\n    age\n\n\n\n        &lt; 40\n—\n—\n\n        ≥ 40\n0.70\n0.61, 0.80\n&lt;0.001\n    sex * age\n\n\n\n        Male * ≥ 40\n0.52\n0.44, 0.62\n&lt;0.001\n  \n  \n  \n    \n      1 RR = Relative Risk, CI = Confidence Interval\n    \n  \n\n\n\n\nNote how the p value for the interaction term is very low - this would be a good indicator that the model fits the data better with the interaction term present than without it (i.e. assuming age as a confounder only).\nAs I mentioned earlier, the model interpretation with an interaction present does become a little more complicated, but let’s break this down (note that I use “effect” in a non-causal way):\n\nThe coefficient for sex = 1.77 (95% CI 1.60, 1.96; p &lt; 0.001). The represents the “effect” of sex (being male relative to female) on hospitalisation risk at the reference level of age, which in this case is the under 40 yrs group. So, for those under 40, there is about a 77% increased risk for males relative to females.\nThe coefficient for age = 0.70 (95% CI 0.61, 0.80; p &lt; 0.001). This represents the “effect” of age (being older than 40 yrs relative to younger than 40 yrs) on hospitalisation risk at the reference level of sex, which in this case is female. So, for females, there is about a 30% risk reduction in the need for hospitalisation for older relative to younger drivers.\nThe coefficient for the interaction term: sex * age = 0.52 (95% CI 0.44, 0.62; p &lt; 0.001). This represents the multiplicative increase in the magnitude of association for males over 40 yrs."
  },
  {
    "objectID": "posts/001_24Nov2023/index.html#effect-modification-means-more-associations-to-estimate",
    "href": "posts/001_24Nov2023/index.html#effect-modification-means-more-associations-to-estimate",
    "title": "Interactions (effect modifiers) are important - don’t ignore them",
    "section": "4 Effect modification means more associations to estimate",
    "text": "4 Effect modification means more associations to estimate\nIn this specific case, when you treat age as a confounder, the model produces two risk ratios - one for sex and one for age. However, when you treat age as an effect modifier, there are now four possible risk ratios to estimate (if you care about age more than it being a “nuisance” variable to control for). These are:\n\nThe effect of being male in younger individuals.\nThe effect of being male in older individuals.\nThe effect of being older in females.\nThe effect of being older in males.\n\nYou can easily enough work these out manually by multiplying the respective reference coefficients with the interaction coefficient. The risk ratios for each of the above would then be:\n\n1.77 (we can just read this one straight off the model output)\n1.77 x 0.52 = 0.92\n0.70 (again we can just read this one straight off)\n0.70 x 0.52 = 0.36\n\nNote that the effects for 1. and 2. are very similar to what we calculated straight from the 2 x 2 tables (1.84 and 0.92, respectively - as previously mentioned, the effects for 3. and 4. aren’t able to be calculated for the stratifying variable)."
  },
  {
    "objectID": "posts/001_24Nov2023/index.html#emmeans-should-be-your-new-best-friend",
    "href": "posts/001_24Nov2023/index.html#emmeans-should-be-your-new-best-friend",
    "title": "Interactions (effect modifiers) are important - don’t ignore them",
    "section": "5 Emmeans should be your new best friend",
    "text": "5 Emmeans should be your new best friend\nPerhaps I am preaching to the converted, but if you don’t know what the emmeans package and specific function in R does, then you should learn about it (the equivalent function in Stata is margins).\nhttps://cran.r-project.org/web/packages/emmeans/index.html\nhttps://aosmith.rbind.io/2019/03/25/getting-started-with-emmeans/\nemmeans does a lot of things, but perhaps its workhorse function is to allow you to take a model and calculate adjusted predictions (either at set values of covariates, or by ‘averaging’ over them). In this case, we can very easily use emmeans to reproduce the manual calculations we just did.\n\n\nCode\nemmeans(mod_interact, ~ sex + age, type = \"response\") |&gt; \n  data.frame() |&gt; \n  flextable() |&gt; \n  colformat_double(digits = 3, na_str = \"N/A\") |&gt;\n  set_table_properties(layout = \"autofit\") |&gt; \n  height(height = 1, unit = \"cm\") |&gt; \n  hrule(rule = \"atleast\", part = \"header\") |&gt; \n  align(align = \"center\", part = \"body\") |&gt;\n  bg(bg = \"white\", part = \"all\") |&gt; \n  flextable::font(fontname = \"Consolas\", part = \"all\") |&gt;\n  theme_vanilla()\n\n\n\nPredicted Probabilities of HospitalisationsexageprobSEdfasymp.LCLasymp.UCLFemale&lt; 400.1330.006 Inf0.1220.145Male&lt; 400.2350.007 Inf0.2220.248Female≥ 400.0930.005 Inf0.0840.103Male≥ 400.0860.004 Inf0.0780.095\n\n\nSpecifying type = \"response\" in the emmeans call indicates that we want to calculate the outcome on the probability (i.e. risk) scale. It is simple enough to plot these predicted probabilities using the emmip function in emmeans.\n\n\nCode\nemmip(mod_interact, age ~ sex, type = \"response\") + \n  theme_bw(base_size = 18)\n\n\n\n\n\nTo get the risk ratios we have been working with until now, we simply add the pairs(rev = T) function to the call:\n\n\nCode\nemmeans(mod_interact, ~ sex + age, type = \"response\") |&gt; pairs(rev = T) |&gt; \n  data.frame() |&gt; \n  flextable() |&gt; \n  colformat_double(digits = 3, na_str = \"N/A\") |&gt;\n  set_table_properties(layout = \"autofit\") |&gt; \n  height(height = 1, unit = \"cm\") |&gt; \n  hrule(rule = \"atleast\", part = \"header\") |&gt; \n  align(align = \"center\", part = \"body\") |&gt;\n  bg(bg = \"white\", part = \"all\") |&gt; \n  flextable::font(fontname = \"Consolas\", part = \"all\") |&gt;\n  theme_vanilla()\n\n\n\nAll Pairwise Risk RatioscontrastratioSEdfnullz.ratiop.valueMale &lt; 40 / Female &lt; 401.7670.091 Inf1.00011.0030.000Female ≥ 40 / Female &lt; 400.6980.047 Inf1.000-5.3560.000Female ≥ 40 / Male &lt; 400.3950.023 Inf1.000-15.9230.000Male ≥ 40 / Female &lt; 400.6460.043 Inf1.000-6.5820.000Male ≥ 40 / Male &lt; 400.3660.021 Inf1.000-17.4990.000Male ≥ 40 / Female ≥ 400.9250.066 Inf1.000-1.0830.700\n\n\nNote that this gives us two extra comparisons we might not really want (the 3rd and 4th lines of the output) as it estimates every single pairwise comparison. We can get a bit fancier and customise the emmeans output to give us only what we want:\n\n\nCode\nemm &lt;- emmeans(mod_interact, ~ sex + age, type = \"response\") # save the estimated risks\ncustom &lt;- list(`The effect of being male in younger individuals` = c(-1,1,0,0),\n               `The effect of being male in older individuals` = c(0,0,-1,1),\n               `The effect of being older in females` = c(-1,0,1,0),\n               `The effect of being older in males` = c(0,-1,0,1)) # create custom grid of RR's to estimate\ncontrast(emm, custom) |&gt; \n  summary(infer = T) |&gt; \n  data.frame() |&gt; \n  flextable() |&gt; \n  colformat_double(digits = 3, na_str = \"N/A\") |&gt;\n  set_table_properties(layout = \"autofit\") |&gt; \n  height(height = 1, unit = \"cm\") |&gt; \n  hrule(rule = \"atleast\", part = \"header\") |&gt; \n  align(align = \"center\", part = \"body\") |&gt;\n  bg(bg = \"white\", part = \"all\") |&gt; \n  flextable::font(fontname = \"Consolas\", part = \"all\") |&gt;\n  theme_vanilla()\n\n\n\nCustom Pairwise Risk RatioscontrastratioSEdfasymp.LCLasymp.UCLnullz.ratiop.valueThe effect of being male in younger individuals1.7670.091 Inf1.5971.9561.00011.0030.000The effect of being male in older individuals0.9250.066 Inf0.8041.0651.000-1.0830.279The effect of being older in females0.6980.047 Inf0.6120.7961.000-5.3560.000The effect of being older in males0.3660.021 Inf0.3270.4091.000-17.4990.000\n\n\nNote, that these match the manual calculations pretty well.\nPlease take some time to learn about emmeans (or margins in Stata). It will make your life so much easier if you plan to have a career in research (and don’t always have access to a statistician)."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Stats Tips - Welcome",
    "section": "",
    "text": "Hi Everyone,\nI’m not sure how much everyone’s getting out of the stats tips that I put on WhatsApp on Fridays, but maybe some of it is helpful. For those of you who are interested, I thought that if I was going to do this on a semi-regular basis, I might as well turn it into a resource. So I am having a go at a blog-style format for posting these tips. It also means I can more easily illustrate concepts where needed with code, etc. And it also helps to not overload your WhatsApp with a bunch of text. Some posts will be short and some will be longer and I may not be able to put something up every week, depending on workload, but will do my best. I hope it’s something people find useful.\nSome general housekeeping:\n\nYou can view and copy the code as blocks just before each set of output (there will be a Copy to Clipboard button at the top right of each code block); or by clicking the &lt;/&gt; Code button at the top right of the page, then View Source and copying the entire block.\nThere is a light/dark mode toggle on the top right of the page, depending on how you like to view your internet.\nFeel free to add any comments/questions to a post and/or provide general feedback."
  },
  {
    "objectID": "posts/002_01Dec_2023/index.html",
    "href": "posts/002_01Dec_2023/index.html",
    "title": "It pays to think like a Bayesian",
    "section": "",
    "text": "Recall that the question this week was to choose between:\nA) Switch to another door.\nB) Stay with your original door.\nC) It doesn’t matter if you switch or stay."
  },
  {
    "objectID": "posts/002_01Dec_2023/index.html#the-question",
    "href": "posts/002_01Dec_2023/index.html#the-question",
    "title": "It pays to think like a Bayesian",
    "section": "",
    "text": "Recall that the question this week was to choose between:\nA) Switch to another door.\nB) Stay with your original door.\nC) It doesn’t matter if you switch or stay."
  },
  {
    "objectID": "posts/002_01Dec_2023/index.html#the-answer",
    "href": "posts/002_01Dec_2023/index.html#the-answer",
    "title": "It pays to think like a Bayesian",
    "section": "The Answer",
    "text": "The Answer\nThe answer is that you should switch doors, and in fact if you do switch, you double your chances of winning the car - from 33.3% to 66.7%.\nThis is known as the Monty Hall problem and when it was first posed in a magazine column in 1975 managed to confuse readers to the extent that even mathematicians were writing in to the magazine to claim that answer was in fact wrong and staying with the originally chosen door was the better strategy for success.\nThe simplest way that I can explain this is that you start out with a 33.3% chance of winning the car and those probabilities don’t change once you lock in your selection and Monty offers you another chance to choose (i.e. the probabilities don’t change to 50/50 once Monty reveals what’s behind one of the doors).\n\nIf you stay\nIf you choose the correct door to start with (for which there is a 33.3% chance), staying will result in you ending up with the car (winning).\nIf you choose the incorrect door to start with (for which there is a 66.7% chance), staying will necessarily result in you ending up with a goat (losing).\n\n\nIf you switch\nIf you choose the correct door to start with (for which there is a 33.3% chance), switching will result in you ending up with a goat (losing).\nIf you choose the incorrect door to start with (for which there is a 66.7% chance), switching will necessarily result in you ending up with the car, because Monty has to pick the only other losing door to open (winning).\nStaying is associated with a 33.3% success rate, whereas switching doubles your chance of success to 66.7%.\nStop here if equations give you the equivalent of the aftermath of eating Mexican food. What I have done below is show how we can arrive at the same answer using an analytical approach when our logic/intuition fails. You may not want to venture that far…\nWhat I think is cool about this problem is that while the result might seem counterintuitive to how we naturally process chance, using Bayesian reasoning provides a formulaic way to get at the right answer. This again uses conditional probabilities as I introduced them a few weeks ago. Bayesian thinking is about utilising prior knowledge in conjunction with new data to improve or update our knowledge (whereas the Frequentist approach to statistics doesn’t care so much about prior knowledge and instead just uses the data at hand).\n\n\n\n\n\n\nImportant Concept\n\n\n\nBayesian reasoning enables the analysis of data under the light of prior knowledge.\n\n\nBayes Theorem can be written as:\n\\[\nPr(\\theta | data) = \\frac{Pr(data | \\theta) Pr(\\theta)}{Pr(data)}\n\\]\nwhere \\(\\theta\\) could be a particular parameter or hypothesis.\nHere:\n\\(Pr(data | \\theta)\\) is the likelihood function (the data, or what we measure)\n\\(Pr(\\theta)\\) is the prior probability of our hypothesis (prior knowledge before we the measurement)\n\\(Pr(data)\\) is the prior probability of the data\n\\(Pr(\\theta | data)\\) is the posterior probability of our hypothesis (i.e. “in light of the data”)\nOn the Bayesian/Frequentist topic, note that \\(Pr(data | \\theta)\\) is what null-hypothesis significance testing (NHST) encapsulates and this is a Frequentist concept. Whenever we calculate a p value we are asking:\n\n“What is the probability of this new data (or data even more extreme) occurring by chance given the null hypothesis is true?”\n\nBut really, what we want to know most of the time is the opposite:\n\n“What is the probability of the null hypothesis being true given this new data?”\n\nThat is a Bayesian concept and is answered with \\(Pr(\\theta | data)\\). Maybe we should become more Bayesian in how we handle our research…\nAnyway, excuse the digression. We can generalise Bayes Theorem to the Monty Hall problem as:\n\\[\nPr(\\text{car behind door x} | \\text{Monty opens door y}) = \\frac{Pr(\\text{Monty opens door y} | \\text{car behind door x}) Pr(\\text{car behind door x})}{Pr(\\text{Monty opens door y})}\n\\]\nFor the sake of the exercise, let x = door 1 and y = door 3.\nSo we are interested in the probability the car is behind door 1 (that means we picked door 1) when Monty opens door 3 to reveal a goat.\nIn calculating the different components of Bayes Theorem, we first need to enumerate the various probabilities.\n\\(Pr(\\text{car behind door 1}) = Pr(\\text{car behind door 2}) = Pr(\\text{car behind door 3}) = 33.3\\%\\)\nThese are the prior probabilities.\nThen:\n\\(Pr(\\text{Monty opens door 3} | \\text{car behind door 1}) = 50\\%\\)\nMonty can only pick doors 2 or 3, as we picked door 1.\n\\(Pr(\\text{Monty opens door 3} | \\text{car behind door 2}) = 100\\%\\)\nMonty can only pick door 3, as we picked door 1 and he doesn’t want to reveal the car behind door 2.\n\\(Pr(\\text{Monty opens door 3} | \\text{car behind door 3}) = 0\\%\\)\nMonty won’t reveal the car as part of his playing rules.\nThese are the likelihoods or the data.\nThe \\(Pr(\\text{Monty opens door 3})\\) is a little trickier to calculate. Here we don’t need to worry about the car being behind any specific door, only that Monty won’t reveal it. Intuitively, this would be \\(50\\%\\) as he only has two doors to choose from. But you can also work this out by summing the product of each of the prior probabilities and the evidence:\n\\(Pr(\\text{Monty opens door 3}) = (0.33 * 0.5) + (0.33 * 1) + (0.33 * 0) = 0.5\\)\nFinally, we can get to working out the posterior probabilities of the car being behind each door given Monty opens door 3. We use Bayes Theorem as shown above to do this.\n\\[\nPr(\\text{car behind door 1} | \\text{Monty opens door 3}) = \\frac{Pr(\\text{Monty opens door 3} | \\text{car behind door 1}) Pr(\\text{car behind door 1})}{Pr(\\text{Monty opens door 3})}\n\\] \\[\n= \\frac{0.5 * 0.33}{0.5} = 33.3\\%\n\\] Likewise:\n\\[\nPr(\\text{car behind door 2} | \\text{Monty opens door 3})  = \\frac{1 * 0.33}{0.5} = 66.7\\%\n\\] and\n\\[\nPr(\\text{car behind door 3} | \\text{Monty opens door 3})  = \\frac{0 * 0.33}{0.5} = 0\\%\n\\]\nRemember, we initially chose door 1. So, when Monty opens door 3 (this could have been door 2 - we just needed to pick a door for the exercise), we double our chances of winning by switching to door 2. And this is how Bayesian reasoning can come to the rescue when our own intuition fails.\nFurther explanation can be found here if you remain unconvinced:\nhttps://en.wikipedia.org/wiki/Monty_Hall_problem\nhttps://statisticsbyjim.com/fun/monty-hall-problem/"
  },
  {
    "objectID": "posts/003/index.html",
    "href": "posts/003/index.html",
    "title": "Immortal time bias - “The fallacy that never dies” (Part 1)",
    "section": "",
    "text": "Code\ndat &lt;- read.csv(\"https://raw.githubusercontent.com/pgseye/data/main/heart.csv\", header = T)\n\n#data(heart, package=\"survival\")\nhead(dat)\n\n\n\n\n\n\nstart\nstop\nevent\nage\nyear\nsurgery\ntransplant\nid\n\n\n\n\n0\n50\n1\n-17.155373\n0.1232033\n0\n0\n1\n\n\n0\n6\n1\n3.835729\n0.2546201\n0\n0\n2\n\n\n0\n1\n0\n6.297057\n0.2655715\n0\n0\n3\n\n\n1\n16\n1\n6.297057\n0.2655715\n0\n1\n3\n\n\n0\n36\n0\n-7.737166\n0.4900753\n0\n0\n4\n\n\n36\n39\n1\n-7.737166\n0.4900753\n0\n1\n4"
  },
  {
    "objectID": "posts/004_09Feb_2024/index.html",
    "href": "posts/004_09Feb_2024/index.html",
    "title": "Immortal time bias - “The fallacy that never dies” (Part 1)",
    "section": "",
    "text": "In 2001, a paper published in the Annals of Internal Medicine reported that Oscar winners had a longer life expectancy - by about 4 years - compared to their less successful peers. The authors conclusions were that:\n“The association of high status with increased longevity that prevails in the public also extends to celebrities, contributes to a large survival advantage, and is partially explained by factors related to success.”\nThe study received widespread attention in the media, with one future Oscar winner acknowledging the work in her acceptance speech.\nThe problem was that the reported survival advantage was illusory, and the reason for this was an invalid analysis that is not alone in the literature. As in any simple time-to-event analysis, two groups may be compared in their respective ‘survival’ times. In this study, subjects were first classified as winners or non-winners and observation time counted as their time alive. The error in this case was to consider winning status time-fixed (A), when in reality it is time-varying (B). By naively assuming it is time-fixed, we are erroneously attributing the time that a winner was in fact a loser prior to getting their gong, to their winning observation time.\n\nThis creates a distortion or bias in the exposure/treatment -&gt; outcome association, usually in a direction that overestimates the benefit of the exposure/treatment. When proper methods are then employed, the perceived benefits are reduced or sometimes reversed. And in fact that is exactly what was found when a re-analysis of the data was conducted in 2006 - the survival advantage was calculated to be closer to 1 year and not deemed statistically significant.\nIn general, the observation time prior to the exposure/treatment commencing (for those exposed/treated) is considered ‘immortal’, because the subject cannot experience the outcome during this period as they have yet to receive the exposure/treatment. If you are like me, this fairly classic description of immortal time hurts my brain and so I just like to simply think of it as the period that a person’s observation time has been misclassified.\nObservational research that involves time-to-event outcomes is particularly prone to immortal time bias and central to the problem is the specification of ‘time zero’ - i.e. when does the clock start? There are several examples of study design choices that can lead you down the wrong analysis path if you are not careful, and an especially pertinent one in this field is drawing contrasts between treated and untreated patients (hint: a patient is not ‘treated’ for the duration of their observation time if they were only on treatment for the last 10% of that time).\nSo, what’s the solution?\nTime-varying covariates\nI will illustrate their use in an example next week."
  },
  {
    "objectID": "posts/003_08Dec_2023/index.html",
    "href": "posts/003_08Dec_2023/index.html",
    "title": "A Very Merry Christmas",
    "section": "",
    "text": "This figure was made using ggplot2 and while I can’t take credit for coming up with the idea (source), I have added a couple of flourishes including the animated lights.\nI hope everyone has a safe, happy and enjoyable holiday period.\n\n\nCode\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(gifski)\nlibrary(extrafont)\nloadfonts()\n\n# Read in the base Christmas tree data\nChristmasTree &lt;- read.csv(\"https://raw.githubusercontent.com/t-redactyl/Blog-posts/master/Christmas%20tree%20base%20data.csv\")\n\n# Change tree colour\nChristmasTree$Tree.Colour[ChristmasTree$Tree.Colour == \"#143306\"] &lt;- \"green4\"\n\n# Generate the \"lights\"\nDesired.Lights &lt;- 100\nTotal.Lights &lt;- sum(round(Desired.Lights * 0.35) + round(Desired.Lights * 0.20) + \n                    round(Desired.Lights * 0.17) + round(Desired.Lights * 0.13) +\n                    round(Desired.Lights * 0.10) + round(Desired.Lights * 0.05))\n\nLights &lt;- data.frame(Lights.X = c(round(runif(round(Desired.Lights * 0.35), 4, 18), 0),\n                                  round(runif(round(Desired.Lights * 0.20), 5, 17), 0),\n                                  round(runif(round(Desired.Lights * 0.17), 6, 16), 0),\n                                  round(runif(round(Desired.Lights * 0.13), 7, 15), 0),\n                                  round(runif(round(Desired.Lights * 0.10), 8, 14), 0),\n                                  round(runif(round(Desired.Lights * 0.05), 10, 12), 0)))\nLights$Lights.Y &lt;- c(round(runif(round(Desired.Lights * 0.35), 4, 6), 0),\n                     round(runif(round(Desired.Lights * 0.20), 7, 8), 0),\n                     round(runif(round(Desired.Lights * 0.17), 9, 10), 0),\n                     round(runif(round(Desired.Lights * 0.13), 11, 12), 0),\n                     round(runif(round(Desired.Lights * 0.10), 13, 14), 0),\n                     round(runif(round(Desired.Lights * 0.05), 15, 17), 0))\nLights$Lights.Colour &lt;- c(round(runif(Total.Lights, 1, 3), 0))\n\n# Generate the \"baubles\"\nBaubles &lt;- data.frame(Bauble.X = c(6, 9, 15, 17, 5, 13, 16, 7, 10, 14, 7, 9, 11, 14, 8, 14, 9, 12, 11, 12, 14, 11, 17, 10))\nBaubles$Bauble.Y &lt;- c(4, 5, 4, 4, 5, 5, 5, 6, 6, 6, 8, 8, 8, 8, 10, 10, 11, 11, 12, 13, 10, 16, 7, 14)\nBaubles$Bauble.Colour &lt;- factor(c(1, 2, 2, 3, 2, 3, 1, 3, 1, 1, 1, 2, 1, 2, 3, 3, 2, 1, 3, 2, 1, 3, 3, 1))\nBaubles$Bauble.Size &lt;- c(6, 18, 6, 6, 12, 6, 12, 12, 12, 6, 6, 6, 18, 18, 18, 12, 18, 6, 6, 12, 12, 18, 18, 12)\n\n# Generate the plot\np &lt;- ggplot() + \n  geom_tile(data = ChristmasTree, aes(x = Tree.X, y = Tree.Y, fill = Tree.Colour)) +\n  scale_fill_identity() + \n  geom_point(data = Lights, aes(x = Lights.X, y = Lights.Y), color = \"lightgoldenrodyellow\", shape = 8) +\n  geom_point(data = Baubles, aes(x = Bauble.X, y = Bauble.Y, colour = Bauble.Colour), size = Baubles$Bauble.Size, shape = 16) +\n  scale_colour_manual(values = c(\"firebrick2\", \"gold\", \"blue3\")) +\n  scale_size_area(max_size = 12) +\n  theme_bw() +\n  scale_x_continuous(breaks = NULL) + \n  scale_y_continuous(breaks = NULL) +\n  geom_segment(aes(x = 2.5, xend = 4.5, y = 1.5, yend = 1.5), colour = \"blueviolet\", size = 2) +\n  geom_segment(aes(x = 5.5, xend = 8.5, y = 1.5, yend = 1.5), colour = \"dodgerblue3\", size = 2) +\n  geom_segment(aes(x = 13.5, xend = 16.5, y = 1.5, yend = 1.5), colour = \"blueviolet\", size = 2) +\n  geom_segment(aes(x = 17.5, xend = 19.5, y = 1.5, yend = 1.5), colour = \"dodgerblue3\", size = 2) +\n  geom_segment(aes(x = 3.5, xend = 3.5, y = 0.5, yend = 2.5), colour = \"blueviolet\", size = 2) +\n  geom_segment(aes(x = 7.0, xend = 7.0, y = 0.5, yend = 2.5), colour = \"dodgerblue3\", size = 2) +\n  geom_segment(aes(x = 15.0, xend = 15.0, y = 0.5, yend = 2.5), colour = \"blueviolet\", size = 2) +\n  geom_segment(aes(x = 18.5, xend = 18.5, y = 0.5, yend = 2.5), colour = \"dodgerblue3\", size = 2) +\n  annotate(\"text\", x = 11, y = 20, label = \"Merry Christmas!\",family = \"Luminari\", color = \"white\", size = 12) +\n  transition_states(states=Lights.Colour, transition_length = 0, state_length = 0.0001) +\n  labs(x = \"\", y = \"\") +\n  theme(legend.position = \"none\") +\n  theme(panel.background = element_rect(fill = 'midnightblue', colour = \"yellow\"))\n\n# Animate\nanimate(p, nframe = 20, fps = 20)"
  },
  {
    "objectID": "posts/005_16Feb_2024/index.html",
    "href": "posts/005_16Feb_2024/index.html",
    "title": "Immortal time bias - “The fallacy that never dies” (Part 2)",
    "section": "",
    "text": "Last week I introduced the concept of immortal time bias and how it can distort associations in your survival analysis, if you naively misclassify unexposed/untreated observation time as exposed/treated. This week I am going to illustrate the concept with some data and R code. It would have been good to analyse the Oscar Winner’s data but as I could not locate that anywhere online, we are instead going to look at one of the first studies in which immortal time bias was subsequently recognised to be a problem.\nThe work came out of Stanford University in the early 1970s and assessed the survival benefit of potential heart transplant recipients. In the analysis, the event of interest was death and the primary treatment was heart transplantation - so survival amongst transplant recipients was compared to that amongst accepted patients into the program that did not end up receiving a transplant. Treatment was initially considered time-fixed and the patients divided into two groups - ‘ever transplanted’ vs ‘never transplanted’. Survival time amongst recipients was found to be longer than those who didn’t receive transplantation.\nThe immortal time bias here involves the waiting time of those patients who survived to make it to the transplant. Because this portion of the observation time was classified as exposed to transplantation instead of unexposed, it offered a guaranteed survival time to the transplanted group. The result of this misclassification was to produce an artificial increase in the mortality rate of the reference group, thus suggesting a benefit of heart transplant surgery. In a later reanalysis of the data, the apparent survival benefit of the transplanted group disappeared when the immortal time was properly accounted for by a time-dependent analysis."
  },
  {
    "objectID": "posts/005_16Feb_2024/index.html#load-data",
    "href": "posts/005_16Feb_2024/index.html#load-data",
    "title": "Immortal time bias - “The fallacy that never dies” (Part 2)",
    "section": "1 Load data",
    "text": "1 Load data\nAs this study is considered a canonical example of immortal time bias, the data comes built into R’s survival package. We can load the data and inspect the relevant jasa dataframe as below.\n\n\nCode\nlibrary(survival)\nlibrary(survminer)\nlibrary(gtsummary)\nlibrary(dplyr)\ndata(heart, package = \"survival\")\nhead(jasa)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbirth.dt\naccept.dt\ntx.date\nfu.date\nfustat\nsurgery\nage\nfutime\nwait.time\ntransplant\nmismatch\nhla.a2\nmscore\nreject\n\n\n\n\n1937-01-10\n1967-11-15\nNA\n1968-01-03\n1\n0\n30.84463\n49\nNA\n0\nNA\nNA\nNA\nNA\n\n\n1916-03-02\n1968-01-02\nNA\n1968-01-07\n1\n0\n51.83573\n5\nNA\n0\nNA\nNA\nNA\nNA\n\n\n1913-09-19\n1968-01-06\n1968-01-06\n1968-01-21\n1\n0\n54.29706\n15\n0\n1\n2\n0\n1.11\n0\n\n\n1927-12-23\n1968-03-28\n1968-05-02\n1968-05-05\n1\n0\n40.26283\n38\n35\n1\n3\n0\n1.66\n0\n\n\n1947-07-28\n1968-05-10\nNA\n1968-05-27\n1\n0\n20.78576\n17\nNA\n0\nNA\nNA\nNA\nNA\n\n\n1913-11-08\n1968-06-13\nNA\n1968-06-15\n1\n0\n54.59548\n2\nNA\n0\nNA\nNA\nNA\nNA\n\n\n\n\n\n\nThe variables that we’re going to use are:\n\nfustat - the ‘event’ variable; 0 = alive, 1 = dead at the end of follow-up.\nfutime - the primary ‘time’ variable; time (days) from acceptance into the transplant program until death or censoring.\nwait.time - the secondary ‘time’ variable; time (days) from acceptance into the transplant program until receiving a heart if transplanted (NA for those who never underwent transplant surgery).\ntransplant - the ‘treatment/exposure’ variable; 0 = did not receive heart, 1 = received heart."
  },
  {
    "objectID": "posts/005_16Feb_2024/index.html#visualise-individual-survival-trajectories",
    "href": "posts/005_16Feb_2024/index.html#visualise-individual-survival-trajectories",
    "title": "Immortal time bias - “The fallacy that never dies” (Part 2)",
    "section": "2 Visualise individual survival trajectories",
    "text": "2 Visualise individual survival trajectories\nUsing a bit of ggplot2 magic, we can now plot the individual observation times for the 103 patients in the study. Note that I have stratified observation time by transplant status (orange for the period a patient remains untransplanted and blue for the period following a transplant).\n\n\nCode\n# Create 'id' variable\njasa$id &lt;- seq(1:dim(jasa)[1])\n# Replace wait.time with futime if didn't undergo transplant\njasa$wait.time[is.na(jasa$wait.time)] &lt;- jasa$futime[is.na(jasa$wait.time)]\n# Plot\njasa |&gt;\n  ggplot(aes(x = id, y = futime)) +\n  geom_linerange(aes(ymin = 0, ymax = wait.time), color = \"#E7B800\", linewidth = 1) +\n  geom_linerange(aes(ymin = wait.time, ymax = futime), color = \"#2E9FDF\", linewidth = 1) +\n  geom_point(aes(shape = factor(fustat)), stroke = 1, cex = 1, color = \"black\") +\n  scale_shape_manual(values = c(1, 3), labels = c(\"Censored\", \"Died\"), name = \"Outcome\") +\n  annotate(\"text\", x = 95, y = 1400, label = \"Observation time = yellow - untransplanted\", size = 5, color = \"#E7B800\") +\n  annotate(\"text\", x = 92, y = 1380, label = \"Observation time = blue - post-transplant\", size = 5, color = \"#2E9FDF\") +\n  ggtitle(\"Survival Trajectories for Heart Transplant Patients\") +   \n  ylab(\"Time (days)\") +\n  xlab(\"Patient Number\") + \n  coord_flip() + \n  theme_bw(base_size = 20) +\n  theme(axis.text.y = element_text(size = 15))"
  },
  {
    "objectID": "posts/005_16Feb_2024/index.html#naive-analysis-assuming-treatment-status-is-time-fixed",
    "href": "posts/005_16Feb_2024/index.html#naive-analysis-assuming-treatment-status-is-time-fixed",
    "title": "Immortal time bias - “The fallacy that never dies” (Part 2)",
    "section": "3 Naive analysis assuming treatment status is time-fixed",
    "text": "3 Naive analysis assuming treatment status is time-fixed\n\n3.1 Visualise survival curves\nPlotting the Kaplan-Meier survival curves are easy by first saving the survfit object:\nfit &lt;- survfit(Surv(futime, fustat) ~ transplant, data = jasa)\nand then passing this ggsurvplot which does a nicer job of plotting survival data then using R’s base functions. Note that we ignore wait.time and only specify futime in our fit function. This is because we are assuming if a patient was transplanted, the entire duration of their observation period was considered as such.\n\n\nCode\nfit_naive &lt;- survfit(Surv(futime, fustat) ~ transplant, data = jasa)\nggsurvplot(fit_naive,\n          risk.table = TRUE,\n          risk.table.col = \"strata\",\n          linetype = \"strata\",\n          surv.median.line = \"hv\",\n          ggtheme = theme_bw(base_size = 20),\n          palette = c(\"#E7B800\", \"#2E9FDF\"))\n\n\n\n\n\n\n\n3.2 Cox model\nFitting a Cox model is also simple with:\nmod_naive &lt;- coxph(Surv(futime, fustat) ~ transplant, data = jasa)\n\n\nCode\nmod_naive &lt;- coxph(Surv(futime, fustat) ~ transplant, data = jasa)\ntbl_regression(mod_naive, exp = T)\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      HR1\n      95% CI1\n      p-value\n    \n  \n  \n    transplant\n0.27\n0.17, 0.43\n&lt;0.001\n  \n  \n  \n    \n      1 HR = Hazard Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nThis gives a HR = 0.27 (95% CI 0.17, 0.43; p &lt; 0.001) indicating that there is about a 73% reduction in the risk of death with transplantation. Pretty effective, right?"
  },
  {
    "objectID": "posts/005_16Feb_2024/index.html#correct-analysis-assuming-treatment-status-is-time-varying",
    "href": "posts/005_16Feb_2024/index.html#correct-analysis-assuming-treatment-status-is-time-varying",
    "title": "Immortal time bias - “The fallacy that never dies” (Part 2)",
    "section": "4 Correct analysis assuming treatment status is time-varying",
    "text": "4 Correct analysis assuming treatment status is time-varying\nUp until now we have just used the data as it’s been presented to us. Each patient has a single observation with all information about them contained in that row of data. However, to perform the correct time-dependent analysis we first need to construct a time-varying version of the treatment (i.e. transplant) variable. This data format is known as ‘counting process’ and in the general case involves creating potentially multiple rows of data for each patient with each row corresponding to a different exposure/treatment period of that patients observation time. In this specific example, we will create an additional row of data for transplanted patients splitting time at the point of transplant, so that the first row contains the time from acceptance into the transplant program to the point of transplant, and the second row contains the time from transplant to either death or censoring. We specify this in ‘start, stop’ format rather than the duration of the interval itself. We will use the tmerge function to do this, although a little bit of manual programming can also achieve the same result.\n\n\nCode\n# Create subset of data selecting relevant variables\njasa_subset &lt;- jasa |&gt; \n  select(id, wait.time, futime, fustat, transplant)\n# Can't have an end time of 0 (one obs) - change this to 0.5\njasa_subset$futime[jasa_subset$futime == 0] &lt;- 0.5\n# Create dataframe in counting process format\njasa_cp &lt;- tmerge(data1 = jasa_subset |&gt; select(id, futime, fustat), \n                  data2 = jasa_subset |&gt; select(id, futime, fustat, wait.time, transplant), \n                  id = id, \n                  death = event(futime, fustat),\n                  transplant = tdc(wait.time)) |&gt; \n            select(-c(futime, fustat))\n\n\nRemember that the original data looked like:\n\n\nCode\nhead(jasa_subset, 7)\n\n\n\n\n\n\nid\nwait.time\nfutime\nfustat\ntransplant\n\n\n\n\n1\n49\n49\n1\n0\n\n\n2\n5\n5\n1\n0\n\n\n3\n0\n15\n1\n1\n\n\n4\n35\n38\n1\n1\n\n\n5\n17\n17\n1\n0\n\n\n6\n2\n2\n1\n0\n\n\n7\n50\n674\n1\n1\n\n\n\n\n\n\nAnd the newly created dataframe in counting process format:\n\n\nCode\nhead(jasa_cp, 9)\n\n\n\n\n\n\nid\ntstart\ntstop\ndeath\ntransplant\n\n\n\n\n1\n0\n49\n1\n0\n\n\n2\n0\n5\n1\n0\n\n\n3\n0\n15\n1\n1\n\n\n4\n0\n35\n0\n0\n\n\n4\n35\n38\n1\n1\n\n\n5\n0\n17\n1\n0\n\n\n6\n0\n2\n1\n0\n\n\n7\n0\n50\n0\n0\n\n\n7\n50\n674\n1\n1\n\n\n\n\n\n\nNote the new ‘start, stop’ time variables. We have also renamed fustat to death for a more intuitive name.\n\n4.1 Visualise survival curves\nPlotting the Kaplan-Meier survival curves for the data in this correct format reveals a vastly different result to that which we viewed earlier. There is now almost no separation in the curves.\n\n\nCode\nfit_correct &lt;- survfit(Surv(tstart, tstop, death) ~ transplant, data = jasa_cp)\nggsurvplot(fit_correct,\n          risk.table = TRUE,\n          risk.table.col = \"strata\",\n          linetype = \"strata\",\n          surv.median.line = \"hv\",\n          ggtheme = theme_bw(base_size = 20),\n          palette = c(\"#E7B800\", \"#2E9FDF\"))\n\n\n\n\n\n\n\n4.2 Cox model\nCommensurately, the output of the Cox model now gives a HR = 1.13 (95% CI 0.63, 2.04; p = 0.7) indicating that there is about a 14% increase in the risk of death with transplantation - but this could be as much as a 104% increase or even a 37% decrease. That is, we can’t be confident the observed effect didn’t occur just by chance. Clearly, this tells a different story to the naive analysis we previously conducted.\n\n\nCode\nmod_correct &lt;- coxph(Surv(tstart, tstop, death) ~ transplant, data = jasa_cp)\ntbl_regression(mod_correct, exp = T)\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      HR1\n      95% CI1\n      p-value\n    \n  \n  \n    transplant\n1.13\n0.63, 2.04\n0.7\n  \n  \n  \n    \n      1 HR = Hazard Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nThe lesson here is to always think about whether your exposure or treatment changes over the course of an individual’s observation time, and if it does, to account for that in your survival model by constructing a time-varying covariate."
  },
  {
    "objectID": "posts/004_02Feb_2024/index.html",
    "href": "posts/004_02Feb_2024/index.html",
    "title": "Put your ggplot on steroids",
    "section": "",
    "text": "Welcome back to Stats Tips for 2024 - hope you managed a nice break.\nIt’s a short one today. If you didn’t already now it existed, check out plotly for taking your ggplots to the next level.\nSometimes it can be extremely helpful to quickly link discrete elements of a plot to the corresponding observation/s in your dataframe. For example, you have a suspected outlier in a scatterplot and you want to know which individual that belongs to. Or, you have an unavoidably busy plot; for example, plotting the predictions from a mixed model for longitudinal data overlaid on the observed data for comparison. In these cases it’s nearly impossible to discern the origin of the plotted data. In both use-case scenarios (and many more), plotly can help.\nIn this example of the latter use-case, we are going to use data from a built-in dataset in the lme4 package. The sleepstudy data looks at reaction times over time in sleep-deprived individuals. For the sake of the exercise we will fit a mixed model with reaction time (ms) as the outcome, time (days) as a fixed-effect and time (days) and individual as random-effects. So this is a random slopes model allowing the ‘effect’ of sleep-deprivation on reaction time to vary over time for each individual. We fit the model and view a few lines of the dataframe which now contains the fixed (mod_pred_fix) and random (mod_pred_ran) predictions.\n\n\nCode\nlibrary(lme4)\nlibrary(ggplot2)\nlibrary(plotly)\n# Load data\ndata(\"sleepstudy\")\n# Model\nmod &lt;- lmer(Reaction ~ Days + (Days | Subject), data = sleepstudy)\n# Predict\nsleepstudy$mod_pred_fix &lt;- predict(mod, re.form = NA) # predict fixed effects\nsleepstudy$mod_pred_ran &lt;- predict(mod) # predict random effects\n# View data\nhead(sleepstudy, 10)\n\n\n\n\n\n\nReaction\nDays\nSubject\nmod_pred_fix\nmod_pred_ran\n\n\n\n\n249.5600\n0\n308\n251.4051\n253.6637\n\n\n258.7047\n1\n308\n261.8724\n273.3299\n\n\n250.8006\n2\n308\n272.3397\n292.9962\n\n\n321.4398\n3\n308\n282.8070\n312.6624\n\n\n356.8519\n4\n308\n293.2742\n332.3287\n\n\n414.6901\n5\n308\n303.7415\n351.9950\n\n\n382.2038\n6\n308\n314.2088\n371.6612\n\n\n290.1486\n7\n308\n324.6761\n391.3275\n\n\n430.5853\n8\n308\n335.1434\n410.9937\n\n\n466.3535\n9\n308\n345.6107\n430.6600\n\n\n\n\n\n\nWe can then plot the data interactively by simply ‘wrapping’ the ggplot object in a plotly call. If you hover over a data point you can easily identify which individual it belongs to as well as the observed reaction time. Similarly, by hovering over one of the random slopes you will see the predicted reaction time and the individual that corresponds to.\nYou won’t want to do this for every plot you make but it does provide a simple way to make some of your more complex visualisations using ggplot that bit more useful (and fun!) in helping to understand your data.\n\n\nCode\n# Plot\np &lt;- sleepstudy |&gt;\n    ggplot(aes(x = Days, y = Reaction, color = factor(Subject))) +\n    geom_line(aes(x = Days, y = mod_pred_ran)) +\n    geom_line(aes(x = Days, y = mod_pred_fix), linewidth = 2, color = \"blue\") +\n    geom_point(alpha = 0.5) +\n    xlab(\"Time (days)\") + ylab(\"Reaction Time (ms)\") +\n    guides(color = \"none\") +\n    theme_bw(base_size = 15)\nggplotly(p)"
  },
  {
    "objectID": "posts/006_16Feb_2024/index.html",
    "href": "posts/006_16Feb_2024/index.html",
    "title": "Immortal time bias - “The fallacy that never dies” (Part 2)",
    "section": "",
    "text": "Last week I introduced the concept of immortal time bias and how it can distort associations in your survival analysis, if you naively misclassify unexposed/untreated observation time as exposed/treated. This week I am going to illustrate the concept with some data and R code. It would have been good to analyse the Oscar Winner’s data but as I could not locate that anywhere online, we are instead going to look at one of the first studies in which immortal time bias was subsequently recognised to be a problem.\nThe work came out of Stanford University in the early 1970s and assessed the survival benefit of potential heart transplant recipients. In the analysis, the event of interest was death and the primary treatment was heart transplantation - so survival amongst transplant recipients was compared to that amongst accepted patients into the program that did not end up receiving a transplant. Treatment was initially considered time-fixed and the patients divided into two groups - ‘ever transplanted’ vs ‘never transplanted’. Survival time amongst recipients was found to be longer than those who didn’t receive transplantation.\nThe immortal time bias here involves the waiting time of those patients who survived to make it to the transplant. Because this portion of the observation time was classified as exposed to transplantation instead of unexposed, it offered a guaranteed survival time to the transplanted group. The result of this misclassification was to produce an artificial increase in the mortality rate of the reference group, thus suggesting a benefit of heart transplant surgery. In a later reanalysis of the data, the apparent survival benefit of the transplanted group disappeared when the immortal time was properly accounted for by a time-dependent analysis."
  },
  {
    "objectID": "posts/006_16Feb_2024/index.html#load-data",
    "href": "posts/006_16Feb_2024/index.html#load-data",
    "title": "Immortal time bias - “The fallacy that never dies” (Part 2)",
    "section": "1 Load data",
    "text": "1 Load data\nAs this study is considered a canonical example of immortal time bias, the data comes built into R’s survival package. We can load the data and inspect the relevant jasa dataframe as below.\n\n\nCode\nlibrary(survival)\nlibrary(survminer)\nlibrary(gtsummary)\nlibrary(dplyr)\ndata(heart, package = \"survival\")\nhead(jasa)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbirth.dt\naccept.dt\ntx.date\nfu.date\nfustat\nsurgery\nage\nfutime\nwait.time\ntransplant\nmismatch\nhla.a2\nmscore\nreject\n\n\n\n\n1937-01-10\n1967-11-15\nNA\n1968-01-03\n1\n0\n30.84463\n49\nNA\n0\nNA\nNA\nNA\nNA\n\n\n1916-03-02\n1968-01-02\nNA\n1968-01-07\n1\n0\n51.83573\n5\nNA\n0\nNA\nNA\nNA\nNA\n\n\n1913-09-19\n1968-01-06\n1968-01-06\n1968-01-21\n1\n0\n54.29706\n15\n0\n1\n2\n0\n1.11\n0\n\n\n1927-12-23\n1968-03-28\n1968-05-02\n1968-05-05\n1\n0\n40.26283\n38\n35\n1\n3\n0\n1.66\n0\n\n\n1947-07-28\n1968-05-10\nNA\n1968-05-27\n1\n0\n20.78576\n17\nNA\n0\nNA\nNA\nNA\nNA\n\n\n1913-11-08\n1968-06-13\nNA\n1968-06-15\n1\n0\n54.59548\n2\nNA\n0\nNA\nNA\nNA\nNA\n\n\n\n\n\n\nThe variables that we’re going to use are:\n\nfustat - the ‘event’ variable; 0 = alive, 1 = dead at the end of follow-up.\nfutime - the primary ‘time’ variable; time (days) from acceptance into the transplant program until death or censoring.\nwait.time - the secondary ‘time’ variable; time (days) from acceptance into the transplant program until receiving a heart if transplanted (NA for those who never underwent transplant surgery).\ntransplant - the ‘treatment/exposure’ variable; 0 = did not receive heart, 1 = received heart."
  },
  {
    "objectID": "posts/006_16Feb_2024/index.html#visualise-individual-survival-trajectories",
    "href": "posts/006_16Feb_2024/index.html#visualise-individual-survival-trajectories",
    "title": "Immortal time bias - “The fallacy that never dies” (Part 2)",
    "section": "2 Visualise individual survival trajectories",
    "text": "2 Visualise individual survival trajectories\nUsing a bit of ggplot2 magic, we can now plot the individual observation times for the 103 patients in the study. Note that I have stratified observation time by transplant status (orange for the period a patient remains untransplanted and blue for the period following a transplant).\n\n\nCode\n# Create 'id' variable\njasa$id &lt;- seq(1:dim(jasa)[1])\n# Replace wait.time with futime if didn't undergo transplant\njasa$wait.time[is.na(jasa$wait.time)] &lt;- jasa$futime[is.na(jasa$wait.time)]\n# Plot\njasa |&gt;\n  ggplot(aes(x = id, y = futime)) +\n  geom_linerange(aes(ymin = 0, ymax = wait.time), color = \"#E7B800\", linewidth = 1) +\n  geom_linerange(aes(ymin = wait.time, ymax = futime), color = \"#2E9FDF\", linewidth = 1) +\n  geom_point(aes(shape = factor(fustat)), stroke = 1, cex = 1, color = \"black\") +\n  scale_shape_manual(values = c(1, 3), labels = c(\"Censored\", \"Died\"), name = \"Outcome\") +\n  annotate(\"text\", x = 95, y = 1400, label = \"Observation time = yellow - untransplanted\", size = 5, color = \"#E7B800\") +\n  annotate(\"text\", x = 92, y = 1380, label = \"Observation time = blue - post-transplant\", size = 5, color = \"#2E9FDF\") +\n  ggtitle(\"Survival Trajectories for Heart Transplant Patients\") +   \n  ylab(\"Time (days)\") +\n  xlab(\"Patient Number\") + \n  coord_flip() + \n  theme_bw(base_size = 20) +\n  theme(axis.text.y = element_text(size = 15))"
  },
  {
    "objectID": "posts/006_16Feb_2024/index.html#naive-analysis-assuming-treatment-status-is-time-fixed",
    "href": "posts/006_16Feb_2024/index.html#naive-analysis-assuming-treatment-status-is-time-fixed",
    "title": "Immortal time bias - “The fallacy that never dies” (Part 2)",
    "section": "3 Naive analysis assuming treatment status is time-fixed",
    "text": "3 Naive analysis assuming treatment status is time-fixed\n\n3.1 Visualise survival curves\nPlotting the Kaplan-Meier survival curves are easy by first saving the survfit object:\nfit &lt;- survfit(Surv(futime, fustat) ~ transplant, data = jasa)\nand then passing this ggsurvplot which does a nicer job of plotting survival data then using R’s base functions. Note that we ignore wait.time and only specify futime in our fit function. This is because we are assuming if a patient was transplanted, the entire duration of their observation period was considered as such.\n\n\nCode\nfit_naive &lt;- survfit(Surv(futime, fustat) ~ transplant, data = jasa)\nggsurvplot(fit_naive,\n          risk.table = TRUE,\n          risk.table.col = \"strata\",\n          linetype = \"strata\",\n          surv.median.line = \"hv\",\n          ggtheme = theme_bw(base_size = 20),\n          palette = c(\"#E7B800\", \"#2E9FDF\"))\n\n\n\n\n\n\n\n3.2 Cox model\nFitting a Cox model is also simple with:\nmod_naive &lt;- coxph(Surv(futime, fustat) ~ transplant, data = jasa)\n\n\nCode\nmod_naive &lt;- coxph(Surv(futime, fustat) ~ transplant, data = jasa)\ntbl_regression(mod_naive, exp = T)\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      HR1\n      95% CI1\n      p-value\n    \n  \n  \n    transplant\n0.27\n0.17, 0.43\n&lt;0.001\n  \n  \n  \n    \n      1 HR = Hazard Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nThis gives a HR = 0.27 (95% CI 0.17, 0.43; p &lt; 0.001) indicating that there is about a 73% reduction in the risk of death with transplantation. Pretty effective, right?"
  },
  {
    "objectID": "posts/006_16Feb_2024/index.html#correct-analysis-assuming-treatment-status-is-time-varying",
    "href": "posts/006_16Feb_2024/index.html#correct-analysis-assuming-treatment-status-is-time-varying",
    "title": "Immortal time bias - “The fallacy that never dies” (Part 2)",
    "section": "4 Correct analysis assuming treatment status is time-varying",
    "text": "4 Correct analysis assuming treatment status is time-varying\nUp until now we have just used the data as it’s been presented to us. Each patient has a single observation with all information about them contained in that row of data. However, to perform the correct time-dependent analysis we first need to construct a time-varying version of the treatment (i.e. transplant) variable. This data format is known as ‘counting process’ and in the general case involves creating potentially multiple rows of data for each patient with each row corresponding to a different exposure/treatment period of that patients observation time. In this specific example, we will create an additional row of data for transplanted patients splitting time at the point of transplant, so that the first row contains the time from acceptance into the transplant program to the point of transplant, and the second row contains the time from transplant to either death or censoring. We specify this in ‘start, stop’ format rather than the duration of the interval itself. We will use the tmerge function to do this, although a little bit of manual programming can also achieve the same result.\n\n\nCode\n# Create subset of data selecting relevant variables\njasa_subset &lt;- jasa |&gt; \n  select(id, wait.time, futime, fustat, transplant)\n# Can't have an end time of 0 (one obs) - change this to 0.5\njasa_subset$futime[jasa_subset$futime == 0] &lt;- 0.5\n# Create dataframe in counting process format\njasa_cp &lt;- tmerge(data1 = jasa_subset |&gt; select(id, futime, fustat), \n                  data2 = jasa_subset |&gt; select(id, futime, fustat, wait.time, transplant), \n                  id = id, \n                  death = event(futime, fustat),\n                  transplant = tdc(wait.time)) |&gt; \n            select(-c(futime, fustat))\n\n\nRemember that the original data looked like:\n\n\nCode\nhead(jasa_subset, 7)\n\n\n\n\n\n\nid\nwait.time\nfutime\nfustat\ntransplant\n\n\n\n\n1\n49\n49\n1\n0\n\n\n2\n5\n5\n1\n0\n\n\n3\n0\n15\n1\n1\n\n\n4\n35\n38\n1\n1\n\n\n5\n17\n17\n1\n0\n\n\n6\n2\n2\n1\n0\n\n\n7\n50\n674\n1\n1\n\n\n\n\n\n\nAnd the newly created dataframe in counting process format:\n\n\nCode\nhead(jasa_cp, 9)\n\n\n\n\n\n\nid\ntstart\ntstop\ndeath\ntransplant\n\n\n\n\n1\n0\n49\n1\n0\n\n\n2\n0\n5\n1\n0\n\n\n3\n0\n15\n1\n1\n\n\n4\n0\n35\n0\n0\n\n\n4\n35\n38\n1\n1\n\n\n5\n0\n17\n1\n0\n\n\n6\n0\n2\n1\n0\n\n\n7\n0\n50\n0\n0\n\n\n7\n50\n674\n1\n1\n\n\n\n\n\n\nNote the new ‘start, stop’ time variables. We have also renamed fustat to death for a more intuitive name.\n\n4.1 Visualise survival curves\nPlotting the Kaplan-Meier survival curves for the data in this correct format reveals a vastly different result to that which we viewed earlier. There is now almost no separation in the curves.\n\n\nCode\nfit_correct &lt;- survfit(Surv(tstart, tstop, death) ~ transplant, data = jasa_cp)\nggsurvplot(fit_correct,\n          risk.table = TRUE,\n          risk.table.col = \"strata\",\n          linetype = \"strata\",\n          surv.median.line = \"hv\",\n          ggtheme = theme_bw(base_size = 20),\n          palette = c(\"#E7B800\", \"#2E9FDF\"))\n\n\n\n\n\n\n\n4.2 Cox model\nCommensurately, the output of the Cox model now gives a HR = 1.13 (95% CI 0.63, 2.04; p = 0.7) indicating that there is about a 14% increase in the risk of death with transplantation - but this could be as much as a 104% increase or even a 37% decrease. That is, we can’t be confident the observed effect didn’t occur just by chance. Clearly, this tells a different story to the naive analysis we previously conducted.\n\n\nCode\nmod_correct &lt;- coxph(Surv(tstart, tstop, death) ~ transplant, data = jasa_cp)\ntbl_regression(mod_correct, exp = T)\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      HR1\n      95% CI1\n      p-value\n    \n  \n  \n    transplant\n1.13\n0.63, 2.04\n0.7\n  \n  \n  \n    \n      1 HR = Hazard Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nThe lesson here is to always think about whether your exposure or treatment changes over the course of an individual’s observation time, and if it does, to account for that in your survival model by constructing a time-varying covariate."
  },
  {
    "objectID": "posts/005_09Feb_2024/index.html",
    "href": "posts/005_09Feb_2024/index.html",
    "title": "Immortal time bias - “The fallacy that never dies” (Part 1)",
    "section": "",
    "text": "In 2001, a paper published in the Annals of Internal Medicine reported that Oscar winners had a longer life expectancy - by about 4 years - compared to their less successful peers. The authors conclusions were that:\n“The association of high status with increased longevity that prevails in the public also extends to celebrities, contributes to a large survival advantage, and is partially explained by factors related to success.”\nThe study received widespread attention in the media, with one future Oscar winner acknowledging the work in her acceptance speech.\nThe problem was that the reported survival advantage was illusory, and the reason for this was an invalid analysis that is not alone in the literature. As in any simple time-to-event analysis, two groups may be compared in their respective ‘survival’ times. In this study, subjects were first classified as winners or non-winners and observation time counted as their time alive. The error in this case was to consider winning status time-fixed (A), when in reality it is time-varying (B). By naively assuming it is time-fixed, we are erroneously attributing the time that a winner was in fact a loser prior to getting their gong, to their winning observation time.\n\nThis creates a distortion or bias in the exposure/treatment -&gt; outcome association, usually in a direction that overestimates the benefit of the exposure/treatment. When proper methods are then employed, the perceived benefits are reduced or sometimes reversed. And in fact that is exactly what was found when a re-analysis of the data was conducted in 2006 - the survival advantage was calculated to be closer to 1 year and not deemed statistically significant.\nIn general, the observation time prior to the exposure/treatment commencing (for those exposed/treated) is considered ‘immortal’, because the subject cannot experience the outcome during this period as they have yet to receive the exposure/treatment. If you are like me, this fairly classic description of immortal time hurts my brain and so I just like to simply think of it as the period that a person’s observation time has been misclassified.\nObservational research that involves time-to-event outcomes is particularly prone to immortal time bias and central to the problem is the specification of ‘time zero’ - i.e. when does the clock start? There are several examples of study design choices that can lead you down the wrong analysis path if you are not careful, and an especially pertinent one in this field is drawing contrasts between treated and untreated patients (hint: a patient is not ‘treated’ for the duration of their observation time if they were only on treatment for the last 10% of that time).\nSo, what’s the solution?\nTime-varying covariates\nI will illustrate their use in an example next week."
  },
  {
    "objectID": "posts/007_23Feb_2024/index.html",
    "href": "posts/007_23Feb_2024/index.html",
    "title": "Easily view your data by a grouping variable",
    "section": "",
    "text": "It is easy enough to view a dataframe in RStudio by opening the dataframe in the viewer or printing the dataframe (or part of it) to the console. However, this can be messy if you want to quickly identify data by a grouping variable (usually the patient id). The by() function can help you to do this. Let’s illustrate its utility with the sleepstudy dataset from the lme4 package. To start with I’ll print the data for the first 3 subjects as one might.\nsleepstudy |&gt; as_tibble() |&gt; print(n = 30)\n\n\nCode\nlibrary(lme4)\nlibrary(dplyr)\n# Load data\ndata(\"sleepstudy\")\nsleepstudy |&gt; as_tibble() |&gt; print(n = 30)\n\n\n# A tibble: 180 × 3\n   Reaction  Days Subject\n      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  \n 1     250.     0 308    \n 2     259.     1 308    \n 3     251.     2 308    \n 4     321.     3 308    \n 5     357.     4 308    \n 6     415.     5 308    \n 7     382.     6 308    \n 8     290.     7 308    \n 9     431.     8 308    \n10     466.     9 308    \n11     223.     0 309    \n12     205.     1 309    \n13     203.     2 309    \n14     205.     3 309    \n15     208.     4 309    \n16     216.     5 309    \n17     214.     6 309    \n18     218.     7 309    \n19     224.     8 309    \n20     237.     9 309    \n21     199.     0 310    \n22     194.     1 310    \n23     234.     2 310    \n24     233.     3 310    \n25     229.     4 310    \n26     220.     5 310    \n27     235.     6 310    \n28     256.     7 310    \n29     261.     8 310    \n30     248.     9 310    \n# ℹ 150 more rows\n\n\nBut we can do this better with:\nby(sleepstudy, sleepstudy$PATIENT_ID, identity)[1:3]\nNote that the [1:3] indicates the range of group indices that you want to view.\n\n\nCode\nby(sleepstudy, sleepstudy$Subject, identity)[1:3]\n\n\n$`308`\n   Reaction Days Subject\n1  249.5600    0     308\n2  258.7047    1     308\n3  250.8006    2     308\n4  321.4398    3     308\n5  356.8519    4     308\n6  414.6901    5     308\n7  382.2038    6     308\n8  290.1486    7     308\n9  430.5853    8     308\n10 466.3535    9     308\n\n$`309`\n   Reaction Days Subject\n11 222.7339    0     309\n12 205.2658    1     309\n13 202.9778    2     309\n14 204.7070    3     309\n15 207.7161    4     309\n16 215.9618    5     309\n17 213.6303    6     309\n18 217.7272    7     309\n19 224.2957    8     309\n20 237.3142    9     309\n\n$`310`\n   Reaction Days Subject\n21 199.0539    0     310\n22 194.3322    1     310\n23 234.3200    2     310\n24 232.8416    3     310\n25 229.3074    4     310\n26 220.4579    5     310\n27 235.4208    6     310\n28 255.7511    7     310\n29 261.0125    8     310\n30 247.5153    9     310\n\n\nIf you want to take this a step further, you can generalise this with a function that will allow you to quickly view the data in any range that you want, without having to continually copy and paste that line of code. Just call the function with your dataframe and group id names and the range of group indices that you want to view (interestingly while writing this function I worked out you don’t even need the by() function to achieve the same result).\nprint_groups(sleepstudy, Subject, 1, 3)\n\n\nCode\n# Create function\nprint_groups &lt;- function(df, id, index1, index2) {\n  ids_all &lt;-  unique(eval(substitute(id), df))\n  ids_range &lt;- ids_all[index1:index2]\n  if (index1 &lt;= length(ids_all) & index2 &lt;= length(ids_all)) {\n    for (id2 in ids_range) {\n      cat(paste0(\"id = \", id2, \"\\n\"))\n      print(df[eval(substitute(id), df) %in% id2,])\n      cat(\"----------------------------\\n\\n\")\n    }\n  } else {\n    print(\"There aren't that many groups in your dataset\")\n  }\n}\n\n# Use function\nprint_groups(sleepstudy, Subject, 1, 3)\n\n\nid = 308\n   Reaction Days Subject\n1  249.5600    0     308\n2  258.7047    1     308\n3  250.8006    2     308\n4  321.4398    3     308\n5  356.8519    4     308\n6  414.6901    5     308\n7  382.2038    6     308\n8  290.1486    7     308\n9  430.5853    8     308\n10 466.3535    9     308\n----------------------------\n\nid = 309\n   Reaction Days Subject\n11 222.7339    0     309\n12 205.2658    1     309\n13 202.9778    2     309\n14 204.7070    3     309\n15 207.7161    4     309\n16 215.9618    5     309\n17 213.6303    6     309\n18 217.7272    7     309\n19 224.2957    8     309\n20 237.3142    9     309\n----------------------------\n\nid = 310\n   Reaction Days Subject\n21 199.0539    0     310\n22 194.3322    1     310\n23 234.3200    2     310\n24 232.8416    3     310\n25 229.3074    4     310\n26 220.4579    5     310\n27 235.4208    6     310\n28 255.7511    7     310\n29 261.0125    8     310\n30 247.5153    9     310\n----------------------------\n\n\nAnd there you have it!"
  }
]