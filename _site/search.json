[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nA Gentle Introduction to the Bootstrap\n\n\n\n\n\n\ncode\n\n\nconcept\n\n\nmodelling\n\n\n\nPrimer and Intuition for Computational Non-Parametric Resampling\n\n\n\n\n\nAug 28, 2026\n\n\n29 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe Perils of Stepwise Regression - And what to do instead\n\n\n\n\n\n\ncode\n\n\nconcept\n\n\n\nData-driven variable selection does not represent best practice in applied statistics.\n\n\n\n\n\nMay 29, 2026\n\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\n\nA Gentle Introduction to Directed Acyclic Graphs (DAGs)\n\n\n\n\n\n\ncode\n\n\nconcept\n\n\nmodelling\n\n\n\nDon‚Äôt be a DAG, draw one instead.\n\n\n\n\n\nFeb 27, 2026\n\n\n21 min\n\n\n\n\n\n\n\n\n\n\n\n\nMore DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())\n\n\n\n\n\n\ncode\n\n\nconcept\n\n\n\nCreate or edit multiple columns efficiently.\n\n\n\n\n\nJan 30, 2026\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nLarge Language Models are Just Statistics\n\n\n\n\n\n\nconcept\n\n\n\nA plain-language look at how large language models like ChatGPT are really just massive probabilistic models ‚Äî not magic, just statistics at scale.\n\n\n\n\n\nNov 21, 2025\n\n\n24 min\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelated Data\n\n\n\n\n\n\ncode\n\n\nconcept\n\n\nmodelling\n\n\n\nA broad overview of implications for power, sample size and precision.\n\n\n\n\n\nOct 17, 2025\n\n\n26 min\n\n\n\n\n\n\n\n\n\n\n\n\nAre your Hazards Proportional? - What to do when it just ain‚Äôt so (Part 2)\n\n\n\n\n\n\ncode\n\n\nconcept\n\n\nmodelling\n\n\nvisualisation\n\n\n\nThe hazards of misunderstanding the hazard ratio.\n\n\n\n\n\nJul 25, 2025\n\n\n25 min\n\n\n\n\n\n\n\n\n\n\n\n\nAre your Hazards Proportional? - What to do when it just ain‚Äôt so (Part 1)\n\n\n\n\n\n\ncode\n\n\nconcept\n\n\nmodelling\n\n\nvisualisation\n\n\n\nThe hazards of misunderstanding the hazard ratio.\n\n\n\n\n\nJun 27, 2025\n\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\n\nModel Prediction/Visualisation  6. Survival Analysis - Cox Model\n\n\n\n\n\n\ncode\n\n\nconcept\n\n\nmodelling\n\n\nvisualisation\n\n\n\nHow to survive visualising the Cox model.\n\n\n\n\n\nJun 13, 2025\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nModel Prediction/Visualisation  5. Linear Mixed Models (LMMs)\n\n\n\n\n\n\ncode\n\n\nconcept\n\n\nmodelling\n\n\nvisualisation\n\n\n\nVisualising fixed vs random effects.\n\n\n\n\n\nMay 30, 2025\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nModel Prediction/Visualisation  4. Generalised Linear Models (GLMs)\n\n\n\n\n\n\ncode\n\n\nconcept\n\n\nmodelling\n\n\nvisualisation\n\n\n\nKnow what you are predicting when using GLMs.\n\n\n\n\n\nMay 16, 2025\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nModel Prediction/Visualisation  3. Non-Linear Functional Forms\n\n\n\n\n\n\ncode\n\n\nconcept\n\n\nmodelling\n\n\nvisualisation\n\n\n\nRelaxing the linearity assumption does not make prediction/visualisation any more difficult.\n\n\n\n\n\nMay 2, 2025\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nModel Prediction/Visualisation  2. How Many Dimensions?\n\n\n\n\n\n\ncode\n\n\nconcept\n\n\nmodelling\n\n\nvisualisation\n\n\n\nThere is a practical limit to the number of predictors you can visualise.\n\n\n\n\n\nApr 4, 2025\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nModel Prediction/Visualisation  1. The Basics\n\n\n\n\n\n\ncode\n\n\nconcept\n\n\nmodelling\n\n\nvisualisation\n\n\n\nHow to predict from your model.\n\n\n\n\n\nMar 21, 2025\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\njanitor - Your local R handypackage\n\n\n\n\n\n\ncode\n\n\n\nWork smarter, not harder.\n\n\n\n\n\nMar 7, 2025\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nMissing Data and Multiple Imputation for Dummies  (Part 2)\n\n\n\n\n\n\ncode\n\n\nconcept\n\n\nanalysis\n\n\n\nThe core concepts and a simple example to get you up and running with MI in your next analysis.\n\n\n\n\n\nFeb 21, 2025\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\nMissing Data and Multiple Imputation for Dummies  (Part 1)\n\n\n\n\n\n\ncode\n\n\nconcept\n\n\nanalysis\n\n\n\nThe core concepts and a simple example to get you up and running with MI in your next analysis.\n\n\n\n\n\nFeb 7, 2025\n\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\n\nOn the 12th Day of Christmas, a Statistician Sent to Me‚Ä¶\n\n\n\n\n\n\nfun\n\n\n\n\n\n\n\n\n\nNov 29, 2024\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nR Programming - Try for DRY\n\n\n\n\n\n\ncode\n\n\nconcept\n\n\n\nA general philosophy on making your code more efficient.\n\n\n\n\n\nNov 15, 2024\n\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Analysis - Under the Hood\n\n\n\n\n\n\ncode\n\n\nconcept\n\n\nsurvival\n\n\n\nIf you find survival analysis confusing - this post is for you.\n\n\n\n\n\nNov 1, 2024\n\n\n28 min\n\n\n\n\n\n\n\n\n\n\n\n\nBreaking Free From ‚ÄòThe Cult of P‚Äô\n\n\n\n\n\n\nconcept\n\n\nfun\n\n\n\nA reminder not to hang all your hopes and dreams on one insignificant number (see what I did there)\n\n\n\n\n\nOct 18, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nEpi. 101 Lesson - Confounding\n\n\n\n\n\n\nanalysis\n\n\ncode\n\n\nconcept\n\n\nmodelling\n\n\n\nConfounding - how statistical adjustment relates to stratification.\n\n\n\n\n\nOct 4, 2024\n\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\n\nBiostats Book Club\n\n\n\n\n\n\nfun\n\n\nresources\n\n\n\nRule # 1. You do not talk about Biostats Book Club.\n\n\n\n\n\nSep 6, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\ntidylog - Console Messaging in R\n\n\n\n\n\n\ncode\n\n\n\nFinally get some feedback about what your data manipulations are actually doing.\n\n\n\n\n\nAug 23, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nLogarithms and Why They‚Äôre Important in Statistics\n\n\n\n\n\n\ncode\n\n\nconcept\n\n\nvisualisation\n\n\n\nHaving to deal with logarithms may send a shiver down your spine, but they‚Äôre not as hard as you may think\n\n\n\n\n\nAug 9, 2024\n\n\n21 min\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression - Under the Hood\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nmodelling\n\n\nvisualisation\n\n\nprobability\n\n\n\nSee how log-odds, odds and probability are all simply versions of each other - and fundamental to logistic regression.\n\n\n\n\n\nJul 26, 2024\n\n\n32 min\n\n\n\n\n\n\n\n\n\n\n\n\nAcademic Research - ‚ÄúWhat‚Äôs in a Title?‚Äù\n\n\n\n\n\n\nfun\n\n\n\n‚ÄúThat which we call a paper by any other title would have as much impact?‚Äù\n\n\n\n\n\nJun 14, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nReshaping Data - Think Before you Pivot\n\n\n\n\n\n\nconcept\n\n\ncode\n\n\n\nSome common-use scenarios for reshaping your data from wide to long format\n\n\n\n\n\nMay 31, 2024\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nOrdinal Logistic Regression for Ordinal Outcomes?\n\n\n\n\n\n\nanalysis\n\n\nconcept\n\n\ncode\n\n\nlogistic\n\n\nmodelling\n\n\n\nThe proportional odds model is the best choice when we are regressing an ordinal outcome.\n\n\n\n\n\nMay 17, 2024\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\ngtsummary - Your New Go-To for Tables\n\n\n\n\n\n\ncode\n\n\npresentation\n\n\n\nCreate summary and regression tables in a flash.\n\n\n\n\n\nMay 3, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nEverything is a Linear Model\n\n\n\n\n\n\nanalysis\n\n\nconcept\n\n\ncode\n\n\nmodelling\n\n\n\nThe t-test and linear model with one grouping variable are two sides of the same coin.\n\n\n\n\n\nApr 19, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nDon‚Äôt be Scared of Splines\n\n\n\n\n\n\nanalysis\n\n\nconcept\n\n\ncode\n\n\nmodelling\n\n\nvisualisation\n\n\n\nRestricted cubic splines give you the ultimate flexiblity in modelling continuous predictors.\n\n\n\n\n\nApr 5, 2024\n\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\n\nEasily view your data by a grouping variable\n\n\n\n\n\n\ncode\n\n\n\nUse by() to view your data by a grouping variable.\n\n\n\n\n\nMar 22, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nSimpson‚Äôs Paradox - Contextualised for Research Students\n\n\n\n\n\n\nconcept\n\n\ncode\n\n\nvisualisation\n\n\n\nSimpson‚Äôs Paradox is essentially an extreme form of confounding, characterised by a reversal of the effect estimate sign.\n\n\n\n\n\nMar 8, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nImmortal time bias - ‚ÄúThe fallacy that never dies‚Äù (Part 2)\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nmodelling\n\n\nsurvival\n\n\nvisualisation\n\n\n\nLet‚Äôs investigate immortal time bias with a coded example.\n\n\n\n\n\nFeb 23, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nImmortal time bias - ‚ÄúThe fallacy that never dies‚Äù (Part 1)\n\n\n\n\n\n\nconcept\n\n\nsurvival\n\n\n\nLearn why this misclassification of time in a survival analysis can seriously bias your results.\n\n\n\n\n\nFeb 16, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nPut your ggplot on steroids\n\n\n\n\n\n\nvisualisation\n\n\ncode\n\n\n\nPlotly adds some interactivity and can help clarify your data.\n\n\n\n\n\nFeb 2, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nA Very Merry Christmas\n\n\n\n\n\n\nfun\n\n\ncode\n\n\nvisualisation\n\n\n\nHo, Ho, Ho\n\n\n\n\n\nDec 8, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nIt pays to think like a Bayesian\n\n\n\n\n\n\npuzzle\n\n\nprobability\n\n\nBayesian\n\n\n\nBayesian reasoning is more in line with how we process chance in everyday life.\n\n\n\n\n\nDec 1, 2023\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nInteractions (effect modifiers) are important - don‚Äôt ignore them\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nmodelling\n\n\nlogistic\n\n\n\nKeep an open mind to interactions in your next model.\n\n\n\n\n\nNov 24, 2023\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nStats Tips - Welcome\n\n\n\n\n\n\nnews\n\n\n\nWelcome to what I hope can become a useful stats resource.\n\n\n\n\n\nNov 22, 2023\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/007_08Mar_2024/index.html",
    "href": "posts/007_08Mar_2024/index.html",
    "title": "Simpson‚Äôs Paradox - Contextualised for Research Students",
    "section": "",
    "text": "A thought exercise this week in possible ways to interpret the following plot. This comes from hypothetical (simulated) data that purports to assess research students productivity (measured by how quickly they can type) as a function of the number of standard cups of coffee they drink in a day.\n\n\n\n\n\n\nNote\n\n\n\nI am no way suggesting that these findings apply to MSNI research students who no doubt remain highly productive throughout their research careers.\n\n\n\n\nCode\nlibrary(bayestestR)\nlibrary(ggplot2)\nlibrary(dplyr)\n# Simulate some Simpson's paradox style data \nset.seed(253445)\ndat &lt;- simulate_simpson(\n  n = 50,\n  r = 0.5,\n  groups = 3,\n  difference = 2,\n  group_prefix = \"G_\"\n)\n# A couple of variable manipulations to get the variables to look the way I want\ndat$V2 &lt;- (dat$V2*10)+90\ndat$V1 &lt;- (dat$V1+1)/2\n# Rename groups\ndat &lt;- dat |&gt; \n  mutate(Group = case_when(Group == \"G_1\" ~ \"Honours\",\n                           Group == \"G_2\" ~ \"Masters\",\n                           Group == \"G_3\" ~ \"PhD\")) |&gt; \n  rename(`Student Type` = Group)\n# Plot aggregated data\nggplot(dat, aes(x = V1, y = V2)) + \n  geom_point(size = 3) + \n  geom_smooth(method = \"lm\", se = F) +\n  ggtitle(\"Research students - Typing speed as a function of coffee consumption\") +\n  xlab(\"Number of (standard) cups of coffee\") + ylab(\"Words Per Minute (WPM)\") +\n  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, by = 20)) +\n  scale_x_continuous(limits = c(0, 5), breaks = seq(0, 5, by = 1)) +\n  theme_bw(base_size = 18)\n\n\n\n\n\n\n\n\n\nIf they define typing speed as total words/total time elapsed then the time spent buying/making coffee, holding cups and drinking would inflate the denominator without the numerator increasing and make it appear that typing speed has decreased as more time is spent not typing. Would be very hard to get the speed back up just by typing faster in that scenario. So clearly the workplace should be providing each worker with a pre-made, heated camelbak filled with coffee each day to improve productivity. Hands free coffee drinking! (Robb - former Research Student)\nThe best response of the week has to go to Robb for this very well considered and justified attempt to have the bosses supply one of these (I‚Äôm assuming this is what you really meant Robb):\n\n\n\nNew MSNI Coffee Facilities?\n\n\nSo what is this scatterplot showing? Well there appears to be a relationship between coffee drinking and typing speed - drinking more cups of coffee seems to be associated with reduced typing speed (note that I am not suggesting that drinking more coffee causes reduced typing speed, merely that there is a correlation). Fitting a regression model to these data produces a best-fitting line with a negative slope or coefficient. It is interesting though - is this direction of ‚Äòeffect‚Äô what one might reasonably expect? Perhaps, but my intuition would be that drinking more coffee might naturally correlate with a faster typing ability (don‚Äôt forget I‚Äôve made up the data to suit the story I‚Äôm telling - I can only guess as to whether these associations are real or not).\nSo what else could be going on to produce the pattern that you see? Well, you might then naturally think that there is some other ‚Äòlurking‚Äô variable (i.e.¬†a confounder) that is distorting or masking the true relationship between coffee consumption and typing speed leading to the association that we actually observe. And, you‚Äôd be right‚Ä¶\nIt just so turns out that the program that the research student is enrolled in (Honours, Masters, PhD) is an important factor in teasing apart the coffee -&gt; typing speed association. If we now condition on or control for Student Type we see a completely different picture regarding that association. As before, we can similarly fit regression lines to these three subgroups. For each Student Type drinking more coffee is now associated with a faster typing speed (the slope/coefficient of those lines are now positive), but we can also observe some other interesting findings. On average, Honours students drink the fewest cups of coffee and have the fastest typing speed - they are fresh and motivated. In contrast, PhD students drink the most coffee and have the slowest typing speed. My take on this (and again I‚Äôm sure this doesn‚Äôt apply to MSNI students), is that by the time you‚Äôve become a PhD student, you have more caffeine coursing through your system than actual blood, but in fact this does little to help your productivity which is more impaired by your sheer exhaustion, increasing cynicism towards academic life and typing-related repetitive strain injury (maybe this is just a realisation of my own PhD experience ü§î).\n\n\nCode\nggplot(dat, aes(x = V1, y = V2, color = `Student Type`)) + \n  geom_point(size = 3) + \n  geom_smooth(method = \"lm\", se = F) +\n  ggtitle(\"Research students - Typing speed as a function of coffee consumption\") +\n  xlab(\"Number of (standard) cups of coffee\") + ylab(\"Words Per Minute (WPM)\") +\n  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, by = 20)) +\n  scale_x_continuous(limits = c(0, 5), breaks = seq(0, 5, by = 1)) + \n  theme_bw(base_size = 18) +\n  theme(legend.position = c(1,1), legend.justification = c(1.1,1.1))\n\n\n\n\n\n\n\n\n\nYou might have come across Simpson‚Äôs Paradox in your statistical reading - it‚Äôs a fairly common epidemiological bias but it can have important implications for the interpretation of evidence from observational studies, including yours. And so to that end it‚Äôs not purely an exercise for academic interest. Simpson‚Äôs Paradox is a version of Lord‚Äôs Paradox (differentiated by whether exposure and outcome variables are categorical, or continuous, or a combination of both) but at the end of the day they are both a type of Reversal Paradox. Regardless of the variable type, a common characteristic is shared in the Reversal Paradox: the association between two variables can be reversed, diminished, or enhanced when another variable (confounder) is statistically controlled for.\n\n\n\n\n\n\nImportant\n\n\n\nObserved associations at the aggregated level - when important underlying group structures aren‚Äôt realised - or worse still, ignored - can potentially reverse when the data are disaggregated and those underlying group structures are considered in the analysis. In other words, the observed association across all groups can be quite different to that within each group. This is Simpson‚Äôs Paradox in a nutshell.\n\n\nA canonical example of Simpson‚Äôs Paradox is the relationship between body mass and longevity across different species of animals. In general, across all animal species, larger animals (elephants, whales, etc) tend to live longer than smaller animals (rodents, birds, etc). There are of course exceptions to that rule - I‚Äôm looking at you Mr Tortoise. But when you look within species, an inverse correlation typically exists - being heavier tends to be associated with a shorter life.\nThese examples highlight the importance of understanding your data and the research questions you are asking. Prior knowledge of potential variable relationships (measured and unmeasured) and underlying causal theory should be the primary considerations guiding you in the modelling of your data. Simply following variable selection techniques based on statistical criteria can still lead to models that are consistent and replicable, but also very easily lead to erroneous conclusions because you haven‚Äôt considered a pesky ‚Äòlurking‚Äô factor that can leave you with the equivalent of statistical and scientific egg on your face."
  },
  {
    "objectID": "posts/100__2024/index.html",
    "href": "posts/100__2024/index.html",
    "title": "Logistic regression under the hood",
    "section": "",
    "text": "Code\n# Recreate data from Ophthalmic statistics note 11: logistic regression.\n# Original source: A comparison of several methods of macular hole measurement using optical coherence tomography, and their value in predicting anatomical and visual outcomes.\n\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(ggmagnify)\nlibrary(emmeans)\n\n# Simulate data ----\nn &lt;- 1000                    # don't change this unless necessary (plots might be fragile)\nset.seed(1234)\nx  &lt;-  rnorm(n, 486, 142)    # generate macular hole inner opening data with mean 486 and sd = 152\nz  &lt;-  10.89 - 0.016 * x     # generate variable that is linear combination of intercept = 10.89 and coefficient for macular hole -0.016 (logit scale)\npr  &lt;-  1/(1 + exp(-z))      # generate probabilities from this\ny  &lt;-  rbinom(n, 1, pr)      # generate outcome variable as a function of those probabilities\n\n# Create dataframe from these:\ndf &lt;-  data.frame(y = y, x = x, z = z, pr = pr)\ndf &lt;- df |&gt; \n  filter(x &gt; 100) # only include those with thickness &gt; 100\n\n# Logistic regression model ----\n# Rescale x to 1 unit = 100 microns instead of 1 micron\nsummary(mod_logistic &lt;- glm(y ~ I(x/100), data = df, family = \"binomial\"))\n\n\n\nCall:\nglm(formula = y ~ I(x/100), family = \"binomial\", data = df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  10.3501     0.7456   13.88   &lt;2e-16 ***\nI(x/100)     -1.5045     0.1212  -12.42   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 773.74  on 989  degrees of freedom\nResidual deviance: 494.67  on 988  degrees of freedom\nAIC: 498.67\n\nNumber of Fisher Scoring iterations: 6\n\n\nCode\n# Emmeans of logodds at 600 and 700 microns\nemmeans_df &lt;- data.frame(emmeans(mod_logistic, ~ x, at = list(x = c(600, 700))))\n# Create df of emmeans and corresponding odds and probs, for plotting\nemmeans_df &lt;- emmeans_df |&gt; \n  select(x, emmean) |&gt; \n  rename(logodds = emmean) |&gt; \n  mutate(odds = round(exp(logodds), 3),\n         probs = round(plogis(logodds), 2),\n         logodds = round(logodds, 3))\n\n# Predictions ----\n# Create new df to predict on new values of x\nnew_dat &lt;- data.frame(x = seq(from = 0, to = 1200, length.out = 100))\n# Predict new fitted values and SE's on logodds scale\npred_logodds &lt;- predict(mod_logistic, newdata = new_dat, type = \"link\", se = TRUE)\nnew_dat &lt;- cbind(new_dat, pred_logodds)\n# Create new df of predictions\npredictions &lt;- new_dat |&gt; \n  rename(pred_logodds_est = fit) |&gt; \n  mutate(pred_logodds_LL = pred_logodds_est - (1.96 * se.fit),\n         pred_logodds_UL = pred_logodds_est + (1.96 * se.fit)) |&gt; \n  select(-c(se.fit, residual.scale))\n# Predict new fitted values and SE's on odds scale\npredictions &lt;- predictions |&gt; \n  mutate(pred_odds_est = exp(pred_logodds_est),\n         pred_odds_LL = exp(pred_logodds_LL),\n         pred_odds_UL = exp(pred_logodds_UL))\n# Predict new fitted values and SE's on probability scale\npred_probs &lt;- predict(mod_logistic, newdata = new_dat, type = \"response\", se = TRUE)\nnew_dat &lt;- cbind(new_dat[1], pred_probs)\nnew_dat &lt;- new_dat |&gt; \n  mutate(pred_probs_LL = fit - (1.96 * se.fit),\n         pred_probs_UL = fit + (1.96 * se.fit))\n# Add predicted probs and CIs to predictions df\npredictions &lt;- cbind(predictions, \n                     pred_probs_est = new_dat$fit, \n                     pred_probs_LL = new_dat$pred_probs_LL,\n                     pred_probs_UL = new_dat$pred_probs_UL)\n\n# Reformat plots slightly for ggarrange ----\np3a &lt;- ggplot(predictions, aes(x = x, y = pred_logodds_est)) + \n  geom_ribbon(aes(ymin = pred_logodds_LL, ymax = pred_logodds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = x, y = y), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1150, y = 6, label = \"log-odds\", size = 10) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-100, 100), breaks = seq(-100, 100, by = 2)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(-8, 8)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(x, logodds),\n                            label = emmeans_df$logodds, \n                            nudge_x = c(-50, 50), nudge_y = c(4, -4),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  theme_bw(base_size = 25) +\n  ylab(\"\") +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\n\np4a &lt;- ggplot(predictions, aes(x = x, y = pred_odds_est)) + \n  geom_ribbon(aes(ymin = pred_odds_LL, ymax = pred_odds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = x, y = y), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1150, y = 6000, label = \"odds\", size = 10) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(seq(0, 1000000, by = 1000))) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(0, 7000)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ylab(\"\") +\n  theme_bw(base_size = 25) +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\np4a_inset &lt;- p4a +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(1,2,3,4,5, seq(0, 1000000, by = 1000))) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(x, odds),\n                            label = emmeans_df$odds, \n                            nudge_x = c(-50, 50), nudge_y = c(-1, 2),\n                            color = \"red\", segment.size = 0.2, size = 5)\np4a &lt;- p4a + geom_magnify(from = c(xmin = 500, xmax = 1000, ymin = 0, ymax = 5), \n                          to = c(xmin = 465, xmax = 1010, ymin = 1000, ymax = 5000), \n                          shadow = T, axes = \"y\", plot = p4a_inset)\n\np5a &lt;- ggplot(predictions, aes(x = x, y = pred_probs_est)) + \n  geom_ribbon(aes(ymin = pred_probs_LL, ymax = pred_probs_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = x, y = y), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1150, y = 0.8, label = \"probability\", size = 10) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(x, probs),\n                            label = emmeans_df$probs, \n                            nudge_x = c(-50, 50), nudge_y = c(-0.1, 0.1),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  ylab(\"\") + xlab(\"Macular hole thickness\") +\n  theme_bw(base_size = 25)\nggarrange(p3a, p4a, p5a, align = \"v\", ncol = 1, heights = c(1,1,1.2))"
  },
  {
    "objectID": "posts/008_22Mar_2024/index.html",
    "href": "posts/008_22Mar_2024/index.html",
    "title": "Easily view your data by a grouping variable",
    "section": "",
    "text": "It is easy enough to view a dataframe in RStudio by opening the dataframe in the viewer or printing the dataframe (or part of it) to the console. However, this can be messy if you want to quickly identify data by a grouping variable (usually the patient id). The by() function can help you to do this. Let‚Äôs illustrate its utility with the sleepstudy dataset from the lme4 package. To start with I‚Äôll print the data for the first 3 subjects as one might.\nsleepstudy |&gt; as_tibble() |&gt; print(n = 30)\n\n\nCode\nlibrary(lme4)\nlibrary(dplyr)\n# Load data\ndata(\"sleepstudy\")\nsleepstudy |&gt; as_tibble() |&gt; print(n = 30)\n\n\n# A tibble: 180 √ó 3\n   Reaction  Days Subject\n      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  \n 1     250.     0 308    \n 2     259.     1 308    \n 3     251.     2 308    \n 4     321.     3 308    \n 5     357.     4 308    \n 6     415.     5 308    \n 7     382.     6 308    \n 8     290.     7 308    \n 9     431.     8 308    \n10     466.     9 308    \n11     223.     0 309    \n12     205.     1 309    \n13     203.     2 309    \n14     205.     3 309    \n15     208.     4 309    \n16     216.     5 309    \n17     214.     6 309    \n18     218.     7 309    \n19     224.     8 309    \n20     237.     9 309    \n21     199.     0 310    \n22     194.     1 310    \n23     234.     2 310    \n24     233.     3 310    \n25     229.     4 310    \n26     220.     5 310    \n27     235.     6 310    \n28     256.     7 310    \n29     261.     8 310    \n30     248.     9 310    \n# ‚Ñπ 150 more rows\n\n\nBut we can do this better with:\nby(sleepstudy, sleepstudy$PATIENT_ID, identity)[1:3]\nNote that the [1:3] indicates the range of group indices that you want to view.\n\n\nCode\nby(sleepstudy, sleepstudy$Subject, identity)[1:3]\n\n\n$`308`\n   Reaction Days Subject\n1  249.5600    0     308\n2  258.7047    1     308\n3  250.8006    2     308\n4  321.4398    3     308\n5  356.8519    4     308\n6  414.6901    5     308\n7  382.2038    6     308\n8  290.1486    7     308\n9  430.5853    8     308\n10 466.3535    9     308\n\n$`309`\n   Reaction Days Subject\n11 222.7339    0     309\n12 205.2658    1     309\n13 202.9778    2     309\n14 204.7070    3     309\n15 207.7161    4     309\n16 215.9618    5     309\n17 213.6303    6     309\n18 217.7272    7     309\n19 224.2957    8     309\n20 237.3142    9     309\n\n$`310`\n   Reaction Days Subject\n21 199.0539    0     310\n22 194.3322    1     310\n23 234.3200    2     310\n24 232.8416    3     310\n25 229.3074    4     310\n26 220.4579    5     310\n27 235.4208    6     310\n28 255.7511    7     310\n29 261.0125    8     310\n30 247.5153    9     310\n\n\nIf you want to take this a step further, you can generalise this with a function that will allow you to quickly view the data in any range that you want, without having to continually copy and paste that line of code. Just call the function with your dataframe and group id names and the range of group indices that you want to view (interestingly while writing this function I worked out you don‚Äôt even need the by() function to achieve the same result).\nprint_groups(sleepstudy, Subject, 1, 3)\n\n\nCode\n# Create function\nprint_groups &lt;- function(df, id, index1, index2) {\n  df &lt;- data.frame(df)\n  ids_all &lt;-  unique(eval(substitute(id), df))\n  ids_range &lt;- ids_all[index1:index2]\n  if (index1 &lt;= length(ids_all) & index2 &lt;= length(ids_all)) {\n    for (id2 in ids_range) {\n      cat(paste0(\"id = \", id2, \"\\n\"))\n      print(df[eval(substitute(id), df) %in% id2,])\n      cat(\"----------------------------\\n\\n\")\n    }\n  } else {\n    print(\"There aren't that many groups in your dataset\")\n  }\n}\n\n# Use function\nprint_groups(sleepstudy, Subject, 1, 3)\n\n\nid = 308\n   Reaction Days Subject\n1  249.5600    0     308\n2  258.7047    1     308\n3  250.8006    2     308\n4  321.4398    3     308\n5  356.8519    4     308\n6  414.6901    5     308\n7  382.2038    6     308\n8  290.1486    7     308\n9  430.5853    8     308\n10 466.3535    9     308\n----------------------------\n\nid = 309\n   Reaction Days Subject\n11 222.7339    0     309\n12 205.2658    1     309\n13 202.9778    2     309\n14 204.7070    3     309\n15 207.7161    4     309\n16 215.9618    5     309\n17 213.6303    6     309\n18 217.7272    7     309\n19 224.2957    8     309\n20 237.3142    9     309\n----------------------------\n\nid = 310\n   Reaction Days Subject\n21 199.0539    0     310\n22 194.3322    1     310\n23 234.3200    2     310\n24 232.8416    3     310\n25 229.3074    4     310\n26 220.4579    5     310\n27 235.4208    6     310\n28 255.7511    7     310\n29 261.0125    8     310\n30 247.5153    9     310\n----------------------------\n\n\nAnd there you have it!"
  },
  {
    "objectID": "posts/001_24Nov2023/index.html",
    "href": "posts/001_24Nov2023/index.html",
    "title": "Interactions (effect modifiers) are important - don‚Äôt ignore them",
    "section": "",
    "text": "Code\nlibrary(knitr)\nlibrary(quarto)\nlibrary(emmeans)\nlibrary(flextable)\nlibrary(ggplot2)\nsuppressPackageStartupMessages(library(gtsummary))\nopts_chunk$set(echo = T,\n               cache = F,\n               prompt = F,\n               tidy = F,\n               message = F,\n               warning = F)"
  },
  {
    "objectID": "posts/001_24Nov2023/index.html#the-question",
    "href": "posts/001_24Nov2023/index.html#the-question",
    "title": "Interactions (effect modifiers) are important - don‚Äôt ignore them",
    "section": "1 The Question",
    "text": "1 The Question\n\nRecall that the question this week was to choose between:\nA) Age is a confounder in the relationship between sex and hospitalisation from car crash.\nB) Age is an effect modifier in the relationship between sex and hospitalisation from car crash.\nusing the data supplied below."
  },
  {
    "objectID": "posts/001_24Nov2023/index.html#the-answer",
    "href": "posts/001_24Nov2023/index.html#the-answer",
    "title": "Interactions (effect modifiers) are important - don‚Äôt ignore them",
    "section": "2 The Answer",
    "text": "2 The Answer\n\nThe answer is B. Younger male drivers tend to be more stupid and injure themselves more seriously than their female counterparts. Upon reaching a suitable age of maturity their risk of hospitalisation reduces to about the same (if we assume the 95% CI for the 8% reduction includes 0) as that for females.\nWhen a third variable plays a role in the association between an exposure and an outcome it may act as a confounder OR effect modifier (AND sometimes both).\nA simple confounder (e.g.¬†age) will show the same exposure -&gt; outcome association across all of its categories (e.g.¬†same risk ratio in &lt; 40 yrs and ‚â• 40 yrs)\nAn effect modifier will show different magnitudes of association across its categories (e.g.¬†the risk ratio will differ in those &lt; 40 yrs and ‚â• 40 yrs).\nStratification is the simplest form of exploring and adjusting for confounding/interaction effects (used before we had all this computing power).\nSubgroups of data are created for each category of confounder/effect modifier and estimates of interest (mean differences, risk ratios, etc) calculated in each.\nThese can then be combined in a weighted manner to give an overall (adjusted) estimate if NO effect modification is present.\nThis is equivalent to including the third variable as a covariate in our regression model (we now use models rather than stratification methods).\nSimple inclusion in the regression model (using + in R) FORCES the exposure -&gt; outcome association to be the same across all categories of the effect modifier even if in reality it‚Äôs not.\n+ assumes confounding ONLY and NO effect modification.\nA problem arises, however, when the third variable is more an effect modifier, rather than confounder.\nIf we suspect effect modification is present, we need to include this third variable in the model as an interaction term (using * in R)\nThis will allow the exposure -&gt; outcome association to differ across categories of the effect modifier.\n* assumes effect modification is present.\nThis is a more flexible model specification, than simply ‚Äòadjusting‚Äô for a variable.\nInterpretation is a little more involved (always happy to help with this) but the point is it‚Äôs important not to blindly assume a third variable can only ever be a confounder.\nIf effect modification is present, you need to know about it.\nIt is simple to test for effect modification in R, Stata, etc. Include the interaction term and then drop it if not clinically/statistically significant at some level.\nI have included some R output below showing the equivalence of stratification and modelling approaches to interaction effects.\n\nBefore we get to that - a simple set of guidelines for how to think about crude vs stratified associations:\n\nEquivalence of model-derived crude estimate\nRecall that the aggregated data that the crude estimate is calculated from is:\n\n\nCode\ndat_agg &lt;- data.frame(sex = c(\"Male\", \"Female\"),\n                      hospitalised = as.numeric(c(1330, 798)), \n                      not_hospitalised = as.numeric(c(7018,6400)))\ndat_agg\n\n\n\n\n\n\nsex\nhospitalised\nnot_hospitalised\n\n\n\n\nMale\n1330\n7018\n\n\nFemale\n798\n6400\n\n\n\n\n\n\nTo estimate this model in R we essentially run a logistic regression but instead of outputting an odds ratio, we calculate a risk ratio by specifying a log rather than the default logit link. We will also use the aggregate model specification, as we don‚Äôt have the individual-level data.\nThe model-derived crude risk ratio for sex = 1.44 (95% CI 1.32, 1.56; p &lt; 0.001). This is very close to the estimate that we initially calculated manually from the 2 x 2 table (1.45).\n\n\nCode\nmod_crude &lt;- glm(cbind(hospitalised, not_hospitalised) ~ sex, data = dat_agg, family = binomial(link = \"log\"))\ntbl_regression(mod_crude, exp = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nRR1\n95% CI1\np-value\n\n\n\n\nsex\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†Female\n‚Äî\n‚Äî\n\n\n\n\n¬†¬†¬†¬†Male\n1.44\n1.32, 1.56\n&lt;0.001\n\n\n\n1 RR = Relative Risk, CI = Confidence Interval"
  },
  {
    "objectID": "posts/001_24Nov2023/index.html#lets-introduce-age-40-vs-40-as-a-third-variable",
    "href": "posts/001_24Nov2023/index.html#lets-introduce-age-40-vs-40-as-a-third-variable",
    "title": "Interactions (effect modifiers) are important - don‚Äôt ignore them",
    "section": "3 Let‚Äôs introduce age (<40 vs ‚â• 40) as a third variable",
    "text": "3 Let‚Äôs introduce age (&lt;40 vs ‚â• 40) as a third variable\n\n\nCode\ndat_disagg &lt;- data.frame(sex = c(\"Male\", \"Female\", \"Male\", \"Female\"),\n                         age = c(\"&lt; 40\", \"&lt; 40\", \"‚â• 40\", \"‚â• 40\"),\n                         hospitalised = as.numeric(c(966, 460, 364, 348)), \n                         not_hospitalised = as.numeric(c(3146, 3000, 3872, 3400)))\ndat_disagg\n\n\n\n\n\n\nsex\nage\nhospitalised\nnot_hospitalised\n\n\n\n\nMale\n&lt; 40\n966\n3146\n\n\nFemale\n&lt; 40\n460\n3000\n\n\nMale\n‚â• 40\n364\n3872\n\n\nFemale\n‚â• 40\n348\n3400\n\n\n\n\n\n\n\n3.1 Age as a confounder\nFor now, let‚Äôs just assume age is a confounder in the association between sex and hospitalisation risk. The model formulation in R is then:\nmod_adj &lt;- glm(cbind(hospitalised, not_hospitalised) ~ sex + age, data = dat_agg, family = binomial(link = \"log\"))\nand the risk ratios we get are:\n\n\nCode\nmod_confound &lt;- glm(cbind(hospitalised, not_hospitalised) ~ sex + age, data = dat_disagg, family = binomial(link = \"log\"))\ntbl_regression(mod_confound, exp = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nRR1\n95% CI1\np-value\n\n\n\n\nsex\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†Female\n‚Äî\n‚Äî\n\n\n\n\n¬†¬†¬†¬†Male\n1.43\n1.32, 1.55\n&lt;0.001\n\n\nage\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†&lt; 40\n‚Äî\n‚Äî\n\n\n\n\n¬†¬†¬†¬†‚â• 40\n0.47\n0.43, 0.51\n&lt;0.001\n\n\n\n1 RR = Relative Risk, CI = Confidence Interval\n\n\n\n\n\n\n\n\nSo, what we are seeing here is that the magnitude of association between sex and hospitalisation risk is averaged (in a weighted way) over both categories of age to produce one effect estimate sex = 1.43 (95% CI 1.32, 1.55; p &lt; 0.001). This just so happens to be almost the same as the crude estimate when you ignore age altogether.\nThe effect for age in this model is such that whatever your sex, there is about a 53% reduction in the risk of hospitalisation if you are over 40 vs under 40. Note that in the stratification approach, you aren‚Äôt able to calculate an effect for age because you are stratifying by it (essentially treating it as a nuisance variable).\n\n\n3.2 Age as an effect modifier\nNow, let‚Äôs correctly model age as an effect modifier in the association between sex and hospitalisation risk. The model formulation in R is then (note the * operator):\nmod_adj &lt;- glm(cbind(hospitalised, not_hospitalised) ~ sex * age, data = dat_agg, family = binomial(link = \"log\"))\nand the risk ratios we get are:\n\n\nCode\nmod_interact &lt;- glm(cbind(hospitalised, not_hospitalised) ~ sex * age, data = dat_disagg, family = binomial(link = \"log\"))\ntbl_regression(mod_interact, exp = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nRR1\n95% CI1\np-value\n\n\n\n\nsex\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†Female\n‚Äî\n‚Äî\n\n\n\n\n¬†¬†¬†¬†Male\n1.77\n1.60, 1.96\n&lt;0.001\n\n\nage\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†&lt; 40\n‚Äî\n‚Äî\n\n\n\n\n¬†¬†¬†¬†‚â• 40\n0.70\n0.61, 0.80\n&lt;0.001\n\n\nsex * age\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†Male * ‚â• 40\n0.52\n0.44, 0.62\n&lt;0.001\n\n\n\n1 RR = Relative Risk, CI = Confidence Interval\n\n\n\n\n\n\n\n\nNote how the p value for the interaction term is very low - this would be a good indicator that the model fits the data better with the interaction term present than without it (i.e.¬†assuming age as a confounder only).\nAs I mentioned earlier, the model interpretation with an interaction present does become a little more complicated, but let‚Äôs break this down (note that I use ‚Äúeffect‚Äù in a non-causal way):\n\nThe coefficient for sex = 1.77 (95% CI 1.60, 1.96; p &lt; 0.001). The represents the ‚Äúeffect‚Äù of sex (being male relative to female) on hospitalisation risk at the reference level of age, which in this case is the under 40 yrs group. So, for those under 40, there is about a 77% increased risk for males relative to females.\nThe coefficient for age = 0.70 (95% CI 0.61, 0.80; p &lt; 0.001). This represents the ‚Äúeffect‚Äù of age (being older than 40 yrs relative to younger than 40 yrs) on hospitalisation risk at the reference level of sex, which in this case is female. So, for females, there is about a 30% risk reduction in the need for hospitalisation for older relative to younger drivers.\nThe coefficient for the interaction term: sex * age = 0.52 (95% CI 0.44, 0.62; p &lt; 0.001). This represents the multiplicative increase in the magnitude of association for males over 40 yrs."
  },
  {
    "objectID": "posts/001_24Nov2023/index.html#effect-modification-means-more-associations-to-estimate",
    "href": "posts/001_24Nov2023/index.html#effect-modification-means-more-associations-to-estimate",
    "title": "Interactions (effect modifiers) are important - don‚Äôt ignore them",
    "section": "4 Effect modification means more associations to estimate",
    "text": "4 Effect modification means more associations to estimate\nIn this specific case, when you treat age as a confounder, the model produces two risk ratios - one for sex and one for age. However, when you treat age as an effect modifier, there are now four possible risk ratios to estimate (if you care about age more than it being a ‚Äúnuisance‚Äù variable to control for). These are:\n\nThe effect of being male in younger individuals.\nThe effect of being male in older individuals.\nThe effect of being older in females.\nThe effect of being older in males.\n\nYou can easily enough work these out manually by multiplying the respective reference coefficients with the interaction coefficient. The risk ratios for each of the above would then be:\n\n1.77 (we can just read this one straight off the model output)\n1.77 x 0.52 = 0.92\n0.70 (again we can just read this one straight off)\n0.70 x 0.52 = 0.36\n\nNote that the effects for 1. and 2. are very similar to what we calculated straight from the 2 x 2 tables (1.84 and 0.92, respectively - as previously mentioned, the effects for 3. and 4. aren‚Äôt able to be calculated for the stratifying variable)."
  },
  {
    "objectID": "posts/001_24Nov2023/index.html#emmeans-should-be-your-new-best-friend",
    "href": "posts/001_24Nov2023/index.html#emmeans-should-be-your-new-best-friend",
    "title": "Interactions (effect modifiers) are important - don‚Äôt ignore them",
    "section": "5 Emmeans should be your new best friend",
    "text": "5 Emmeans should be your new best friend\nPerhaps I am preaching to the converted, but if you don‚Äôt know what the emmeans package and specific function in R does, then you should learn about it (the equivalent function in Stata is margins).\nhttps://cran.r-project.org/web/packages/emmeans/index.html\nhttps://aosmith.rbind.io/2019/03/25/getting-started-with-emmeans/\nemmeans does a lot of things, but perhaps its workhorse function is to allow you to take a model and calculate adjusted predictions (either at set values of covariates, or by ‚Äòaveraging‚Äô over them). In this case, we can very easily use emmeans to reproduce the manual calculations we just did.\n\n\nCode\nemmeans(mod_interact, ~ sex + age, type = \"response\") |&gt; \n  data.frame() |&gt; \n  flextable() |&gt; \n  colformat_double(digits = 3, na_str = \"N/A\") |&gt;\n  set_table_properties(layout = \"autofit\") |&gt; \n  height(height = 1, unit = \"cm\") |&gt; \n  hrule(rule = \"atleast\", part = \"header\") |&gt; \n  align(align = \"center\", part = \"body\") |&gt;\n  bg(bg = \"white\", part = \"all\") |&gt; \n  flextable::font(fontname = \"Consolas\", part = \"all\") |&gt;\n  theme_vanilla()\n\n\nPredicted Probabilities of HospitalisationsexageprobSEdfasymp.LCLasymp.UCLFemale&lt; 400.1330.006 Inf0.1220.145Male&lt; 400.2350.007 Inf0.2220.248Female‚â• 400.0930.005 Inf0.0840.103Male‚â• 400.0860.004 Inf0.0780.095\n\n\nSpecifying type = \"response\" in the emmeans call indicates that we want to calculate the outcome on the probability (i.e.¬†risk) scale. It is simple enough to plot these predicted probabilities using the emmip function in emmeans.\n\n\nCode\nemmip(mod_interact, age ~ sex, type = \"response\") + \n  theme_bw(base_size = 18)\n\n\n\n\n\n\n\n\n\nTo get the risk ratios we have been working with until now, we simply add the pairs(rev = T) function to the call:\n\n\nCode\nemmeans(mod_interact, ~ sex + age, type = \"response\") |&gt; pairs(rev = T) |&gt; \n  data.frame() |&gt; \n  flextable() |&gt; \n  colformat_double(digits = 3, na_str = \"N/A\") |&gt;\n  set_table_properties(layout = \"autofit\") |&gt; \n  height(height = 1, unit = \"cm\") |&gt; \n  hrule(rule = \"atleast\", part = \"header\") |&gt; \n  align(align = \"center\", part = \"body\") |&gt;\n  bg(bg = \"white\", part = \"all\") |&gt; \n  flextable::font(fontname = \"Consolas\", part = \"all\") |&gt;\n  theme_vanilla()\n\n\nAll Pairwise Risk RatioscontrastratioSEdfnullz.ratiop.valueMale &lt; 40 / Female &lt; 401.7670.091 Inf1.00011.0030.000Female ‚â• 40 / Female &lt; 400.6980.047 Inf1.000-5.3560.000Female ‚â• 40 / Male &lt; 400.3950.023 Inf1.000-15.9230.000Male ‚â• 40 / Female &lt; 400.6460.043 Inf1.000-6.5820.000Male ‚â• 40 / Male &lt; 400.3660.021 Inf1.000-17.4990.000Male ‚â• 40 / Female ‚â• 400.9250.066 Inf1.000-1.0830.700\n\n\nNote that this gives us two extra comparisons we might not really want (the 3rd and 4th lines of the output) as it estimates every single pairwise comparison. We can get a bit fancier and customise the emmeans output to give us only what we want:\n\n\nCode\nemm &lt;- emmeans(mod_interact, ~ sex + age, type = \"response\") # save the estimated risks\ncustom &lt;- list(`The effect of being male in younger individuals` = c(-1,1,0,0),\n               `The effect of being male in older individuals` = c(0,0,-1,1),\n               `The effect of being older in females` = c(-1,0,1,0),\n               `The effect of being older in males` = c(0,-1,0,1)) # create custom grid of RR's to estimate\ncontrast(emm, custom) |&gt; \n  summary(infer = T) |&gt; \n  data.frame() |&gt; \n  flextable() |&gt; \n  colformat_double(digits = 3, na_str = \"N/A\") |&gt;\n  set_table_properties(layout = \"autofit\") |&gt; \n  height(height = 1, unit = \"cm\") |&gt; \n  hrule(rule = \"atleast\", part = \"header\") |&gt; \n  align(align = \"center\", part = \"body\") |&gt;\n  bg(bg = \"white\", part = \"all\") |&gt; \n  flextable::font(fontname = \"Consolas\", part = \"all\") |&gt;\n  theme_vanilla()\n\n\nCustom Pairwise Risk RatioscontrastratioSEdfasymp.LCLasymp.UCLnullz.ratiop.valueThe effect of being male in younger individuals1.7670.091 Inf1.5971.9561.00011.0030.000The effect of being male in older individuals0.9250.066 Inf0.8041.0651.000-1.0830.279The effect of being older in females0.6980.047 Inf0.6120.7961.000-5.3560.000The effect of being older in males0.3660.021 Inf0.3270.4091.000-17.4990.000\n\n\nNote, that these match the manual calculations pretty well.\nPlease take some time to learn about emmeans (or margins in Stata). It will make your life so much easier if you plan to have a career in research (and don‚Äôt always have access to a statistician)."
  },
  {
    "objectID": "posts/009_05Apr_2024/index.html",
    "href": "posts/009_05Apr_2024/index.html",
    "title": "Don‚Äôt be Scared of Splines",
    "section": "",
    "text": "Have you heard the term restricted cubic spline (RCS) and thought ‚Äòthat‚Äôs just seems too hard but I should learn about it one day‚Äô and then stuck to modelling your continuous predictor as you always do, assuming it has a linear relationship with the outcome? Well I hope that by the end of this post you have a better basic understanding of what RCS‚Äôs actually are, how they give you so much more flexibility in your modelling toolkit, and above all else, how they are really not that hard to use.\nWhen you want to explore the association between a continuous predictor variable and an outcome (of any form really - binary, count, continuous) you have choices to make about how you parameterise that predictor. In fact, one could consider a hierarchy of such choices that range from downright egregious through to ‚Äòwe‚Äôll just do it how it‚Äôs always done‚Äô through to what is becoming thought more of these days as ‚Äòbest-practice‚Äô in statistical modelling. These approaches include:\n\nCategorising the predictor - Egregious.\nAssuming the predictor has a linear relationship with the outcome (or its link function if the outcome is not continuous) - ‚ÄòWe‚Äôll just do it how it‚Äôs always done‚Äô.\nAssuming the predictor has a non-linear relationship with the outcome and using a piece-wise model (segmented regression) to model smaller segments of the data where linearity does in fact hold - A better alternative than assuming linearity.\nAssuming the predictor has a non-linear relationship with the outcome and using polynomial (e.g.¬†quadratic/cubic/etc) regression - Again, a better alternative than assuming linearity.\nAssuming the predictor has a non-linear relationship with the outcome and using RCS‚Äôs - arguably ‚Äòbest practice‚Äô.\n\n\n\n\n\n\n\nImportant\n\n\n\nDon‚Äôt forget it‚Äôs always a good idea to plot your data first to visualise the relationship (using a lowess smoother helps). There is no need to worry about non-linearity if in fact the relationship between your predictor and outcome isn‚Äôt - just model it as linear and you‚Äôre done.\n\n\nNow, having made that point, let me follow by saying that this is relatively easy when you have a continuous predictor and a continuous outcome. But what do you do with other outcome types - for example, in the case of a binary outcome (successes/failures) your outcome just consists of a bunch of 0's and 1's. This is more challenging to visualise but it is possible. Here, a good way to visualise your observed data is to ‚Äòbin‚Äô the predictor into discrete categories (arbitrarily decided by you), counting the number of successes out of the total in each bin - this will give the proportion of successes - i.e.¬†the observed probability of success in each bin. You then convert each proportion into its equivalent logit (log-odds) and plot this (on the Y axis) against the mid-point of each bin of the predictor on the X axis. If the best-fitting line in that plot is approximately linear, then the assumption of linearity of the continuous predictor with the binary outcome is upheld.\nWell I did say this was more challenging‚Ä¶\nUltimately, visualise your data where it‚Äôs easy enough to do so (which will primarily be the continuous predictor vs continuous outcome case). But when this isn‚Äôt so straightforward or practical, as in the binary outcome case above, we can in lieu use model-fitting statistics to help us decide whether incorporating non-linear predictor terms in our model is of value or not. In the basic comparison we fit two models - one assuming linearity and one assuming non-linearity (via one of the other methods) and let either some information criterion (e.g.¬†the AIC), or a likelihood ratio test guide us as to the better fit. So it is certainly still possible to incorporate non-linear terms in a statistical model without having first plotted that data. I‚Äôll illustrate this shortly.\nNow let‚Äôs have a look at some simulated data that demonstrates a non-linear relationship of the predictor with the outcome, and how these different approaches may be applied to model this."
  },
  {
    "objectID": "posts/003_08Dec_2023/index.html",
    "href": "posts/003_08Dec_2023/index.html",
    "title": "A Very Merry Christmas",
    "section": "",
    "text": "This figure was made using ggplot2 and while I can‚Äôt take credit for coming up with the idea (source), I have added a couple of flourishes including the animated lights.\nI hope everyone has a safe, happy and enjoyable holiday period.\n\n\nCode\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(gifski)\nlibrary(extrafont)\nloadfonts()\n\n# Read in the base Christmas tree data\nChristmasTree &lt;- read.csv(\"https://raw.githubusercontent.com/t-redactyl/Blog-posts/master/Christmas%20tree%20base%20data.csv\")\n\n# Change tree colour\nChristmasTree$Tree.Colour[ChristmasTree$Tree.Colour == \"#143306\"] &lt;- \"green4\"\n\n# Generate the \"lights\"\nDesired.Lights &lt;- 100\nTotal.Lights &lt;- sum(round(Desired.Lights * 0.35) + round(Desired.Lights * 0.20) + \n                    round(Desired.Lights * 0.17) + round(Desired.Lights * 0.13) +\n                    round(Desired.Lights * 0.10) + round(Desired.Lights * 0.05))\n\nLights &lt;- data.frame(Lights.X = c(round(runif(round(Desired.Lights * 0.35), 4, 18), 0),\n                                  round(runif(round(Desired.Lights * 0.20), 5, 17), 0),\n                                  round(runif(round(Desired.Lights * 0.17), 6, 16), 0),\n                                  round(runif(round(Desired.Lights * 0.13), 7, 15), 0),\n                                  round(runif(round(Desired.Lights * 0.10), 8, 14), 0),\n                                  round(runif(round(Desired.Lights * 0.05), 10, 12), 0)))\nLights$Lights.Y &lt;- c(round(runif(round(Desired.Lights * 0.35), 4, 6), 0),\n                     round(runif(round(Desired.Lights * 0.20), 7, 8), 0),\n                     round(runif(round(Desired.Lights * 0.17), 9, 10), 0),\n                     round(runif(round(Desired.Lights * 0.13), 11, 12), 0),\n                     round(runif(round(Desired.Lights * 0.10), 13, 14), 0),\n                     round(runif(round(Desired.Lights * 0.05), 15, 17), 0))\nLights$Lights.Colour &lt;- c(round(runif(Total.Lights, 1, 3), 0))\n\n# Generate the \"baubles\"\nBaubles &lt;- data.frame(Bauble.X = c(6, 9, 15, 17, 5, 13, 16, 7, 10, 14, 7, 9, 11, 14, 8, 14, 9, 12, 11, 12, 14, 11, 17, 10))\nBaubles$Bauble.Y &lt;- c(4, 5, 4, 4, 5, 5, 5, 6, 6, 6, 8, 8, 8, 8, 10, 10, 11, 11, 12, 13, 10, 16, 7, 14)\nBaubles$Bauble.Colour &lt;- factor(c(1, 2, 2, 3, 2, 3, 1, 3, 1, 1, 1, 2, 1, 2, 3, 3, 2, 1, 3, 2, 1, 3, 3, 1))\nBaubles$Bauble.Size &lt;- c(6, 18, 6, 6, 12, 6, 12, 12, 12, 6, 6, 6, 18, 18, 18, 12, 18, 6, 6, 12, 12, 18, 18, 12)\n\n# Generate the plot\np &lt;- ggplot() + \n  geom_tile(data = ChristmasTree, aes(x = Tree.X, y = Tree.Y, fill = Tree.Colour)) +\n  scale_fill_identity() + \n  geom_point(data = Lights, aes(x = Lights.X, y = Lights.Y), color = \"lightgoldenrodyellow\", shape = 8) +\n  geom_point(data = Baubles, aes(x = Bauble.X, y = Bauble.Y, colour = Bauble.Colour), size = Baubles$Bauble.Size, shape = 16) +\n  scale_colour_manual(values = c(\"firebrick2\", \"gold\", \"blue3\")) +\n  scale_size_area(max_size = 12) +\n  theme_bw() +\n  scale_x_continuous(breaks = NULL) + \n  scale_y_continuous(breaks = NULL) +\n  geom_segment(aes(x = 2.5, xend = 4.5, y = 1.5, yend = 1.5), colour = \"blueviolet\", size = 2) +\n  geom_segment(aes(x = 5.5, xend = 8.5, y = 1.5, yend = 1.5), colour = \"dodgerblue3\", size = 2) +\n  geom_segment(aes(x = 13.5, xend = 16.5, y = 1.5, yend = 1.5), colour = \"blueviolet\", size = 2) +\n  geom_segment(aes(x = 17.5, xend = 19.5, y = 1.5, yend = 1.5), colour = \"dodgerblue3\", size = 2) +\n  geom_segment(aes(x = 3.5, xend = 3.5, y = 0.5, yend = 2.5), colour = \"blueviolet\", size = 2) +\n  geom_segment(aes(x = 7.0, xend = 7.0, y = 0.5, yend = 2.5), colour = \"dodgerblue3\", size = 2) +\n  geom_segment(aes(x = 15.0, xend = 15.0, y = 0.5, yend = 2.5), colour = \"blueviolet\", size = 2) +\n  geom_segment(aes(x = 18.5, xend = 18.5, y = 0.5, yend = 2.5), colour = \"dodgerblue3\", size = 2) +\n  annotate(\"text\", x = 11, y = 20, label = \"Merry Christmas!\",family = \"Luminari\", color = \"white\", size = 12) +\n  transition_states(states=Lights.Colour, transition_length = 0, state_length = 0.0001) +\n  labs(x = \"\", y = \"\") +\n  theme(legend.position = \"none\") +\n  theme(panel.background = element_rect(fill = 'midnightblue', colour = \"yellow\"))\n\n# Animate\nanimate(p, nframe = 20, fps = 20)"
  },
  {
    "objectID": "posts/004_02Feb_2024/index.html",
    "href": "posts/004_02Feb_2024/index.html",
    "title": "Put your ggplot on steroids",
    "section": "",
    "text": "Welcome back to Stats Tips for 2024 - hope you managed a nice break.\nIt‚Äôs a short one today. If you didn‚Äôt already now it existed, check out plotly for taking your ggplots to the next level.\nSometimes it can be extremely helpful to quickly link discrete elements of a plot to the corresponding observation/s in your dataframe. For example, you have a suspected outlier in a scatterplot and you want to know which individual that belongs to. Or, you have an unavoidably busy plot; for example, plotting the predictions from a mixed model for longitudinal data overlaid on the observed data for comparison. In these cases it‚Äôs nearly impossible to discern the origin of the plotted data. In both use-case scenarios (and many more), plotly can help.\nIn this example of the latter use-case, we are going to use data from a built-in dataset in the lme4 package. The sleepstudy data looks at reaction times over time in sleep-deprived individuals. For the sake of the exercise we will fit a mixed model with reaction time (ms) as the outcome, time (days) as a fixed-effect and time (days) and individual as random-effects. So this is a random slopes model allowing the ‚Äòeffect‚Äô of sleep-deprivation on reaction time to vary over time for each individual. We fit the model and view a few lines of the dataframe which now contains the fixed (mod_pred_fix) and random (mod_pred_ran) predictions.\n\n\nCode\nlibrary(lme4)\nlibrary(ggplot2)\nlibrary(plotly)\n# Load data\ndata(\"sleepstudy\")\n# Model\nmod &lt;- lmer(Reaction ~ Days + (Days | Subject), data = sleepstudy)\n# Predict\nsleepstudy$mod_pred_fix &lt;- predict(mod, re.form = NA) # predict fixed effects\nsleepstudy$mod_pred_ran &lt;- predict(mod) # predict random effects\n# View data\nhead(sleepstudy, 10)\n\n\n\n\n\n\nReaction\nDays\nSubject\nmod_pred_fix\nmod_pred_ran\n\n\n\n\n249.5600\n0\n308\n251.4051\n253.6637\n\n\n258.7047\n1\n308\n261.8724\n273.3299\n\n\n250.8006\n2\n308\n272.3397\n292.9962\n\n\n321.4398\n3\n308\n282.8070\n312.6624\n\n\n356.8519\n4\n308\n293.2742\n332.3287\n\n\n414.6901\n5\n308\n303.7415\n351.9950\n\n\n382.2038\n6\n308\n314.2088\n371.6612\n\n\n290.1486\n7\n308\n324.6761\n391.3275\n\n\n430.5853\n8\n308\n335.1434\n410.9937\n\n\n466.3535\n9\n308\n345.6107\n430.6600\n\n\n\n\n\n\nWe can then plot the data interactively by simply ‚Äòwrapping‚Äô the ggplot object in a plotly call. If you hover over a data point you can easily identify which individual it belongs to as well as the observed reaction time. Similarly, by hovering over one of the random slopes you will see the predicted reaction time and the individual that corresponds to.\nYou won‚Äôt want to do this for every plot you make but it does provide a simple way to make some of your more complex visualisations using ggplot that bit more useful (and fun!) in helping to understand your data.\n\n\nCode\n# Plot\np &lt;- sleepstudy |&gt;\n    ggplot(aes(x = Days, y = Reaction, color = factor(Subject))) +\n    geom_line(aes(x = Days, y = mod_pred_ran)) +\n    geom_line(aes(x = Days, y = mod_pred_fix), linewidth = 2, color = \"blue\") +\n    geom_point(alpha = 0.5) +\n    xlab(\"Time (days)\") + ylab(\"Reaction Time (ms)\") +\n    guides(color = \"none\") +\n    theme_bw(base_size = 15)\nggplotly(p)"
  },
  {
    "objectID": "posts/005_16Feb_2024/index.html",
    "href": "posts/005_16Feb_2024/index.html",
    "title": "Immortal time bias - ‚ÄúThe fallacy that never dies‚Äù (Part 1)",
    "section": "",
    "text": "In 2001, a paper published in the Annals of Internal Medicine reported that Oscar winners had a longer life expectancy - by about 4 years - compared to their less successful peers. The authors conclusions were that:\n‚ÄúThe association of high status with increased longevity that prevails in the public also extends to celebrities, contributes to a large survival advantage, and is partially explained by factors related to success.‚Äù\nThe study received widespread attention in the media, with one future Oscar winner acknowledging the work in her acceptance speech.\nThe problem was that the reported survival advantage was illusory, and the reason for this was an invalid analysis that is not alone in the literature. As in any simple time-to-event analysis, two groups may be compared in their respective ‚Äòsurvival‚Äô times. In this study, subjects were first classified as winners or non-winners and observation time counted as their time alive. The error in this case was to consider winning status time-fixed (A), when in reality it is time-varying (B). By naively assuming it is time-fixed, we are erroneously attributing the time that a winner was in fact a loser prior to getting their gong, to their winning observation time.\n\nThis creates a distortion or bias in the exposure/treatment -&gt; outcome association, usually in a direction that overestimates the benefit of the exposure/treatment. When proper methods are then employed, the perceived benefits are reduced or sometimes reversed. And in fact that is exactly what was found when a re-analysis of the data was conducted in 2006 - the survival advantage was calculated to be closer to 1 year and not deemed statistically significant.\nIn general, the observation time prior to the exposure/treatment commencing (for those exposed/treated) is considered ‚Äòimmortal‚Äô, because the subject cannot experience the outcome during this period as they have yet to receive the exposure/treatment. If you are like me, this fairly classic description of immortal time hurts my brain and so I just like to simply think of it as the period that a person‚Äôs observation time has been misclassified.\nObservational research that involves time-to-event outcomes is particularly prone to immortal time bias and central to the problem is the specification of ‚Äòtime zero‚Äô - i.e.¬†when does the clock start? There are several examples of study design choices that can lead you down the wrong analysis path if you are not careful, and an especially pertinent one in this field is drawing contrasts between treated and untreated patients (hint: a patient is not ‚Äòtreated‚Äô for the duration of their observation time if they were only on treatment for the last 10% of that time).\nSo, what‚Äôs the solution?\nTime-varying covariates\nI will illustrate their use in an example in the next post."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Stats Tips - Welcome",
    "section": "",
    "text": "Hi Everyone,\nI‚Äôm not sure how much everyone‚Äôs getting out of the stats tips that I put on WhatsApp on Fridays, but maybe some of it is helpful. For those of you who are interested, I thought that if I was going to do this on a semi-regular basis, I might as well turn it into a resource. So I am having a go at a blog-style format for posting these tips. It also means I can more easily illustrate concepts where needed with code, etc. And it also helps to not overload your WhatsApp with a bunch of text. Some posts will be short and some will be longer and I may not be able to put something up every week, depending on workload, but will do my best. I hope it‚Äôs something people find useful.\nSome general housekeeping:\n\nYou can view and copy the code as blocks just before each set of output (there will be a Copy to Clipboard button at the top right of each code block); or by clicking the &lt;/&gt; Code button at the top right of the page, then View Source and copying the entire block.\nThere is a light/dark mode toggle on the top right of the page, depending on how you like to view your internet.\nFeel free to add any comments/questions to a post and/or provide general feedback."
  },
  {
    "objectID": "posts/002_01Dec_2023/index.html",
    "href": "posts/002_01Dec_2023/index.html",
    "title": "It pays to think like a Bayesian",
    "section": "",
    "text": "Recall that the question this week was to choose between:\nA) Switch to another door.\nB) Stay with your original door.\nC) It doesn‚Äôt matter if you switch or stay."
  },
  {
    "objectID": "posts/002_01Dec_2023/index.html#the-question",
    "href": "posts/002_01Dec_2023/index.html#the-question",
    "title": "It pays to think like a Bayesian",
    "section": "",
    "text": "Recall that the question this week was to choose between:\nA) Switch to another door.\nB) Stay with your original door.\nC) It doesn‚Äôt matter if you switch or stay."
  },
  {
    "objectID": "posts/002_01Dec_2023/index.html#the-answer",
    "href": "posts/002_01Dec_2023/index.html#the-answer",
    "title": "It pays to think like a Bayesian",
    "section": "The Answer",
    "text": "The Answer\nThe answer is that you should switch doors, and in fact if you do switch, you double your chances of winning the car - from 33.3% to 66.7%.\nThis is known as the Monty Hall problem and when it was first posed in a magazine column in 1975 managed to confuse readers to the extent that even mathematicians were writing in to the magazine to claim that answer was in fact wrong and staying with the originally chosen door was the better strategy for success.\nThe simplest way that I can explain this is that you start out with a 33.3% chance of winning the car and those probabilities don‚Äôt change once you lock in your selection and Monty offers you another chance to choose (i.e.¬†the probabilities don‚Äôt change to 50/50 once Monty reveals what‚Äôs behind one of the doors).\n\nIf you stay\nIf you choose the correct door to start with (for which there is a 33.3% chance), staying will result in you ending up with the car (winning).\nIf you choose the incorrect door to start with (for which there is a 66.7% chance), staying will necessarily result in you ending up with a goat (losing).\n\n\nIf you switch\nIf you choose the correct door to start with (for which there is a 33.3% chance), switching will result in you ending up with a goat (losing).\nIf you choose the incorrect door to start with (for which there is a 66.7% chance), switching will necessarily result in you ending up with the car, because Monty has to pick the only other losing door to open (winning).\nStaying is associated with a 33.3% success rate, whereas switching doubles your chance of success to 66.7%.\nStop here if equations give you the equivalent of the aftermath of eating Mexican food. What I have done below is show how we can arrive at the same answer using an analytical approach when our logic/intuition fails. You may not want to venture that far‚Ä¶\nWhat I think is cool about this problem is that while the result might seem counterintuitive to how we naturally process chance, using Bayesian reasoning provides a formulaic way to get at the right answer. This again uses conditional probabilities as I introduced them a few weeks ago. Bayesian thinking is about utilising prior knowledge in conjunction with new data to improve or update our knowledge (whereas the Frequentist approach to statistics doesn‚Äôt care so much about prior knowledge and instead just uses the data at hand).\n\n\n\n\n\n\nImportant Concept\n\n\n\nBayesian reasoning enables the analysis of data under the light of prior knowledge.\n\n\nBayes Theorem can be written as:\n\\[\nPr(\\theta | data) = \\frac{Pr(data | \\theta) Pr(\\theta)}{Pr(data)}\n\\]\nwhere \\(\\theta\\) could be a particular parameter or hypothesis.\nHere:\n\\(Pr(data | \\theta)\\) is the likelihood function (the data, or what we measure)\n\\(Pr(\\theta)\\) is the prior probability of our hypothesis (prior knowledge before we the measurement)\n\\(Pr(data)\\) is the prior probability of the data\n\\(Pr(\\theta | data)\\) is the posterior probability of our hypothesis (i.e.¬†‚Äúin light of the data‚Äù)\nOn the Bayesian/Frequentist topic, note that \\(Pr(data | \\theta)\\) is what null-hypothesis significance testing (NHST) encapsulates and this is a Frequentist concept. Whenever we calculate a p value we are asking:\n\n‚ÄúWhat is the probability of this new data (or data even more extreme) occurring by chance given the null hypothesis is true?‚Äù\n\nBut really, what we want to know most of the time is the opposite:\n\n‚ÄúWhat is the probability of the null hypothesis being true given this new data?‚Äù\n\nThat is a Bayesian concept and is answered with \\(Pr(\\theta | data)\\). Maybe we should become more Bayesian in how we handle our research‚Ä¶\nAnyway, excuse the digression. We can generalise Bayes Theorem to the Monty Hall problem as:\n\\[\nPr(\\text{car behind door x} | \\text{Monty opens door y}) = \\frac{Pr(\\text{Monty opens door y} | \\text{car behind door x}) Pr(\\text{car behind door x})}{Pr(\\text{Monty opens door y})}\n\\]\nFor the sake of the exercise, let x = door 1 and y = door 3.\nSo we are interested in the probability the car is behind door 1 (that means we picked door 1) when Monty opens door 3 to reveal a goat.\nIn calculating the different components of Bayes Theorem, we first need to enumerate the various probabilities.\n\\(Pr(\\text{car behind door 1}) = Pr(\\text{car behind door 2}) = Pr(\\text{car behind door 3}) = 33.3\\%\\)\nThese are the prior probabilities.\nThen:\n\\(Pr(\\text{Monty opens door 3} | \\text{car behind door 1}) = 50\\%\\)\nMonty can only pick doors 2 or 3, as we picked door 1.\n\\(Pr(\\text{Monty opens door 3} | \\text{car behind door 2}) = 100\\%\\)\nMonty can only pick door 3, as we picked door 1 and he doesn‚Äôt want to reveal the car behind door 2.\n\\(Pr(\\text{Monty opens door 3} | \\text{car behind door 3}) = 0\\%\\)\nMonty won‚Äôt reveal the car as part of his playing rules.\nThese are the likelihoods or the data.\nThe \\(Pr(\\text{Monty opens door 3})\\) is a little trickier to calculate. Here we don‚Äôt need to worry about the car being behind any specific door, only that Monty won‚Äôt reveal it. Intuitively, this would be \\(50\\%\\) as he only has two doors to choose from. But you can also work this out by summing the product of each of the prior probabilities and the evidence:\n\\(Pr(\\text{Monty opens door 3}) = (0.33 * 0.5) + (0.33 * 1) + (0.33 * 0) = 0.5\\)\nFinally, we can get to working out the posterior probabilities of the car being behind each door given Monty opens door 3. We use Bayes Theorem as shown above to do this.\n\\[\nPr(\\text{car behind door 1} | \\text{Monty opens door 3}) = \\frac{Pr(\\text{Monty opens door 3} | \\text{car behind door 1}) Pr(\\text{car behind door 1})}{Pr(\\text{Monty opens door 3})}\n\\] \\[\n= \\frac{0.5 * 0.33}{0.5} = 33.3\\%\n\\] Likewise:\n\\[\nPr(\\text{car behind door 2} | \\text{Monty opens door 3})  = \\frac{1 * 0.33}{0.5} = 66.7\\%\n\\] and\n\\[\nPr(\\text{car behind door 3} | \\text{Monty opens door 3})  = \\frac{0 * 0.33}{0.5} = 0\\%\n\\]\nRemember, we initially chose door 1. So, when Monty opens door 3 (this could have been door 2 - we just needed to pick a door for the exercise), we double our chances of winning by switching to door 2. And this is how Bayesian reasoning can come to the rescue when our own intuition fails.\nFurther explanation can be found here if you remain unconvinced:\nhttps://en.wikipedia.org/wiki/Monty_Hall_problem\nhttps://statisticsbyjim.com/fun/monty-hall-problem/"
  },
  {
    "objectID": "posts/006_23Feb_2024/index.html",
    "href": "posts/006_23Feb_2024/index.html",
    "title": "Immortal time bias - ‚ÄúThe fallacy that never dies‚Äù (Part 2)",
    "section": "",
    "text": "In the last post I introduced the concept of immortal time bias and how it can distort associations in your survival analysis, if you naively misclassify unexposed/untreated observation time as exposed/treated. This week I am going to illustrate the concept with some data and R code. It would have been good to analyse the Oscar Winner‚Äôs data but as I could not locate that anywhere online, we are instead going to look at one of the first studies in which immortal time bias was subsequently recognised to be a problem.\nThe work came out of Stanford University in the early 1970s and assessed the survival benefit of potential heart transplant recipients. In the analysis, the event of interest was death and the primary treatment was heart transplantation - so survival amongst transplant recipients was compared to that amongst accepted patients into the program that did not end up receiving a transplant. Treatment was initially considered time-fixed and the patients divided into two groups - ‚Äòever transplanted‚Äô vs ‚Äònever transplanted‚Äô. Survival time amongst recipients was found to be longer than those who didn‚Äôt receive transplantation.\nThe immortal time bias here involves the waiting time of those patients who survived to make it to the transplant. Because this portion of the observation time was classified as exposed to transplantation instead of unexposed, it offered a guaranteed survival time to the transplanted group. The result of this misclassification was to produce an artificial increase in the mortality rate of the reference group, thus suggesting a benefit of heart transplant surgery. In a later reanalysis of the data, the apparent survival benefit of the transplanted group disappeared when the immortal time was properly accounted for by a time-dependent analysis."
  },
  {
    "objectID": "posts/006_23Feb_2024/index.html#load-data",
    "href": "posts/006_23Feb_2024/index.html#load-data",
    "title": "Immortal time bias - ‚ÄúThe fallacy that never dies‚Äù (Part 2)",
    "section": "1 Load data",
    "text": "1 Load data\nAs this study is considered a canonical example of immortal time bias, the data comes built into R‚Äôs survival package. We can load the data and inspect the relevant jasa dataframe as below.\n\n\nCode\nlibrary(survival)\nlibrary(survminer)\nlibrary(gtsummary)\nlibrary(dplyr)\ndata(heart, package = \"survival\")\nhead(jasa)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbirth.dt\naccept.dt\ntx.date\nfu.date\nfustat\nsurgery\nage\nfutime\nwait.time\ntransplant\nmismatch\nhla.a2\nmscore\nreject\n\n\n\n\n1937-01-10\n1967-11-15\nNA\n1968-01-03\n1\n0\n30.84463\n49\nNA\n0\nNA\nNA\nNA\nNA\n\n\n1916-03-02\n1968-01-02\nNA\n1968-01-07\n1\n0\n51.83573\n5\nNA\n0\nNA\nNA\nNA\nNA\n\n\n1913-09-19\n1968-01-06\n1968-01-06\n1968-01-21\n1\n0\n54.29706\n15\n0\n1\n2\n0\n1.11\n0\n\n\n1927-12-23\n1968-03-28\n1968-05-02\n1968-05-05\n1\n0\n40.26283\n38\n35\n1\n3\n0\n1.66\n0\n\n\n1947-07-28\n1968-05-10\nNA\n1968-05-27\n1\n0\n20.78576\n17\nNA\n0\nNA\nNA\nNA\nNA\n\n\n1913-11-08\n1968-06-13\nNA\n1968-06-15\n1\n0\n54.59548\n2\nNA\n0\nNA\nNA\nNA\nNA\n\n\n\n\n\n\nThe variables that we‚Äôre going to use are:\n\nfustat - the ‚Äòevent‚Äô variable; 0 = alive, 1 = dead at the end of follow-up.\nfutime - the primary ‚Äòtime‚Äô variable; time (days) from acceptance into the transplant program until death or censoring.\nwait.time - the secondary ‚Äòtime‚Äô variable; time (days) from acceptance into the transplant program until receiving a heart if transplanted (NA for those who never underwent transplant surgery).\ntransplant - the ‚Äòtreatment/exposure‚Äô variable; 0 = did not receive heart, 1 = received heart."
  },
  {
    "objectID": "posts/006_23Feb_2024/index.html#visualise-individual-survival-trajectories",
    "href": "posts/006_23Feb_2024/index.html#visualise-individual-survival-trajectories",
    "title": "Immortal time bias - ‚ÄúThe fallacy that never dies‚Äù (Part 2)",
    "section": "2 Visualise individual survival trajectories",
    "text": "2 Visualise individual survival trajectories\nUsing a bit of ggplot2 magic, we can now plot the individual observation times for the 103 patients in the study. Note that I have stratified observation time by transplant status (orange for the period a patient remains untransplanted and blue for the period following a transplant).\n\n\nCode\n# Create 'id' variable\njasa$id &lt;- seq(1:dim(jasa)[1])\n# Replace wait.time with futime if didn't undergo transplant\njasa$wait.time[is.na(jasa$wait.time)] &lt;- jasa$futime[is.na(jasa$wait.time)]\n# Plot\njasa |&gt;\n  ggplot(aes(x = id, y = futime)) +\n  geom_linerange(aes(ymin = 0, ymax = wait.time), color = \"#E7B800\", linewidth = 1) +\n  geom_linerange(aes(ymin = wait.time, ymax = futime), color = \"#2E9FDF\", linewidth = 1) +\n  geom_point(aes(shape = factor(fustat)), stroke = 1, cex = 1, color = \"black\") +\n  scale_shape_manual(values = c(1, 3), labels = c(\"Censored\", \"Died\"), name = \"Outcome\") +\n  annotate(\"text\", x = 95, y = 1400, label = \"Observation time = yellow - untransplanted\", size = 5, color = \"#E7B800\") +\n  annotate(\"text\", x = 92, y = 1380, label = \"Observation time = blue - post-transplant\", size = 5, color = \"#2E9FDF\") +\n  ggtitle(\"Survival Trajectories for Heart Transplant Patients\") +   \n  ylab(\"Time (days)\") +\n  xlab(\"Patient Number\") + \n  coord_flip() + \n  theme_bw(base_size = 20) +\n  theme(axis.text.y = element_text(size = 15))"
  },
  {
    "objectID": "posts/006_23Feb_2024/index.html#naive-analysis-assuming-treatment-status-is-time-fixed",
    "href": "posts/006_23Feb_2024/index.html#naive-analysis-assuming-treatment-status-is-time-fixed",
    "title": "Immortal time bias - ‚ÄúThe fallacy that never dies‚Äù (Part 2)",
    "section": "3 Naive analysis assuming treatment status is time-fixed",
    "text": "3 Naive analysis assuming treatment status is time-fixed\n\n3.1 Visualise survival curves\nPlotting the Kaplan-Meier survival curves are easy by first saving the survfit object:\nfit &lt;- survfit(Surv(futime, fustat) ~ transplant, data = jasa)\nand then passing this ggsurvplot which does a nicer job of plotting survival data then using R‚Äôs base functions. Note that we ignore wait.time and only specify futime in our fit function. This is because we are assuming if a patient was transplanted, the entire duration of their observation period was considered as such.\n\n\nCode\nfit_naive &lt;- survfit(Surv(futime, fustat) ~ transplant, data = jasa)\nggsurvplot(fit_naive,\n          risk.table = TRUE,\n          risk.table.col = \"strata\",\n          linetype = \"strata\",\n          surv.median.line = \"hv\",\n          ggtheme = theme_bw(base_size = 20),\n          palette = c(\"#E7B800\", \"#2E9FDF\"))\n\n\n\n\n\n\n\n\n\n\n\n3.2 Cox model\nFitting a Cox model is also simple with:\nmod_naive &lt;- coxph(Surv(futime, fustat) ~ transplant, data = jasa)\n\n\nCode\nmod_naive &lt;- coxph(Surv(futime, fustat) ~ transplant, data = jasa)\ntbl_regression(mod_naive, exp = T)\n\n\n\n\n\n\n\n\nCharacteristic\nHR1\n95% CI1\np-value\n\n\n\n\ntransplant\n0.27\n0.17, 0.43\n&lt;0.001\n\n\n\n1 HR = Hazard Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nThis gives a HR = 0.27 (95% CI 0.17, 0.43; p &lt; 0.001) indicating that there is about a 73% reduction in the risk of death with transplantation. Pretty effective, right?"
  },
  {
    "objectID": "posts/006_23Feb_2024/index.html#correct-analysis-assuming-treatment-status-is-time-varying",
    "href": "posts/006_23Feb_2024/index.html#correct-analysis-assuming-treatment-status-is-time-varying",
    "title": "Immortal time bias - ‚ÄúThe fallacy that never dies‚Äù (Part 2)",
    "section": "4 Correct analysis assuming treatment status is time-varying",
    "text": "4 Correct analysis assuming treatment status is time-varying\nUp until now we have just used the data as it‚Äôs been presented to us. Each patient has a single observation with all information about them contained in that row of data. However, to perform the correct time-dependent analysis we first need to construct a time-varying version of the treatment (i.e.¬†transplant) variable. This data format is known as ‚Äòcounting process‚Äô and in the general case involves creating potentially multiple rows of data for each patient with each row corresponding to a different exposure/treatment period of that patients observation time. In this specific example, we will create an additional row of data for transplanted patients splitting time at the point of transplant, so that the first row contains the time from acceptance into the transplant program to the point of transplant, and the second row contains the time from transplant to either death or censoring. We specify this in ‚Äòstart, stop‚Äô format rather than the duration of the interval itself. We will use the tmerge function to do this, although a little bit of manual programming can also achieve the same result.\n\n\nCode\n# Create subset of data selecting relevant variables\njasa_subset &lt;- jasa |&gt; \n  select(id, wait.time, futime, fustat, transplant)\n# Can't have an end time of 0 (one obs) - change this to 0.5\njasa_subset$futime[jasa_subset$futime == 0] &lt;- 0.5\n# Create dataframe in counting process format\njasa_cp &lt;- tmerge(data1 = jasa_subset |&gt; select(id, futime, fustat), \n                  data2 = jasa_subset |&gt; select(id, futime, fustat, wait.time, transplant), \n                  id = id, \n                  death = event(futime, fustat),\n                  transplant = tdc(wait.time)) |&gt; \n            select(-c(futime, fustat))\n\n\nRemember that the original data looked like:\n\n\nCode\nhead(jasa_subset, 7)\n\n\n\n\n\n\nid\nwait.time\nfutime\nfustat\ntransplant\n\n\n\n\n1\n49\n49\n1\n0\n\n\n2\n5\n5\n1\n0\n\n\n3\n0\n15\n1\n1\n\n\n4\n35\n38\n1\n1\n\n\n5\n17\n17\n1\n0\n\n\n6\n2\n2\n1\n0\n\n\n7\n50\n674\n1\n1\n\n\n\n\n\n\nAnd the newly created dataframe in counting process format:\n\n\nCode\nhead(jasa_cp, 9)\n\n\n\n\n\n\nid\ntstart\ntstop\ndeath\ntransplant\n\n\n\n\n1\n0\n49\n1\n0\n\n\n2\n0\n5\n1\n0\n\n\n3\n0\n15\n1\n1\n\n\n4\n0\n35\n0\n0\n\n\n4\n35\n38\n1\n1\n\n\n5\n0\n17\n1\n0\n\n\n6\n0\n2\n1\n0\n\n\n7\n0\n50\n0\n0\n\n\n7\n50\n674\n1\n1\n\n\n\n\n\n\nNote the new ‚Äòstart, stop‚Äô time variables. We have also renamed fustat to death for a more intuitive name. In this small data subset, Subject‚Äôs 3, 4 and 7 underwent a transplant, but only the latter two had both unexposed and exposed time periods during observation (Subject 3 was transplanted at the beginning of their observation), hence each subject now has two rows of data.\n\n4.1 Visualise survival curves\nPlotting the Kaplan-Meier survival curves for the data in this correct format reveals a vastly different result to that which we viewed earlier. There is now almost no separation in the curves.\n\n\nCode\nfit_correct &lt;- survfit(Surv(tstart, tstop, death) ~ transplant, data = jasa_cp)\nggsurvplot(fit_correct,\n          risk.table = TRUE,\n          risk.table.col = \"strata\",\n          linetype = \"strata\",\n          surv.median.line = \"hv\",\n          ggtheme = theme_bw(base_size = 20),\n          palette = c(\"#E7B800\", \"#2E9FDF\"))\n\n\n\n\n\n\n\n\n\n\n\n4.2 Cox model\nCommensurately, the output of the Cox model now gives a HR = 1.13 (95% CI 0.63, 2.04; p = 0.7) indicating that there is about a 13% increase in the risk of death with transplantation - but this could be as much as a 104% increase or even a 37% decrease. That is, we can‚Äôt be confident the observed effect didn‚Äôt occur just by chance. Clearly, this tells a different story to the naive analysis we previously conducted.\n\n\nCode\nmod_correct &lt;- coxph(Surv(tstart, tstop, death) ~ transplant, data = jasa_cp)\ntbl_regression(mod_correct, exp = T)\n\n\n\n\n\n\n\n\nCharacteristic\nHR1\n95% CI1\np-value\n\n\n\n\ntransplant\n1.13\n0.63, 2.04\n0.7\n\n\n\n1 HR = Hazard Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nThe lesson here is to always think about whether your exposure or treatment changes over the course of an individual‚Äôs observation time, and if it does, to account for that in your survival model by constructing a time-varying covariate."
  },
  {
    "objectID": "posts/009_05Apr_2024/index.html#categorising-the-predictor",
    "href": "posts/009_05Apr_2024/index.html#categorising-the-predictor",
    "title": "Don‚Äôt be Scared of Splines",
    "section": "3.1 Categorising the Predictor",
    "text": "3.1 Categorising the Predictor\nI‚Äôm not even really going to talk about this - it‚Äôs very rarely a good thing to categorise a continuous variable. The loss of information and power and introduction of spurious threshold effects (e.g., by grouping 20- to 29-year-olds in one category and 30- to 39-year-olds in another, we create the impression that 20- and 29-year-olds tend to be more alike than 29- and 30-year-olds) are just some of the reasons why this is a bad idea."
  },
  {
    "objectID": "posts/009_05Apr_2024/index.html#lowess-smoother",
    "href": "posts/009_05Apr_2024/index.html#lowess-smoother",
    "title": "Don‚Äôt be Scared of Splines",
    "section": "2.1 Lowess Smoother",
    "text": "2.1 Lowess Smoother\nI mentioned using a lowess smoother above to help visualise any potential association in your data. Lowess regression is a type of non-parametric regression method that fits a smooth curve to your data by calculating a weighted average of Y across a moving span (or window) of X. It is a great initial exploratory method for looking at your data. If we fit a lowess curve to these data, we can see the following:\n\n\nCode\n# Plot lowess\nggplot(dat, aes(x = x, y = y)) +\n  geom_point(size = 3) +\n  geom_smooth(method = \"loess\", se = F, linewidth = 2, color = \"#1F77B4FF\") + \n  xlab(\"x\") + ylab(\"y\") + \n  theme_bw(base_size = 20)"
  },
  {
    "objectID": "posts/009_05Apr_2024/index.html#assuming-linearity",
    "href": "posts/009_05Apr_2024/index.html#assuming-linearity",
    "title": "Don‚Äôt be Scared of Splines",
    "section": "3.2 Assuming Linearity",
    "text": "3.2 Assuming Linearity\nIf you bothered to plot the data you would know the association between X and Y was non-linear. But plenty of people don‚Äôt bother to do this and just go ahead and fit a model under the assumption of linearity. If we erroneously did this, the best-fitting regression line would appear as in the following plot. Clearly, for these data, this is a bad modelling choice leading one to think there is no association at all between X and Y.\n\n\nCode\n# Model\nmod1 &lt;- lm(y ~ x, data = dat)\n# Predict Y from model\ndat$mod1_pred &lt;- predict(mod1, dat)\n# Plot linear\nggplot(dat, aes(x = x, y = mod1_pred)) +\n  geom_line(linewidth = 2, color = \"deeppink\") + \n  geom_point(data = dat, aes(x = x, y = y), size = 3) +\n  xlab(\"x\") + ylab(\"y\") + \n  theme_bw(base_size = 20)"
  },
  {
    "objectID": "posts/009_05Apr_2024/index.html#assuming-non-linearity---segmented-regression",
    "href": "posts/009_05Apr_2024/index.html#assuming-non-linearity---segmented-regression",
    "title": "Don‚Äôt be Scared of Splines",
    "section": "3.3 Assuming Non-Linearity - Segmented Regression",
    "text": "3.3 Assuming Non-Linearity - Segmented Regression\nLet‚Äôs now look at a piecewise or linear spline model. Another name for this is segmented regression - a method in which the predictor is partitioned into intervals and a separate line segment is fit to each interval. Essentially, we are fitting multiple, linked linear regression models. To do this we need to first decide where sensible threshold/s exist in the data for us to partition the predictor, allowing approximate linearity within those partitions. In this case, if we look back at the lowess plot, the vertex of the curve represents a reasonable threshold and so we might decide to use a predictor cut-point at X = 50.\nI won‚Äôt go into the details of the parameterisation (it‚Äôs there in the code), but if we fit such a model and then make model predictions from that, we get the following plot. Clearly this is a much better representation of the actual trend in the data, compared to assuming a linear relationship.\n\n\nCode\n# Create a new variable corresponding to change in slope (using 50 as threshold)\ndat &lt;- dat |&gt; \n  mutate(x50 = (x - 50) * (x &gt;= 50))  # will be 0 if x &lt; 50\n# Model\nmod2 &lt;- lm(y ~ x + x50, data = dat)\n# Predict Y from model\ndat$mod2_pred &lt;- predict(mod2, dat)\n# Plot piecewise\nggplot(dat, aes(x = x, y = mod2_pred)) +\n  geom_line(linewidth = 2, color = \"chartreuse\") + \n  geom_point(data = dat, aes(x = x, y = y), size = 3) +\n  xlab(\"x\") + ylab(\"y\") + \n  theme_bw(base_size = 20)"
  },
  {
    "objectID": "posts/009_05Apr_2024/index.html#assuming-non-linearity---polynomial-regression",
    "href": "posts/009_05Apr_2024/index.html#assuming-non-linearity---polynomial-regression",
    "title": "Don‚Äôt be Scared of Splines",
    "section": "3.4 Assuming Non-Linearity - Polynomial Regression",
    "text": "3.4 Assuming Non-Linearity - Polynomial Regression\nPolynomial regression takes this idea further by allowing smoothness to be incorporated into the modelling of the non-linearity. It is a form of regression analysis in which the association between X and Y is modelled as an nth degree polynomial in X. It is important to keep in mind that while the model fits a non-linear curve to the data, the statistical estimation of the model is still considered linear (in the unknown parameters). This differentiates this and models with RCS splines from true non-linear models. See here for some further explanation.\nSo now let‚Äôs fit a quadratic model to these data. It doesn‚Äôt look like too bad a fit either, does it.\n\n\nCode\n# Model - quadratic\nmod3 &lt;- lm(y ~ x + I(x^2), data = dat)\n# Predict Y from models\ndat$mod3_pred &lt;- predict(mod3, dat)\n# Plot piecewise\nggplot(dat, aes(x = x, y = mod3_pred)) +\n  geom_line(linewidth = 2, color = \"chocolate1\") + \n  geom_point(data = dat, aes(x = x, y = y), size = 3) +\n  xlab(\"x\") + ylab(\"y\") + \n  theme_bw(base_size = 20)"
  },
  {
    "objectID": "posts/009_05Apr_2024/index.html#assuming-non-linearity---rcss",
    "href": "posts/009_05Apr_2024/index.html#assuming-non-linearity---rcss",
    "title": "Don‚Äôt be Scared of Splines",
    "section": "3.5 Assuming Non-Linearity - RCS‚Äôs",
    "text": "3.5 Assuming Non-Linearity - RCS‚Äôs\n\n3.5.1 The Basics of RCS‚Äôs\nI‚Äôd like to make an important first point in that RCS‚Äôs can be applied in any statistical model that linearly relates a predictor to an outcome. We have been emphasising continuous (predictor) vs continuous (outcome) associations because these are the simplest to conceptualise. But the same applies to any generalised linear or survival model.\n\n\n\n\n\n\nNote\n\n\n\nGeneralised linear models use link functions to linearise a predictor on the link scale and it is that scale that we are interested in knowing whether the predictor has an approximately linear relationship with the outcome to ensure model assumptions are met. E.g. for binary outcomes we want to know if the association of the continuous predictor and the logit (log-odds) of the outcome is linear or not. We don‚Äôt so much care about the association of the continuous predictor with the odds or the probability of the outcome because we know these to be non-linear. I will hopefully elaborate on this idea in a future post.\n\n\nThe intuition behind RCS‚Äôs is that the continuous predictor is broken into multiple intervals at locations called knots and for each interval the association between the predictor and the outcome is estimated separately. The association within each interval can be estimated with increasing complexity - from the linear splines that we have already explored in segmented regression, to cubic splines (polynomials) as we describe them here. I won‚Äôt go into detail of the underlying maths because it does get complicated, but a series of spline basis functions are used to ‚Äòbuild‚Äô the resulting cubic spline within each interval (please see the papers below for more detail.) The cubic splines within each interval are restricted in a couple of ways: firstly, adjacent splines join smoothly at knot locations because their slopes are constrained to be equal at these boundaries, and secondly, the spline functions are constrained to be linear in the tails (i.e., before the Ô¨Årst and after the last knot)\n\n\n3.5.2 Fitting RCS‚Äôs to the Current Data\nThere are multiple packages in R that allow you to fit RCS‚Äôs to your data but my go to is the ns() function in the splines package. I would encourage you to look at the papers listed below if you want to explore alternatives. Using ns(), the way to specify a RCS term on the relevant predictor in your model is really quite simple. The main argument that you need to specify is the degrees of freedom (df) which is equivalent to the number of different intervals that you want to model in your predictor-outcome relationship. This also corresponds to the number of RCS coefficients in your model output. However, as alluded to above, this DOES NOT represent the number of internal knots that the model uses under the hood to achieve this - which is always one less. So the take home here is that if you want to model 4 intervals of your predictor for which you feel the association differs with your outcome, you specify df = 4 which signals to ns() that it needs to define 3 internal knots.\nNow the placement of the knots can also be specified, but to be honest I‚Äôve never felt the need to do this. For the most part things seem to work fine with ns() default placement of knots at quantiles of the distribution of the predictor. For example, if you specified df = 4, then 3 internal knots would be placed at the 25th, 50th and 75th percentiles of the distribution of X.\nSo the basic form of a linear model with an ns() term included is then:\nlm(y ~ ns(x, df = 4), data = dat)\nand if you wish to check at what actual values of your predictor ns() has placed the knots, you can use:\nattr(ns(x, df = 4), \"knots\")\nFor an interesting comparison, we are now going to fit 4 models with RCS‚Äôs:\n\n2 knots (df = 3)\n3 knots (df = 4)\n4 knots (df = 5)\n20 knots (df = 21)\n\nThe resultant model predictions are shown below.\n\n\nCode\n# Model - rcs with 2 knots\nmod4a &lt;- lm(y ~ ns(x, df = 3), data = dat)\n# Model - rcs with 3 knots\nmod4b &lt;- lm(y ~ ns(x, 4), data = dat)\n# Model - rcs with 4 knots\nmod4c &lt;- lm(y ~ ns(x, 5), data = dat)\n# Model - rcs with 20 knots\nmod4d &lt;- lm(y ~ ns(x, 21), data = dat)\n# Predict Y from models\ndat$mod4a_pred &lt;- predict(mod4a, dat)\ndat$mod4b_pred &lt;- predict(mod4b, dat)\ndat$mod4c_pred &lt;- predict(mod4c, dat)\ndat$mod4d_pred &lt;- predict(mod4d, dat)\n# Convert to long format for easier plotting\ndat_long &lt;- dat |&gt; \n  select(1:2, 7:10) |&gt; \n  pivot_longer(3:6)\n# Plot rcs \nggplot() +\n  geom_line(data = dat_long, aes(x = x, y = value, color = name), linewidth = 2) + \n  geom_point(data = dat, aes(x = x, y = y), size = 3) +\n  scale_color_paletteer_d(\"ggsci::category20_d3\", name = \"RCS - # of knots\", labels = c(\"2\", \"3\", \"4\", \"20\")) +\n  xlab(\"x\") + ylab(\"y\") + \n  theme_bw(base_size = 20) +\n  theme(legend.position = c(1,1), legend.justification = c(2.8,1.1))\n\n\n\n\n\n\n\n\n\nWhat can we gather from this. Well there is no doubt some subjectivity to the interpretation of these plots, but my take is that the RCS with 2 internal knots is actually very similar to the quadratic (polynomial) model - these are both quite ‚Äòsmoothed‚Äô. We then start to see a little more flexibility in the model fit for the models with 3 and 4 internal knots - and really I‚Äôd be hard-pushed to say they‚Äôre that different. We can unequivocally say, though, that the model with 20 internal knots looks like it‚Äôs picking up a lot of noise in the data - this is a classic case of model over-fitting and we want to avoid this as much as possible. The main issue with overfit models is that they appear to work very well with the data that they were fit to - the predictions are excellent! But the catch is that the model has been fit to the idiosyncrasies of that specific dataset and consequently doesn‚Äôt generalise well to any other dataset that you might want to test your model on - for these new data the predictions are now terrible! We should always keep this in mind when we are formulating models to fit to our data, irrespective of whether we are using RCS‚Äôs or not.\nWhen we are using RCS‚Äôs though, for most applications, three to five internal knots strike a nice balance between complicating the model needlessly and fitting data pleasingly. For these data, it would not be unreasonable to suggest that the models with 3 or 4 internal knots capture the fit nicely (one could argue that the 2-knot model is a little underfit, and of course the 20 knot model is grossly overfit). So, from purely eyeballing the plots, I would tend to settle on the 3-knot model.\n\n\n3.5.3 Model Comparisons\nWe can add some statistical rigour to this intuition by calculating model-fit statistics, and I have done this using the AIC for all the models we have considered in this post. When we use the AIC to help decide on model fit we are looking for (relatively) lower (i.e.¬†more negative numbers). When we calculate these we can see some interesting results. The model assuming linearity is comparatively a terrible fit (AIC = 227). The piecewise model isn‚Äôt too bad though (AIC = -23) if we are looking to the RCS models as a gold standard. The quadratic model has a comparatively poorer fit (AIC = 31) and this is actually fairly similar to the 2-knot RCS model (AIC = 11 - remember we said they looked similar). The 3-knot and greater RCS models seem to perform the best, but this is really a case of diminishing returns. We can fairly justify either a 3- (AIC = -34) or 4-knot (AIC = -38) model and I don‚Äôt think any reasonable reviewer would criticise you for either choice.\n\n\nCode\n# AIC for each model\nmods_aic &lt;- data.frame(AIC(mod1, mod2, mod3, mod4a, mod4b, mod4c, mod4d))\nmods_aic &lt;- tibble::rownames_to_column(mods_aic, var = \"Model\")\nmods_aic &lt;- mods_aic |&gt; \n  select(-df) |&gt; \n  mutate(Model = case_when(Model == \"mod1\" ~ \"Linear Regression\",\n                           Model == \"mod2\" ~ \"Segmented Regression\",\n                           Model == \"mod3\" ~ \"Polynomial Regression\",\n                           Model == \"mod4a\" ~ \"RCS - 2 knots\",\n                           Model == \"mod4b\" ~ \"RCS - 3 knots\",\n                           Model == \"mod4c\" ~ \"RCS - 4 knots\",\n                           Model == \"mod4d\" ~ \"RCS - 20 knots\"))\nmods_aic\n\n\n\n\n\n\nModel\nAIC\n\n\n\n\nLinear Regression\n226.55549\n\n\nSegmented Regression\n-22.98867\n\n\nPolynomial Regression\n30.77121\n\n\nRCS - 2 knots\n10.55193\n\n\nRCS - 3 knots\n-33.66342\n\n\nRCS - 4 knots\n-38.49374\n\n\nRCS - 20 knots\n-34.52662\n\n\n\n\n\n\n\n\n3.5.4 Interpretation and Presentation of your Results\nOk, so we‚Äôre nearly done. You‚Äôve done the hard work of recognising your predictor has a non-linear relationship with your outcome, assessed multiple approaches to modelling that non-linearity and settled on a RCS with 3 knots on the predictor as the best-fitting model. You excitedly run your model and get the following output:\n\n\nCode\n# Model - rcs with 3 knots\nmod4b &lt;- lm(y ~ ns(x, 4), data = dat)\n# Format model results in a table\nmod4b |&gt; gtsummary::tbl_regression()\n\n\n\n\n\n\nModel Output\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nns(x, 4)\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†ns(x, 4)1\n-2.8\n-3.0, -2.6\n&lt;0.001\n\n\n¬†¬†¬†¬†ns(x, 4)2\n-0.69\n-0.89, -0.50\n&lt;0.001\n\n\n¬†¬†¬†¬†ns(x, 4)3\n-1.2\n-1.6, -0.82\n&lt;0.001\n\n\n¬†¬†¬†¬†ns(x, 4)4\n0.53\n0.34, 0.71\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nWTH?! What does that all mean? Paul, what are you doing to me? I just want the simple output that I‚Äôm used to where I can say that a one-unit change in my predictor corresponds to some change in my outcome!\nI apologise for my facetiousness - no doubt you have cottoned on to the fact that you when you specifically model a non-linear association between two variables, there is no longer any constancy in the relationship between the two variables. A one-unit change in X will give a different change in Y depending on the values of X.\nSo, what to do? First recognise that the RCS coefficients presented to you in a regression output are essentially useless from an interpretation point of view. You can‚Äôt easily use these to describe the association between X and Y in reporting your results.\nThe general approach to interpretation and reporting of results in the presence of non-linearity in a regression model is to pick salient values (biological, clinical) of X to predict model-estimated values of Y. You can quite easily calculate model-estimated means and differences using our friend emmeans which I described to you in an earlier post.\nFor illustrative purposes, let‚Äôs say we‚Äôre interested in knowing the model-estimated values of Y corresponding to X values of 20, 30, 50 and 60. We can estimate these values and differences (contrasts) of interest using emmeans.\n\n\nCode\n# Plot rcs \nggplot() +\n  geom_line(data = dat, aes(x = x, y = mod4b_pred), color = \"#EE8635\", linewidth = 2) + \n  geom_point(data = dat, aes(x = x, y = y), size = 3) +\n  geom_vline(xintercept = c(20,30,50,60), color = \"red\", linetype = \"dotted\", linewidth = 1) +\n  scale_x_continuous(limits = c(0, 100), breaks = c(20, 30, 50, 60)) +\n  xlab(\"x\") + ylab(\"y\") + \n  theme_bw(base_size = 20)\n\n\n\n\n\n\n\n\n\nCode\n# emmeans\nemmeans(mod4b, ~ x, at = list(x = c(20,30,50,60))) |&gt; \n  data.frame() |&gt; \n  select(-df) |&gt; \n  rename(\"X\" = \"x\",\n         \"Emmean (Y)\" = \"emmean\",\n         \"95% C.I. (lower)\" = \"lower.CL\",\n         \"95% C.I. (upper)\" = \"upper.CL\") |&gt; \n  flextable() |&gt; \n  colformat_double(j = c(2:5), digits = 3, na_str = \"N/A\") |&gt;\n  set_table_properties(layout = \"autofit\") |&gt; \n  height(height = 1, unit = \"cm\") |&gt; \n  hrule(rule = \"atleast\", part = \"header\") |&gt; \n  align(align = \"center\", part = \"body\") |&gt;\n  bg(bg = \"white\", part = \"all\") |&gt; \n  flextable::font(fontname = \"Consolas\", part = \"all\") |&gt;\n  theme_vanilla()\n\n\nEstimated Marginal MeansXEmmean (Y)SE95% C.I. (lower)95% C.I. (upper)200.3050.0420.2220.38830-0.2480.040-0.328-0.16850-1.0120.043-1.098-0.92560-0.7650.036-0.837-0.693\n\n\n\n\nCode\n# contrasts\nemm &lt;- emmeans(mod4b, ~ x, at = list(x = c(20,30,50,60)))\ncustom &lt;- list(`Change in Y corresponding to change in X from 20 to 30` = c(-1,1,0,0),\n               `Change in Y corresponding to change in X from 50 to 60` = c(0,0,-1,1))\ncontrast(emm, custom) |&gt; \n  summary(infer = T) |&gt; \n  data.frame() |&gt; \n  select(c(-df, -t.ratio)) |&gt; \n  rename(\"Contrast\" = \"contrast\",\n         \"Estimate\" = \"estimate\",\n         \"95% C.I. (lower)\" = \"lower.CL\",\n         \"95% C.I. (upper)\" = \"upper.CL\",\n         \"p\" = \"p.value\") |&gt; \n  flextable() |&gt; \n  colformat_double(digits = 3, na_str = \"N/A\") |&gt;\n  set_table_properties(layout = \"autofit\") |&gt; \n  height(height = 1, unit = \"cm\") |&gt; \n  hrule(rule = \"atleast\", part = \"header\") |&gt; \n  align(align = \"center\", part = \"body\") |&gt;\n  bg(bg = \"white\", part = \"all\") |&gt; \n  flextable::font(fontname = \"Consolas\", part = \"all\") |&gt;\n  theme_vanilla()\n\n\nContrasts of Estimated Marginal MeansContrastEstimateSE95% C.I. (lower)95% C.I. (upper)pChange in Y corresponding to change in X from 20 to 30-0.5530.019-0.592-0.5150.000Change in Y corresponding to change in X from 50 to 600.2470.0260.1960.2970.000\n\n\nThat‚Äôs probably enough on RCS‚Äôs for now - this post has turned out longer than I initially anticipated. I hope you have found it helpful and above all, found some motivation to using RCS‚Äôs in your modelling endeavours if you are not already.\n\n\n3.5.5 Extra Reading\nIf you want to dive a little deeper into RCS‚Äôs than we‚Äôve done here, I can thoroughly recommend the following 3 papers:\nModeling non-linear relationships in epidemiological data: The application and interpretation of spline models\nCubic splines to model relationships between continuous variables and outcomes: a guide for clinicians\nA review of spline function procedures in R"
  },
  {
    "objectID": "posts/010_19Apr_2024/index.html",
    "href": "posts/010_19Apr_2024/index.html",
    "title": "Everything is a Linear Model",
    "section": "",
    "text": "I want to share with you a secret - maybe you already know it. It took me a while into my statistical learnings to realise this and since then I‚Äôve seen people write about it (see here and here for examples). But the basic idea is that many of the common statistical tests that we use (e.g.¬†t-test, ANOVA, etc) are really nothing more than variations on the general linear model that we‚Äôre all accustomed to:\n\\[ y = ax + b \\]\nThe former are specific-use tests, whereas the latter is an ‚Äòumbrella‚Äô model that can be broadly adapted to accomplish each of the same tasks - perhaps there‚Äôs something to be said for learning just one set of syntax. Let me illustrate this to you with one example using the two-sample t-test. We‚Äôll use the genderweight dataset from the datarium package in R which consists of the bodyweights of 40 subjects (20 males, 20 females). We‚Äôre interested in working out whether there is a gender difference. A look at the data shows:\n\n\nCode\nlibrary(ggplot2)\ndata(\"genderweight\", package = \"datarium\")\nhead(genderweight, 10)\n\n\n\n\n\n\nid\ngroup\nweight\n\n\n\n\n1\nF\n61.58587\n\n\n2\nF\n64.55486\n\n\n3\nF\n66.16888\n\n\n4\nF\n59.30860\n\n\n5\nF\n64.85825\n\n\n6\nF\n65.01211\n\n\n7\nF\n62.85052\n\n\n8\nF\n62.90674\n\n\n9\nF\n62.87110\n\n\n10\nF\n62.21992\n\n\n\n\n\n\n\n1 Plot the Data\nIt‚Äôs always helpful to first plot the data:\n\n\nCode\nggplot(genderweight, aes(x = group, y = weight)) +\n  geom_jitter(size = 3, width = 0.05) +\n  scale_y_continuous(limits = c(50, 100), breaks = seq(50, 100, by = 10)) +\n  stat_summary(fun = mean, \n               geom = \"errorbar\", \n               aes(ymax = after_stat(y), ymin = after_stat(y)), \n               width = 0.25) +\n  theme_bw(base_size = 20)\n\n\n\n\n\n\n\n\n\n\n\n2 Two-Sample t-Test\nNow, we can run our standard t-test as follows (by default, computing the Welch version of the test which does not assume the same variances in each group). In words, we are asking to test the difference in weight by group (i.e.¬†males vs females).\nt.test(weight ~ group, data = genderweight)\n\n\nCode\nt.test(weight ~ group, data = genderweight)\n\n\n\n    Welch Two Sample t-test\n\ndata:  weight by group\nt = -20.791, df = 26.872, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n95 percent confidence interval:\n -24.53135 -20.12353\nsample estimates:\nmean in group F mean in group M \n       63.49867        85.82612 \n\n\nThis output tells us that the mean weight in females and males is 63.5 kg and 85.8 kg, respectively. Furthermore, the 95% C.I. for the difference (note that is does not give us the actual difference) in those two weights is -24.5, -20.1 and as the interval does not contain 0 this is statistically significant (as also reflected in the p-value).\n\n\n3 Linear Model\nNow, the equivalent linear model (i.e.¬†linear regression) in R is simply:\nsummary(lm(weight ~ group, data = genderweight))\n\n\nCode\nsummary(lm(weight ~ group, data = genderweight))\n\n\n\nCall:\nlm(formula = weight ~ group, data = genderweight)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8163 -1.3647 -0.4869  1.3980  9.2365 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  63.4987     0.7593   83.62   &lt;2e-16 ***\ngroupM       22.3274     1.0739   20.79   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.396 on 38 degrees of freedom\nMultiple R-squared:  0.9192,    Adjusted R-squared:  0.9171 \nF-statistic: 432.3 on 1 and 38 DF,  p-value: &lt; 2.2e-16\n\n\nThe output is slightly different but the information contained is almost the same. (Intercept) represents the mean weight in the reference category of the group variable (in this case females). groupM represents the difference in means between females and males (22.3 kg). Note that the 95% C.I.‚Äôs aren‚Äôt presented as part of this standard output, but we can obtain that information easily enough with:\nconfint(lm(weight ~ group, data = genderweight))\n\n\nCode\nconfint(lm(weight ~ group, data = genderweight))\n\n\n               2.5 %   97.5 %\n(Intercept) 61.96145 65.03589\ngroupM      20.15349 24.50140\n\n\nNote the slight difference in the 95% C.I.‚Äôs to that obtained from the t-test. The general linear model, by assumption, assumes homogeneity of variances among the two groups.\nFinally, if you would prefer to know the actual mean values of each group as well, it‚Äôs possible to amend the lm call slightly by removing the intercept term. This gives:\nsummary(lm(weight ~ group - 1, data = genderweight))\n\n\nCode\nsummary(lm(weight ~ group - 1, data = genderweight))\n\n\n\nCall:\nlm(formula = weight ~ group - 1, data = genderweight)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8163 -1.3647 -0.4869  1.3980  9.2365 \n\nCoefficients:\n       Estimate Std. Error t value Pr(&gt;|t|)    \ngroupF  63.4987     0.7593   83.62   &lt;2e-16 ***\ngroupM  85.8261     0.7593  113.03   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.396 on 38 degrees of freedom\nMultiple R-squared:  0.9981,    Adjusted R-squared:  0.998 \nF-statistic:  9884 on 2 and 38 DF,  p-value: &lt; 2.2e-16\n\n\nThe two-sample t-test is just one example of a special case of the general linear model. The first link I provided above contains a neat pdf describing many other special cases and I would encourage you to have a look at these. While you might still use these specific tests in your day to day work, it is nonetheless helpful to broaden your statistical knowledge in the realisation that the general linear model is fundamental to all of these."
  },
  {
    "objectID": "posts/013_31May_2024/index.html",
    "href": "posts/013_31May_2024/index.html",
    "title": "Reshaping Data - Think Before you Pivot",
    "section": "",
    "text": "In your day-to-day data analysis work you will probably find yourself at some point needing to reshape data, and this is usually to suit an analytical need. Reshaping is changing the rectangular structure of the columns and rows in your dataset without altering the content. Data comes in two basic shapes: wide and long."
  },
  {
    "objectID": "posts/011_03May_2024/index.html",
    "href": "posts/011_03May_2024/index.html",
    "title": "gtsummary - Your New Go-To for Tables",
    "section": "",
    "text": "I thought I should bring this excellent package to your attention if you weren‚Äôt aware that it exists, as I have taken gtsummary somewhat for granted over the last few years since it first appeared on CRAN. I‚Äôm prompted in part due to a research student having to recently remake several ‚ÄúTable 1‚Äù - style tables (following a data change) in manuscript preparation for submission and they were going to redo this manually. When they realised what gtsummary could do in terms of saving them time, I think they were fairly impressed. So today, I‚Äôm just going to show you a couple of basic functionalities of this package. It is extremely extensible and if you can‚Äôt find answers for your own customisation needs on the homepage or vignette, I have found googling the issue often brings an answer. The developer is also quite active on stackoverflow.com. The homepage can be found at:\nhttps://www.danieldsjoberg.com/gtsummary/index.html\nWe going to use a publicly available MS dataset, so if you want to run the code yourself you will first need to download the data from:\nBrain MRI dataset of multiple sclerosis with consensus manual lesion segmentation and patient meta information\nThis dataset contains the demographic and clinical data on 60 patients (MRI data in accompanying datasets available at link).\n\n1 Load and Inspect the Data\nLet‚Äôs have a look at the first few lines:\n\n\nCode\nhead(dat, 10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nGender\nAge\nAge.of.onset\nEDSS\nDoes.the.time.difference.between.MRI.acquisition.and.EDSS‚Ä¶two.months\nTypes.of.Medicines\nPresenting.Symptom\nDose.the.patient.has.Co.moroidity\nPyramidal\nCerebella\nBrain.stem\nSensory\nSphincters\nVisual\nMental\nSpeech\nMotor.System\nSensory.System\nCoordination\nGait\nBowel.and.bladder.function\nMobility\nMental.State\nOptic.discs\nFields\nNystagmus\nOcular.Movement\nSwallowing\n\n\n\n\n1\nF\n56\n43\n3.0\nNo\nGelenia\nMotor\nNo\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n2\nF\n29\n19\n1.5\nNo\nGelenia\nSensory\nNo\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n3\nF\n15\n8\n4.0\nNo\nTysabri\nMotor\nNo\n1\n1\n0\n0\n0\n1\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n4\nF\n24\n20\n6.0\nNo\nTysabri\nSensory\nNo\n1\n1\n1\n0\n1\n0\n0\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n5\nF\n33\n31\n0.0\nNo\nAvonex\nPain\nNo\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n6\nF\n44\n40\n5.0\nNo\nAvonex\nMotor\nNo\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n1\n0\n0\n0\n0\n1\n0\n0\n\n\n7\nM\n43\n40\n3.5\nNo\nBetaferon\nMotor & Visual\nNo\n0\n1\n0\n0\n0\n0\n1\n0\n1\n1\n1\n0\n0\n1\n0\n1\n0\n0\n0\n0\n\n\n8\nF\n32\n30\n1.0\nNo\nGelenia\nVisual\nNo\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n9\nF\n36\n33\n6.0\nNo\nGelenia\nMotore\nNo\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n10\nF\n39\n35\n3.0\nNo\nBetaferon\nMotor & Behavioural\nNo\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n2 Summary Table\nLet‚Äôs say you want to create a summary table showing descriptive statistics of the various demographic and clinical characteristics, stratified by DMT (Types.of.Medicines). In the first instance, this can be a basic call of tbl_summary() specifying Types.of.Medicines as the stratifying variable. We want to specify medians (IQR) and n‚Äôs (%‚Äôs) as the summary statistics.\n\n\nCode\nlibrary(gtsummary)\ndat |&gt; \n  select(-ID) |&gt; \n  tbl_summary(\n    by = Types.of.Medicines,\n    statistic = list(all_continuous() ~ \"{median} ({p25},{p75})\",\n                     all_categorical() ~ \"{n}/{N} ({p}%)\"),\n    digits = all_continuous() ~ 1) |&gt; \n  add_overall()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOverall, N = 601\nAvonex, N = 51\nBetaferon, N = 241\nGelenia, N = 91\nRebif, N = 141\nTysabri, N = 81\n\n\n\n\nGender\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†F\n46/60 (77%)\n5/5 (100%)\n15/24 (63%)\n9/9 (100%)\n10/14 (71%)\n7/8 (88%)\n\n\n¬†¬†¬†¬†M\n13/60 (22%)\n0/5 (0%)\n8/24 (33%)\n0/9 (0%)\n4/14 (29%)\n1/8 (13%)\n\n\n¬†¬†¬†¬†N\n1/60 (1.7%)\n0/5 (0%)\n1/24 (4.2%)\n0/9 (0%)\n0/14 (0%)\n0/8 (0%)\n\n\nAge\n33.0 (20.0,42.3)\n24.0 (23.0,33.0)\n37.5 (23.8,43.0)\n42.0 (36.0,52.0)\n32.5 (18.5,38.0)\n20.5 (15.0,24.3)\n\n\nAge.of.onset\n30.5 (19.8,40.0)\n20.0 (20.0,31.0)\n35.0 (23.0,41.0)\n40.0 (30.0,42.0)\n31.0 (18.5,37.0)\n17.0 (16.3,21.3)\n\n\nEDSS\n2.0 (1.0,3.5)\n1.5 (1.0,4.0)\n2.3 (1.0,3.1)\n3.0 (1.5,3.0)\n1.3 (1.0,2.4)\n3.0 (1.4,4.3)\n\n\nDoes.the.time.difference.between.MRI.acquisition.and.EDSS...two.months\n26/60 (43%)\n0/5 (0%)\n10/24 (42%)\n3/9 (33%)\n11/14 (79%)\n2/8 (25%)\n\n\nPresenting.Symptom\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†Balance\n4/60 (6.7%)\n0/5 (0%)\n2/24 (8.3%)\n0/9 (0%)\n2/14 (14%)\n0/8 (0%)\n\n\n¬†¬†¬†¬†Balance &Motor\n1/60 (1.7%)\n0/5 (0%)\n0/24 (0%)\n0/9 (0%)\n0/14 (0%)\n1/8 (13%)\n\n\n¬†¬†¬†¬†Motor\n10/60 (17%)\n1/5 (20%)\n3/24 (13%)\n1/9 (11%)\n3/14 (21%)\n2/8 (25%)\n\n\n¬†¬†¬†¬†Motor & Behavioural\n1/60 (1.7%)\n0/5 (0%)\n1/24 (4.2%)\n0/9 (0%)\n0/14 (0%)\n0/8 (0%)\n\n\n¬†¬†¬†¬†Motor & Sensory\n1/60 (1.7%)\n0/5 (0%)\n1/24 (4.2%)\n0/9 (0%)\n0/14 (0%)\n0/8 (0%)\n\n\n¬†¬†¬†¬†Motor & Visual\n2/60 (3.3%)\n0/5 (0%)\n2/24 (8.3%)\n0/9 (0%)\n0/14 (0%)\n0/8 (0%)\n\n\n¬†¬†¬†¬†Motore\n1/60 (1.7%)\n0/5 (0%)\n0/24 (0%)\n1/9 (11%)\n0/14 (0%)\n0/8 (0%)\n\n\n¬†¬†¬†¬†Pain\n1/60 (1.7%)\n1/5 (20%)\n0/24 (0%)\n0/9 (0%)\n0/14 (0%)\n0/8 (0%)\n\n\n¬†¬†¬†¬†Sensory\n19/60 (32%)\n0/5 (0%)\n8/24 (33%)\n3/9 (33%)\n7/14 (50%)\n1/8 (13%)\n\n\n¬†¬†¬†¬†Sensory & Visual\n1/60 (1.7%)\n0/5 (0%)\n1/24 (4.2%)\n0/9 (0%)\n0/14 (0%)\n0/8 (0%)\n\n\n¬†¬†¬†¬†Sensory & Motor\n1/60 (1.7%)\n0/5 (0%)\n1/24 (4.2%)\n0/9 (0%)\n0/14 (0%)\n0/8 (0%)\n\n\n¬†¬†¬†¬†Sensory & Visual\n1/60 (1.7%)\n0/5 (0%)\n0/24 (0%)\n1/9 (11%)\n0/14 (0%)\n0/8 (0%)\n\n\n¬†¬†¬†¬†Sensory & Visual ,Balance , Motor, Sexual,Fatigue\n1/60 (1.7%)\n0/5 (0%)\n1/24 (4.2%)\n0/9 (0%)\n0/14 (0%)\n0/8 (0%)\n\n\n¬†¬†¬†¬†Sensory &Motor\n1/60 (1.7%)\n0/5 (0%)\n0/24 (0%)\n0/9 (0%)\n0/14 (0%)\n1/8 (13%)\n\n\n¬†¬†¬†¬†Visual\n14/60 (23%)\n3/5 (60%)\n4/24 (17%)\n2/9 (22%)\n2/14 (14%)\n3/8 (38%)\n\n\n¬†¬†¬†¬†Visual & Balance\n1/60 (1.7%)\n0/5 (0%)\n0/24 (0%)\n1/9 (11%)\n0/14 (0%)\n0/8 (0%)\n\n\nDose.the.patient.has.Co.moroidity\n13/60 (22%)\n0/5 (0%)\n8/24 (33%)\n3/9 (33%)\n2/14 (14%)\n0/8 (0%)\n\n\nPyramidal\n31/60 (52%)\n2/5 (40%)\n14/24 (58%)\n5/9 (56%)\n4/14 (29%)\n6/8 (75%)\n\n\nCerebella\n17/60 (28%)\n1/5 (20%)\n8/24 (33%)\n2/9 (22%)\n3/14 (21%)\n3/8 (38%)\n\n\nBrain.stem\n5/60 (8.3%)\n1/5 (20%)\n1/24 (4.2%)\n0/9 (0%)\n1/14 (7.1%)\n2/8 (25%)\n\n\nSensory\n18/60 (30%)\n1/5 (20%)\n8/24 (33%)\n3/9 (33%)\n3/14 (21%)\n3/8 (38%)\n\n\nSphincters\n9/60 (15%)\n0/5 (0%)\n5/24 (21%)\n0/9 (0%)\n2/14 (14%)\n2/8 (25%)\n\n\nVisual\n17/60 (28%)\n3/5 (60%)\n6/24 (25%)\n2/9 (22%)\n2/14 (14%)\n4/8 (50%)\n\n\nMental\n2/60 (3.3%)\n0/5 (0%)\n2/24 (8.3%)\n0/9 (0%)\n0/14 (0%)\n0/8 (0%)\n\n\nSpeech\n6/60 (10%)\n0/5 (0%)\n4/24 (17%)\n0/9 (0%)\n1/14 (7.1%)\n1/8 (13%)\n\n\nMotor.System\n35/60 (58%)\n3/5 (60%)\n14/24 (58%)\n5/9 (56%)\n6/14 (43%)\n7/8 (88%)\n\n\nSensory.System\n19/60 (32%)\n0/5 (0%)\n8/24 (33%)\n4/9 (44%)\n4/14 (29%)\n3/8 (38%)\n\n\nCoordination\n17/60 (28%)\n2/5 (40%)\n6/24 (25%)\n2/9 (22%)\n2/14 (14%)\n5/8 (63%)\n\n\nGait\n17/60 (28%)\n2/5 (40%)\n7/24 (29%)\n1/9 (11%)\n4/14 (29%)\n3/8 (38%)\n\n\nBowel.and.bladder.function\n9/60 (15%)\n1/5 (20%)\n2/24 (8.3%)\n1/9 (11%)\n3/14 (21%)\n2/8 (25%)\n\n\nMobility\n4/60 (6.7%)\n0/5 (0%)\n2/24 (8.3%)\n1/9 (11%)\n1/14 (7.1%)\n0/8 (0%)\n\n\nMental.State\n3/60 (5.0%)\n0/5 (0%)\n2/24 (8.3%)\n0/9 (0%)\n1/14 (7.1%)\n0/8 (0%)\n\n\nOptic.discs\n22/60 (37%)\n2/5 (40%)\n8/24 (33%)\n3/9 (33%)\n4/14 (29%)\n5/8 (63%)\n\n\nFields\n0/60 (0%)\n0/5 (0%)\n0/24 (0%)\n0/9 (0%)\n0/14 (0%)\n0/8 (0%)\n\n\nNystagmus\n7/60 (12%)\n1/5 (20%)\n3/24 (13%)\n2/9 (22%)\n0/14 (0%)\n1/8 (13%)\n\n\nOcular.Movement\n2/60 (3.3%)\n0/5 (0%)\n0/24 (0%)\n1/9 (11%)\n0/14 (0%)\n1/8 (13%)\n\n\nSwallowing\n3/60 (5.0%)\n0/5 (0%)\n3/24 (13%)\n0/9 (0%)\n0/14 (0%)\n0/8 (0%)\n\n\n\n1 n/N (%); Median (25%,75%)\n\n\n\n\n\n\n\n\nIn fact, that‚Äôs a pretty good start. However, we think that including the column frequency as the denominator in every cell is just clutter, so let‚Äôs remove that. We‚Äôll also include an argument for reporting missingness if any exists. Additionally, we want to tidy up some of the variable names - I‚Äôll just do Age, Age.of.onset and the somewhat convoluted Does.the.time.difference.between.MRI.acquisition.and.EDSS...two.months for now. In fact, for the latter we‚Äôll make it a short name and include a footnote to expand on the variable description.\n\n\nCode\ndat |&gt; \n  select(-ID) |&gt; \n  tbl_summary(\n    by = Types.of.Medicines,\n    statistic = list(all_continuous() ~ \"{median} ({p25},{p75})\",\n                     all_categorical() ~ \"{n} ({p}%)\"),\n    digits = all_continuous() ~ 1,\n    missing_text = \"(Missing)\",\n    label = c(Age ~ \"Age, yrs - median (IQR)\",\n              Age.of.onset ~ \"Age onset, yrs - median (IQR)\",\n              Does.the.time.difference.between.MRI.acquisition.and.EDSS...two.months ~ \"Time difference &lt; 2 months\")) |&gt; \n    modify_table_styling(columns = label,\n                         rows = label == \"Time difference &lt; 2 months\",\n                         footnote = \"Does the time difference between MRI acquisition and EDSS &lt; two months\") |&gt; \n  add_overall()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOverall, N = 601\nAvonex, N = 51\nBetaferon, N = 241\nGelenia, N = 91\nRebif, N = 141\nTysabri, N = 81\n\n\n\n\nGender\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†F\n46 (77%)\n5 (100%)\n15 (63%)\n9 (100%)\n10 (71%)\n7 (88%)\n\n\n¬†¬†¬†¬†M\n13 (22%)\n0 (0%)\n8 (33%)\n0 (0%)\n4 (29%)\n1 (13%)\n\n\n¬†¬†¬†¬†N\n1 (1.7%)\n0 (0%)\n1 (4.2%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\nAge, yrs - median (IQR)\n33.0 (20.0,42.3)\n24.0 (23.0,33.0)\n37.5 (23.8,43.0)\n42.0 (36.0,52.0)\n32.5 (18.5,38.0)\n20.5 (15.0,24.3)\n\n\nAge onset, yrs - median (IQR)\n30.5 (19.8,40.0)\n20.0 (20.0,31.0)\n35.0 (23.0,41.0)\n40.0 (30.0,42.0)\n31.0 (18.5,37.0)\n17.0 (16.3,21.3)\n\n\nEDSS\n2.0 (1.0,3.5)\n1.5 (1.0,4.0)\n2.3 (1.0,3.1)\n3.0 (1.5,3.0)\n1.3 (1.0,2.4)\n3.0 (1.4,4.3)\n\n\nTime difference &lt; 2 months2\n26 (43%)\n0 (0%)\n10 (42%)\n3 (33%)\n11 (79%)\n2 (25%)\n\n\nPresenting.Symptom\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†Balance\n4 (6.7%)\n0 (0%)\n2 (8.3%)\n0 (0%)\n2 (14%)\n0 (0%)\n\n\n¬†¬†¬†¬†Balance &Motor\n1 (1.7%)\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n1 (13%)\n\n\n¬†¬†¬†¬†Motor\n10 (17%)\n1 (20%)\n3 (13%)\n1 (11%)\n3 (21%)\n2 (25%)\n\n\n¬†¬†¬†¬†Motor & Behavioural\n1 (1.7%)\n0 (0%)\n1 (4.2%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\n¬†¬†¬†¬†Motor & Sensory\n1 (1.7%)\n0 (0%)\n1 (4.2%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\n¬†¬†¬†¬†Motor & Visual\n2 (3.3%)\n0 (0%)\n2 (8.3%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\n¬†¬†¬†¬†Motore\n1 (1.7%)\n0 (0%)\n0 (0%)\n1 (11%)\n0 (0%)\n0 (0%)\n\n\n¬†¬†¬†¬†Pain\n1 (1.7%)\n1 (20%)\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\n¬†¬†¬†¬†Sensory\n19 (32%)\n0 (0%)\n8 (33%)\n3 (33%)\n7 (50%)\n1 (13%)\n\n\n¬†¬†¬†¬†Sensory & Visual\n1 (1.7%)\n0 (0%)\n1 (4.2%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\n¬†¬†¬†¬†Sensory & Motor\n1 (1.7%)\n0 (0%)\n1 (4.2%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\n¬†¬†¬†¬†Sensory & Visual\n1 (1.7%)\n0 (0%)\n0 (0%)\n1 (11%)\n0 (0%)\n0 (0%)\n\n\n¬†¬†¬†¬†Sensory & Visual ,Balance , Motor, Sexual,Fatigue\n1 (1.7%)\n0 (0%)\n1 (4.2%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\n¬†¬†¬†¬†Sensory &Motor\n1 (1.7%)\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n1 (13%)\n\n\n¬†¬†¬†¬†Visual\n14 (23%)\n3 (60%)\n4 (17%)\n2 (22%)\n2 (14%)\n3 (38%)\n\n\n¬†¬†¬†¬†Visual & Balance\n1 (1.7%)\n0 (0%)\n0 (0%)\n1 (11%)\n0 (0%)\n0 (0%)\n\n\nDose.the.patient.has.Co.moroidity\n13 (22%)\n0 (0%)\n8 (33%)\n3 (33%)\n2 (14%)\n0 (0%)\n\n\nPyramidal\n31 (52%)\n2 (40%)\n14 (58%)\n5 (56%)\n4 (29%)\n6 (75%)\n\n\nCerebella\n17 (28%)\n1 (20%)\n8 (33%)\n2 (22%)\n3 (21%)\n3 (38%)\n\n\nBrain.stem\n5 (8.3%)\n1 (20%)\n1 (4.2%)\n0 (0%)\n1 (7.1%)\n2 (25%)\n\n\nSensory\n18 (30%)\n1 (20%)\n8 (33%)\n3 (33%)\n3 (21%)\n3 (38%)\n\n\nSphincters\n9 (15%)\n0 (0%)\n5 (21%)\n0 (0%)\n2 (14%)\n2 (25%)\n\n\nVisual\n17 (28%)\n3 (60%)\n6 (25%)\n2 (22%)\n2 (14%)\n4 (50%)\n\n\nMental\n2 (3.3%)\n0 (0%)\n2 (8.3%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\nSpeech\n6 (10%)\n0 (0%)\n4 (17%)\n0 (0%)\n1 (7.1%)\n1 (13%)\n\n\nMotor.System\n35 (58%)\n3 (60%)\n14 (58%)\n5 (56%)\n6 (43%)\n7 (88%)\n\n\nSensory.System\n19 (32%)\n0 (0%)\n8 (33%)\n4 (44%)\n4 (29%)\n3 (38%)\n\n\nCoordination\n17 (28%)\n2 (40%)\n6 (25%)\n2 (22%)\n2 (14%)\n5 (63%)\n\n\nGait\n17 (28%)\n2 (40%)\n7 (29%)\n1 (11%)\n4 (29%)\n3 (38%)\n\n\nBowel.and.bladder.function\n9 (15%)\n1 (20%)\n2 (8.3%)\n1 (11%)\n3 (21%)\n2 (25%)\n\n\nMobility\n4 (6.7%)\n0 (0%)\n2 (8.3%)\n1 (11%)\n1 (7.1%)\n0 (0%)\n\n\nMental.State\n3 (5.0%)\n0 (0%)\n2 (8.3%)\n0 (0%)\n1 (7.1%)\n0 (0%)\n\n\nOptic.discs\n22 (37%)\n2 (40%)\n8 (33%)\n3 (33%)\n4 (29%)\n5 (63%)\n\n\nFields\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\nNystagmus\n7 (12%)\n1 (20%)\n3 (13%)\n2 (22%)\n0 (0%)\n1 (13%)\n\n\nOcular.Movement\n2 (3.3%)\n0 (0%)\n0 (0%)\n1 (11%)\n0 (0%)\n1 (13%)\n\n\nSwallowing\n3 (5.0%)\n0 (0%)\n3 (13%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\n\n1 n (%); Median (25%,75%)\n\n\n2 Does the time difference between MRI acquisition and EDSS &lt; two months\n\n\n\n\n\n\n\n\nIf you want to save the created table, you can do this in one of two ways. The first is save it directly as a .docx file which should work most of the time. However, if you notice any formatting issues, change the save target file extension to .html, then open that in Word and you should be ok as well. An important point is to first save the table in your R script to an object - e.g.\ntbl &lt;- dat |&gt; tbl_summary(...\nThe command to save the table as a Word (or html file is then):\ngt::gtsave(as_gt(tbl), filename = \"summary_table.docx\", path = \"...your_path.../\")\n\n\n3 Regression Table\ngtsummary‚Äôs other strength is in making regression tables, and the relevant workhorse function here is tbl_regression().\nLet‚Äôs say we‚Äôre interested in the association between Age onset and the presence of Sensory symptoms (I don‚Äôt really know whether this makes sense or not but it‚Äôs just to run a regression). The outcome variable here is binary, so we‚Äôll need to specify a logistic regression model. We can do that as follows in R and we obtain the standard (fairly bland from the point of view of presentation/collaboration) ouput:\n\n\nCode\nmod &lt;- glm(Sensory ~ Age.of.onset, family = 'binomial', data = dat)\nsummary(mod)\n\n\n\nCall:\nglm(formula = Sensory ~ Age.of.onset, family = \"binomial\", data = dat)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -1.75743    0.87101  -2.018   0.0436 *\nAge.of.onset  0.02987    0.02641   1.131   0.2581  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 73.304  on 59  degrees of freedom\nResidual deviance: 71.994  on 58  degrees of freedom\nAIC: 75.994\n\nNumber of Fisher Scoring iterations: 4\n\n\nLet‚Äôs pretty this up by passing the model results through tbl_regression():\n\n\nCode\nmod |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\nCharacteristic\nlog(OR)1\n95% CI1\np-value\n\n\n\n\nAge.of.onset\n0.03\n-0.02, 0.08\n0.3\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nNot bad, but we‚Äôd like the output to be in terms of odds-ratios rather than log odds-ratios. That‚Äôs actually quite simple to do:\n\n\nCode\nmod |&gt; \n  tbl_regression(exponentiate = T)\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nAge.of.onset\n1.03\n0.98, 1.09\n0.3\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nWhat if you want to include some model summary fit-statistics:\n\n\nCode\nmod |&gt; \n  tbl_regression(exponentiate = T) |&gt; \n  add_glance_source_note()\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    Age.of.onset\n1.03\n0.98, 1.09\n0.3\n  \n  \n    \n      Null deviance = 73.3; Null df = 59.0; Log-likelihood = -36.0; AIC = 76.0; BIC = 80.2; Deviance = 72.0; Residual df = 58; No. Obs. = 60\n    \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nThat‚Äôs all great, but I‚Äôve just noticed that the predictor variable isn‚Äôt formatted so well, so let‚Äôs change that.\n\n\nCode\nmod |&gt; \n  tbl_regression(exponentiate = T,\n                 label = c(Age.of.onset ~ \"Age onset\")) |&gt; \n  add_glance_source_note()\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    Age onset\n1.03\n0.98, 1.09\n0.3\n  \n  \n    \n      Null deviance = 73.3; Null df = 59.0; Log-likelihood = -36.0; AIC = 76.0; BIC = 80.2; Deviance = 72.0; Residual df = 58; No. Obs. = 60\n    \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\ntbl_regression() supports almost any model you can throw at it.\n\n\n4 Last Word\nI hope you find both of these functions useful in your day-to-day coding and data analysis - they are great additions to your R toolkit, not only for their time-saving capabilities, but also the fantastic improvements to the visual style of results formatting that you can achieve, for which base R often falls far short."
  },
  {
    "objectID": "posts/012_17May_2024/index.html",
    "href": "posts/012_17May_2024/index.html",
    "title": "Ordinal Logistic Regression for Ordinal Outcomes?",
    "section": "",
    "text": "It has become clear to me over the years that there is usually no one correct way to kick a statistical goal - several approaches might all converge to the same ‚Äúbest‚Äù answer. To that end it‚Äôs also often difficult to identify an unequivocally incorrect way to do something. So data analysis is not black and white - there are shades of grey (not 50 though). However, there are some approaches that are well-intentioned but ill-considered and some that are performed purely as shortcuts - whether that be for the sake of time or simplicity. Both conditions can lead to results and their interpretation that are misleading at best.\nWhere am I heading with this? Well, today‚Äôs post is based on something as fundamental as the scale that we use to measure or record our data on. It might be good at this point to take a few moments to look over the following link (there is no point in me re-inventing the wheel as Harvey Motulsky - the author of Prism - explains things so eloquently):\nMeasurement/Variable Scales\nThis page describes a ‚Äòhierarchy‚Äô of measurement/variable scales (nominal -&gt; ordinal -&gt; interval -&gt; ratio) as well as their differences. If you‚Äôre unfamiliar with these concepts, it would be worthwhile brushing up on them as they are integral to the discussion that follows."
  },
  {
    "objectID": "posts/012_17May_2024/index.html#the-data",
    "href": "posts/012_17May_2024/index.html#the-data",
    "title": "Ordinal Logistic Regression for Ordinal Outcomes?",
    "section": "3.1 The Data",
    "text": "3.1 The Data\nWe first need to load some data to experiment on, and we‚Äôll use the same publicly available MS dataset that we used in the last post. So if you want to run the code yourself you will first need to download the data from:\nBrain MRI dataset of multiple sclerosis with consensus manual lesion segmentation and patient meta information\nThis dataset contains the demographic and clinical data on 60 patients (MRI data in accompanying datasets available at link)."
  },
  {
    "objectID": "posts/012_17May_2024/index.html#load-and-inspect-the-data",
    "href": "posts/012_17May_2024/index.html#load-and-inspect-the-data",
    "title": "Ordinal Logistic Regression for Ordinal Outcomes?",
    "section": "3.1 Load and Inspect the Data",
    "text": "3.1 Load and Inspect the Data\nWe first need to load some data to experiment on, and we‚Äôll use the same publicly available MS dataset that we used in the last post. So if you want to run the code yourself you will first need to download the data from:\nBrain MRI dataset of multiple sclerosis with consensus manual lesion segmentation and patient meta information\nThis dataset contains the demographic and clinical data on 60 patients (MRI data in accompanying datasets available at link).\nLet‚Äôs have a look at the first few lines:\n\n\nCode\nhead(dat, 10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nGender\nAge\nAge.of.onset\nEDSS\nDoes.the.time.difference.between.MRI.acquisition.and.EDSS‚Ä¶two.months\nTypes.of.Medicines\nPresenting.Symptom\nDose.the.patient.has.Co.moroidity\nPyramidal\nCerebella\nBrain.stem\nSensory\nSphincters\nVisual\nMental\nSpeech\nMotor.System\nSensory.System\nCoordination\nGait\nBowel.and.bladder.function\nMobility\nMental.State\nOptic.discs\nFields\nNystagmus\nOcular.Movement\nSwallowing\n\n\n\n\n1\nF\n56\n43\n3.0\nNo\nGelenia\nMotor\nNo\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n2\nF\n29\n19\n1.5\nNo\nGelenia\nSensory\nNo\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n3\nF\n15\n8\n4.0\nNo\nTysabri\nMotor\nNo\n1\n1\n0\n0\n0\n1\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n4\nF\n24\n20\n6.0\nNo\nTysabri\nSensory\nNo\n1\n1\n1\n0\n1\n0\n0\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n5\nF\n33\n31\n0.0\nNo\nAvonex\nPain\nNo\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n6\nF\n44\n40\n5.0\nNo\nAvonex\nMotor\nNo\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n1\n0\n0\n0\n0\n1\n0\n0\n\n\n7\nM\n43\n40\n3.5\nNo\nBetaferon\nMotor & Visual\nNo\n0\n1\n0\n0\n0\n0\n1\n0\n1\n1\n1\n0\n0\n1\n0\n1\n0\n0\n0\n0\n\n\n8\nF\n32\n30\n1.0\nNo\nGelenia\nVisual\nNo\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n9\nF\n36\n33\n6.0\nNo\nGelenia\nMotore\nNo\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n10\nF\n39\n35\n3.0\nNo\nBetaferon\nMotor & Behavioural\nNo\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n\n\n\n\nLet‚Äôs say we want to regress EDSS on Sensory (the presence of sensory symptoms). In the first instance it‚Äôs always good to visualise the data, so let‚Äôs do that by using some boxplots. It appears that sensory symptoms are associated with higher EDSS - eyeballing the plot suggests that the median EDSS is ~ 1.5 in the absence of Sensory symptoms and ~ 3 when Sensory symptoms are present.\n\n\nCode\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(gtsummary) \nlibrary(ordinal)\nlibrary(ggeffects)\nggplot(dat, aes(x = factor(Sensory), y = EDSS)) +\n  geom_boxplot() +\n  geom_dotplot(binaxis = 'y', stackdir = 'center', position = position_dodge(1), dotsize = 0.8) +\n  xlab(\"Sensory Symptoms\") +\n  theme_bw(base_size = 20)"
  },
  {
    "objectID": "posts/012_17May_2024/index.html#linear-regression",
    "href": "posts/012_17May_2024/index.html#linear-regression",
    "title": "Ordinal Logistic Regression for Ordinal Outcomes?",
    "section": "3.2 Linear Regression",
    "text": "3.2 Linear Regression\nOne way to model the association between the two variables would be to simply treat EDSS as a numeric variable. That‚Äôs very straightforward and I‚Äôm sure you‚Äôve run a linear regression model before. I‚Äôll use the gtsummary package that I highlighted in the last post to format the results.\n\n\nCode\nmod1 &lt;- lm(EDSS ~ Sensory, data = dat)\nmod1 |&gt; tbl_regression(intercept = T)\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n2.0\n1.5, 2.5\n&lt;0.001\n\n\nSensory\n0.93\n0.05, 1.8\n0.038\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nWhile we are now estimating means and not medians, the model results are mostly in line with trend observed in the boxplot. The EDSS in the reference group (no symptoms) is on average, 2.0, and the difference in EDSS in the group with symptoms is an additional 0.93 units (i.e.¬†2.9 on average).\nThere is nothing too difficult about that - if you are willing to accept that the difference between any two equally spaced EDSS scores has about the same clinical impact on the patient, and you are willing to accept the assumptions that go along with linear regression."
  },
  {
    "objectID": "posts/012_17May_2024/index.html#ordinal-logistic-regression",
    "href": "posts/012_17May_2024/index.html#ordinal-logistic-regression",
    "title": "Ordinal Logistic Regression for Ordinal Outcomes?",
    "section": "3.3 Ordinal Logistic Regression",
    "text": "3.3 Ordinal Logistic Regression\nAn alternative approach to model the association between the two variables is to treat EDSS as an ordinal variable. Ordinal logistic regression (using the proportional-odds or the cumulative-logit model) may be used with an outcome variable that consists of three or more categories to model the cumulative probability of falling in any particular category or those below, versus all categories above. In other words we are considering cumulative probabilities up to a threshold, thereby making the whole range of ordinal categories binary at that threshold. Another way to think about the cumulative-logit model is that it essentially consists of a set of binary logistic regression models for each possible binary dichotomisation (i.e.¬†threshold/cut-point) of the ordinal outcome. For example, in the simplest case of an (ordered) three-category outcome, there are two possible thresholds and thus two possible binary logistic regression models. The proportional-odds model allows a comparison of category 1 vs categories 2-3, and simultaneously categories 1-2 vs 3, producing a kind of averaged or summary odds ratio reflecting one overall ‚Äòeffect‚Äô estimate (if one traditionally considered ‚Äòimportant‚Äô assumption is met - more on this soon).\nConceptually, the proportional-odds model is based on the idea of a continuous latent outcome - think of a normal distribution. While we cannot observe or measure this construct, the actual observed ordered categories are mapped to this latent continuous variable and used in the model estimation process. In addition to the ‚Äòeffect‚Äô estimates for each predictor in the model, a series of intercepts (which represent the thresholds or cutpoints) are also typically reported, depending on your software. Let‚Äôs actually run one of these models now.\nThere are a couple of packages in R that estimate proportional-odds models and today we‚Äôll use the ordinal package. The essential function here is clm(). An important thing to note with these models is that your outcome has to be formatted as a factor. The basic specification is simple and in line with typical R models:\nclm(EDSS ~ Sensory, data = dat, link = \"logit\")\nWe will format our results using the excellent gtsummary package I introduced you to in the last post.\n\n\nCode\ndat$EDSS &lt;- factor(dat$EDSS) # format EDSS as factor\nmod2 &lt;- clm(EDSS ~ factor(Sensory), data = dat, link = \"logit\")\nmod2 |&gt; tbl_regression(exponentiate = T, intercept = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\n0|0.5\n0.12\n\n\n&lt;0.001\n\n\n0.5|1\n0.23\n\n\n&lt;0.001\n\n\n1|1.5\n0.81\n\n\n0.5\n\n\n1.5|2\n1.36\n\n\n0.3\n\n\n2|2.5\n1.82\n\n\n0.062\n\n\n2.5|3\n2.12\n\n\n0.022\n\n\n3|3.5\n3.91\n\n\n&lt;0.001\n\n\n3.5|4\n5.62\n\n\n&lt;0.001\n\n\n4|4.5\n14.6\n\n\n&lt;0.001\n\n\n4.5|5\n18.0\n\n\n&lt;0.001\n\n\n5|6\n47.7\n\n\n&lt;0.001\n\n\nfactor(Sensory)\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†0\n‚Äî\n‚Äî\n\n\n\n\n¬†¬†¬†¬†1\n3.08\n1.18, 8.31\n0.024\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nSo let‚Äôs not worry too much about the intercepts and instead just focus on the OR for the predictor of interest (Sensory). The basic interpretation of this is that sensory symptoms are associated with higher EDSS - specifically, there is an ~ 3-fold increase in the odds of having a higher EDSS vs a lower EDSS at any of the possible cutpoints that result in a hypothetical dichotomisation of the data. An example of this might be a group of patients with EDSS above 2 vs below 2, or equivalently a group of patients with EDSS above 5 vs below 5. Note that we are getting the same direction of effect that we observed with a simple linear regression (really, we shouldn‚Äôt have expected anything else).\nNow, let‚Äôs talk about model assumptions. Compared to the linear regression model, the proportional-odds model is relatively assumption-free. This actually makes it an alternative modelling option even for continuous (interval) outcomes when these fail to meet the standard assumptions (mostly to do with model residuals). This has been described here. There is one assumption, however, that has historically been deemed important to the validity of the proportional-odds model, and that is aptly called the proportional odds assumption. In a nutshell, violation of this assumption suggests that the association of predictor variable with the ordinal outcome depends on the level (category) of the outcome. Hence, the assumption of a single summary OR then supposedly becomes untenable.\nThere are a couple of ways to test the proportional-odds assumption and the simplest is with the Brant-Wald test. A p-value of less than 0.05 on this test ‚Äî particularly on the Omnibus plus at least one of the variables (if you have multiple predictors) ‚Äî should be interpreted as a failure of the proportional odds assumption.\n\n\nCode\nlibrary(gofcat)\nbrant.test(mod2)\n\n\n\nBrant Test:\n                    chi-sq   df   pr(&gt;chi)  \nOmnibus               17.3   10      0.068 .\nfactor(Sensory)1      17.3   10      0.068 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nH0: Proportional odds assumption holds\n\n\nThis is a very simple model, so I would have hoped that the test would pass. But I must say that I have not had a lot of luck in my own experience - proportional-odds fail by this test more often than they pass. It does seem well known, however, that the test can falsely reject the null hypothesis that the assumption is satisfied, leading to an incorrect conclusion that the analysis is invalid. On this topic, I am willing to pay more attention to what Frank Harrell on his blog (not that I am using it as a get-out-of-jail-free card):\nViolation of Proportional Odds is Not Fatal\nOne last thing before we finish this section. I‚Äôm an advocate of not just stopping at the model results. Model coefficients tell you something, for sure - the ‚Äòeffect‚Äô on the outcome of a one-unit change in the predictor. But that doesn‚Äôt really tell you what the predicted values of the outcome (for given values of the predictor) actually are. So I always think it‚Äôs a good idea to plot the model predictions. Let‚Äôs do that now with the following code:\n\n\nCode\n# Create new dataframe to predict on\nnewdat &lt;- data.frame(Sensory = c(0, 1)) |&gt; \n  mutate(Sensory = factor(Sensory))\n# Predict on the linear (log-odds) scale\nmod2predict  &lt;- cbind(newdat, predict(mod2, newdat, interval = T, type = \"prob\"))\n# Put estimated probabilities into long format\ndat_long_est &lt;- mod2predict |&gt;\n  pivot_longer(2:13,\n    names_to = \"outcome_val\",\n    values_to = \"pred_prob\") |&gt; \n  select(Sensory, outcome_val, pred_prob)\n# Put estimated lower CI into long format\ndat_long_lowerci &lt;- mod2predict |&gt;\n  pivot_longer(14:25,\n    names_to = \"outcome_val\",\n    values_to = \"pred_lowerci\") |&gt; \n  select(Sensory, outcome_val, pred_lowerci)\n# Put estimated upper CI into long format\ndat_long_upperci &lt;- mod2predict |&gt;\n  pivot_longer(26:37,\n    names_to = \"outcome_val\",\n    values_to = \"pred_upperci\") |&gt; \n  select(Sensory, outcome_val, pred_upperci)\n# cbind together\ndat_long &lt;- cbind(dat_long_est, dat_long_lowerci[3], dat_long_upperci[3])\n# Create EDSS variable from outcome_val\ndat_long &lt;- dat_long |&gt; \n  mutate(EDSS = as.numeric(str_sub(outcome_val, 5, -1)))\n# Plot\nggplot(dat_long, aes(x = EDSS, y = pred_prob)) + \n  geom_point(aes(color = Sensory), position = position_dodge(width = 0.4), size = 2) +\n  geom_errorbar(aes(ymin = pred_lowerci, ymax = pred_upperci, color = Sensory), position = position_dodge(width = 0.4), width = 0.4, linewidth = 0.8) + \n  scale_x_continuous(limits = c(0, 6), breaks = seq(0, 6, by = 0.5)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.1)) +\n  scale_color_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n  xlab(\"EDSS\") + ylab(\"Predicted Probability\") +\n  theme_bw(base_size = 20)\n\n\n\n\n\n\n\n\n\nThis shows the predicted probability of having each level of EDSS, stratified by the presence or absence of sensory symptoms. Compared to being symptom-free, the presence of sensory symptoms are associated with higher predicted probabilities as one‚Äôs EDSS increases."
  },
  {
    "objectID": "posts/013_31May_2024/index.html#primary-transformations",
    "href": "posts/013_31May_2024/index.html#primary-transformations",
    "title": "Logistic Regression - Under the Hood",
    "section": "3.1 Primary Transformations",
    "text": "3.1 Primary Transformations\nIf we have the odds of an event, then we can transform that to a probability - or a risk, which is another way to refer to the chance of an outcome - by dividing the odds by the odds plus 1.\n\\[\\text{p} = \\frac{\\text{odds}}{1 + \\text{odds}}\\]\nIf instead we have the probability of an event, we can transform that to the equivalent odds of the event, by dividing the probability by its complement (that is 1 minus the probability).\n\\[\\text{odds} = \\frac{\\text{p}}{1 - \\text{p}}\\]\nAnd the log-odds is simply taking the natural logarithm of the odds - there is nothing difficult about that.\n\\[\\text{log-odds} = \\text{log(odds)}\\]\nSo these three are what I consider primary transformations and we can cycle between them quite easily with a calculator."
  },
  {
    "objectID": "posts/013_31May_2024/index.html#secondary-transformations",
    "href": "posts/013_31May_2024/index.html#secondary-transformations",
    "title": "Logistic Regression - Under the Hood",
    "section": "3.2 Secondary Transformations",
    "text": "3.2 Secondary Transformations\nThere are additional transformations that you can make and while these aren‚Äôt quite as straightforward, they are even more fundamental to logistic regression, which ultimately uses the log-odds as the scale to estimate its parameters on.\nThe first is that we can get to a probability directly from the log-odds by taking the exponent of the log-odds divided by the exponent of the log-odds plus one.\n\\[\\text{p} = \\frac{e^{\\text{log-odds}}}{1 + e^{\\text{log-odds}}}\\]\nAnd as you can see, that‚Äôs really no different to the first equation above because the exponent of the log-odds is simply the odds (the reverse transformation of logging a number is to take the exponent of that logged number). i.e.\n\\[\\text{odds} = e^{\\text{log-odds}}\\]\nNow, the final transformation, and the one that‚Äôs most relevant to logistic regression shows the relationship between the log-odds and probability and is known as the logit function (red box) - and we‚Äôll talk more about this in a minute.\n\\[\\text{log-odds} = \\text{log}\\Bigl(\\frac{\\text{p}}{1 - \\text{p}}\\Bigr)\\]"
  },
  {
    "objectID": "posts/013_31May_2024/index.html#applied-to-the-research-question",
    "href": "posts/013_31May_2024/index.html#applied-to-the-research-question",
    "title": "Logistic Regression - Under the Hood",
    "section": "3.3 Applied to the Research Question",
    "text": "3.3 Applied to the Research Question\n\n\nCode\n# Recreate data from Ophthalmic statistics note 11: logistic regression.\n# Original source: A comparison of several methods of macular hole measurement using optical coherence tomography, and their value in predicting anatomical and visual outcomes.\n\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(ggmagnify)\nlibrary(emmeans)\n\n# Simulate data ----\nn &lt;- 1000                    # don't change this unless necessary (plots might be fragile)\nset.seed(1234)\nx  &lt;-  rnorm(n, 486, 142)    # generate macular hole inner opening data with mean 486 and sd = 152\nz  &lt;-  10.89 - 0.016 * x     # generate variable that is linear combination of intercept = 10.89 and coefficient for macular hole -0.016 (logit scale)\npr  &lt;-  1/(1 + exp(-z))      # generate probabilities from this\ny  &lt;-  rbinom(n, 1, pr)      # generate outcome variable as a function of those probabilities\n\n# Create dataframe from these:\ndf &lt;-  data.frame(y = y, x = x, z = z, pr = pr)\ndf &lt;- df |&gt; \n  filter(x &gt; 100) # only include those with thickness &gt; 100\n\n# Logistic regression model ----\n# Rescale x to 1 unit = 100 microns instead of 1 micron\nsummary(mod_logistic &lt;- glm(y ~ I(x/100), data = df, family = \"binomial\"))\n\n\n\nCall:\nglm(formula = y ~ I(x/100), family = \"binomial\", data = df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  10.3501     0.7456   13.88   &lt;2e-16 ***\nI(x/100)     -1.5045     0.1212  -12.42   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 773.74  on 989  degrees of freedom\nResidual deviance: 494.67  on 988  degrees of freedom\nAIC: 498.67\n\nNumber of Fisher Scoring iterations: 6\n\n\nCode\n# Emmeans of logodds at 600 and 700 microns\nemmeans_df &lt;- data.frame(emmeans(mod_logistic, ~ x, at = list(x = c(600, 700))))\n# Create df of emmeans and corresponding odds and probs, for plotting\nemmeans_df &lt;- emmeans_df |&gt; \n  select(x, emmean) |&gt; \n  rename(logodds = emmean) |&gt; \n  mutate(odds = round(exp(logodds), 3),\n         probs = round(plogis(logodds), 2),\n         logodds = round(logodds, 3))\n\n# Predictions ----\n# Create new df to predict on new values of x\nnew_dat &lt;- data.frame(x = seq(from = 0, to = 1200, length.out = 100))\n# Predict new fitted values and SE's on logodds scale\npred_logodds &lt;- predict(mod_logistic, newdata = new_dat, type = \"link\", se = TRUE)\nnew_dat &lt;- cbind(new_dat, pred_logodds)\n# Create new df of predictions\npredictions &lt;- new_dat |&gt; \n  rename(pred_logodds_est = fit) |&gt; \n  mutate(pred_logodds_LL = pred_logodds_est - (1.96 * se.fit),\n         pred_logodds_UL = pred_logodds_est + (1.96 * se.fit)) |&gt; \n  select(-c(se.fit, residual.scale))\n# Predict new fitted values and SE's on odds scale\npredictions &lt;- predictions |&gt; \n  mutate(pred_odds_est = exp(pred_logodds_est),\n         pred_odds_LL = exp(pred_logodds_LL),\n         pred_odds_UL = exp(pred_logodds_UL))\n# Predict new fitted values and SE's on probability scale\npred_probs &lt;- predict(mod_logistic, newdata = new_dat, type = \"response\", se = TRUE)\nnew_dat &lt;- cbind(new_dat[1], pred_probs)\nnew_dat &lt;- new_dat |&gt; \n  mutate(pred_probs_LL = fit - (1.96 * se.fit),\n         pred_probs_UL = fit + (1.96 * se.fit))\n# Add predicted probs and CIs to predictions df\npredictions &lt;- cbind(predictions, \n                     pred_probs_est = new_dat$fit, \n                     pred_probs_LL = new_dat$pred_probs_LL,\n                     pred_probs_UL = new_dat$pred_probs_UL)\n\n# Reformat plots slightly for ggarrange ----\np3a &lt;- ggplot(predictions, aes(x = x, y = pred_logodds_est)) + \n  geom_ribbon(aes(ymin = pred_logodds_LL, ymax = pred_logodds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = x, y = y), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1150, y = 6, label = \"log-odds\", size = 10) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-100, 100), breaks = seq(-100, 100, by = 2)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(-8, 8)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(x, logodds),\n                            label = emmeans_df$logodds, \n                            nudge_x = c(-50, 50), nudge_y = c(4, -4),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  theme_bw(base_size = 25) +\n  ylab(\"\") +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\n\np4a &lt;- ggplot(predictions, aes(x = x, y = pred_odds_est)) + \n  geom_ribbon(aes(ymin = pred_odds_LL, ymax = pred_odds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = x, y = y), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1150, y = 6000, label = \"odds\", size = 10) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(seq(0, 1000000, by = 1000))) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(0, 7000)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ylab(\"\") +\n  theme_bw(base_size = 25) +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\np4a_inset &lt;- p4a +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(1,2,3,4,5, seq(0, 1000000, by = 1000))) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(x, odds),\n                            label = emmeans_df$odds, \n                            nudge_x = c(-50, 50), nudge_y = c(-1, 2),\n                            color = \"red\", segment.size = 0.2, size = 5)\np4a &lt;- p4a + geom_magnify(from = c(xmin = 500, xmax = 1000, ymin = 0, ymax = 5), \n                          to = c(xmin = 465, xmax = 1010, ymin = 1000, ymax = 5000), \n                          shadow = T, axes = \"y\", plot = p4a_inset)\n\np5a &lt;- ggplot(predictions, aes(x = x, y = pred_probs_est)) + \n  geom_ribbon(aes(ymin = pred_probs_LL, ymax = pred_probs_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = x, y = y), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1150, y = 0.8, label = \"probability\", size = 10) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(x, probs),\n                            label = emmeans_df$probs, \n                            nudge_x = c(-50, 50), nudge_y = c(-0.1, 0.1),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  ylab(\"\") + xlab(\"Macular hole thickness\") +\n  theme_bw(base_size = 25)\nggarrange(p3a, p4a, p5a, align = \"v\", ncol = 1, heights = c(1,1,1.2))"
  },
  {
    "objectID": "posts/013_31May_2024/index.html#probability-vs-odds-as-applied-to-the-research-question",
    "href": "posts/013_31May_2024/index.html#probability-vs-odds-as-applied-to-the-research-question",
    "title": "Logistic Regression - Under the Hood",
    "section": "3.3 Probability vs Odds as Applied to the Research Question",
    "text": "3.3 Probability vs Odds as Applied to the Research Question\nLet‚Äôs go back now to our original research question and specifically look at the difference between the probability and odds of an event - I want to solidify this concept in your minds. For most of us probability (or chance) is much more intuitive than the notion of odds. If I think about the likelihood of it being a wet day, I think there‚Äôs (for example) an 80% chance of it raining, rather than 4 to 1 odds in favour of it raining. However, if you like to wager on the horses than you may actually be quite comfortable with thinking in terms of odds - as those are the terms bets are placed in.\nIn the following I‚Äôve simulated some data from the original paper (based on the summary statistics and regression model provided). Obviously macular hole size is a continuous variable, but for the sake of this illustration I am going to dichotomise it at it‚Äôs approximate mean value (480 microns). From this we can draw up a 2 x 2 contingency table of the number of successful and failed macular hole repairs as a function of the dichotomised version of macular hole size.\nWhen we talk about probabilities we focus on the proportion of successes out of the total for each row. So, the probability (or risk) of successful repair for those patients with a macular hole size less than 480 microns is therefore 479/487 or about 0.98. That‚Äôs in contrast to the probability of successful repair in those patients with a size greater than 480 microns - where the chance of success is 380/503 or 0.76 - so lower as one would expect. You can then take a ratio of those two probabilities or risks to give the classic risk ratio and that equals 1.3 which tells you that the chance of successful repair is 30% greater if you have a smaller macular hole at initial presentation.\nNow let‚Äôs look at the equivalent odds. When we talk about odds we focus on the ratio of successes to failures and don‚Äôt really worry about the totals. So the odds of successful repair for those patients with a smaller initial hole is 479/8 or 60 to 1. The odds of success with a larger hole are a lot more modest - 380/123 or about 3 to 1. We can then calculate the classic odds ratio which works out at about 20, meaning the odds of successful repair is about 20 times greater in those with thinner initial holes.\nNote that both of these effect measures are commensurate - they point in exactly the same direction - that you fare better with a smaller hole at diagnosis. They are just different ways of expressing the same thing."
  },
  {
    "objectID": "posts/013_31May_2024/index.html#probability-vs-odds-and-as-applied-to-the-research-question",
    "href": "posts/013_31May_2024/index.html#probability-vs-odds-and-as-applied-to-the-research-question",
    "title": "Logistic Regression - Under the Hood",
    "section": "3.3 Probability vs Odds (and as applied to the Research Question)",
    "text": "3.3 Probability vs Odds (and as applied to the Research Question)\nLet‚Äôs go back now to our original research question and specifically look at the difference between the probability and odds of an event - I want to solidify this concept in your minds. For most of us probability (or chance) is much more intuitive than the notion of odds. If I think about the likelihood of it being a wet day, I think there‚Äôs (for example) an 80% chance of it raining, rather than 4 to 1 odds in favour of it raining. However, if you like to wager on the horses than you may actually be quite comfortable with thinking in terms of odds - as those are the terms bets are placed in.\nIn the following I‚Äôve simulated some data from the original paper (based on the summary statistics and regression model provided). Obviously macular hole size is a continuous variable, but for the sake of this illustration I am going to dichotomise it at it‚Äôs approximate mean value (480 microns). From this we can draw up a 2 x 2 contingency table of the number of successful and failed macular hole repairs as a function of the dichotomised version of macular hole size.\nWhen we talk about probabilities we focus on the proportion of successes out of the total for each row. So, the probability (or risk) of successful repair for those patients with a macular hole size less than 480 microns is therefore 479/487 or about 0.98. That‚Äôs in contrast to the probability of successful repair in those patients with a size greater than 480 microns - where the chance of success is 380/503 or 0.76 - so lower as one would expect. You can then take a ratio of those two probabilities or risks to give the classic risk ratio and that equals 1.3 which tells you that the chance of successful repair is 30% greater if you have a smaller macular hole at initial presentation.\nNow let‚Äôs look at the equivalent odds. When we talk about odds we focus on the ratio of successes to failures and don‚Äôt really worry about the totals. So the odds of successful repair for those patients with a smaller initial hole is 479/8 or 60 to 1. The odds of success with a larger hole are a lot more modest - 380/123 or about 3 to 1. We can then calculate the classic odds ratio which works out at about 20, meaning the odds of successful repair is about 20 times greater in those with thinner initial holes.\nNote that both of these effect measures are commensurate - they point in exactly the same direction - that you fare better with a smaller hole at diagnosis. They are just different ways of expressing the same thing."
  },
  {
    "objectID": "posts/013_31May_2024/index.html#the-logit-link-function-and-why-log-odds-are-actually-important",
    "href": "posts/013_31May_2024/index.html#the-logit-link-function-and-why-log-odds-are-actually-important",
    "title": "Logistic Regression - Under the Hood",
    "section": "4.1 The Logit Link Function (and why log-odds are actually important)",
    "text": "4.1 The Logit Link Function (and why log-odds are actually important)\nThe log-odds or the logit function is pivotal to how logistic regression works. The logit function is a type of link function and there‚Äôs really nothing mysterious or difficult about this. A link function is just a function of the mean of Y - in other words a transformation of the mean of Y - and we use this link function as the outcome instead of Y itself.\n\\[\\text{logit(p)} = \\text{log}\\Bigl(\\frac{\\text{p}}{1 - \\text{p}}\\Bigr)\\] The logistic model then becomes:\n\\[\\text{Y} = \\text{logit(p)} = \\beta_0 \\; + \\; \\beta_1x\\]\nThis allows the outcome to become continuous and unbounded and the association between Y and X to become linear, therefore satisfying those fundamental assumptions of the linear model that I mentioned earlier.\nThus, the logit is a type of transformation that links or maps probability values from \\((0, 1)\\) to real numbers \\({\\displaystyle (-\\infty ,+\\infty )}\\). Now, on the log-odds/logit scale the effect of a one-unit change in X on Y becomes constant once again."
  },
  {
    "objectID": "posts/013_31May_2024/index.html#wth-log-oddsoddsprobabilities---youre-confusing-me---pick-one-please",
    "href": "posts/013_31May_2024/index.html#wth-log-oddsoddsprobabilities---youre-confusing-me---pick-one-please",
    "title": "Logistic Regression - Under the Hood",
    "section": "4.2 WTH? log-odds/odds/probabilities - You‚Äôre confusing me - pick one please!!",
    "text": "4.2 WTH? log-odds/odds/probabilities - You‚Äôre confusing me - pick one please!!\nIt‚Äôs important to remember that logistic regression is really about probabilities, not odds. Probabilities are more intuitive than odds (unless you like to put a bet on the horses), so why use odds? Well, model results have historically been expressed in terms of odds and odds ratios for mathematical convenience. As it is for the log-odds scale, the ‚Äòeffect‚Äô of a one-unit change in X on Y is also constant on the odds scale (but as a ratio of two odds, not as a difference in two log-odds). So an odds ratio describes a constant ‚Äòeffect‚Äô of a predictor on the odds of an outcome, irrespective of the value of the predictor. This is not the case on the probability scale, where both differences and ratios vary per unit change depending on the value of the predictor (I will illustrate this later).\nThe TLDR:\n\nModel is estimated (under the hood) on the log-odds scale to satisfy modelling assumptions.\nModel results are displayed on the odds scale (as odds ratios) for their constancy.\nProbabilities are calculated post-hoc with not much extra effort."
  },
  {
    "objectID": "posts/013_31May_2024/index.html#back-to-the-research-question",
    "href": "posts/013_31May_2024/index.html#back-to-the-research-question",
    "title": "Logistic Regression - Under the Hood",
    "section": "4.3 Back to the Research Question",
    "text": "4.3 Back to the Research Question\nLet‚Äôs go back to the original research question for the final time and now illustrate these concepts in that context. The regression equation for surgical repair success as a function of macular hole size that was estimated in our paper is shown here.\n\\[\\text{logit(p)} = 10.89 - 0.016 \\; \\text{x} \\;\\text{macular hole size}\\;(\\mu m)\\]\nBasically, the log-odds of successful repair can be predicted by calculating the result of 10.89 minus the product of 0.016 and whatever the macular hole size is in microns.\n\n4.3.1 Simulate Some Data\nWe can simulate some data from that regression equation - essentially reverse engineering fake data from the estimated best fit equation on the actual data. Why do this? Well, it gives us some sense of what the actual data may have looked like that the researchers collected empirically. We can then do all sorts of cool things like refit a model, generate model predictions, etc. Data simulation is something I am still learning more about but it opens up a world of possibilities in your analytical work.\nFor this simulation I used a normal distribution with a mean and SD for macular hole size of 486 and 142, respectively (the numbers came from the original research paper). The code to show how to reproduce this in R is included below but you can do something similar in Stata and in most other scriptable statistical software packages.\n\n\nCode\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(emmeans)\nlibrary(gtsummary)\nlibrary(ggmagnify)\nlibrary(ggpubr)\n# Simulation\nn &lt;- 1000                                          # set number of obs to simulate\nset.seed(1234)                                     # set seed for reproducibility\nmac_hole_size  &lt;-  rnorm(n, 486, 142)              # generate macular hole inner opening data with mean 486 and sd = 152\nlogodds_success  &lt;-  10.89 - 0.016 * mac_hole_size # generate variable that is linear combination of intercept = 10.89 and coefficient for macular hole -0.016 (logit scale)\nodds_success  &lt;-  exp(logodds_success)             # exponentiate logodds to get odds\nprob_success  &lt;-  odds_success/(1 + odds_success)  # generate probabilities from this\ny_success  &lt;-  rbinom(n, 1, prob_success)          # generate outcome variable as a function of those probabilities\ndf &lt;-  data.frame(cbind(mac_hole_size,             # combine into dataframe\n                        logodds_success,\n                        odds_success,\n                        prob_success, \n                        y_success))\ndf &lt;- df |&gt; \n  filter(mac_hole_size &gt; 100)                       # only include those with size &gt; 100\n\n\nEssentially, we generate some random values of macular hole size based on the specified distribution and then plug those values in to the regression equation and calculate the various outcome measures. The resultant data look like (first 20 rows shown):\n\n\nCode\nhead(df, 20) |&gt; \n  kable(align = \"c\", digits = 2) |&gt; \n  kable_styling() |&gt; \n  row_spec(1, bold = T, background = \"skyblue1\") |&gt; \n  row_spec(20, bold = T, background = \"lightgoldenrod1\")\n\n\n\n\n\nmac_hole_size\nlogodds_success\nodds_success\nprob_success\ny_success\n\n\n\n\n314.60\n5.86\n349.48\n1.00\n1\n\n\n525.39\n2.48\n11.99\n0.92\n1\n\n\n639.99\n0.65\n1.92\n0.66\n1\n\n\n152.91\n8.44\n4644.44\n1.00\n1\n\n\n546.94\n2.14\n8.49\n0.89\n1\n\n\n557.86\n1.96\n7.13\n0.88\n1\n\n\n404.39\n4.42\n83.08\n0.99\n1\n\n\n408.38\n4.36\n77.94\n0.99\n1\n\n\n405.85\n4.40\n81.16\n0.99\n1\n\n\n359.61\n5.14\n170.06\n0.99\n1\n\n\n418.24\n4.20\n66.57\n0.99\n1\n\n\n344.23\n5.38\n217.53\n1.00\n1\n\n\n375.77\n4.88\n131.32\n0.99\n1\n\n\n495.15\n2.97\n19.44\n0.95\n1\n\n\n622.25\n0.93\n2.54\n0.72\n1\n\n\n470.34\n3.36\n28.92\n0.97\n1\n\n\n413.44\n4.28\n71.88\n0.99\n1\n\n\n356.61\n5.18\n178.44\n0.99\n1\n\n\n367.12\n5.02\n150.82\n0.99\n1\n\n\n829.05\n-2.37\n0.09\n0.09\n0\n\n\n\n\n\n\n\n\nLet‚Äôs pick a couple of data points to examine more closely.\nIf we look at the first row (blue), the macular hole size is about 315 microns and this translates to a log-odds of successful repair of 5.9 (calculated straight from the regression equation), an odds of successful repair of about 350 to 1 (transformation calculation) and a probability of successful repair of 0.997 (transformation calculation). We can then generate the actual outcome status using that probability as a random variable in a binomial distribution. So in this case a relatively low macular hole size translates to a successful repair, as we might have expected.\nNow if we look at the last row of data (yellow), we see something different. Here we have a simulated size of 829 microns and this translates to a log-odds of successful repair of -2.4, an odds of 1 to 10 against and a probability of less than 0.1. Not surprisingly, the outcome status in this case is simulated as a failure.\n\n\n4.3.2 Plot the Observed Data\nWhat if we now plot the observed data, that is, plot the outcome status as a function of macular hole size?\nI‚Äôm a big advocate of always plotting your data first - before you calculate any descriptive statistics and certainly before running any statistical tests. In fact I‚Äôm not being melodramatic when I say that plotting your data can save you from looking like an analytical fool when you‚Äôre potentially ill-founded assumptions aren‚Äôt realised in the actual data. A good visualisation can reveal these kinds of problems instantly.\nAfter that big recommendation, we might be left feeling slightly deflated when we see that plotting binary data doesn‚Äôt appear that informative - I mean where‚Äôs the nice scatter plot and imaginary trend line that we‚Äôre used to thinking about?\n\n\nCode\n# Plot observed data\nggplot(df, aes(mac_hole_size, y_success)) + \n  geom_point(size = 4, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 1)) +\n  xlab(\"Macular hole size\") + ylab(\"Surgical repair success\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\nWell it‚Äôs not entirely helpless - we can see that there‚Äôs more successes skewed towards smaller macular holes and more failures skewed towards larger macular holes, so that does tell us something useful. But there are better ways to plot binary data and I‚Äôll take you through this shortly.\n\n\n4.3.3 Fit Linear Model (Naive Approach)\nWhat if we fit a standard regression line to binary data, effectively treating Y as continuous? Well people have done this in the past, and in some parts of the data analysis world, still do. It kind of works, but it‚Äôs not the best tool for the job and we can certainly do better. The best fit line is negative suggesting that as macular hole size increases, repair success decreases, so it is in line with what we know to be true. Note that I have rescaled macular hole size so that the resulting coefficient isn‚Äôt so small (now 1 ‚Äòunit‚Äô = 100 microns instead of 1 micron).\n\n\nCode\n# Linear model\nmod_linear &lt;- lm(y_success ~ I(mac_hole_size/100), data = df) \nmod_linear |&gt; \n  tbl_regression(intercept = T)\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n1.5\n1.4, 1.5\n&lt;0.001\n\n\nI(mac_hole_size/100)\n-0.12\n-0.14, -0.11\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nCode\n# Plot data with regression line\nggplot(df, aes(mac_hole_size, y_success)) + \n  geom_point(size = 2, alpha = 0.1) +\n  geom_abline(slope = coef(mod_linear)[[\"I(mac_hole_size/100)\"]]/100, \n              intercept = coef(mod_linear)[[\"(Intercept)\"]], \n              color = \"cornflowerblue\", linewidth = 1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 1)) +\n  xlab(\"Macular hole size\") + ylab(\"Surgical repair success\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.4 Problem - Predictions are made Outside the Probability Domain\nThe problem with naively fitting a standard linear model is that because we are know treating Y as unbounded and continuous, the model no longer knows that the outcome has to be constrained between 0 and 1, and predictions can easily fall outside this range. And that‚Äôs exactly what we can see here - a macular hole size of 100 microns (blue) predicts outcome success at 1.35 and a size of 1200 microns (yellow) predicts outcome success at -0.02. So fitting a standard linear model to binary data is really not a good approach.\n\n\nCode\n# Create new df to predict on new values of x\nnew_dat &lt;- data.frame(mac_hole_size = seq(from = 0, to = 1200, by = 50))\n# Predict new fitted values\npred &lt;- cbind(new_dat, prediction = predict(mod_linear, newdata = new_dat))\n# Display predictions\npred |&gt; \n  kable(align = \"c\", digits = 2) |&gt; \n  kable_styling() |&gt; \n  row_spec(3, bold = T, background = \"skyblue1\") |&gt; \n  row_spec(25, bold = T, background = \"lightgoldenrod1\")\n\n\n\n\n\nmac_hole_size\nprediction\n\n\n\n\n0\n1.47\n\n\n50\n1.41\n\n\n100\n1.35\n\n\n150\n1.29\n\n\n200\n1.22\n\n\n250\n1.16\n\n\n300\n1.10\n\n\n350\n1.04\n\n\n400\n0.98\n\n\n450\n0.91\n\n\n500\n0.85\n\n\n550\n0.79\n\n\n600\n0.73\n\n\n650\n0.66\n\n\n700\n0.60\n\n\n750\n0.54\n\n\n800\n0.48\n\n\n850\n0.42\n\n\n900\n0.35\n\n\n950\n0.29\n\n\n1000\n0.23\n\n\n1050\n0.17\n\n\n1100\n0.10\n\n\n1150\n0.04\n\n\n1200\n-0.02\n\n\n\n\n\n\n\n\n\n4.3.5 Plot the ‚ÄòBinned‚Äô Observed Data\nSo I said above that there was a better way to plot binary data in an effort to make the visualisation more information, and there is. Instead of just plotting the raw data - just the 0‚Äòs and 1‚Äôs - you can create ‚Äôbins‚Äô of data based on (some fairly arbitrary) thresholds of the predictor you are plotting against and then plot the proportion of successes within each bin. Here I created ‚Äòbins‚Äô of data for each 50 \\(\\mu m\\)‚Äôs of macular hole size and then calculated the mean success rate (i.e.¬†the proportion of 1‚Äôs) within each bin.\n\n\nCode\n# Create bins of data for each 50 microns of macular hole size\ndf$bins &lt;- cut(df$mac_hole_size, breaks = seq(0, 1000, by = 50))\n# Calculate means (number of successful repairs divided by total number of repairs) in each bin\ndf &lt;- df |&gt; \n  group_by(bins) |&gt; \n  mutate(mean_y = mean(y_success))\n# Plot observed (averaged) data \nggplot(df, aes(as.numeric(bins)*50-25, mean_y)) + \n  geom_point(size = 2) +\n  geom_line(color = \"orange\", linewidth = 1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Proportion\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\nNow we are starting to see the classic sigmoid shape of the relationship between probability and a continuous predictor.\n\n\n4.3.6 Fit a Logistic Model to the Simulated Data\nNow, let‚Äôs finally fit a logistic model to these simulated data.\n\n\nCode\n# Logistic model\nmod_logistic &lt;- glm(y_success ~ I(mac_hole_size/100), data = df, family = \"binomial\")\nmod_logistic |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\nCharacteristic\nlog(OR)1\n95% CI1\np-value\n\n\n\n\nI(mac_hole_size/100)\n-1.5\n-1.8, -1.3\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nOn the model estimation log-odds scale, the parameter estimate for the effect of macular hole size is -1.5, meaning for every 100 micron increase in macular hole size, the log-odds of success decreases by 1.5 units. All stats software will also provide you with parameter estimates as odds ratios, because that‚Äôs how we‚Äôre accustomed to reporting effects. And the equivalent odds ratio in this case is 0.22. In other words, for every 100 micron increase in macular hole size there is about a 78% reduction in the odds of success.\n\n\nCode\nmod_logistic |&gt; \n  tbl_regression(exponentiate = T)\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nI(mac_hole_size/100)\n0.22\n0.17, 0.28\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nNote that the odds ratio coefficient is a simple transformation of the log-odds coefficient (as we described earlier.) That is:\n\\[e^{-1.5} = 0.22\\]\n\n\n4.3.7 Plot Predicted log-odds of Surgical Repair Success\nNow we can plot the predicted log-odds of surgical repair success. Note that this is a straight line effectively fitted to our binary outcome, and the reason again that we can do this is because the binary outcome has been ‚Äòremapped‚Äô to log-odds and the log-odds exist on an unbounded and continuous scale. So this is a valid plot, unlike that before where we plotted the predictions treating Y as if it were a continuous outcome (I‚Äôm not necessarily suggesting this is a useful plot, just that it‚Äôs valid).\n\n\nCode\n# Predictions\n# Emmeans of logodds at 600 and 700 microns\nemmeans_df &lt;- data.frame(emmeans(mod_logistic, ~ mac_hole_size, at = list(mac_hole_size = c(600, 700, 800))))\n# Create df of emmeans and corresponding odds and probs, for plotting\nemmeans_df &lt;- emmeans_df |&gt; \n  select(mac_hole_size, emmean) |&gt; \n  rename(logodds = emmean) |&gt; \n  mutate(odds = round(exp(logodds), 3),\n         probs = round(plogis(logodds), 2),\n         logodds = round(logodds, 3))\n\n# Predictions ----\n# Create new df to predict on new values of x\nnew_dat &lt;- data.frame(mac_hole_size = seq(from = 0, to = 1200, length.out = 100))\n# Predict new fitted values and SE's on logodds scale\npred_logodds &lt;- predict(mod_logistic, newdata = new_dat, type = \"link\", se = TRUE)\nnew_dat &lt;- cbind(new_dat, pred_logodds)\n# Create new df of predictions\npredictions &lt;- new_dat |&gt; \n  rename(pred_logodds_est = fit) |&gt; \n  mutate(pred_logodds_LL = pred_logodds_est - (1.96 * se.fit),\n         pred_logodds_UL = pred_logodds_est + (1.96 * se.fit)) |&gt; \n  select(-c(se.fit, residual.scale))\n# Predict new fitted values and SE's on odds scale\npredictions &lt;- predictions |&gt; \n  mutate(pred_odds_est = exp(pred_logodds_est),\n         pred_odds_LL = exp(pred_logodds_LL),\n         pred_odds_UL = exp(pred_logodds_UL))\n# Predict new fitted values and SE's on probability scale\npred_probs &lt;- predict(mod_logistic, newdata = new_dat, type = \"response\", se = TRUE)\nnew_dat &lt;- cbind(new_dat[1], pred_probs)\nnew_dat &lt;- new_dat |&gt; \n  mutate(pred_probs_LL = fit - (1.96 * se.fit),\n         pred_probs_UL = fit + (1.96 * se.fit))\n# Add predicted probs and CIs to predictions df\npredictions &lt;- cbind(predictions, \n                     pred_probs_est = new_dat$fit, \n                     pred_probs_LL = new_dat$pred_probs_LL,\n                     pred_probs_UL = new_dat$pred_probs_UL)\n# Plot predicted log-odds\nggplot(predictions, aes(x = mac_hole_size, y = pred_logodds_est)) + \n  geom_ribbon(aes(ymin = pred_logodds_LL, ymax = pred_logodds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-100, 100), breaks = seq(-100, 100, by = 2)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(-8, 8)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted log-odds\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.8 Plot Predicted Odds of Surgical Repair Success\nWe can also plot the predicted odds of surgical repair success. Note that this best-fitting line is not linear, but instead looks like an exponential plot. We established previously that odds exist on a scale from 0 to \\(+\\infty\\) and that‚Äôs what we can appeciate here. Remember that this is just another transformation of the log-odds and the same trend is seen - as hole size increases, the odds of success decline.\n\n\nCode\n# Plot predicted odds\nggplot(predictions, aes(x = mac_hole_size, y = pred_odds_est)) + \n  geom_ribbon(aes(ymin = pred_odds_LL, ymax = pred_odds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1000000), breaks = seq(0, 1000000, by = 1000)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(0, 7000)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted odds\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.9 Plot Predicted Probabilities of Surgical Repair Success\nAnd we can also plot the predicted probabilities of surgical repair success. Again this is just another transformation of the log-odds or the odds. Note the classic sigmoid shape which emulates what we saw earlier when plotting the binned raw data. To my mind this plot is more useful than either plots of predicted log-odds or odds, as it tells us the likely chance of successful repair conditional on the initial macular hole size.\n\n\nCode\n# Plot predicted probabilities\nggplot(predictions, aes(x = mac_hole_size, y = pred_probs_est)) + \n  geom_ribbon(aes(ymin = pred_probs_LL, ymax = pred_probs_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted probability\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.10 Plot Observed and Predicted Probabilities of Surgical Repair Success\nAnd in fact what you should ideally find if you plot both your observed and predicted probabilities is that they are quite similar, reflecting that the model actually fits the data quite well. Of course, in this case the convergence of observed and predicted probabilities is unsurprising given that we know the data-generating proces - i.e.¬†we fitted a model to data that came from a model. But, in general, this is a good way to assess your model fit.\n\n\nCode\n# Plot observed and predicted data\nggplot(df, aes(as.numeric(bins)*50-25, mean_y)) + \n  geom_point(size = 2) +\n  geom_line(aes(color = \"observed\"), linewidth = 1) +\n  geom_line(data = predictions, aes(x = mac_hole_size, y = pred_probs_est, color = \"predicted\"), linewidth = 1) +\n  scale_color_manual(name = \"\", values = c(\"predicted\" = \"cornflowerblue\", \"observed\" = \"orange\")) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Probability\") +\n  theme_bw(base_size = 25) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n4.3.11 Putting It All Together\nWhat I want to try and reinforce in the figure below is how the log-odds, the odds and the probability of the outcome are all transformations of each other.\nIn logistic regression we are ultimately interested in the probability of the outcome occurring. But we use the log-odds or logit link function to map probabilities onto a linear real-number range. This means that the change in Y for each unit increase in X remains the same and we can see that in the top plot. So the log-odds coefficient that our stats software gives us is constant across all values of macular hole size. This represents the difference in the log-odds of success for each unit change in X.\nOne unit increase in log-odds:\n\n600 -&gt; 700 \\(\\mu m\\)\n= -0.181 - 1.323 = -1.504\n\n700 -&gt; 800 \\(\\mu m\\)\n= -1.686 - -0.181 = -1.505\n\n\nThe middle plot shows the equivalent odds of success across the same range of macular hole size. Here the difference in Y is no longer constant for each unit increase in X, but the commensurate ratio is constant - and this gives us the odds ratio. This means that the odds ratio is constant across all values of macular hole size.\nOne unit increase in odds:\n\n600 -&gt; 700 \\(\\mu m\\)\n= 0.834/3.756 = 0.22\n\n700 -&gt; 800 \\(\\mu m\\)\n= 0.185/0.834 = 0.22\n\n\nFinally the lower plot shows the equivalent probabilities of success and now there is no constancy in either unit differences or ratios. So, if we are interested in expressing our logistic regression model results as differences or ratios of predicted probabilities, we need to make sure that we qualify the macular hole size that a particular predicted probability corresponds to, as this will change across the range of X.\nOne unit increase in probability:\n\n600 -&gt; 700 \\(\\mu m\\)\n= 0.45 - 0.79 = -0.34\n= 0.45/0.79 = 0.57\n\n700 -&gt; 800 \\(\\mu m\\)\n= 0.16 - 0.45 = -0.29\n= 0.16/0.45 = 0.36\n\n\n\n\nCode\n# Reformat plots slightly for ggarrange ----\np1 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_logodds_est)) + \n  geom_ribbon(aes(ymin = pred_logodds_LL, ymax = pred_logodds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1150, y = 6, label = \"log-odds\", size = 10) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  annotate(\"text\", x = 1110, y = 3.5, label = \"unit differences = constant\", size = 5) +\n  scale_y_continuous(limits = c(-100, 100), breaks = seq(-100, 100, by = 2)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(-8, 8)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, logodds),\n                            label = emmeans_df$logodds, \n                            nudge_x = c(50, 50, 50), nudge_y = c(4, 4, 4),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  theme_bw(base_size = 20) +\n  ylab(\"\") +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\n\np2 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_odds_est)) + \n  geom_ribbon(aes(ymin = pred_odds_LL, ymax = pred_odds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1180, y = 6000, label = \"odds\", size = 10) +\n  annotate(\"text\", x = 1136, y = 5000, label = \"unit ratios = constant\", size = 5) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(seq(0, 1000000, by = 1000))) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(0, 7000)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ylab(\"\") +\n  theme_bw(base_size = 20) +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\np2_inset &lt;- p2 +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(1,2,3,4,5, seq(0, 1000000, by = 1000))) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, odds),\n                            label = emmeans_df$odds, \n                            nudge_x = c(50, 50, 50), nudge_y = c(0.5, 1, 1),\n                            color = \"red\", segment.size = 0.2, size = 5)\np2 &lt;- p2 + geom_magnify(from = c(xmin = 500, xmax = 1000, ymin = 0, ymax = 5), \n                          to = c(xmin = 470, xmax = 1010, ymin = 1000, ymax = 5000), \n                          shadow = T, axes = \"y\", plot = p2_inset)\n\np3 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_probs_est)) + \n  geom_ribbon(aes(ymin = pred_probs_LL, ymax = pred_probs_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1140, y = 0.8, label = \"probability\", size = 10) +\n  annotate(\"text\", x = 1075, y = 0.65, label = \"unit differences/ratios = not constant\", size = 5) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, probs),\n                            label = emmeans_df$probs, \n                            nudge_x = c(50, 50, 50), nudge_y = c(0.1, 0.1, 0.1),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  ylab(\"\") + xlab(\"Macular hole size\") +\n  theme_bw(base_size = 20)\nggarrange(p1, p2, p3, align = \"v\", ncol = 1, heights = c(1,1,1.2))\n\n\n\n\n\n\n\n\n\n\n\n4.3.12 Key points\nThis has been a long post but we are finally at the end. I hope that reading this has demystified what goes on under the hood in logistic regression as it‚Äôs really not that difficult once you understand the purpose of the link function.\nThese are the main summary points:\n\nLogistic regression is a type of generalised linear model that allows the estimation of associations of potential predictors with a binary outcome.\nThe logit link function transforms the probabilities of a binary outcome variable to a continuous, unbounded scale (log-odds). Standard linear modelling methods can then be applied to estimate the model.\nLog-odds, odds and probabilities are all simple transformations of one another.\nThe model is estimated on the log-odds scale where the association between the predictor and the log-odds of the outcome is linear, and the unit difference is constant.\nResults may be reported in terms of odds or probabilities:\n\nOn the odds scale the association between the predictor and the odds of the outcome is non-linear, but the unit ratio is constant.\nOn the probability scale the association between the predictor and the probability of the outcome is non-linear, and the unit difference/ratio is not constant.\nThis means probability estimates (risk differences of ratios) depend on the value of the predictor and this needs to be specified."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html",
    "href": "posts/014_14Jun_2024/index.html",
    "title": "Academic Research - ‚ÄúWhat‚Äôs in a Title?‚Äù",
    "section": "",
    "text": "So that was a pretty poor attempt to adapt a famous adage from Romeo and Juliet - ‚ÄúWhat‚Äôs in a name? That which we call a rose by any other name would smell just as sweet‚Äù - for my own expository purposes.\nBefore taking a short mid-year break (and also because I didn‚Äôt have time to prepare anything formal this week), I thought we could have a little fun in taking a look at some clever, bizarre, and in some cases laugh-out-loud titles that academic researchers have published with over the years.\nBut we can‚Äôt have fun without a little science as well. So, is there actually anything to be gained by giving your research paper a less serious title? Interestingly, there is a recent preprint paper out there that has attempted to assess this relationship - specifically between humour in article titles and the all-important citation impact.\nIf this title is funny, will you cite me? Citation impacts of humour and other features of article titles in ecology and evolution (and the Nature opinion piece of the same paper).\nThe TL;DR is that articles with humourous titles tend to be cited less, but article ‚Äòimportance‚Äô was considered to be a confounder in this association, and once adjustment was made for that, articles with humourous titles were in fact cited more. If you‚Äôre interested, have a read of the paper. I‚Äôm not that convinced as the authors used self-citation as a proxy for their definition of importance and I‚Äôm sure there are better ways to assess this.\nBut anyway, now that the science is out of the way, let‚Äôs get to the fun bit. These are a collection of papers that I‚Äôve come across over the years and no doubt some will be familiar to you, but please indulge me anyway. Many of these have used clever wordplay or puns, but some are also serious (and unfortunate) titles.\nI am going to try my best to broadly group them by some linking theme, if that is even possible‚Ä¶."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#case-1-one-name-column-one-value-column",
    "href": "posts/014_14Jun_2024/index.html#case-1-one-name-column-one-value-column",
    "title": "Reshaping Data - Think Before you Pivot",
    "section": "2.1 Case 1: One Name Column, One Value Column",
    "text": "2.1 Case 1: One Name Column, One Value Column\nThis is the most common and most straight-forward application of reshaping to long that you might be required to perform. In this case we have multiple value columns in wide format that we want to reshape to one name column and one value column. The good news is that we have already done this in the example above. The code is also shown but just to revisit that briefly for clarity:\n\ndf_long &lt;- df_wide |&gt; \n  pivot_longer(cols = month_0:month_3,\n               names_to = \"month\",\n               names_prefix = \"month_\",\n               values_to = \"bp\")\n\nSo we take the df_wide dataframe and ‚Äòpipe‚Äô it to the pivot_longer() function where we specify that we want to take the columns from (and including) month_0 to month_3, assigning those column names as category labels in the new month name variable, while also placing each corresponding BP measurement into the new bp value variable. The names_prefix argument is optional but was used here to strip out the somewhat redundant month_ text from each column name prior to labelling. You could certainly leave this in if you wanted and the result would then be:\n\n\n\n\n\nid\nmonth\nbp\n\n\n\n\n1\nmonth_0\n136.79\n\n\n1\nmonth_1\n170.78\n\n\n1\nmonth_2\n130.11\n\n\n1\nmonth_3\n136.40\n\n\n2\nmonth_0\n99.08\n\n\n2\nmonth_1\n110.87\n\n\n2\nmonth_2\n118.15\n\n\n2\nmonth_3\n141.49\n\n\n\n\n\nBut I‚Äôm sure you‚Äôd agree that the results looks cleaner without all that unnecessary repetition."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#case-2-multiple-name-columns-one-value-column",
    "href": "posts/014_14Jun_2024/index.html#case-2-multiple-name-columns-one-value-column",
    "title": "Reshaping Data - Think Before you Pivot",
    "section": "2.2 Case 2: Multiple Name Columns, One Value Column",
    "text": "2.2 Case 2: Multiple Name Columns, One Value Column\nLet‚Äôs now extend this idea a little. Imagine that in addition to BP measurements, subjects also had their weight measured at the same time points (simulated with a mean of 70 kg and SD 15 kg). Now we have data that could potentially look like:\n\n\nCode\nid &lt;- seq(1:5)\nfor(i in 0:3){\n  var_name_bp &lt;- paste0(\"month_\",i,\"_bp\")\n  assign(var_name_bp, rnorm(5, 130, 20))\n  var_name_wt &lt;- paste0(\"month_\",i,\"_wt\")\n  assign(var_name_wt, rnorm(5, 70, 15))\n}\ndf_wide &lt;- data.frame(cbind(id, month_0_bp, month_0_wt, month_1_bp, month_1_wt, month_2_bp, month_2_wt, month_3_bp, month_3_wt))\ndf_wide |&gt;  \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nmonth_0_bp\nmonth_0_wt\nmonth_1_bp\nmonth_1_wt\nmonth_2_bp\nmonth_2_wt\nmonth_3_bp\nmonth_3_wt\n\n\n\n\n1\n139.56\n64.44\n142.76\n88.21\n94.20\n31.81\n123.68\n110.41\n\n\n2\n159.89\n58.74\n136.44\n58.65\n137.26\n60.98\n135.68\n56.16\n\n\n3\n118.89\n62.49\n101.90\n56.53\n103.91\n54.20\n129.39\n91.74\n\n\n4\n106.44\n90.90\n138.15\n64.54\n126.27\n73.29\n120.39\n59.81\n\n\n5\n141.39\n74.20\n109.42\n62.05\n114.40\n51.03\n73.31\n82.06\n\n\n\n\n\nWhat to do here?\nActually, some thought is required at this point as there are two potential paths you could go down and it all depends on what you want to achieve. Let‚Äôs assume that you want to put all measurement values in one column. Once you have decided on this final form, the code is not challenging. We will necessarily end up with two names columns instead of just one, one for time (month) and one for the all the clinical measures (BP and weight). The main changes to the code are to now supply two new variable names to the names_to argument as well as tell the function how to source the new category labels with the names_sep argument. This will split the currently wide variable names at the second _ (after stripping out the redundant month_ text) and use the number as the month label and the type of measurement as the clinical measure label.\n\ndf_long &lt;- df_wide |&gt; \n  pivot_longer(cols = month_0_bp:month_3_wt,\n               names_to = c(\"month\", \"clinical_measure\"),\n               names_prefix = \"month_\",\n               names_sep = \"_\",\n               values_to = \"value\")\n\nand the data looks like:\n\n\nCode\ndf_long |&gt;  \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nid\nmonth\nclinical_measure\nvalue\n\n\n\n\n1\n0\nbp\n139.56\n\n\n1\n0\nwt\n64.44\n\n\n1\n1\nbp\n142.76\n\n\n1\n1\nwt\n88.21\n\n\n1\n2\nbp\n94.20\n\n\n1\n2\nwt\n31.81\n\n\n1\n3\nbp\n123.68\n\n\n1\n3\nwt\n110.41\n\n\n2\n0\nbp\n159.89\n\n\n2\n0\nwt\n58.74\n\n\n2\n1\nbp\n136.44\n\n\n2\n1\nwt\n58.65\n\n\n2\n2\nbp\n137.26\n\n\n2\n2\nwt\n60.98\n\n\n2\n3\nbp\n135.68\n\n\n2\n3\nwt\n56.16\n\n\n3\n0\nbp\n118.89\n\n\n3\n0\nwt\n62.49\n\n\n3\n1\nbp\n101.90\n\n\n3\n1\nwt\n56.53\n\n\n3\n2\nbp\n103.91\n\n\n3\n2\nwt\n54.20\n\n\n3\n3\nbp\n129.39\n\n\n3\n3\nwt\n91.74\n\n\n4\n0\nbp\n106.44\n\n\n4\n0\nwt\n90.90\n\n\n4\n1\nbp\n138.15\n\n\n4\n1\nwt\n64.54\n\n\n4\n2\nbp\n126.27\n\n\n4\n2\nwt\n73.29\n\n\n4\n3\nbp\n120.39\n\n\n4\n3\nwt\n59.81\n\n\n5\n0\nbp\n141.39\n\n\n5\n0\nwt\n74.20\n\n\n5\n1\nbp\n109.42\n\n\n5\n1\nwt\n62.05\n\n\n5\n2\nbp\n114.40\n\n\n5\n2\nwt\n51.03\n\n\n5\n3\nbp\n73.31\n\n\n5\n3\nwt\n82.06\n\n\n\n\n\nI tend to think of this as a complete reshaping to long format."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#case-3-multiple-name-columns-multiple-value-columns",
    "href": "posts/014_14Jun_2024/index.html#case-3-multiple-name-columns-multiple-value-columns",
    "title": "Reshaping Data - Think Before you Pivot",
    "section": "2.3 Case 3: Multiple Name Columns, Multiple Value Columns",
    "text": "2.3 Case 3: Multiple Name Columns, Multiple Value Columns\nBut what if didn‚Äôt want to do this and instead wanted the values of BP and weight to appear in their own columns - a partial reshaping to long format if you like.\n\ndf_long &lt;- df_wide |&gt; \n  pivot_longer(cols = month_0_bp:month_3_wt,\n               names_to = c(\"month\", \".value\"),\n               names_prefix = \"month_\",\n               names_sep = \"_\")\n\nand the data now looks like:\n\n\nCode\ndf_long |&gt;  \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nid\nmonth\nbp\nwt\n\n\n\n\n1\n0\n121.12\n89.31\n\n\n1\n1\n147.56\n62.52\n\n\n1\n2\n124.86\n90.79\n\n\n1\n3\n119.11\n68.30\n\n\n2\n0\n146.37\n77.34\n\n\n2\n1\n128.50\n73.84\n\n\n2\n2\n143.65\n77.60\n\n\n2\n3\n149.05\n75.75\n\n\n3\n0\n101.35\n77.29\n\n\n3\n1\n139.16\n95.57\n\n\n3\n2\n143.61\n70.25\n\n\n3\n3\n110.51\n67.72\n\n\n4\n0\n135.15\n83.72\n\n\n4\n1\n114.41\n49.70\n\n\n4\n2\n116.78\n64.98\n\n\n4\n3\n143.78\n54.83\n\n\n5\n0\n117.96\n57.93\n\n\n5\n1\n160.39\n65.14\n\n\n5\n2\n127.80\n39.26\n\n\n5\n3\n102.54\n74.54"
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#case-3-one-name-columns-multiple-value-columns",
    "href": "posts/014_14Jun_2024/index.html#case-3-one-name-columns-multiple-value-columns",
    "title": "Reshaping Data - Think Before you Pivot",
    "section": "2.3 Case 3: One Name Columns, Multiple Value Columns",
    "text": "2.3 Case 3: One Name Columns, Multiple Value Columns\nBut what if didn‚Äôt want to do this and instead wanted the values of BP and weight to appear in their own columns - a partial reshaping to long format if you like. To my mind this is probably a more useful long format than what we considered in the last example, although there may be some niche use-case scenarios that require data to be in that format for analysis (they just elude me right now).\nSo let‚Äôs now assume that you want separate columns of values for each type of measurement. Now we will end up with one name column and two value columns - one for BP and one for weight. The general form of the code doesn‚Äôt change a lot in this case - the main thing being that we replace ‚Äúclinical_measure‚Äù in the names_to argument with a special term .value which indicates that the pivoted (new) columns will be split by the text after the second _ in the currently wide column names - i.e.¬†taking on the value of bp or wt. In those two new columns the corresponding measurement values will be placed. The code looks like:\n\ndf_long &lt;- df_wide |&gt; \n  pivot_longer(cols = month_0_bp:month_3_wt,\n               names_to = c(\"month\", \".value\"),\n               names_prefix = \"month_\",\n               names_sep = \"_\")\n\nand the data now looks like:\n\n\nCode\ndf_long |&gt;  \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nid\nmonth\nbp\nwt\n\n\n\n\n1\n0\n125.77\n58.28\n\n\n1\n1\n151.33\n60.70\n\n\n1\n2\n148.67\n53.45\n\n\n1\n3\n90.00\n58.16\n\n\n2\n0\n155.15\n51.05\n\n\n2\n1\n138.30\n70.95\n\n\n2\n2\n131.13\n73.50\n\n\n2\n3\n94.79\n64.58\n\n\n3\n0\n109.26\n45.20\n\n\n3\n1\n150.97\n79.06\n\n\n3\n2\n126.63\n25.21\n\n\n3\n3\n136.61\n55.40\n\n\n4\n0\n171.49\n71.59\n\n\n4\n1\n109.88\n88.51\n\n\n4\n2\n98.91\n64.93\n\n\n4\n3\n164.71\n83.97\n\n\n5\n0\n92.08\n81.82\n\n\n5\n1\n129.54\n67.93\n\n\n5\n2\n141.80\n77.97\n\n\n5\n3\n150.33\n88.55"
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#case-3-one-name-column-multiple-value-columns",
    "href": "posts/014_14Jun_2024/index.html#case-3-one-name-column-multiple-value-columns",
    "title": "Reshaping Data - Think Before you Pivot",
    "section": "2.3 Case 3: One Name Column, Multiple Value Columns",
    "text": "2.3 Case 3: One Name Column, Multiple Value Columns\nBut what if didn‚Äôt want to do this and instead wanted the values of BP and weight to appear in their own columns - a partial reshaping to long format if you like. To my mind this is probably a more useful long format than what we considered in the last example, although there may be some niche use-case scenarios that require data to be in that format for analysis (they just elude me right now).\nSo let‚Äôs now assume that you want separate columns of values for each type of measurement. Now we will end up with one name column and two value columns - one for BP and one for weight. The general form of the code doesn‚Äôt change a lot in this case - the main thing being that we replace ‚Äúclinical_measure‚Äù in the names_to argument with a special term .value which indicates that the pivoted (new) columns will be split by the text after the second _ in the currently wide column names - i.e.¬†taking on the value of bp or wt (so it‚Äôs not necessary to specify a values_to term this time around). In those two new columns the corresponding measurement values will be placed. The code looks like:\n\ndf_long &lt;- df_wide |&gt; \n  pivot_longer(cols = month_0_bp:month_3_wt,\n               names_to = c(\"month\", \".value\"),\n               names_prefix = \"month_\",\n               names_sep = \"_\")\n\nand the data now looks like:\n\n\nCode\ndf_long |&gt;  \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nid\nmonth\nbp\nwt\n\n\n\n\n1\n0\n139.56\n64.44\n\n\n1\n1\n142.76\n88.21\n\n\n1\n2\n94.20\n31.81\n\n\n1\n3\n123.68\n110.41\n\n\n2\n0\n159.89\n58.74\n\n\n2\n1\n136.44\n58.65\n\n\n2\n2\n137.26\n60.98\n\n\n2\n3\n135.68\n56.16\n\n\n3\n0\n118.89\n62.49\n\n\n3\n1\n101.90\n56.53\n\n\n3\n2\n103.91\n54.20\n\n\n3\n3\n129.39\n91.74\n\n\n4\n0\n106.44\n90.90\n\n\n4\n1\n138.15\n64.54\n\n\n4\n2\n126.27\n73.29\n\n\n4\n3\n120.39\n59.81\n\n\n5\n0\n141.39\n74.20\n\n\n5\n1\n109.42\n62.05\n\n\n5\n2\n114.40\n51.03\n\n\n5\n3\n73.31\n82.06"
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#primary-transformations",
    "href": "posts/014_14Jun_2024/index.html#primary-transformations",
    "title": "Logistic Regression - Under the Hood",
    "section": "3.1 Primary Transformations",
    "text": "3.1 Primary Transformations\nIf we have the odds of an event, then we can transform that to a probability - or a risk, which is another way to refer to the chance of an outcome - by dividing the odds by the odds plus 1.\n\\[\\text{p} = \\frac{\\text{odds}}{1 + \\text{odds}}\\]\nIf instead we have the probability of an event, we can transform that to the equivalent odds of the event, by dividing the probability by its complement (that is 1 minus the probability).\n\\[\\text{odds} = \\frac{\\text{p}}{1 - \\text{p}}\\]\nAnd the log-odds is simply taking the natural logarithm of the odds - there is nothing difficult about that.\n\\[\\text{log-odds} = \\text{log(odds)}\\]\nSo these three are what I consider primary transformations and we can cycle between them quite easily with a calculator."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#secondary-transformations",
    "href": "posts/014_14Jun_2024/index.html#secondary-transformations",
    "title": "Logistic Regression - Under the Hood",
    "section": "3.2 Secondary Transformations",
    "text": "3.2 Secondary Transformations\nThere are additional transformations that you can make and while these aren‚Äôt quite as straightforward, they are even more fundamental to logistic regression, which ultimately uses the log-odds as the scale to estimate its parameters on.\nThe first is that we can get to a probability directly from the log-odds by taking the exponent of the log-odds divided by the exponent of the log-odds plus one.\n\\[\\text{p} = \\frac{e^{\\text{log-odds}}}{1 + e^{\\text{log-odds}}}\\]\nAnd as you can see, that‚Äôs really no different to the first equation above because the exponent of the log-odds is simply the odds (the reverse transformation of logging a number is to take the exponent of that logged number). i.e.\n\\[\\text{odds} = e^{\\text{log-odds}}\\]\nNow, the final transformation, and the one that‚Äôs most relevant to logistic regression shows the relationship between the log-odds and probability and is known as the logit function (red box) - and we‚Äôll talk more about this in a minute.\n\\[\\text{log-odds} = \\text{log}\\Bigl(\\frac{\\text{p}}{1 - \\text{p}}\\Bigr)\\]"
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#probability-vs-odds-and-as-applied-to-the-research-question",
    "href": "posts/014_14Jun_2024/index.html#probability-vs-odds-and-as-applied-to-the-research-question",
    "title": "Logistic Regression - Under the Hood",
    "section": "3.3 Probability vs Odds (and as applied to the Research Question)",
    "text": "3.3 Probability vs Odds (and as applied to the Research Question)\nLet‚Äôs go back now to our original research question and specifically look at the difference between the probability and odds of an event - I want to solidify this concept in your minds. For most of us probability (or chance) is much more intuitive than the notion of odds. If I think about the likelihood of it being a wet day, I think there‚Äôs (for example) an 80% chance of it raining, rather than 4 to 1 odds in favour of it raining. However, if you like to wager on the horses than you may actually be quite comfortable with thinking in terms of odds - as those are the terms bets are placed in.\nIn the following I‚Äôve simulated some data from the original paper (based on the summary statistics and regression model provided). Obviously macular hole size is a continuous variable, but for the sake of this illustration I am going to dichotomise it at it‚Äôs approximate mean value (480 microns). From this we can draw up a 2 x 2 contingency table of the number of successful and failed macular hole repairs as a function of the dichotomised version of macular hole size.\nWhen we talk about probabilities we focus on the proportion of successes out of the total for each row. So, the probability (or risk) of successful repair for those patients with a macular hole size less than 480 microns is therefore 479/487 or about 0.98. That‚Äôs in contrast to the probability of successful repair in those patients with a size greater than 480 microns - where the chance of success is 380/503 or 0.76 - so lower as one would expect. You can then take a ratio of those two probabilities or risks to give the classic risk ratio and that equals 1.3 which tells you that the chance of successful repair is 30% greater if you have a smaller macular hole at initial presentation.\nNow let‚Äôs look at the equivalent odds. When we talk about odds we focus on the ratio of successes to failures and don‚Äôt really worry about the totals. So the odds of successful repair for those patients with a smaller initial hole is 479/8 or 60 to 1. The odds of success with a larger hole are a lot more modest - 380/123 or about 3 to 1. We can then calculate the classic odds ratio which works out at about 20, meaning the odds of successful repair is about 20 times greater in those with thinner initial holes.\nNote that both of these effect measures are commensurate - they point in exactly the same direction - that you fare better with a smaller hole at diagnosis. They are just different ways of expressing the same thing."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#the-logit-link-function-and-why-log-odds-are-actually-important",
    "href": "posts/014_14Jun_2024/index.html#the-logit-link-function-and-why-log-odds-are-actually-important",
    "title": "Logistic Regression - Under the Hood",
    "section": "4.1 The Logit Link Function (and why log-odds are actually important)",
    "text": "4.1 The Logit Link Function (and why log-odds are actually important)\nThe log-odds or the logit function is pivotal to how logistic regression works. The logit function is a type of link function and there‚Äôs really nothing mysterious or difficult about this. A link function is just a function of the mean of Y - in other words a transformation of the mean of Y - and we use this link function as the outcome instead of Y itself.\n\\[\\text{logit(p)} = \\text{log}\\Bigl(\\frac{\\text{p}}{1 - \\text{p}}\\Bigr)\\] The logistic model then becomes:\n\\[\\text{Y} = \\text{logit(p)} = \\beta_0 \\; + \\; \\beta_1x\\]\nThis allows the outcome to become continuous and unbounded and the association between Y and X to become linear, therefore satisfying those fundamental assumptions of the linear model that I mentioned earlier.\nThus, the logit is a type of transformation that links or maps probability values from \\((0, 1)\\) to real numbers \\({\\displaystyle (-\\infty ,+\\infty )}\\). Now, on the log-odds/logit scale the effect of a one-unit change in X on Y becomes constant once again."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#wth-log-oddsoddsprobabilities---youre-confusing-me---pick-one-please",
    "href": "posts/014_14Jun_2024/index.html#wth-log-oddsoddsprobabilities---youre-confusing-me---pick-one-please",
    "title": "Logistic Regression - Under the Hood",
    "section": "4.2 WTH? log-odds/odds/probabilities - You‚Äôre confusing me - pick one please!!",
    "text": "4.2 WTH? log-odds/odds/probabilities - You‚Äôre confusing me - pick one please!!\nIt‚Äôs important to remember that logistic regression is really about probabilities, not odds. Probabilities are more intuitive than odds (unless you like to put a bet on the horses), so why use odds? Well, model results have historically been expressed in terms of odds and odds ratios for mathematical convenience. As it is for the log-odds scale, the ‚Äòeffect‚Äô of a one-unit change in X on Y is also constant on the odds scale (but as a ratio of two odds, not as a difference in two log-odds). So an odds ratio describes a constant ‚Äòeffect‚Äô of a predictor on the odds of an outcome, irrespective of the value of the predictor. This is not the case on the probability scale, where both differences and ratios vary per unit change depending on the value of the predictor (I will illustrate this later).\nThe TLDR:\n\nModel is estimated (under the hood) on the log-odds scale to satisfy modelling assumptions.\nModel results are displayed on the odds scale (as odds ratios) for their constancy.\nProbabilities are calculated post-hoc with not much extra effort."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#back-to-the-research-question",
    "href": "posts/014_14Jun_2024/index.html#back-to-the-research-question",
    "title": "Logistic Regression - Under the Hood",
    "section": "4.3 Back to the Research Question",
    "text": "4.3 Back to the Research Question\nLet‚Äôs go back to the original research question for the final time and now illustrate these concepts in that context. The regression equation for surgical repair success as a function of macular hole size that was estimated in our paper is shown here.\n\\[\\text{logit(p)} = 10.89 - 0.016 \\; \\text{x} \\;\\text{macular hole size}\\;(\\mu m)\\]\nBasically, the log-odds of successful repair can be predicted by calculating the result of 10.89 minus the product of 0.016 and whatever the macular hole size is in microns.\n\n4.3.1 Simulate Some Data\nWe can simulate some data from that regression equation - essentially reverse engineering fake data from the estimated best fit equation on the actual data. Why do this? Well, it gives us some sense of what the actual data may have looked like that the researchers collected empirically. We can then do all sorts of cool things like refit a model, generate model predictions, etc. Data simulation is something I am still learning more about but it opens up a world of possibilities in your analytical work.\nFor this simulation I used a normal distribution with a mean and SD for macular hole size of 486 and 142, respectively (the numbers came from the original research paper). The code to show how to reproduce this in R is included below but you can do something similar in Stata and in most other scriptable statistical software packages.\n\n\nCode\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(emmeans)\nlibrary(gtsummary)\nlibrary(ggmagnify)\nlibrary(ggpubr)\n# Simulation\nn &lt;- 1000                                          # set number of obs to simulate\nset.seed(1234)                                     # set seed for reproducibility\nmac_hole_size  &lt;-  rnorm(n, 486, 142)              # generate macular hole inner opening data with mean 486 and sd = 152\nlogodds_success  &lt;-  10.89 - 0.016 * mac_hole_size # generate variable that is linear combination of intercept = 10.89 and coefficient for macular hole -0.016 (logit scale)\nodds_success  &lt;-  exp(logodds_success)             # exponentiate logodds to get odds\nprob_success  &lt;-  odds_success/(1 + odds_success)  # generate probabilities from this\ny_success  &lt;-  rbinom(n, 1, prob_success)          # generate outcome variable as a function of those probabilities\ndf &lt;-  data.frame(cbind(mac_hole_size,             # combine into dataframe\n                        logodds_success,\n                        odds_success,\n                        prob_success, \n                        y_success))\ndf &lt;- df |&gt; \n  filter(mac_hole_size &gt; 100)                       # only include those with size &gt; 100\n\n\nEssentially, we generate some random values of macular hole size based on the specified distribution and then plug those values in to the regression equation and calculate the various outcome measures. The resultant data look like (first 20 rows shown):\n\n\nCode\nhead(df, 20) |&gt; \n  kable(align = \"c\", digits = 2) |&gt; \n  kable_styling() |&gt; \n  row_spec(1, bold = T, background = \"skyblue1\") |&gt; \n  row_spec(20, bold = T, background = \"lightgoldenrod1\")\n\n\n\n\n\nmac_hole_size\nlogodds_success\nodds_success\nprob_success\ny_success\n\n\n\n\n314.60\n5.86\n349.48\n1.00\n1\n\n\n525.39\n2.48\n11.99\n0.92\n1\n\n\n639.99\n0.65\n1.92\n0.66\n1\n\n\n152.91\n8.44\n4644.44\n1.00\n1\n\n\n546.94\n2.14\n8.49\n0.89\n1\n\n\n557.86\n1.96\n7.13\n0.88\n1\n\n\n404.39\n4.42\n83.08\n0.99\n1\n\n\n408.38\n4.36\n77.94\n0.99\n1\n\n\n405.85\n4.40\n81.16\n0.99\n1\n\n\n359.61\n5.14\n170.06\n0.99\n1\n\n\n418.24\n4.20\n66.57\n0.99\n1\n\n\n344.23\n5.38\n217.53\n1.00\n1\n\n\n375.77\n4.88\n131.32\n0.99\n1\n\n\n495.15\n2.97\n19.44\n0.95\n1\n\n\n622.25\n0.93\n2.54\n0.72\n1\n\n\n470.34\n3.36\n28.92\n0.97\n1\n\n\n413.44\n4.28\n71.88\n0.99\n1\n\n\n356.61\n5.18\n178.44\n0.99\n1\n\n\n367.12\n5.02\n150.82\n0.99\n1\n\n\n829.05\n-2.37\n0.09\n0.09\n0\n\n\n\n\n\n\n\n\nLet‚Äôs pick a couple of data points to examine more closely.\nIf we look at the first row (blue), the macular hole size is about 315 microns and this translates to a log-odds of successful repair of 5.9 (calculated straight from the regression equation), an odds of successful repair of about 350 to 1 (transformation calculation) and a probability of successful repair of 0.997 (transformation calculation). We can then generate the actual outcome status using that probability as a random variable in a binomial distribution. So in this case a relatively low macular hole size translates to a successful repair, as we might have expected.\nNow if we look at the last row of data (yellow), we see something different. Here we have a simulated size of 829 microns and this translates to a log-odds of successful repair of -2.4, an odds of 1 to 10 against and a probability of less than 0.1. Not surprisingly, the outcome status in this case is simulated as a failure.\n\n\n4.3.2 Plot the Observed Data\nWhat if we now plot the observed data, that is, plot the outcome status as a function of macular hole size?\nI‚Äôm a big advocate of always plotting your data first - before you calculate any descriptive statistics and certainly before running any statistical tests. In fact I‚Äôm not being melodramatic when I say that plotting your data can save you from looking like an analytical fool when you‚Äôre potentially ill-founded assumptions aren‚Äôt realised in the actual data. A good visualisation can reveal these kinds of problems instantly.\nAfter that big recommendation, we might be left feeling slightly deflated when we see that plotting binary data doesn‚Äôt appear that informative - I mean where‚Äôs the nice scatter plot and imaginary trend line that we‚Äôre used to thinking about?\n\n\nCode\n# Plot observed data\nggplot(df, aes(mac_hole_size, y_success)) + \n  geom_point(size = 4, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 1)) +\n  xlab(\"Macular hole size\") + ylab(\"Surgical repair success\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\nWell it‚Äôs not entirely helpless - we can see that there‚Äôs more successes skewed towards smaller macular holes and more failures skewed towards larger macular holes, so that does tell us something useful. But there are better ways to plot binary data and I‚Äôll take you through this shortly.\n\n\n4.3.3 Fit Linear Model (Naive Approach)\nWhat if we fit a standard regression line to binary data, effectively treating Y as continuous? Well people have done this in the past, and in some parts of the data analysis world, still do. It kind of works, but it‚Äôs not the best tool for the job and we can certainly do better. The best fit line is negative suggesting that as macular hole size increases, repair success decreases, so it is in line with what we know to be true. Note that I have rescaled macular hole size so that the resulting coefficient isn‚Äôt so small (now 1 ‚Äòunit‚Äô = 100 microns instead of 1 micron).\n\n\nCode\n# Linear model\nmod_linear &lt;- lm(y_success ~ I(mac_hole_size/100), data = df) \nmod_linear |&gt; \n  tbl_regression(intercept = T)\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n1.5\n1.4, 1.5\n&lt;0.001\n\n\nI(mac_hole_size/100)\n-0.12\n-0.14, -0.11\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nCode\n# Plot data with regression line\nggplot(df, aes(mac_hole_size, y_success)) + \n  geom_point(size = 2, alpha = 0.1) +\n  geom_abline(slope = coef(mod_linear)[[\"I(mac_hole_size/100)\"]]/100, \n              intercept = coef(mod_linear)[[\"(Intercept)\"]], \n              color = \"cornflowerblue\", linewidth = 1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 1)) +\n  xlab(\"Macular hole size\") + ylab(\"Surgical repair success\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.4 Problem - Predictions are made Outside the Probability Domain\nThe problem with naively fitting a standard linear model is that because we are know treating Y as unbounded and continuous, the model no longer knows that the outcome has to be constrained between 0 and 1, and predictions can easily fall outside this range. And that‚Äôs exactly what we can see here - a macular hole size of 100 microns (blue) predicts outcome success at 1.35 and a size of 1200 microns (yellow) predicts outcome success at -0.02. So fitting a standard linear model to binary data is really not a good approach.\n\n\nCode\n# Create new df to predict on new values of x\nnew_dat &lt;- data.frame(mac_hole_size = seq(from = 0, to = 1200, by = 50))\n# Predict new fitted values\npred &lt;- cbind(new_dat, prediction = predict(mod_linear, newdata = new_dat))\n# Display predictions\npred |&gt; \n  kable(align = \"c\", digits = 2) |&gt; \n  kable_styling() |&gt; \n  row_spec(3, bold = T, background = \"skyblue1\") |&gt; \n  row_spec(25, bold = T, background = \"lightgoldenrod1\")\n\n\n\n\n\nmac_hole_size\nprediction\n\n\n\n\n0\n1.47\n\n\n50\n1.41\n\n\n100\n1.35\n\n\n150\n1.29\n\n\n200\n1.22\n\n\n250\n1.16\n\n\n300\n1.10\n\n\n350\n1.04\n\n\n400\n0.98\n\n\n450\n0.91\n\n\n500\n0.85\n\n\n550\n0.79\n\n\n600\n0.73\n\n\n650\n0.66\n\n\n700\n0.60\n\n\n750\n0.54\n\n\n800\n0.48\n\n\n850\n0.42\n\n\n900\n0.35\n\n\n950\n0.29\n\n\n1000\n0.23\n\n\n1050\n0.17\n\n\n1100\n0.10\n\n\n1150\n0.04\n\n\n1200\n-0.02\n\n\n\n\n\n\n\n\n\n4.3.5 Plot the ‚ÄòBinned‚Äô Observed Data\nSo I said above that there was a better way to plot binary data in an effort to make the visualisation more information, and there is. Instead of just plotting the raw data - just the 0‚Äòs and 1‚Äôs - you can create ‚Äôbins‚Äô of data based on (some fairly arbitrary) thresholds of the predictor you are plotting against and then plot the proportion of successes within each bin. Here I created ‚Äòbins‚Äô of data for each 50 \\(\\mu m\\)‚Äôs of macular hole size and then calculated the mean success rate (i.e.¬†the proportion of 1‚Äôs) within each bin.\n\n\nCode\n# Create bins of data for each 50 microns of macular hole size\ndf$bins &lt;- cut(df$mac_hole_size, breaks = seq(0, 1000, by = 50))\n# Calculate means (number of successful repairs divided by total number of repairs) in each bin\ndf &lt;- df |&gt; \n  group_by(bins) |&gt; \n  mutate(mean_y = mean(y_success))\n# Plot observed (averaged) data \nggplot(df, aes(as.numeric(bins)*50-25, mean_y)) + \n  geom_point(size = 2) +\n  geom_line(color = \"orange\", linewidth = 1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Proportion\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\nNow we are starting to see the classic sigmoid shape of the relationship between probability and a continuous predictor.\n\n\n4.3.6 Fit a Logistic Model to the Simulated Data\nNow, let‚Äôs finally fit a logistic model to these simulated data.\n\n\nCode\n# Logistic model\nmod_logistic &lt;- glm(y_success ~ I(mac_hole_size/100), data = df, family = \"binomial\")\nmod_logistic |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\nCharacteristic\nlog(OR)1\n95% CI1\np-value\n\n\n\n\nI(mac_hole_size/100)\n-1.5\n-1.8, -1.3\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nOn the model estimation log-odds scale, the parameter estimate for the effect of macular hole size is -1.5, meaning for every 100 micron increase in macular hole size, the log-odds of success decreases by 1.5 units. All stats software will also provide you with parameter estimates as odds ratios, because that‚Äôs how we‚Äôre accustomed to reporting effects. And the equivalent odds ratio in this case is 0.22. In other words, for every 100 micron increase in macular hole size there is about a 78% reduction in the odds of success.\n\n\nCode\nmod_logistic |&gt; \n  tbl_regression(exponentiate = T)\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nI(mac_hole_size/100)\n0.22\n0.17, 0.28\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nNote that the odds ratio coefficient is a simple transformation of the log-odds coefficient (as we described earlier.) That is:\n\\[e^{-1.5} = 0.22\\]\n\n\n4.3.7 Plot Predicted log-odds of Surgical Repair Success\nNow we can plot the predicted log-odds of surgical repair success. Note that this is a straight line effectively fitted to our binary outcome, and the reason again that we can do this is because the binary outcome has been ‚Äòremapped‚Äô to log-odds and the log-odds exist on an unbounded and continuous scale. So this is a valid plot, unlike that before where we plotted the predictions treating Y as if it were a continuous outcome (I‚Äôm not necessarily suggesting this is a useful plot, just that it‚Äôs valid).\n\n\nCode\n# Predictions\n# Emmeans of logodds at 600 and 700 microns\nemmeans_df &lt;- data.frame(emmeans(mod_logistic, ~ mac_hole_size, at = list(mac_hole_size = c(600, 700, 800))))\n# Create df of emmeans and corresponding odds and probs, for plotting\nemmeans_df &lt;- emmeans_df |&gt; \n  select(mac_hole_size, emmean) |&gt; \n  rename(logodds = emmean) |&gt; \n  mutate(odds = round(exp(logodds), 3),\n         probs = round(plogis(logodds), 2),\n         logodds = round(logodds, 3))\n\n# Predictions ----\n# Create new df to predict on new values of x\nnew_dat &lt;- data.frame(mac_hole_size = seq(from = 0, to = 1200, length.out = 100))\n# Predict new fitted values and SE's on logodds scale\npred_logodds &lt;- predict(mod_logistic, newdata = new_dat, type = \"link\", se = TRUE)\nnew_dat &lt;- cbind(new_dat, pred_logodds)\n# Create new df of predictions\npredictions &lt;- new_dat |&gt; \n  rename(pred_logodds_est = fit) |&gt; \n  mutate(pred_logodds_LL = pred_logodds_est - (1.96 * se.fit),\n         pred_logodds_UL = pred_logodds_est + (1.96 * se.fit)) |&gt; \n  select(-c(se.fit, residual.scale))\n# Predict new fitted values and SE's on odds scale\npredictions &lt;- predictions |&gt; \n  mutate(pred_odds_est = exp(pred_logodds_est),\n         pred_odds_LL = exp(pred_logodds_LL),\n         pred_odds_UL = exp(pred_logodds_UL))\n# Predict new fitted values and SE's on probability scale\npred_probs &lt;- predict(mod_logistic, newdata = new_dat, type = \"response\", se = TRUE)\nnew_dat &lt;- cbind(new_dat[1], pred_probs)\nnew_dat &lt;- new_dat |&gt; \n  mutate(pred_probs_LL = fit - (1.96 * se.fit),\n         pred_probs_UL = fit + (1.96 * se.fit))\n# Add predicted probs and CIs to predictions df\npredictions &lt;- cbind(predictions, \n                     pred_probs_est = new_dat$fit, \n                     pred_probs_LL = new_dat$pred_probs_LL,\n                     pred_probs_UL = new_dat$pred_probs_UL)\n# Plot predicted log-odds\nggplot(predictions, aes(x = mac_hole_size, y = pred_logodds_est)) + \n  geom_ribbon(aes(ymin = pred_logodds_LL, ymax = pred_logodds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-100, 100), breaks = seq(-100, 100, by = 2)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(-8, 8)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted log-odds\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.8 Plot Predicted Odds of Surgical Repair Success\nWe can also plot the predicted odds of surgical repair success. Note that this best-fitting line is not linear, but instead looks like an exponential plot. We established previously that odds exist on a scale from 0 to \\(+\\infty\\) and that‚Äôs what we can appeciate here. Remember that this is just another transformation of the log-odds and the same trend is seen - as hole size increases, the odds of success decline.\n\n\nCode\n# Plot predicted odds\nggplot(predictions, aes(x = mac_hole_size, y = pred_odds_est)) + \n  geom_ribbon(aes(ymin = pred_odds_LL, ymax = pred_odds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1000000), breaks = seq(0, 1000000, by = 1000)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(0, 7000)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted odds\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.9 Plot Predicted Probabilities of Surgical Repair Success\nAnd we can also plot the predicted probabilities of surgical repair success. Again this is just another transformation of the log-odds or the odds. Note the classic sigmoid shape which emulates what we saw earlier when plotting the binned raw data. To my mind this plot is more useful than either plots of predicted log-odds or odds, as it tells us the likely chance of successful repair conditional on the initial macular hole size.\n\n\nCode\n# Plot predicted probabilities\nggplot(predictions, aes(x = mac_hole_size, y = pred_probs_est)) + \n  geom_ribbon(aes(ymin = pred_probs_LL, ymax = pred_probs_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted probability\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.10 Plot Observed and Predicted Probabilities of Surgical Repair Success\nAnd in fact what you should ideally find if you plot both your observed and predicted probabilities is that they are quite similar, reflecting that the model actually fits the data quite well. Of course, in this case the convergence of observed and predicted probabilities is unsurprising given that we know the data-generating proces - i.e.¬†we fitted a model to data that came from a model. But, in general, this is a good way to assess your model fit.\n\n\nCode\n# Plot observed and predicted data\nggplot(df, aes(as.numeric(bins)*50-25, mean_y)) + \n  geom_point(size = 2) +\n  geom_line(aes(color = \"observed\"), linewidth = 1) +\n  geom_line(data = predictions, aes(x = mac_hole_size, y = pred_probs_est, color = \"predicted\"), linewidth = 1) +\n  scale_color_manual(name = \"\", values = c(\"predicted\" = \"cornflowerblue\", \"observed\" = \"orange\")) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Probability\") +\n  theme_bw(base_size = 25) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n4.3.11 Putting It All Together\nWhat I want to try and reinforce in the figure below is how the log-odds, the odds and the probability of the outcome are all transformations of each other.\nIn logistic regression we are ultimately interested in the probability of the outcome occurring. But we use the log-odds or logit link function to map probabilities onto a linear real-number range. This means that the change in Y for each unit increase in X remains the same and we can see that in the top plot. So the log-odds coefficient that our stats software gives us is constant across all values of macular hole size. This represents the difference in the log-odds of success for each unit change in X.\nOne unit increase in log-odds:\n\n600 -&gt; 700 \\(\\mu m\\)\n= -0.181 - 1.323 = -1.504\n\n700 -&gt; 800 \\(\\mu m\\)\n= -1.686 - -0.181 = -1.505\n\n\nThe middle plot shows the equivalent odds of success across the same range of macular hole size. Here the difference in Y is no longer constant for each unit increase in X, but the commensurate ratio is constant - and this gives us the odds ratio. This means that the odds ratio is constant across all values of macular hole size.\nOne unit increase in odds:\n\n600 -&gt; 700 \\(\\mu m\\)\n= 0.834/3.756 = 0.22\n\n700 -&gt; 800 \\(\\mu m\\)\n= 0.185/0.834 = 0.22\n\n\nFinally the lower plot shows the equivalent probabilities of success and now there is no constancy in either unit differences or ratios. So, if we are interested in expressing our logistic regression model results as differences or ratios of predicted probabilities, we need to make sure that we qualify the macular hole size that a particular predicted probability corresponds to, as this will change across the range of X.\nOne unit increase in probability:\n\n600 -&gt; 700 \\(\\mu m\\)\n= 0.45 - 0.79 = -0.34\n= 0.45/0.79 = 0.57\n\n700 -&gt; 800 \\(\\mu m\\)\n= 0.16 - 0.45 = -0.29\n= 0.16/0.45 = 0.36\n\n\n\n\nCode\n# Reformat plots slightly for ggarrange ----\np1 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_logodds_est)) + \n  geom_ribbon(aes(ymin = pred_logodds_LL, ymax = pred_logodds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1150, y = 6, label = \"log-odds\", size = 10) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  annotate(\"text\", x = 1110, y = 3.5, label = \"unit differences = constant\", size = 5) +\n  scale_y_continuous(limits = c(-100, 100), breaks = seq(-100, 100, by = 2)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(-8, 8)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, logodds),\n                            label = emmeans_df$logodds, \n                            nudge_x = c(50, 50, 50), nudge_y = c(4, 4, 4),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  theme_bw(base_size = 20) +\n  ylab(\"\") +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\n\np2 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_odds_est)) + \n  geom_ribbon(aes(ymin = pred_odds_LL, ymax = pred_odds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1180, y = 6000, label = \"odds\", size = 10) +\n  annotate(\"text\", x = 1136, y = 5000, label = \"unit ratios = constant\", size = 5) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(seq(0, 1000000, by = 1000))) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(0, 7000)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ylab(\"\") +\n  theme_bw(base_size = 20) +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\np2_inset &lt;- p2 +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(1,2,3,4,5, seq(0, 1000000, by = 1000))) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, odds),\n                            label = emmeans_df$odds, \n                            nudge_x = c(50, 50, 50), nudge_y = c(0.5, 1, 1),\n                            color = \"red\", segment.size = 0.2, size = 5)\np2 &lt;- p2 + geom_magnify(from = c(xmin = 500, xmax = 1000, ymin = 0, ymax = 5), \n                          to = c(xmin = 470, xmax = 1010, ymin = 1000, ymax = 5000), \n                          shadow = T, axes = \"y\", plot = p2_inset)\n\np3 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_probs_est)) + \n  geom_ribbon(aes(ymin = pred_probs_LL, ymax = pred_probs_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1140, y = 0.8, label = \"probability\", size = 10) +\n  annotate(\"text\", x = 1075, y = 0.65, label = \"unit differences/ratios = not constant\", size = 5) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, probs),\n                            label = emmeans_df$probs, \n                            nudge_x = c(50, 50, 50), nudge_y = c(0.1, 0.1, 0.1),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  ylab(\"\") + xlab(\"Macular hole size\") +\n  theme_bw(base_size = 20)\nggarrange(p1, p2, p3, align = \"v\", ncol = 1, heights = c(1,1,1.2))\n\n\n\n\n\n\n\n\n\n\n\n4.3.12 Key points\nThis has been a long post but we are finally at the end. I hope that reading this has demystified what goes on under the hood in logistic regression as it‚Äôs really not that difficult once you understand the purpose of the link function.\nThese are the main summary points:\n\nLogistic regression is a type of generalised linear model that allows the estimation of associations of potential predictors with a binary outcome.\nThe logit link function transforms the probabilities of a binary outcome variable to a continuous, unbounded scale (log-odds). Standard linear modelling methods can then be applied to estimate the model.\nLog-odds, odds and probabilities are all simple transformations of one another.\nThe model is estimated on the log-odds scale where the association between the predictor and the log-odds of the outcome is linear, and the unit difference is constant.\nResults may be reported in terms of odds or probabilities:\n\nOn the odds scale the association between the predictor and the odds of the outcome is non-linear, but the unit ratio is constant.\nOn the probability scale the association between the predictor and the probability of the outcome is non-linear, and the unit difference/ratio is not constant.\nThis means probability estimates (risk differences of ratios) depend on the value of the predictor and this needs to be specified."
  },
  {
    "objectID": "posts/013_31May_2024/index.html#case-1-one-name-column-one-value-column",
    "href": "posts/013_31May_2024/index.html#case-1-one-name-column-one-value-column",
    "title": "Reshaping Data - Think Before you Pivot",
    "section": "2.1 Case 1: One Name Column, One Value Column",
    "text": "2.1 Case 1: One Name Column, One Value Column\nThis is the most common and most straight-forward application of reshaping to long that you might be required to perform. In this case we have multiple value columns in wide format that we want to reshape to one name column and one value column. The good news is that we have already done this in the example above. The code is also shown but just to revisit that briefly for clarity:\n\ndf_long &lt;- df_wide |&gt; \n  pivot_longer(cols = month_0:month_3,\n               names_to = \"month\",\n               names_prefix = \"month_\",\n               values_to = \"bp\")\n\nSo we take the df_wide dataframe and ‚Äòpipe‚Äô it to the pivot_longer() function where we specify that we want to take the columns from (and including) month_0 to month_3, assigning those column names as category labels in the new month name variable, while also placing each corresponding BP measurement into the new bp value variable. The names_prefix argument is optional but was used here to strip out the somewhat redundant month_ text from each column name prior to labelling. You could certainly leave this in if you wanted and the result would then be:\n\n\n\n\n\nid\nmonth\nbp\n\n\n\n\n1\nmonth_0\n88.29\n\n\n1\nmonth_1\n145.65\n\n\n1\nmonth_2\n100.05\n\n\n1\nmonth_3\n132.61\n\n\n2\nmonth_0\n134.50\n\n\n2\nmonth_1\n140.99\n\n\n2\nmonth_2\n115.06\n\n\n2\nmonth_3\n157.35\n\n\n\n\n\nBut I‚Äôm sure you‚Äôd agree that the results looks cleaner without all that unnecessary repetition."
  },
  {
    "objectID": "posts/013_31May_2024/index.html#case-2-multiple-name-columns-one-value-column",
    "href": "posts/013_31May_2024/index.html#case-2-multiple-name-columns-one-value-column",
    "title": "Reshaping Data - Think Before you Pivot",
    "section": "2.2 Case 2: Multiple Name Columns, One Value Column",
    "text": "2.2 Case 2: Multiple Name Columns, One Value Column\nLet‚Äôs now extend this idea a little. Imagine that in addition to BP measurements, subjects also had their weight measured at the same time points (simulated with a mean of 70 kg and SD 15 kg). Now we have data that could potentially look like:\n\n\nCode\nid &lt;- seq(1:5)\nfor(i in 0:3){\n  var_name_bp &lt;- paste0(\"month_\",i,\"_bp\")\n  assign(var_name_bp, rnorm(5, 130, 20))\n  var_name_wt &lt;- paste0(\"month_\",i,\"_wt\")\n  assign(var_name_wt, rnorm(5, 70, 15))\n}\ndf_wide &lt;- data.frame(cbind(id, month_0_bp, month_0_wt, month_1_bp, month_1_wt, month_2_bp, month_2_wt, month_3_bp, month_3_wt))\ndf_wide |&gt;  \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nmonth_0_bp\nmonth_0_wt\nmonth_1_bp\nmonth_1_wt\nmonth_2_bp\nmonth_2_wt\nmonth_3_bp\nmonth_3_wt\n\n\n\n\n1\n168.71\n70.94\n110.54\n62.65\n141.74\n64.22\n117.43\n42.37\n\n\n2\n99.66\n86.30\n103.43\n69.65\n148.09\n69.11\n139.75\n60.50\n\n\n3\n122.44\n72.88\n120.12\n68.22\n105.90\n83.36\n146.80\n70.25\n\n\n4\n116.68\n52.53\n136.44\n70.32\n123.79\n71.43\n88.53\n51.79\n\n\n5\n122.52\n59.85\n145.43\n80.02\n157.58\n60.87\n137.32\n91.14\n\n\n\n\n\nWhat to do here?\nActually, some thought is required at this point as there are two potential paths you could go down and it all depends on what you want to achieve. Let‚Äôs assume that you want to put all measurement values in one column. Once you have decided on this final form, the code is not challenging. We will necessarily end up with two names columns instead of just one, one for time (month) and one for the all the clinical measures (BP and weight). The main changes to the code are to now supply two new variable names to the names_to argument as well as tell the function how to source the new category labels with the names_sep argument. This will split the currently wide variable names at the second _ (after stripping out the redundant month_ text) and use the number as the month label and the type of measurement as the clinical measure label.\n\ndf_long &lt;- df_wide |&gt; \n  pivot_longer(cols = month_0_bp:month_3_wt,\n               names_to = c(\"month\", \"clinical_measure\"),\n               names_prefix = \"month_\",\n               names_sep = \"_\",\n               values_to = \"value\")\n\nand the data looks like:\n\n\nCode\ndf_long |&gt;  \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nid\nmonth\nclinical_measure\nvalue\n\n\n\n\n1\n0\nbp\n168.71\n\n\n1\n0\nwt\n70.94\n\n\n1\n1\nbp\n110.54\n\n\n1\n1\nwt\n62.65\n\n\n1\n2\nbp\n141.74\n\n\n1\n2\nwt\n64.22\n\n\n1\n3\nbp\n117.43\n\n\n1\n3\nwt\n42.37\n\n\n2\n0\nbp\n99.66\n\n\n2\n0\nwt\n86.30\n\n\n2\n1\nbp\n103.43\n\n\n2\n1\nwt\n69.65\n\n\n2\n2\nbp\n148.09\n\n\n2\n2\nwt\n69.11\n\n\n2\n3\nbp\n139.75\n\n\n2\n3\nwt\n60.50\n\n\n3\n0\nbp\n122.44\n\n\n3\n0\nwt\n72.88\n\n\n3\n1\nbp\n120.12\n\n\n3\n1\nwt\n68.22\n\n\n3\n2\nbp\n105.90\n\n\n3\n2\nwt\n83.36\n\n\n3\n3\nbp\n146.80\n\n\n3\n3\nwt\n70.25\n\n\n4\n0\nbp\n116.68\n\n\n4\n0\nwt\n52.53\n\n\n4\n1\nbp\n136.44\n\n\n4\n1\nwt\n70.32\n\n\n4\n2\nbp\n123.79\n\n\n4\n2\nwt\n71.43\n\n\n4\n3\nbp\n88.53\n\n\n4\n3\nwt\n51.79\n\n\n5\n0\nbp\n122.52\n\n\n5\n0\nwt\n59.85\n\n\n5\n1\nbp\n145.43\n\n\n5\n1\nwt\n80.02\n\n\n5\n2\nbp\n157.58\n\n\n5\n2\nwt\n60.87\n\n\n5\n3\nbp\n137.32\n\n\n5\n3\nwt\n91.14\n\n\n\n\n\nI tend to think of this as a complete reshaping to long format."
  },
  {
    "objectID": "posts/013_31May_2024/index.html#case-3-one-name-column-multiple-value-columns",
    "href": "posts/013_31May_2024/index.html#case-3-one-name-column-multiple-value-columns",
    "title": "Reshaping Data - Think Before you Pivot",
    "section": "2.3 Case 3: One Name Column, Multiple Value Columns",
    "text": "2.3 Case 3: One Name Column, Multiple Value Columns\nBut what if didn‚Äôt want to do this and instead wanted the values of BP and weight to appear in their own columns - a partial reshaping to long format if you like. To my mind this is probably a more useful long format than what we considered in the last example, although there may be some niche use-case scenarios that require data to be in that format for analysis (they just elude me right now).\nSo let‚Äôs now assume that you want separate columns of values for each type of measurement. Now we will end up with one name column and two value columns - one for BP and one for weight. The general form of the code doesn‚Äôt change a lot in this case - the main thing being that we replace ‚Äúclinical_measure‚Äù in the names_to argument with a special term .value which indicates that the pivoted (new) columns will be split by the text after the second _ in the currently wide column names - i.e.¬†taking on the value of bp or wt (so it‚Äôs not necessary to specify a values_to term this time around). In those two new columns the corresponding measurement values will be placed. The code looks like:\n\ndf_long &lt;- df_wide |&gt; \n  pivot_longer(cols = month_0_bp:month_3_wt,\n               names_to = c(\"month\", \".value\"),\n               names_prefix = \"month_\",\n               names_sep = \"_\")\n\nand the data now looks like:\n\n\nCode\ndf_long |&gt;  \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nid\nmonth\nbp\nwt\n\n\n\n\n1\n0\n168.71\n70.94\n\n\n1\n1\n110.54\n62.65\n\n\n1\n2\n141.74\n64.22\n\n\n1\n3\n117.43\n42.37\n\n\n2\n0\n99.66\n86.30\n\n\n2\n1\n103.43\n69.65\n\n\n2\n2\n148.09\n69.11\n\n\n2\n3\n139.75\n60.50\n\n\n3\n0\n122.44\n72.88\n\n\n3\n1\n120.12\n68.22\n\n\n3\n2\n105.90\n83.36\n\n\n3\n3\n146.80\n70.25\n\n\n4\n0\n116.68\n52.53\n\n\n4\n1\n136.44\n70.32\n\n\n4\n2\n123.79\n71.43\n\n\n4\n3\n88.53\n51.79\n\n\n5\n0\n122.52\n59.85\n\n\n5\n1\n145.43\n80.02\n\n\n5\n2\n157.58\n60.87\n\n\n5\n3\n137.32\n91.14"
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html",
    "href": "posts/015_28Jun_2024/index.html",
    "title": "Academic Research - ‚ÄúWhat‚Äôs in a Title?‚Äù",
    "section": "",
    "text": "So that was a pretty poor attempt to adapt a famous adage from Romeo and Juliet - ‚ÄúWhat‚Äôs in a name? That which we call a rose by any other name would smell just as sweet‚Äù - for my own expository purposes.\nBefore taking a short mid-year break (and also because I didn‚Äôt have time to prepare anything formal this week), I thought we could have a little fun in taking a look at some clever, bizarre, and in some cases laugh-out-loud titles that academic researchers have published with over the years.\nBut we can‚Äôt have fun without a little science as well. So, is there actually anything to be gained by giving your research paper a less serious title? Interestingly, there is a recent preprint paper out there that has attempted to assess this relationship - specifically between humour in article titles and the all-important citation impact.\nIf this title is funny, will you cite me? Citation impacts of humour and other features of article titles in ecology and evolution (and the Nature opinion piece of the same paper).\nThe TL;DR is that articles with humourous titles tend to be cited less, but article ‚Äòimportance‚Äô was considered to be a confounder in this association, and once adjustment was made for that, articles with humourous titles were in fact cited more. If you‚Äôre interested, have a read of the paper. I‚Äôm not that convinced as the authors used self-citation as a proxy for their definition of importance and I‚Äôm sure there are better ways to assess this.\nBut anyway, now that the science is out of the way, let‚Äôs get to the fun bit. These are a collection of papers that I‚Äôve come across over the years and no doubt some will be familiar to you, but please indulge me anyway. Many of these have used clever wordplay or puns, but some are also serious (and unfortunate) titles.\nI am going to try my best to broadly group them by some linking theme, if that is even possible‚Ä¶."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#primary-transformations",
    "href": "posts/015_28Jun_2024/index.html#primary-transformations",
    "title": "Academic Research - ‚ÄúWhat‚Äôs in a Title‚Äù",
    "section": "3.1 Primary Transformations",
    "text": "3.1 Primary Transformations\nIf we have the odds of an event, then we can transform that to a probability - or a risk, which is another way to refer to the chance of an outcome - by dividing the odds by the odds plus 1.\n\\[\\text{p} = \\frac{\\text{odds}}{1 + \\text{odds}}\\]\nIf instead we have the probability of an event, we can transform that to the equivalent odds of the event, by dividing the probability by its complement (that is 1 minus the probability).\n\\[\\text{odds} = \\frac{\\text{p}}{1 - \\text{p}}\\]\nAnd the log-odds is simply taking the natural logarithm of the odds - there is nothing difficult about that.\n\\[\\text{log-odds} = \\text{log(odds)}\\]\nSo these three are what I consider primary transformations and we can cycle between them quite easily with a calculator."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#secondary-transformations",
    "href": "posts/015_28Jun_2024/index.html#secondary-transformations",
    "title": "Academic Research - ‚ÄúWhat‚Äôs in a Title‚Äù",
    "section": "3.2 Secondary Transformations",
    "text": "3.2 Secondary Transformations\nThere are additional transformations that you can make and while these aren‚Äôt quite as straightforward, they are even more fundamental to logistic regression, which ultimately uses the log-odds as the scale to estimate its parameters on.\nThe first is that we can get to a probability directly from the log-odds by taking the exponent of the log-odds divided by the exponent of the log-odds plus one.\n\\[\\text{p} = \\frac{e^{\\text{log-odds}}}{1 + e^{\\text{log-odds}}}\\]\nAnd as you can see, that‚Äôs really no different to the first equation above because the exponent of the log-odds is simply the odds (the reverse transformation of logging a number is to take the exponent of that logged number). i.e.\n\\[\\text{odds} = e^{\\text{log-odds}}\\]\nNow, the final transformation, and the one that‚Äôs most relevant to logistic regression shows the relationship between the log-odds and probability and is known as the logit function (red box) - and we‚Äôll talk more about this in a minute.\n\\[\\text{log-odds} = \\text{log}\\Bigl(\\frac{\\text{p}}{1 - \\text{p}}\\Bigr)\\]"
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#probability-vs-odds-and-as-applied-to-the-research-question",
    "href": "posts/015_28Jun_2024/index.html#probability-vs-odds-and-as-applied-to-the-research-question",
    "title": "Academic Research - ‚ÄúWhat‚Äôs in a Title‚Äù",
    "section": "3.3 Probability vs Odds (and as applied to the Research Question)",
    "text": "3.3 Probability vs Odds (and as applied to the Research Question)\nLet‚Äôs go back now to our original research question and specifically look at the difference between the probability and odds of an event - I want to solidify this concept in your minds. For most of us probability (or chance) is much more intuitive than the notion of odds. If I think about the likelihood of it being a wet day, I think there‚Äôs (for example) an 80% chance of it raining, rather than 4 to 1 odds in favour of it raining. However, if you like to wager on the horses than you may actually be quite comfortable with thinking in terms of odds - as those are the terms bets are placed in.\nIn the following I‚Äôve simulated some data from the original paper (based on the summary statistics and regression model provided). Obviously macular hole size is a continuous variable, but for the sake of this illustration I am going to dichotomise it at it‚Äôs approximate mean value (480 microns). From this we can draw up a 2 x 2 contingency table of the number of successful and failed macular hole repairs as a function of the dichotomised version of macular hole size.\nWhen we talk about probabilities we focus on the proportion of successes out of the total for each row. So, the probability (or risk) of successful repair for those patients with a macular hole size less than 480 microns is therefore 479/487 or about 0.98. That‚Äôs in contrast to the probability of successful repair in those patients with a size greater than 480 microns - where the chance of success is 380/503 or 0.76 - so lower as one would expect. You can then take a ratio of those two probabilities or risks to give the classic risk ratio and that equals 1.3 which tells you that the chance of successful repair is 30% greater if you have a smaller macular hole at initial presentation.\nNow let‚Äôs look at the equivalent odds. When we talk about odds we focus on the ratio of successes to failures and don‚Äôt really worry about the totals. So the odds of successful repair for those patients with a smaller initial hole is 479/8 or 60 to 1. The odds of success with a larger hole are a lot more modest - 380/123 or about 3 to 1. We can then calculate the classic odds ratio which works out at about 20, meaning the odds of successful repair is about 20 times greater in those with thinner initial holes.\nNote that both of these effect measures are commensurate - they point in exactly the same direction - that you fare better with a smaller hole at diagnosis. They are just different ways of expressing the same thing."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#the-logit-link-function-and-why-log-odds-are-actually-important",
    "href": "posts/015_28Jun_2024/index.html#the-logit-link-function-and-why-log-odds-are-actually-important",
    "title": "Academic Research - ‚ÄúWhat‚Äôs in a Title‚Äù",
    "section": "4.1 The Logit Link Function (and why log-odds are actually important)",
    "text": "4.1 The Logit Link Function (and why log-odds are actually important)\nThe log-odds or the logit function is pivotal to how logistic regression works. The logit function is a type of link function and there‚Äôs really nothing mysterious or difficult about this. A link function is just a function of the mean of Y - in other words a transformation of the mean of Y - and we use this link function as the outcome instead of Y itself.\n\\[\\text{logit(p)} = \\text{log}\\Bigl(\\frac{\\text{p}}{1 - \\text{p}}\\Bigr)\\] The logistic model then becomes:\n\\[\\text{Y} = \\text{logit(p)} = \\beta_0 \\; + \\; \\beta_1x\\]\nThis allows the outcome to become continuous and unbounded and the association between Y and X to become linear, therefore satisfying those fundamental assumptions of the linear model that I mentioned earlier.\nThus, the logit is a type of transformation that links or maps probability values from \\((0, 1)\\) to real numbers \\({\\displaystyle (-\\infty ,+\\infty )}\\). Now, on the log-odds/logit scale the effect of a one-unit change in X on Y becomes constant once again."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#wth-log-oddsoddsprobabilities---youre-confusing-me---pick-one-please",
    "href": "posts/015_28Jun_2024/index.html#wth-log-oddsoddsprobabilities---youre-confusing-me---pick-one-please",
    "title": "Academic Research - ‚ÄúWhat‚Äôs in a Title‚Äù",
    "section": "4.2 WTH? log-odds/odds/probabilities - You‚Äôre confusing me - pick one please!!",
    "text": "4.2 WTH? log-odds/odds/probabilities - You‚Äôre confusing me - pick one please!!\nIt‚Äôs important to remember that logistic regression is really about probabilities, not odds. Probabilities are more intuitive than odds (unless you like to put a bet on the horses), so why use odds? Well, model results have historically been expressed in terms of odds and odds ratios for mathematical convenience. As it is for the log-odds scale, the ‚Äòeffect‚Äô of a one-unit change in X on Y is also constant on the odds scale (but as a ratio of two odds, not as a difference in two log-odds). So an odds ratio describes a constant ‚Äòeffect‚Äô of a predictor on the odds of an outcome, irrespective of the value of the predictor. This is not the case on the probability scale, where both differences and ratios vary per unit change depending on the value of the predictor (I will illustrate this later).\nThe TLDR:\n\nModel is estimated (under the hood) on the log-odds scale to satisfy modelling assumptions.\nModel results are displayed on the odds scale (as odds ratios) for their constancy.\nProbabilities are calculated post-hoc with not much extra effort."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#back-to-the-research-question",
    "href": "posts/015_28Jun_2024/index.html#back-to-the-research-question",
    "title": "Academic Research - ‚ÄúWhat‚Äôs in a Title‚Äù",
    "section": "4.3 Back to the Research Question",
    "text": "4.3 Back to the Research Question\nLet‚Äôs go back to the original research question for the final time and now illustrate these concepts in that context. The regression equation for surgical repair success as a function of macular hole size that was estimated in our paper is shown here.\n\\[\\text{logit(p)} = 10.89 - 0.016 \\; \\text{x} \\;\\text{macular hole size}\\;(\\mu m)\\]\nBasically, the log-odds of successful repair can be predicted by calculating the result of 10.89 minus the product of 0.016 and whatever the macular hole size is in microns.\n\n4.3.1 Simulate Some Data\nWe can simulate some data from that regression equation - essentially reverse engineering fake data from the estimated best fit equation on the actual data. Why do this? Well, it gives us some sense of what the actual data may have looked like that the researchers collected empirically. We can then do all sorts of cool things like refit a model, generate model predictions, etc. Data simulation is something I am still learning more about but it opens up a world of possibilities in your analytical work.\nFor this simulation I used a normal distribution with a mean and SD for macular hole size of 486 and 142, respectively (the numbers came from the original research paper). The code to show how to reproduce this in R is included below but you can do something similar in Stata and in most other scriptable statistical software packages.\n\n\nCode\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(emmeans)\nlibrary(gtsummary)\nlibrary(ggmagnify)\nlibrary(ggpubr)\n# Simulation\nn &lt;- 1000                                          # set number of obs to simulate\nset.seed(1234)                                     # set seed for reproducibility\nmac_hole_size  &lt;-  rnorm(n, 486, 142)              # generate macular hole inner opening data with mean 486 and sd = 152\nlogodds_success  &lt;-  10.89 - 0.016 * mac_hole_size # generate variable that is linear combination of intercept = 10.89 and coefficient for macular hole -0.016 (logit scale)\nodds_success  &lt;-  exp(logodds_success)             # exponentiate logodds to get odds\nprob_success  &lt;-  odds_success/(1 + odds_success)  # generate probabilities from this\ny_success  &lt;-  rbinom(n, 1, prob_success)          # generate outcome variable as a function of those probabilities\ndf &lt;-  data.frame(cbind(mac_hole_size,             # combine into dataframe\n                        logodds_success,\n                        odds_success,\n                        prob_success, \n                        y_success))\ndf &lt;- df |&gt; \n  filter(mac_hole_size &gt; 100)                       # only include those with size &gt; 100\n\n\nEssentially, we generate some random values of macular hole size based on the specified distribution and then plug those values in to the regression equation and calculate the various outcome measures. The resultant data look like (first 20 rows shown):\n\n\nCode\nhead(df, 20) |&gt; \n  kable(align = \"c\", digits = 2) |&gt; \n  kable_styling() |&gt; \n  row_spec(1, bold = T, background = \"skyblue1\") |&gt; \n  row_spec(20, bold = T, background = \"lightgoldenrod1\")\n\n\n\n\n\nmac_hole_size\nlogodds_success\nodds_success\nprob_success\ny_success\n\n\n\n\n314.60\n5.86\n349.48\n1.00\n1\n\n\n525.39\n2.48\n11.99\n0.92\n1\n\n\n639.99\n0.65\n1.92\n0.66\n1\n\n\n152.91\n8.44\n4644.44\n1.00\n1\n\n\n546.94\n2.14\n8.49\n0.89\n1\n\n\n557.86\n1.96\n7.13\n0.88\n1\n\n\n404.39\n4.42\n83.08\n0.99\n1\n\n\n408.38\n4.36\n77.94\n0.99\n1\n\n\n405.85\n4.40\n81.16\n0.99\n1\n\n\n359.61\n5.14\n170.06\n0.99\n1\n\n\n418.24\n4.20\n66.57\n0.99\n1\n\n\n344.23\n5.38\n217.53\n1.00\n1\n\n\n375.77\n4.88\n131.32\n0.99\n1\n\n\n495.15\n2.97\n19.44\n0.95\n1\n\n\n622.25\n0.93\n2.54\n0.72\n1\n\n\n470.34\n3.36\n28.92\n0.97\n1\n\n\n413.44\n4.28\n71.88\n0.99\n1\n\n\n356.61\n5.18\n178.44\n0.99\n1\n\n\n367.12\n5.02\n150.82\n0.99\n1\n\n\n829.05\n-2.37\n0.09\n0.09\n0\n\n\n\n\n\n\n\n\nLet‚Äôs pick a couple of data points to examine more closely.\nIf we look at the first row (blue), the macular hole size is about 315 microns and this translates to a log-odds of successful repair of 5.9 (calculated straight from the regression equation), an odds of successful repair of about 350 to 1 (transformation calculation) and a probability of successful repair of 0.997 (transformation calculation). We can then generate the actual outcome status using that probability as a random variable in a binomial distribution. So in this case a relatively low macular hole size translates to a successful repair, as we might have expected.\nNow if we look at the last row of data (yellow), we see something different. Here we have a simulated size of 829 microns and this translates to a log-odds of successful repair of -2.4, an odds of 1 to 10 against and a probability of less than 0.1. Not surprisingly, the outcome status in this case is simulated as a failure.\n\n\n4.3.2 Plot the Observed Data\nWhat if we now plot the observed data, that is, plot the outcome status as a function of macular hole size?\nI‚Äôm a big advocate of always plotting your data first - before you calculate any descriptive statistics and certainly before running any statistical tests. In fact I‚Äôm not being melodramatic when I say that plotting your data can save you from looking like an analytical fool when you‚Äôre potentially ill-founded assumptions aren‚Äôt realised in the actual data. A good visualisation can reveal these kinds of problems instantly.\nAfter that big recommendation, we might be left feeling slightly deflated when we see that plotting binary data doesn‚Äôt appear that informative - I mean where‚Äôs the nice scatter plot and imaginary trend line that we‚Äôre used to thinking about?\n\n\nCode\n# Plot observed data\nggplot(df, aes(mac_hole_size, y_success)) + \n  geom_point(size = 4, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 1)) +\n  xlab(\"Macular hole size\") + ylab(\"Surgical repair success\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\nWell it‚Äôs not entirely helpless - we can see that there‚Äôs more successes skewed towards smaller macular holes and more failures skewed towards larger macular holes, so that does tell us something useful. But there are better ways to plot binary data and I‚Äôll take you through this shortly.\n\n\n4.3.3 Fit Linear Model (Naive Approach)\nWhat if we fit a standard regression line to binary data, effectively treating Y as continuous? Well people have done this in the past, and in some parts of the data analysis world, still do. It kind of works, but it‚Äôs not the best tool for the job and we can certainly do better. The best fit line is negative suggesting that as macular hole size increases, repair success decreases, so it is in line with what we know to be true. Note that I have rescaled macular hole size so that the resulting coefficient isn‚Äôt so small (now 1 ‚Äòunit‚Äô = 100 microns instead of 1 micron).\n\n\nCode\n# Linear model\nmod_linear &lt;- lm(y_success ~ I(mac_hole_size/100), data = df) \nmod_linear |&gt; \n  tbl_regression(intercept = T)\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n1.5\n1.4, 1.5\n&lt;0.001\n\n\nI(mac_hole_size/100)\n-0.12\n-0.14, -0.11\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nCode\n# Plot data with regression line\nggplot(df, aes(mac_hole_size, y_success)) + \n  geom_point(size = 2, alpha = 0.1) +\n  geom_abline(slope = coef(mod_linear)[[\"I(mac_hole_size/100)\"]]/100, \n              intercept = coef(mod_linear)[[\"(Intercept)\"]], \n              color = \"cornflowerblue\", linewidth = 1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 1)) +\n  xlab(\"Macular hole size\") + ylab(\"Surgical repair success\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.4 Problem - Predictions are made Outside the Probability Domain\nThe problem with naively fitting a standard linear model is that because we are know treating Y as unbounded and continuous, the model no longer knows that the outcome has to be constrained between 0 and 1, and predictions can easily fall outside this range. And that‚Äôs exactly what we can see here - a macular hole size of 100 microns (blue) predicts outcome success at 1.35 and a size of 1200 microns (yellow) predicts outcome success at -0.02. So fitting a standard linear model to binary data is really not a good approach.\n\n\nCode\n# Create new df to predict on new values of x\nnew_dat &lt;- data.frame(mac_hole_size = seq(from = 0, to = 1200, by = 50))\n# Predict new fitted values\npred &lt;- cbind(new_dat, prediction = predict(mod_linear, newdata = new_dat))\n# Display predictions\npred |&gt; \n  kable(align = \"c\", digits = 2) |&gt; \n  kable_styling() |&gt; \n  row_spec(3, bold = T, background = \"skyblue1\") |&gt; \n  row_spec(25, bold = T, background = \"lightgoldenrod1\")\n\n\n\n\n\nmac_hole_size\nprediction\n\n\n\n\n0\n1.47\n\n\n50\n1.41\n\n\n100\n1.35\n\n\n150\n1.29\n\n\n200\n1.22\n\n\n250\n1.16\n\n\n300\n1.10\n\n\n350\n1.04\n\n\n400\n0.98\n\n\n450\n0.91\n\n\n500\n0.85\n\n\n550\n0.79\n\n\n600\n0.73\n\n\n650\n0.66\n\n\n700\n0.60\n\n\n750\n0.54\n\n\n800\n0.48\n\n\n850\n0.42\n\n\n900\n0.35\n\n\n950\n0.29\n\n\n1000\n0.23\n\n\n1050\n0.17\n\n\n1100\n0.10\n\n\n1150\n0.04\n\n\n1200\n-0.02\n\n\n\n\n\n\n\n\n\n4.3.5 Plot the ‚ÄòBinned‚Äô Observed Data\nSo I said above that there was a better way to plot binary data in an effort to make the visualisation more information, and there is. Instead of just plotting the raw data - just the 0‚Äòs and 1‚Äôs - you can create ‚Äôbins‚Äô of data based on (some fairly arbitrary) thresholds of the predictor you are plotting against and then plot the proportion of successes within each bin. Here I created ‚Äòbins‚Äô of data for each 50 \\(\\mu m\\)‚Äôs of macular hole size and then calculated the mean success rate (i.e.¬†the proportion of 1‚Äôs) within each bin.\n\n\nCode\n# Create bins of data for each 50 microns of macular hole size\ndf$bins &lt;- cut(df$mac_hole_size, breaks = seq(0, 1000, by = 50))\n# Calculate means (number of successful repairs divided by total number of repairs) in each bin\ndf &lt;- df |&gt; \n  group_by(bins) |&gt; \n  mutate(mean_y = mean(y_success))\n# Plot observed (averaged) data \nggplot(df, aes(as.numeric(bins)*50-25, mean_y)) + \n  geom_point(size = 2) +\n  geom_line(color = \"orange\", linewidth = 1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Proportion\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\nNow we are starting to see the classic sigmoid shape of the relationship between probability and a continuous predictor.\n\n\n4.3.6 Fit a Logistic Model to the Simulated Data\nNow, let‚Äôs finally fit a logistic model to these simulated data.\n\n\nCode\n# Logistic model\nmod_logistic &lt;- glm(y_success ~ I(mac_hole_size/100), data = df, family = \"binomial\")\nmod_logistic |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\nCharacteristic\nlog(OR)1\n95% CI1\np-value\n\n\n\n\nI(mac_hole_size/100)\n-1.5\n-1.8, -1.3\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nOn the model estimation log-odds scale, the parameter estimate for the effect of macular hole size is -1.5, meaning for every 100 micron increase in macular hole size, the log-odds of success decreases by 1.5 units. All stats software will also provide you with parameter estimates as odds ratios, because that‚Äôs how we‚Äôre accustomed to reporting effects. And the equivalent odds ratio in this case is 0.22. In other words, for every 100 micron increase in macular hole size there is about a 78% reduction in the odds of success.\n\n\nCode\nmod_logistic |&gt; \n  tbl_regression(exponentiate = T)\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nI(mac_hole_size/100)\n0.22\n0.17, 0.28\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nNote that the odds ratio coefficient is a simple transformation of the log-odds coefficient (as we described earlier.) That is:\n\\[e^{-1.5} = 0.22\\]\n\n\n4.3.7 Plot Predicted log-odds of Surgical Repair Success\nNow we can plot the predicted log-odds of surgical repair success. Note that this is a straight line effectively fitted to our binary outcome, and the reason again that we can do this is because the binary outcome has been ‚Äòremapped‚Äô to log-odds and the log-odds exist on an unbounded and continuous scale. So this is a valid plot, unlike that before where we plotted the predictions treating Y as if it were a continuous outcome (I‚Äôm not necessarily suggesting this is a useful plot, just that it‚Äôs valid).\n\n\nCode\n# Predictions\n# Emmeans of logodds at 600 and 700 microns\nemmeans_df &lt;- data.frame(emmeans(mod_logistic, ~ mac_hole_size, at = list(mac_hole_size = c(600, 700, 800))))\n# Create df of emmeans and corresponding odds and probs, for plotting\nemmeans_df &lt;- emmeans_df |&gt; \n  select(mac_hole_size, emmean) |&gt; \n  rename(logodds = emmean) |&gt; \n  mutate(odds = round(exp(logodds), 3),\n         probs = round(plogis(logodds), 2),\n         logodds = round(logodds, 3))\n\n# Predictions ----\n# Create new df to predict on new values of x\nnew_dat &lt;- data.frame(mac_hole_size = seq(from = 0, to = 1200, length.out = 100))\n# Predict new fitted values and SE's on logodds scale\npred_logodds &lt;- predict(mod_logistic, newdata = new_dat, type = \"link\", se = TRUE)\nnew_dat &lt;- cbind(new_dat, pred_logodds)\n# Create new df of predictions\npredictions &lt;- new_dat |&gt; \n  rename(pred_logodds_est = fit) |&gt; \n  mutate(pred_logodds_LL = pred_logodds_est - (1.96 * se.fit),\n         pred_logodds_UL = pred_logodds_est + (1.96 * se.fit)) |&gt; \n  select(-c(se.fit, residual.scale))\n# Predict new fitted values and SE's on odds scale\npredictions &lt;- predictions |&gt; \n  mutate(pred_odds_est = exp(pred_logodds_est),\n         pred_odds_LL = exp(pred_logodds_LL),\n         pred_odds_UL = exp(pred_logodds_UL))\n# Predict new fitted values and SE's on probability scale\npred_probs &lt;- predict(mod_logistic, newdata = new_dat, type = \"response\", se = TRUE)\nnew_dat &lt;- cbind(new_dat[1], pred_probs)\nnew_dat &lt;- new_dat |&gt; \n  mutate(pred_probs_LL = fit - (1.96 * se.fit),\n         pred_probs_UL = fit + (1.96 * se.fit))\n# Add predicted probs and CIs to predictions df\npredictions &lt;- cbind(predictions, \n                     pred_probs_est = new_dat$fit, \n                     pred_probs_LL = new_dat$pred_probs_LL,\n                     pred_probs_UL = new_dat$pred_probs_UL)\n# Plot predicted log-odds\nggplot(predictions, aes(x = mac_hole_size, y = pred_logodds_est)) + \n  geom_ribbon(aes(ymin = pred_logodds_LL, ymax = pred_logodds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-100, 100), breaks = seq(-100, 100, by = 2)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(-8, 8)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted log-odds\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.8 Plot Predicted Odds of Surgical Repair Success\nWe can also plot the predicted odds of surgical repair success. Note that this best-fitting line is not linear, but instead looks like an exponential plot. We established previously that odds exist on a scale from 0 to \\(+\\infty\\) and that‚Äôs what we can appeciate here. Remember that this is just another transformation of the log-odds and the same trend is seen - as hole size increases, the odds of success decline.\n\n\nCode\n# Plot predicted odds\nggplot(predictions, aes(x = mac_hole_size, y = pred_odds_est)) + \n  geom_ribbon(aes(ymin = pred_odds_LL, ymax = pred_odds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1000000), breaks = seq(0, 1000000, by = 1000)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(0, 7000)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted odds\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.9 Plot Predicted Probabilities of Surgical Repair Success\nAnd we can also plot the predicted probabilities of surgical repair success. Again this is just another transformation of the log-odds or the odds. Note the classic sigmoid shape which emulates what we saw earlier when plotting the binned raw data. To my mind this plot is more useful than either plots of predicted log-odds or odds, as it tells us the likely chance of successful repair conditional on the initial macular hole size.\n\n\nCode\n# Plot predicted probabilities\nggplot(predictions, aes(x = mac_hole_size, y = pred_probs_est)) + \n  geom_ribbon(aes(ymin = pred_probs_LL, ymax = pred_probs_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted probability\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.10 Plot Observed and Predicted Probabilities of Surgical Repair Success\nAnd in fact what you should ideally find if you plot both your observed and predicted probabilities is that they are quite similar, reflecting that the model actually fits the data quite well. Of course, in this case the convergence of observed and predicted probabilities is unsurprising given that we know the data-generating proces - i.e.¬†we fitted a model to data that came from a model. But, in general, this is a good way to assess your model fit.\n\n\nCode\n# Plot observed and predicted data\nggplot(df, aes(as.numeric(bins)*50-25, mean_y)) + \n  geom_point(size = 2) +\n  geom_line(aes(color = \"observed\"), linewidth = 1) +\n  geom_line(data = predictions, aes(x = mac_hole_size, y = pred_probs_est, color = \"predicted\"), linewidth = 1) +\n  scale_color_manual(name = \"\", values = c(\"predicted\" = \"cornflowerblue\", \"observed\" = \"orange\")) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Probability\") +\n  theme_bw(base_size = 25) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n4.3.11 Putting It All Together\nWhat I want to try and reinforce in the figure below is how the log-odds, the odds and the probability of the outcome are all transformations of each other.\nIn logistic regression we are ultimately interested in the probability of the outcome occurring. But we use the log-odds or logit link function to map probabilities onto a linear real-number range. This means that the change in Y for each unit increase in X remains the same and we can see that in the top plot. So the log-odds coefficient that our stats software gives us is constant across all values of macular hole size. This represents the difference in the log-odds of success for each unit change in X.\nOne unit increase in log-odds:\n\n600 -&gt; 700 \\(\\mu m\\)\n= -0.181 - 1.323 = -1.504\n\n700 -&gt; 800 \\(\\mu m\\)\n= -1.686 - -0.181 = -1.505\n\n\nThe middle plot shows the equivalent odds of success across the same range of macular hole size. Here the difference in Y is no longer constant for each unit increase in X, but the commensurate ratio is constant - and this gives us the odds ratio. This means that the odds ratio is constant across all values of macular hole size.\nOne unit increase in odds:\n\n600 -&gt; 700 \\(\\mu m\\)\n= 0.834/3.756 = 0.22\n\n700 -&gt; 800 \\(\\mu m\\)\n= 0.185/0.834 = 0.22\n\n\nFinally the lower plot shows the equivalent probabilities of success and now there is no constancy in either unit differences or ratios. So, if we are interested in expressing our logistic regression model results as differences or ratios of predicted probabilities, we need to make sure that we qualify the macular hole size that a particular predicted probability corresponds to, as this will change across the range of X.\nOne unit increase in probability:\n\n600 -&gt; 700 \\(\\mu m\\)\n= 0.45 - 0.79 = -0.34\n= 0.45/0.79 = 0.57\n\n700 -&gt; 800 \\(\\mu m\\)\n= 0.16 - 0.45 = -0.29\n= 0.16/0.45 = 0.36\n\n\n\n\nCode\n# Reformat plots slightly for ggarrange ----\np1 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_logodds_est)) + \n  geom_ribbon(aes(ymin = pred_logodds_LL, ymax = pred_logodds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1150, y = 6, label = \"log-odds\", size = 10) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  annotate(\"text\", x = 1110, y = 3.5, label = \"unit differences = constant\", size = 5) +\n  scale_y_continuous(limits = c(-100, 100), breaks = seq(-100, 100, by = 2)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(-8, 8)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, logodds),\n                            label = emmeans_df$logodds, \n                            nudge_x = c(50, 50, 50), nudge_y = c(4, 4, 4),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  theme_bw(base_size = 20) +\n  ylab(\"\") +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\n\np2 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_odds_est)) + \n  geom_ribbon(aes(ymin = pred_odds_LL, ymax = pred_odds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1180, y = 6000, label = \"odds\", size = 10) +\n  annotate(\"text\", x = 1136, y = 5000, label = \"unit ratios = constant\", size = 5) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(seq(0, 1000000, by = 1000))) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(0, 7000)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ylab(\"\") +\n  theme_bw(base_size = 20) +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\np2_inset &lt;- p2 +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(1,2,3,4,5, seq(0, 1000000, by = 1000))) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, odds),\n                            label = emmeans_df$odds, \n                            nudge_x = c(50, 50, 50), nudge_y = c(0.5, 1, 1),\n                            color = \"red\", segment.size = 0.2, size = 5)\np2 &lt;- p2 + geom_magnify(from = c(xmin = 500, xmax = 1000, ymin = 0, ymax = 5), \n                          to = c(xmin = 470, xmax = 1010, ymin = 1000, ymax = 5000), \n                          shadow = T, axes = \"y\", plot = p2_inset)\n\np3 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_probs_est)) + \n  geom_ribbon(aes(ymin = pred_probs_LL, ymax = pred_probs_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1140, y = 0.8, label = \"probability\", size = 10) +\n  annotate(\"text\", x = 1075, y = 0.65, label = \"unit differences/ratios = not constant\", size = 5) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, probs),\n                            label = emmeans_df$probs, \n                            nudge_x = c(50, 50, 50), nudge_y = c(0.1, 0.1, 0.1),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  ylab(\"\") + xlab(\"Macular hole size\") +\n  theme_bw(base_size = 20)\nggarrange(p1, p2, p3, align = \"v\", ncol = 1, heights = c(1,1,1.2))\n\n\n\n\n\n\n\n\n\n\n\n4.3.12 Key points\nThis has been a long post but we are finally at the end. I hope that reading this has demystified what goes on under the hood in logistic regression as it‚Äôs really not that difficult once you understand the purpose of the link function.\nThese are the main summary points:\n\nLogistic regression is a type of generalised linear model that allows the estimation of associations of potential predictors with a binary outcome.\nThe logit link function transforms the probabilities of a binary outcome variable to a continuous, unbounded scale (log-odds). Standard linear modelling methods can then be applied to estimate the model.\nLog-odds, odds and probabilities are all simple transformations of one another.\nThe model is estimated on the log-odds scale where the association between the predictor and the log-odds of the outcome is linear, and the unit difference is constant.\nResults may be reported in terms of odds or probabilities:\n\nOn the odds scale the association between the predictor and the odds of the outcome is non-linear, but the unit ratio is constant.\nOn the probability scale the association between the predictor and the probability of the outcome is non-linear, and the unit difference/ratio is not constant.\nThis means probability estimates (risk differences of ratios) depend on the value of the predictor and this needs to be specified."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#simulate-exponential-data",
    "href": "posts/015_28Jun_2024/index.html#simulate-exponential-data",
    "title": "Logarithms and Why They‚Äôre Important in Statistics",
    "section": "2.1 Simulate Exponential Data",
    "text": "2.1 Simulate Exponential Data\nLet‚Äôs first of all visualise my statement regarding the logarithms ability to convert multiplicative effects to additive effects. I‚Äôll create a ‚Äògeometric‚Äô number series of 10 numbers with base 2 - i.e.¬†each subsequent number in the series is double the previous number. In other words, 2 is the multiplying factor in this series. The data looks like:\n\n\nCode\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(gtsummary)\nlibrary(emmeans)\nx &lt;- c(0:10)\ny &lt;- 2^(0:10)\ny2 &lt;- c(paste0(\"1 = 2\\U2070\"),\n        paste0(\"2 = 2\\U00B9\"),\n        paste0(\"2x2 = 2\\U00B2\"),\n        paste0(\"2x2x2 = 2\\U00B3\"),\n        paste0(\"2x2x2x2 = 2\\U2074\"),\n        paste0(\"2x2x2x2x2 = 2\\U2075\"),\n        paste0(\"2x2x2x2x2x2 = 2\\U2076\"),\n        paste0(\"2x2x2x2x2x2x2 = 2\\U2077\"),\n        paste0(\"2x2x2x2x2x2x2x2 = 2\\U2078\"),\n        paste0(\"2x2x2x2x2x2x2x2x2 = 2\\U2079\"),\n        paste0(\"2x2x2x2x2x2x2x2x2x2 = 2\\U00B9\\U2070\"))\ndf &lt;- data.frame(cbind(x = x, y = y, `y_in_exponential_form` = y2))\ndf$x &lt;- as.numeric(df$x); df$y &lt;- as.numeric(df$y)\ndf |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nx\ny\ny_in_exponential_form\n\n\n\n\n0\n1\n1 = 2‚Å∞\n\n\n1\n2\n2 = 2¬π\n\n\n2\n4\n2x2 = 2¬≤\n\n\n3\n8\n2x2x2 = 2¬≥\n\n\n4\n16\n2x2x2x2 = 2‚Å¥\n\n\n5\n32\n2x2x2x2x2 = 2‚Åµ\n\n\n6\n64\n2x2x2x2x2x2 = 2‚Å∂\n\n\n7\n128\n2x2x2x2x2x2x2 = 2‚Å∑\n\n\n8\n256\n2x2x2x2x2x2x2x2 = 2‚Å∏\n\n\n9\n512\n2x2x2x2x2x2x2x2x2 = 2‚Åπ\n\n\n10\n1024\n2x2x2x2x2x2x2x2x2x2 = 2¬π‚Å∞\n\n\n\n\n\nYou can see that the numbers grow large very quickly."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#plot-data-on-original-scale",
    "href": "posts/015_28Jun_2024/index.html#plot-data-on-original-scale",
    "title": "Logarithms and Why They‚Äôre Important in Statistics",
    "section": "2.3 Plot Data on Original Scale",
    "text": "2.3 Plot Data on Original Scale\nLet‚Äôs now plot this data using a normal linear scale:\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(1, 1050), breaks = c(1,100,200,300,400,500,600,700,800,900,1000)) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nIt is not hard to appreciate the exponential nature of the relationship between X and Y in this plot. As X increases, Y increases at a much faster rate, but it‚Äôs hard to tell by how much."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#plot-data-on-original-scale-modified-y-axis",
    "href": "posts/015_28Jun_2024/index.html#plot-data-on-original-scale-modified-y-axis",
    "title": "Logarithms and Why They‚Äôre Important in Statistics",
    "section": "2.4 Plot Data on Original Scale (Modified Y Axis)",
    "text": "2.4 Plot Data on Original Scale (Modified Y Axis)\nWhat does the plot look like if we use the axis tick marks to indicate the actual Y values (keeping the original scale):\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(1, 1050), breaks = c(1,2,4,8,16,32,64,128,256,512,1024)) +\n  geom_segment(aes(x = 0, y = 64, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 6, y = 1, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 128, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 7, y = 1, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 7, ymin = 64, ymax = 128), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 6, xmax = 7, ymin = 1, ymax = 64), fill = \"red\", alpha = 0.02) +\n  geom_segment(aes(x = 0, y = 256, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 8, y = 1, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 512, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 9, y = 1, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 9, ymin = 256, ymax = 512), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 8, xmax = 9, ymin = 1, ymax = 256), fill = \"red\", alpha = 0.02) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nInteresting. Obviously nothing has changed except the values on the Y axis no longer reflect evenly spaced units. In fact if you took a ruler to your screen you would see that the pixel distance between each pair of ascending tick marks is double the previous pair of tick marks. The larger numbers are nicely spread out on the axis, while the smaller numbers are all cramped together.\nWhat is certainly easier to appreciate in this plot compared to the previous one is the doubling of Y for each unit increase in X. We can see for instance that the one-unit increase in X between 6 and 7 corresponds to a doubling of Y from 64 to 128. Similarly, the one-unit increase between 8 and 9 corresponds to a doubling of Y from 256 to 512.\nSo, being good data analysts we always visualise our data before we get too far into analysing it. Although we know the data-generating mechanism for these data (because we simulated it based on what we wanted), we usually don‚Äôt know the data-generating mechanism for most real-world data that we come across. So, if we were in fact naive to the origins of these data an entirely reasonable question we might ask ourselves would be ‚Äúdo these come from an exponential (multiplicative) distribution?‚Äù\nA natural next step would be to see if taking logs of the data linearises (i.e straightens) the association between X and Y. Remember that I mentioned earlier that logs convert numbers that are related on a multiplicative scale to numbers that are related on an additive scale. What this means in practice is that an exponential curve flattens out and becomes linear if the data are truly multiplicative in nature."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#plot-data-on-log-scale",
    "href": "posts/015_28Jun_2024/index.html#plot-data-on-log-scale",
    "title": "Logarithms and Why They‚Äôre Important in Statistics",
    "section": "2.5 Plot Data on Log Scale",
    "text": "2.5 Plot Data on Log Scale\nThere are two ways one can plot data on a log scale using ggplot() in R. The first is to log-transform the data and plot it in the normal way; the second is to leave the data as is and use ggplot() in concert with the scales package to log-transform the axis scales. Let‚Äôs consider the second option first.\nHere we specify trans = \"log2\" within the scale_y_continuous() function to transform the Y axis to a base(2) log scale. The result is:\n\n\nCode\nlibrary(scales)\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(trans = \"log2\", breaks = c(1,2,4,8,16,32,64,128,256,512,1024)) +\n  geom_segment(aes(x = 0, y = 64, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 6, y = 1, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 128, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 7, y = 1, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 7, ymin = 64, ymax = 128), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 6, xmax = 7, ymin = 1, ymax = 64), fill = \"red\", alpha = 0.02) +\n  geom_segment(aes(x = 0, y = 256, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 8, y = 1, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 512, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 9, y = 1, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 9, ymin = 256, ymax = 512), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 8, xmax = 9, ymin = 1, ymax = 256), fill = \"red\", alpha = 0.02) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nNow that the Y axis has been rescaled we can easily see that the association between X and Y is in fact linear on this scale. We can also see that where previously the spacing of the ascending tick marks on the Y axis doubled, these now remain the same. Y is still doubling for every unit increase in X, but the Y scale is now considered additive rather than multiplicative in nature (i.e.¬†each doubling is the now the same pixel distance along the axis in the plot).\nI can hopefully consolidate this multiplicative -&gt; additive transformation in your mind by now replacing the raw values on the Y axis with their log-transformed equivalents. If you ignore the base(2) on the ascending Y axis, each exponent is now simply 1 more than the previous value. In other words, on the base(2) log scale, the ‚Äòeffects‚Äô are additive.\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(trans = log2_trans(),\n    breaks = c(1,2,4,8,16,32,64,128,256,512,1024),\n    labels = trans_format(\"log2\", math_format(2^.x))) +\n  geom_segment(aes(x = 0, y = 64, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 6, y = 1, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 128, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 7, y = 1, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 7, ymin = 64, ymax = 128), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 6, xmax = 7, ymin = 1, ymax = 64), fill = \"red\", alpha = 0.02) +\n  geom_segment(aes(x = 0, y = 256, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 8, y = 1, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 512, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 9, y = 1, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 9, ymin = 256, ymax = 512), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 8, xmax = 9, ymin = 1, ymax = 256), fill = \"red\", alpha = 0.02) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nThe other approach to plotting data on a log scale is to actually log-transform the data, and this is not difficult. If you knew that the data series were multiplicative by a factor of 2 you would naturally transform using a base(2) log scale as you would end up with a nice, natural interpretation of the transformed data - each unit increase in X representing a doubling in Y. Often you won‚Äôt know this, but you can still achieve the goal of linearising your data by using either natural (e) or base(10) logs.\nThe plots below show the association between X and log-transformed Y for all three of the common log transformations. Note that they all produce the same effect on the association between X and Y - just the scale differs. The numbers on each Y axis represent the powers that are raised to each base to calculate the value of Y in its original units. So, for example:\n\\[2^{5} \\approx e^{3.46} \\approx 10^{1.51} \\approx 32\\]\n\n\nCode\n# Here I have performed the log-transformation of Y on-the-fly, within the ggplot call, but you can also do this by explicitly creating a new log-transformed variable in the dataset\np1 &lt;- ggplot(df, aes(x, y = log2(y))) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_segment(aes(x = 5, y = 0, xend = 5, yend = 5), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 5, xend = 5, yend = 5), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(0, 10), breaks = c(0,2,4,6,8,10)) +\n  annotate(geom = \"text\", x = 0.8, y = 5.26, label = \"5.00\", color=\"red\", size = 8) +\n  theme_bw(base_size = 20)\np2 &lt;- ggplot(df, aes(x, y = log(y))) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_segment(aes(x = 5, y = 0, xend = 5, yend = 3.46), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 3.46, xend = 5, yend = 3.46), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  annotate(geom = \"text\", x = 0.8, y = 3.65, label = \"3.46\", color=\"red\", size = 8) +\n  theme_bw(base_size = 20)\np3 &lt;- ggplot(df, aes(x, y = log10(y))) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_segment(aes(x = 5, y = 0, xend = 5, yend = 1.51), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 1.51, xend = 5, yend = 1.51), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  annotate(geom = \"text\", x = 0.8, y = 1.6, label = \"1.51\", color=\"red\", size = 8) +\n  theme_bw(base_size = 20)\ncowplot::plot_grid(p1, p2, p3, labels = c('Base(2) log', 'Natural log', 'Base(10) log'), hjust = c(-0.9,-0.7,-0.6), vjust = 4, ncol = 3, label_size = 20)\n\n\n\n\n\n\n\n\n\nSo you might still be wondering where I am headed with all of this."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#arithmetic-vs-geometric-mean",
    "href": "posts/015_28Jun_2024/index.html#arithmetic-vs-geometric-mean",
    "title": "Logarithms and Why They‚Äôre Important in Statistics",
    "section": "2.2 Arithmetic vs Geometric Mean",
    "text": "2.2 Arithmetic vs Geometric Mean\nIf someone asked you to provide a summary statistic for these data what would you give them? The mean, median or something else? The median is always a good choice when you‚Äôre uncertain about whether your data might conform to parametric distribution assumptions. The median is just the middle value in the series and can be worked out in R as:\n\nmedian(df$y)\n\n[1] 32\n\n\nWhat about the (arithmetic) mean?\n\nmean(df$y)\n\n[1] 186.0909\n\n\nThat seems fairly highly when we see that most values are less than this. But this is symptomatic of data that are related in a multiplicative way - values tend to be condensed towards one end of the scale and skewed towards the other. The fewer, larger values ‚Äòdrag‚Äô the average towards that end of the scale. In these cases, the conventional arithmetic mean is not the best measure of central tendency and instead we should use the geometric mean.\nRemember that the arithmetic mean is calculated as such:\n\\[\\frac{1+2+4+8+16+32+64+128+256+512+1024}{11} = 186.1\\] There are two ways to calculate the geometric mean by hand (but I will also show you how to do it in R as well):\nThe first way is to take the nth root of the product of all the terms:\n\\[\\sqrt[11]{1*2*4*8*16*32*64*128*256*512*1024} = 32\\] and the second way is to take the exponent of the mean of the logged values:\n\\[e\\ ^{\\left( \\frac{log(1)+log(2)+log(4)+log(8)+log(16)+log(32)+log(64)+log(128)+log(256)+log(512)+log(1024)}{11} \\right)} = 32\\]\nIn R:\n\n\nCode\n# nth root method - manual\n(1*2*4*8*16*32*64*128*256*512*1024)^(1/11)\n\n\n[1] 32\n\n\nCode\n# logs method - manual\nexp((log(1)+log(2)+log(4)+log(8)+log(16)+log(32)+log(64)+log(128)+log(256)+log(512)+log(1024))/11)\n\n\n[1] 32\n\n\nCode\n# logs method - quick and easy\nexp(mean(log(df$y)))\n\n\n[1] 32\n\n\nIn a perfectly geometric series the geometric mean will align with the median and is a better measure of central tendency, so keep that in the back of your mind."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#modelling-assumptions",
    "href": "posts/015_28Jun_2024/index.html#modelling-assumptions",
    "title": "Logarithms and Why They‚Äôre Important in Statistics",
    "section": "2.6 Modelling Assumptions",
    "text": "2.6 Modelling Assumptions\nWell, there are several assumptions that the linear model leans on to ensure the parameter estimates it comes up with are valid and these include that the association between a continuous predictor and the outcome is roughly linear and the distribution of the model residuals is roughly normal. Both of these assumptions can fail when you try to model an obviously curved relationship as if it were linear. Certainly if domain-knowledge dictates that the origins of your data come from an exponential distribution (many biological cell process have this basis - e.g.¬†cell division), you need to stop and think about how you will approach your analysis.\nIf you blindly ran a standard linear regression on these data, you would get:\n\n\nCode\nmod_linear &lt;- lm(y ~ x, data = df) \nmod_linear |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nx\n75\n29, 120\n0.005\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_smooth(method = \"lm\", se = F, colour = \"black\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(1, 1050), breaks = c(1,100,200,300,400,500,600,700,800,900,1000)) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nClearly this is not ideal. The model is trying to suggest that for each unit increase in X, Y increases at a constant rate of 75. Let‚Äôs see what the model predicts for the value of Y when X = 5 (when we know the actual value is 32).\n\n\nCode\nemmeans(mod_linear, ~ x, at = (list(x = 5))) |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nx\nemmean\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\n5\n186.09\n64.04\n9\n41.22\n330.97\n\n\n\n\n\nHmmm, further evidence that this is a bad model for these data. Instead, let‚Äôs rerun the regression applying one small change - we will log-transform Y on-the-fly within the model call.\n\n\nCode\nmod_trans &lt;- lm(log(y) ~ x, data = df) \ntbl &lt;- mod_trans |&gt; \n  tbl_regression() # need work around for log transformed response for tbl_regression\ntbl |&gt;\n  # remove character version of 95% CI\n  modify_column_hide(ci) |&gt; \n  # exponentiate the regression estimates\n  modify_table_body(\n    \\(x) x |&gt; mutate(across(c(estimate, conf.low, conf.high), exp))\n  ) |&gt; \n  # merge numeric LB and UB together to display in table\n  modify_column_merge(pattern = \"{conf.low}, {conf.high}\", rows = !is.na(estimate)) |&gt; \n  modify_header(conf.low = \"**95% CI**\") |&gt; \n  as_kable() # convert to kable to display on stackoverflow\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\nx\n2.0\n2.0, 2.0\n&lt;0.001\n\n\n\n\n\nplot with all log versions\n\nlogs feature in GLM link functions - eg log odds (logistic) and log count (Poisson). Which is why we take differences in logs and ratios of exponentiated values‚Ä¶"
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#think-about-modelling-assumptions",
    "href": "posts/015_28Jun_2024/index.html#think-about-modelling-assumptions",
    "title": "Logarithms and Why They‚Äôre Important in Statistics",
    "section": "2.6 Think About Modelling Assumptions",
    "text": "2.6 Think About Modelling Assumptions\nWell, there are several assumptions that the linear model leans on to ensure the parameter estimates it comes up with are valid and these include that the association between a continuous predictor and the outcome is roughly linear and the distribution of the model residuals is roughly normal. Both of these assumptions can fail when you try to model an obviously curved relationship as if it were linear. Certainly if domain-knowledge dictates that the origins of your data come from an exponential distribution (many biological cell process have this basis - e.g.¬†cell division), you need to stop and think about how you will approach your analysis.\nIf you blindly ran a standard linear regression on these data, you would get:\n\n\nCode\nmod_linear &lt;- lm(y ~ x, data = df) \nmod_linear |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nx\n75\n29, 120\n0.005\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_smooth(method = \"lm\", se = F, colour = \"black\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(1, 1050), breaks = c(1,100,200,300,400,500,600,700,800,900,1000)) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nClearly this is not ideal. The model is trying to suggest that for each unit increase in X, Y increases at a constant rate of 75. Let‚Äôs see what the model predicts for the value of Y when X = 5 (when we know the actual value is 32).\n\n\nCode\nemmeans(mod_linear, ~ x, at = (list(x = 5))) |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nx\nemmean\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\n5\n186.09\n64.04\n9\n41.22\n330.97\n\n\n\n\n\nHmmm, further evidence that this is a bad model for these data. Instead, let‚Äôs rerun the regression applying one small change - we will log-transform Y on-the-fly within the model call.\n\n\nCode\nmod_trans &lt;- lm(log(y) ~ x, data = df) \ntbl &lt;- mod_trans |&gt; \n  tbl_regression() # need work around for log transformed response for tbl_regression\ntbl |&gt;\n  # remove character version of 95% CI\n  modify_column_hide(ci) |&gt; \n  # exponentiate the regression estimates\n  modify_table_body(\n    \\(x) x |&gt; mutate(across(c(estimate, conf.low, conf.high), exp))\n  ) |&gt; \n  # merge numeric LB and UB together to display in table\n  modify_column_merge(pattern = \"{conf.low}, {conf.high}\", rows = !is.na(estimate)) |&gt; \n  modify_header(conf.low = \"**95% CI**\") |&gt; \n  as_kable()\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\nx\n2.0\n2.0, 2.0\n&lt;0.001\n\n\n\n\n\nYou will get a warning if you run this model (I have hidden it) as it‚Äôs a perfect fit, because there is no randomness in the data. That doesn‚Äôt really matter though for the sake of the illustration. The Beta value represents the exponentiated coefficient for the association between X and Y and can be considered a ‚Äòresponse ratio‚Äô. This is equivalent to the ratio of each pair of successive values of Y for each unit increase in X. The response ratio of 2 implies that the outcome doubles (or increases by 100%) for each unit increase in the predictor and we know this to be true.\nWhat does this model predict the value of Y at X = 5 should be?\n\n\nCode\nemmeans(mod_trans, ~ x, at = (list(x = 5)), type = \"response\") |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nx\nresponse\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\n5\n32\n0\n9\n32\n32\n\n\n\n\n\nAnd this is what we would expect a good-fitting (perfectly-fitting in this case) model to be able to do - predict values on new data in line with our empirical observations."
  },
  {
    "objectID": "posts/016_26Jul_2024/index.html",
    "href": "posts/016_26Jul_2024/index.html",
    "title": "Logarithms and Why They‚Äôre Important in Statistics",
    "section": "",
    "text": "What do the acidity (pH - power of Hydrogen), sound intensity (dB - decibels) and earthquake intensity (measured on the Richter) scales all have in common?\nThey are all reported on a log scale.\nIn our real-world experience with these scales, I would be willing to bet that you haven‚Äôt put a lot of thought into what the numbers actually mean. Sure, we might remember from high school chemistry that something is more acidic if the pH is lower than 7. We might also know that higher numbers on the decibel scale indicate louder noises, but probably not what sources of sound specific levels relate to. We might also know from it‚Äôs reporting in the news that the most recent (thankfully infrequently occurring) earthquake wasn‚Äôt that severe based on a Richter magnitude of 4. But there‚Äôs actually much more to those numbers than meets the eye.\nLet‚Äôs take a look at each of these scales in a little more detail:\n\n\n\npH (taken from: https://www.pmel.noaa.gov/co2/file/The+pH+scale+by+numbers)\n\n\n\n\n\nSound Intensity\n\n\n\n\n\nRichter (taken from: https://en.m.wikipedia.org/wiki/File:How-the-Richter-Magnitude-Scale-is-determined.jpg)\n\n\nIf you take some time to look at those figures you will realise that there is a commonality among all three of them. In each case, two sets of number scales are presented:\n\nReporting scale\n\nAcidity (0 - 14)\nSound Intensity (0 - 150)\nEarthquake Intensity (0 - 9)\n\nMeasurement scale\n\nAcidity (\\(10^0 - 10^{-14}\\))\nSound Intensity (\\(10^{-12} - 10^{3}\\))\nEarthquake Intensity (\\(10^{-1} - 10^{9}\\))\n\n\nThe reporting scale is the one that we‚Äôre all familiar with, but in each case the actual measurements are recorded on a different scale behind the scenes.\nWhy?\nThe reason is that there is just too much variation on the measurement scale - by orders of magnitude - to make it convenient to also use to describe effects. So we convert the measurement scale to a more interpretable (but somewhat arbitrary) scale for reporting.\nWell hello, logarithms.\nWhen a physical quantity varies over a very large range, it is often convenient to take its logarithm in order to have a more manageable set of numbers (good primers on logarithms and exponents can be found here and here). And that‚Äôs exactly what is happening when we talk about acidity, sound intensity and earthquake intensity.\nThere is a key point to know about logarithms:\nLogarithms convert numbers that are related on a multiplicative (exponential) scale to numbers that are related on an additive (linear) scale.\nYou will see that in each of the above cases, the natural scale that the quantity is measured on is multiplicative in nature. Each ‚Äòunit‚Äô change represents an order of magnitude difference in the quantity. For example, the amplitude of seismic waves (felt as the level of ground shake) in a Richter magnitude 5 earthquake (moderate) are 10 times greater than that of a magnitude 4 earthquake (small). Similarly, a ‚Äòmajor‚Äô earthquake (Richter 7) would be considered 1000 times greater in seismic activity compared to a small earthquake.\nBut when we instead use logarithms, those multiplicative effects are now converted to additive effects. Each one-unit increase in seismic activity on the Richter scale corresponds to a 10 times greater increase in seismic activity on the natural scale.\nSo, how is this relevant in our daily data analysis endeavours?"
  },
  {
    "objectID": "posts/016_26Jul_2024/index.html#simulate-exponential-data",
    "href": "posts/016_26Jul_2024/index.html#simulate-exponential-data",
    "title": "Logarithms and Why They‚Äôre Important in Statistics",
    "section": "2.1 Simulate Exponential Data",
    "text": "2.1 Simulate Exponential Data\nLet‚Äôs first of all visualise my statement regarding the logarithms ability to convert multiplicative effects to additive effects. I‚Äôll create a ‚Äògeometric‚Äô number series of 10 numbers with base 2 - i.e.¬†each subsequent number in the series is double the previous number. In other words, 2 is the multiplying factor in this series. The data looks like:\n\n\nCode\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(gtsummary)\nlibrary(emmeans)\nx &lt;- c(0:10)\ny &lt;- 2^(0:10)\ny2 &lt;- c(paste0(\"1 = 2\\U2070\"),\n        paste0(\"2 = 2\\U00B9\"),\n        paste0(\"2x2 = 2\\U00B2\"),\n        paste0(\"2x2x2 = 2\\U00B3\"),\n        paste0(\"2x2x2x2 = 2\\U2074\"),\n        paste0(\"2x2x2x2x2 = 2\\U2075\"),\n        paste0(\"2x2x2x2x2x2 = 2\\U2076\"),\n        paste0(\"2x2x2x2x2x2x2 = 2\\U2077\"),\n        paste0(\"2x2x2x2x2x2x2x2 = 2\\U2078\"),\n        paste0(\"2x2x2x2x2x2x2x2x2 = 2\\U2079\"),\n        paste0(\"2x2x2x2x2x2x2x2x2x2 = 2\\U00B9\\U2070\"))\ndf &lt;- data.frame(cbind(x = x, y = y, `y_in_exponential_form` = y2))\ndf$x &lt;- as.numeric(df$x); df$y &lt;- as.numeric(df$y)\ndf |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nx\ny\ny_in_exponential_form\n\n\n\n\n0\n1\n1 = 2‚Å∞\n\n\n1\n2\n2 = 2¬π\n\n\n2\n4\n2x2 = 2¬≤\n\n\n3\n8\n2x2x2 = 2¬≥\n\n\n4\n16\n2x2x2x2 = 2‚Å¥\n\n\n5\n32\n2x2x2x2x2 = 2‚Åµ\n\n\n6\n64\n2x2x2x2x2x2 = 2‚Å∂\n\n\n7\n128\n2x2x2x2x2x2x2 = 2‚Å∑\n\n\n8\n256\n2x2x2x2x2x2x2x2 = 2‚Å∏\n\n\n9\n512\n2x2x2x2x2x2x2x2x2 = 2‚Åπ\n\n\n10\n1024\n2x2x2x2x2x2x2x2x2x2 = 2¬π‚Å∞\n\n\n\n\n\nYou can see that the numbers grow large very quickly."
  },
  {
    "objectID": "posts/016_26Jul_2024/index.html#arithmetic-vs-geometric-mean",
    "href": "posts/016_26Jul_2024/index.html#arithmetic-vs-geometric-mean",
    "title": "Logarithms and Why They‚Äôre Important in Statistics",
    "section": "2.2 Arithmetic vs Geometric Mean",
    "text": "2.2 Arithmetic vs Geometric Mean\nIf someone asked you to provide a summary statistic for these data what would you give them? The mean, median or something else? The median is always a good choice when you‚Äôre uncertain about whether your data might conform to parametric distribution assumptions. The median is just the middle value in the series and can be worked out in R as:\n\nmedian(df$y)\n\n[1] 32\n\n\nWhat about the (arithmetic) mean?\n\nmean(df$y)\n\n[1] 186.0909\n\n\nThat seems fairly highly when we see that most values are less than this. But this is symptomatic of data that are related in a multiplicative way - values tend to be condensed towards one end of the scale and skewed towards the other. The fewer, larger values ‚Äòdrag‚Äô the average towards that end of the scale. In these cases, the conventional arithmetic mean is not the best measure of central tendency and instead we should use the geometric mean.\nRemember that the arithmetic mean is calculated as such:\n\\[\\frac{1+2+4+8+16+32+64+128+256+512+1024}{11} = 186.1\\] There are two ways to calculate the geometric mean by hand (but I will also show you how to do it in R as well):\nThe first way is to take the nth root of the product of all the terms:\n\\[\\sqrt[11]{1*2*4*8*16*32*64*128*256*512*1024} = 32\\] and the second way is to take the exponent of the mean of the logged values:\n\\[e\\ ^{\\left( \\frac{log(1)+log(2)+log(4)+log(8)+log(16)+log(32)+log(64)+log(128)+log(256)+log(512)+log(1024)}{11} \\right)} = 32\\]\nIn R:\n\n\nCode\n# nth root method - manual\n(1*2*4*8*16*32*64*128*256*512*1024)^(1/11)\n\n\n[1] 32\n\n\nCode\n# logs method - manual\nexp((log(1)+log(2)+log(4)+log(8)+log(16)+log(32)+log(64)+log(128)+log(256)+log(512)+log(1024))/11)\n\n\n[1] 32\n\n\nCode\n# logs method - quick and easy\nexp(mean(log(df$y)))\n\n\n[1] 32\n\n\nIn a perfectly geometric series the geometric mean will align with the median and is a better measure of central tendency, so keep that in the back of your mind."
  },
  {
    "objectID": "posts/016_26Jul_2024/index.html#plot-data-on-original-scale",
    "href": "posts/016_26Jul_2024/index.html#plot-data-on-original-scale",
    "title": "Logarithms and Why They‚Äôre Important in Statistics",
    "section": "2.3 Plot Data on Original Scale",
    "text": "2.3 Plot Data on Original Scale\nLet‚Äôs now plot this data using a normal linear scale:\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(1, 1050), breaks = c(1,100,200,300,400,500,600,700,800,900,1000)) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nIt is not hard to appreciate the exponential nature of the relationship between X and Y in this plot. As X increases, Y increases at a much faster rate, but it‚Äôs hard to tell by how much."
  },
  {
    "objectID": "posts/016_26Jul_2024/index.html#plot-data-on-original-scale-modified-y-axis",
    "href": "posts/016_26Jul_2024/index.html#plot-data-on-original-scale-modified-y-axis",
    "title": "Logarithms and Why They‚Äôre Important in Statistics",
    "section": "2.4 Plot Data on Original Scale (Modified Y Axis)",
    "text": "2.4 Plot Data on Original Scale (Modified Y Axis)\nWhat does the plot look like if we use the axis tick marks to indicate the actual Y values (keeping the original scale):\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(1, 1050), breaks = c(1,2,4,8,16,32,64,128,256,512,1024)) +\n  geom_segment(aes(x = 0, y = 64, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 6, y = 1, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 128, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 7, y = 1, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 7, ymin = 64, ymax = 128), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 6, xmax = 7, ymin = 1, ymax = 64), fill = \"red\", alpha = 0.02) +\n  geom_segment(aes(x = 0, y = 256, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 8, y = 1, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 512, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 9, y = 1, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 9, ymin = 256, ymax = 512), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 8, xmax = 9, ymin = 1, ymax = 256), fill = \"red\", alpha = 0.02) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nInteresting. Obviously nothing has changed except the values on the Y axis no longer reflect evenly spaced units. In fact if you took a ruler to your screen you would see that the pixel distance between each pair of ascending tick marks is double the previous pair of tick marks. The larger numbers are nicely spread out on the axis, while the smaller numbers are all cramped together.\nWhat is certainly easier to appreciate in this plot compared to the previous one is the doubling of Y for each unit increase in X. We can see for instance that the one-unit increase in X between 6 and 7 corresponds to a doubling of Y from 64 to 128. Similarly, the one-unit increase between 8 and 9 corresponds to a doubling of Y from 256 to 512.\nSo, being good data analysts we always visualise our data before we get too far into analysing it. Although we know the data-generating mechanism for these data (because we simulated it based on what we wanted), we usually don‚Äôt know the data-generating mechanism for most real-world data that we come across. So, if we were in fact naive to the origins of these data an entirely reasonable question we might ask ourselves would be ‚Äúdo these come from an exponential (multiplicative) distribution?‚Äù\nA natural next step would be to see if taking logs of the data linearises (i.e straightens) the association between X and Y. Remember that I mentioned earlier that logs convert numbers that are related on a multiplicative scale to numbers that are related on an additive scale. What this means in practice is that an exponential curve flattens out and becomes linear if the data are truly multiplicative in nature."
  },
  {
    "objectID": "posts/016_26Jul_2024/index.html#plot-data-on-log-scale",
    "href": "posts/016_26Jul_2024/index.html#plot-data-on-log-scale",
    "title": "Logarithms and Why They‚Äôre Important in Statistics",
    "section": "2.5 Plot Data on Log Scale",
    "text": "2.5 Plot Data on Log Scale\nThere are two ways one can plot data on a log scale using ggplot() in R. The first is to log-transform the data and plot it in the normal way; the second is to leave the data as is and use ggplot() in concert with the scales package to log-transform the axis scales. Let‚Äôs consider the second option first.\nHere we specify trans = \"log2\" within the scale_y_continuous() function to transform the Y axis to a base(2) log scale. The result is:\n\n\nCode\nlibrary(scales)\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(trans = \"log2\", breaks = c(1,2,4,8,16,32,64,128,256,512,1024)) +\n  geom_segment(aes(x = 0, y = 64, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 6, y = 1, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 128, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 7, y = 1, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 7, ymin = 64, ymax = 128), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 6, xmax = 7, ymin = 1, ymax = 64), fill = \"red\", alpha = 0.02) +\n  geom_segment(aes(x = 0, y = 256, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 8, y = 1, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 512, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 9, y = 1, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 9, ymin = 256, ymax = 512), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 8, xmax = 9, ymin = 1, ymax = 256), fill = \"red\", alpha = 0.02) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nNow that the Y axis has been rescaled we can easily see that the association between X and Y is in fact linear on this scale. We can also see that where previously the spacing of the ascending tick marks on the Y axis doubled, these now remain the same. Y is still doubling for every unit increase in X, but the Y scale is now considered additive rather than multiplicative in nature (i.e.¬†each doubling is the now the same pixel distance along the axis in the plot).\nI can hopefully consolidate this multiplicative -&gt; additive transformation in your mind by now replacing the raw values on the Y axis with their log-transformed equivalents. If you ignore the base(2) on the ascending Y axis, each exponent is now simply 1 more than the previous value. In other words, on the base(2) log scale, the ‚Äòeffects‚Äô are additive.\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(trans = log2_trans(),\n    breaks = c(1,2,4,8,16,32,64,128,256,512,1024),\n    labels = trans_format(\"log2\", math_format(2^.x))) +\n  geom_segment(aes(x = 0, y = 64, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 6, y = 1, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 128, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 7, y = 1, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 7, ymin = 64, ymax = 128), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 6, xmax = 7, ymin = 1, ymax = 64), fill = \"red\", alpha = 0.02) +\n  geom_segment(aes(x = 0, y = 256, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 8, y = 1, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 512, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 9, y = 1, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 9, ymin = 256, ymax = 512), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 8, xmax = 9, ymin = 1, ymax = 256), fill = \"red\", alpha = 0.02) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nThe other approach to plotting data on a log scale is to actually log-transform the data, and this is not difficult. If you knew that the data series were multiplicative by a factor of 2 you would naturally transform using a base(2) log scale as you would end up with a nice, natural interpretation of the transformed data - each unit increase in X representing a doubling in Y. Often you won‚Äôt know this, but you can still achieve the goal of linearising your data by using either natural (e) or base(10) logs.\nThe plots below show the association between X and log-transformed Y for all three of the common log transformations. Note that they all produce the same effect on the association between X and Y - just the scale differs. The numbers on each Y axis represent the powers that are raised to each base to calculate the value of Y in its original units. So, for example:\n\\[2^{5} \\approx e^{3.46} \\approx 10^{1.51} \\approx 32\\]\n\n\nCode\n# Here I have performed the log-transformation of Y on-the-fly, within the ggplot call, but you can also do this by explicitly creating a new log-transformed variable in the dataset\np1 &lt;- ggplot(df, aes(x, y = log2(y))) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_segment(aes(x = 5, y = 0, xend = 5, yend = 5), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 5, xend = 5, yend = 5), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(0, 10), breaks = c(0,2,4,6,8,10)) +\n  annotate(geom = \"text\", x = 0.8, y = 5.26, label = \"5.00\", color=\"red\", size = 8) +\n  theme_bw(base_size = 20)\np2 &lt;- ggplot(df, aes(x, y = log(y))) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_segment(aes(x = 5, y = 0, xend = 5, yend = 3.46), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 3.46, xend = 5, yend = 3.46), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  annotate(geom = \"text\", x = 0.8, y = 3.65, label = \"3.46\", color=\"red\", size = 8) +\n  theme_bw(base_size = 20)\np3 &lt;- ggplot(df, aes(x, y = log10(y))) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_segment(aes(x = 5, y = 0, xend = 5, yend = 1.51), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 1.51, xend = 5, yend = 1.51), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  annotate(geom = \"text\", x = 0.8, y = 1.6, label = \"1.51\", color=\"red\", size = 8) +\n  theme_bw(base_size = 20)\ncowplot::plot_grid(p1, p2, p3, labels = c('Base(2) log', 'Natural log', 'Base(10) log'), hjust = c(-0.9,-0.7,-0.6), vjust = 4, ncol = 3, label_size = 20)\n\n\n\n\n\n\n\n\n\nSo you might still be wondering where I am headed with all of this."
  },
  {
    "objectID": "posts/016_26Jul_2024/index.html#think-about-modelling-assumptions",
    "href": "posts/016_26Jul_2024/index.html#think-about-modelling-assumptions",
    "title": "Logarithms and Why They‚Äôre Important in Statistics",
    "section": "2.6 Think About Modelling Assumptions",
    "text": "2.6 Think About Modelling Assumptions\nWell, there are several assumptions that the linear model leans on to ensure the parameter estimates it comes up with are valid and these include that the association between a continuous predictor and the outcome is roughly linear and the distribution of the model residuals is roughly normal. Both of these assumptions can fail when you try to model an obviously curved relationship as if it were linear. Certainly if domain-knowledge dictates that the origins of your data come from an exponential distribution (many biological cell process have this basis - e.g.¬†cell division), you need to stop and think about how you will approach your analysis.\nIf you blindly ran a standard linear regression on these data, you would get:\n\n\nCode\nmod_linear &lt;- lm(y ~ x, data = df) \nmod_linear |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nx\n75\n29, 120\n0.005\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_smooth(method = \"lm\", se = F, colour = \"black\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(1, 1050), breaks = c(1,100,200,300,400,500,600,700,800,900,1000)) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nClearly this is not ideal. The model is trying to suggest that for each unit increase in X, Y increases at a constant rate of 75. Let‚Äôs see what the model predicts for the value of Y when X = 5 (when we know the actual value is 32).\n\n\nCode\nemmeans(mod_linear, ~ x, at = (list(x = 5))) |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nx\nemmean\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\n5\n186.09\n64.04\n9\n41.22\n330.97\n\n\n\n\n\nHmmm, further evidence that this is a bad model for these data. Instead, let‚Äôs rerun the regression applying one small change - we will log-transform Y on-the-fly within the model call.\n\n\nCode\nmod_trans &lt;- lm(log(y) ~ x, data = df) \ntbl &lt;- mod_trans |&gt; \n  tbl_regression() # need work around for log transformed response for tbl_regression\ntbl |&gt;\n  # remove character version of 95% CI\n  modify_column_hide(ci) |&gt; \n  # exponentiate the regression estimates\n  modify_table_body(\n    \\(x) x |&gt; mutate(across(c(estimate, conf.low, conf.high), exp))\n  ) |&gt; \n  # merge numeric LB and UB together to display in table\n  modify_column_merge(pattern = \"{conf.low}, {conf.high}\", rows = !is.na(estimate)) |&gt; \n  modify_header(conf.low = \"**95% CI**\") |&gt; \n  as_kable()\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\nx\n2.0\n2.0, 2.0\n&lt;0.001\n\n\n\n\n\nYou will get a warning if you run this model (I have hidden it) as it‚Äôs a perfect fit, because there is no randomness in the data. That doesn‚Äôt really matter though for the sake of the illustration. The Beta value represents the exponentiated coefficient for the association between X and Y and can be considered a ‚Äòresponse ratio‚Äô. This is equivalent to the ratio of each pair of successive values of Y for each unit increase in X. The response ratio of 2 implies that the outcome doubles (or increases by 100%) for each unit increase in the predictor and we know this to be true.\nWhat does this model predict the value of Y at X = 5 should be?\n\n\nCode\nemmeans(mod_trans, ~ x, at = (list(x = 5)), type = \"response\") |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nx\nresponse\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\n5\n32\n0\n9\n32\n32\n\n\n\n\n\nAnd this is what we would expect a good-fitting (perfectly-fitting in this case) model to be able to do - predict values on new data in line with our empirical observations."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#puns-and-plays-on-popular-culture",
    "href": "posts/015_28Jun_2024/index.html#puns-and-plays-on-popular-culture",
    "title": "Academic Research - ‚ÄúWhat‚Äôs in a Title?‚Äù",
    "section": "1 Puns and Plays on Popular Culture",
    "text": "1 Puns and Plays on Popular Culture\nFantastic yeasts and where to find them: the hidden diversity of dimorphic fungal pathogens. For the Harry Potter fans.\nMedical marijuana: can‚Äôt we all just get a bong? This was a conference poster, not a paper.\nmiR miR on the wall, who‚Äôs the most malignant medulloblastoma miR of them all? Sounds like a poisoned apple is the least of anyones worries.\nGut Microbe to Brain Signaling: What Happens in Vagus‚Ä¶ I love this one.\nDie hard: Are cancer stem cells the Bruce Willises of tumor biology? Yippee-ki-yay‚Ä¶\nOne ring to multiplex them all. Well, that‚Äôs just precious.\nLeaf me alone: visual constraints on the ecology of social group formation. How I feel when my kids come up to me and ask for more money.\nHow To Train Your Oncolytic Virus: the Immunological Sequel"
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#simple-and-to-the-point",
    "href": "posts/015_28Jun_2024/index.html#simple-and-to-the-point",
    "title": "Academic Research - ‚ÄúWhat‚Äôs in a Title?‚Äù",
    "section": "3 Simple and to the Point",
    "text": "3 Simple and to the Point\nThere‚Äôs not a lot to add about any of these:\nGreat big boulders I have known"
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#offensive-or-risque",
    "href": "posts/015_28Jun_2024/index.html#offensive-or-risque",
    "title": "Academic Research - ‚ÄúWhat‚Äôs in a Title?‚Äù",
    "section": "5 Offensive or Risque",
    "text": "5 Offensive or Risque\nA couple of papers that may create offence. Click on Details at your peril.\n\nPremature Speculation Concerning Pornography‚Äôs Effects on Relationships. At least read the abstract before coming to your own conclusion.\nGet Me Off Your Fucking Mailing List. This is just awesome.\nStructural and electronic properties of chiral single-wall copper nanotubes. Surely they could have come up with a better abbreviation."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#just-clever",
    "href": "posts/015_28Jun_2024/index.html#just-clever",
    "title": "Academic Research - ‚ÄúWhat‚Äôs in a Title?‚Äù",
    "section": "2 Just Clever",
    "text": "2 Just Clever\nCan you tell your clunis from your cubitus? A benchmark for functional imaging. Or, can you tell your arse from your elbow?\nYou Probably Think this Paper‚Äôs About You: Narcissists‚Äô Perceptions of their Personality and Reputation. So, it is about me?\nChemical processes in the deep interior of Uranus\nFactitious Diarrhea: A Case of Watery Deception"
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#but-why",
    "href": "posts/015_28Jun_2024/index.html#but-why",
    "title": "Academic Research - ‚ÄúWhat‚Äôs in a Title?‚Äù",
    "section": "3 But Why?",
    "text": "3 But Why?\nOk, perhaps these aren‚Äôt funny titles, but certainly they make for interesting, if in some cases questionable, research.\nAre full or empty beer bottles sturdier and does their fracture-threshold suffice to break the human skull? ‚ÄúNow let‚Äôs get ethics approval for an RCT‚Äù.\nImpact of wet underwear on thermoregulatory responses and thermal comfort in the cold. Just letting you know that wet underwear is not comfortable - tell your friends.\nSword swallowing and its side effects. It turns out that sword swallowing is a hazardous activity (please don‚Äôt distract the next sword swallower you meet).\nRole of Childhood Aerobic Fitness in Successful Street Crossing. No children were actually harmed in the conduct of this study.\nA comparison of jump performances of the dog flea, Ctenocephalides canis (Curtis, 1826) and the cat flea, Ctenocephalides felis felis (Bouch√©, 1835). But cats can jump?!\nChickens prefer beautiful humans. Duh - obviously!\nPigeon‚Äôs discrimination of paintings by Monet and Picasso. Clearly more cultured than me.\nEnriched environment exposure accelerates rodent driving skills. Rat designated-drivers, a market ready to exploit.\nExperimental replication shows knives manufactured from frozen human feces do not work. Science at its best.\nTermination of intractable hiccups with digital rectal massage. We should all keep this in mind at our next dinner party.\nFarting as a defence against unspeakable dread. We‚Äôve all been there."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#not-sure-where-to-put-this-one",
    "href": "posts/015_28Jun_2024/index.html#not-sure-where-to-put-this-one",
    "title": "Academic Research - ‚ÄúWhat‚Äôs in a Title?‚Äù",
    "section": "4 Not Sure Where to Put This One‚Ä¶",
    "text": "4 Not Sure Where to Put This One‚Ä¶\nThe effect of having Christmas dinner with in-laws on gut microbiota composition. Well now you can put some science behind your decision to abstain from visiting the in-laws during the festive season - ‚ÄúIn participants visiting in-laws, there was a significant decrease in all¬†Ruminococcus¬†species, known to be associated with psychological stress and depression.‚Äù"
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#to-conclude",
    "href": "posts/015_28Jun_2024/index.html#to-conclude",
    "title": "Academic Research - ‚ÄúWhat‚Äôs in a Title?‚Äù",
    "section": "6 To Conclude",
    "text": "6 To Conclude\nI‚Äôm going to end with two papers that I think are highlights.\nThe first is really a tribute to anyone who has gone through the peer-review process and published an academic paper. At some point - if you haven‚Äôt already - you are going to have to deal with Reviewer 2. While this paper provides weak evidence that Reviewer 2 might actually be the victim of Reviewer-Identity-Theft, you can feel rest assured that you are not alone in having an obviously talentless peer-review hack underappreciate your true brilliance and fine work. We‚Äôve all been there.\nDear Reviewer 2: Go F‚Äô Yourself\nThe second is a classic. To the research students out there - don‚Äôt let a lack of words stop you from publishing your best work.\nThe unsuccessful self-treatment of a case of ‚Äúwriter‚Äôs block‚Äù\nI love the review given of it at the time:\n‚ÄúI have studied this manuscript very carefully with lemon juice and X-rays and have not detected a single flaw in either design or writing style. I suggest it be published without revision. Clearly, it is the most concise manuscript I have ever seen¬†‚Äì yet it contains sufficient detail to allow other investigators to replicate Dr.¬†Upper‚Äôs failure. In comparison with the other manuscripts I get from you containing all that complicated detail, this one was a pleasure to examine. Surely we can find a place for this paper in the Journal¬†‚Äì perhaps on the edge of a blank page.‚Äù\nI didn‚Äôt realise this was just the first in a series, and in fact there have been both success and failures in replication of the study. Unfortunately, the more recent meta-analysis still leaves the jury out as far as I‚Äôm concerned‚Ä¶\nThe Unsuccessful Self-Treatment of a Case of ‚ÄúWriter‚Äôs Block‚Äù: A Replication\nUnsuccessful Self-Treatment of a Case of ‚ÄúWriter‚Äôs Block‚Äù: A Partial Failure to Replicate\nUnsuccessful Self-Treatment of ‚ÄúWriter‚Äôs Block‚Äù: A Review of the Literature\nThe Unsuccessful Group-Treatment of ‚ÄúWriter‚Äôs Block‚Äù\nThe Unsuccessful Group Treatment of ‚ÄúWriter‚Äôs Block‚Äù: A Ten-Year Follow-up\nA Multisite Cross-Cultural Replication of Upper‚Äôs (1974) Unsuccessful Self-Treatment of Writer‚Äôs Block\nUnsuccessful Treatments of ‚ÄúWriter‚Äôs Block‚Äù: A Meta-Analysis\nUntil next time‚Ä¶"
  },
  {
    "objectID": "posts/017_09Aug_2024/index.html",
    "href": "posts/017_09Aug_2024/index.html",
    "title": "tidylog - Console Messaging in R",
    "section": "",
    "text": "Today‚Äôs post is really quite short (no thanks needed). It‚Äôs really to point out a super-handy little package that you should load at the beginning of every one of your R scripts (but only useful if you‚Äôre a tidyverse user).\nThe package is called tidylog and it‚Äôs designed to provide immediate feedback about what the data manipulations you make with dplyr and tidyr functions (e.g.¬†filter, select,mutate, group_by, the various join functions, etc) are actually doing to your datasets.\nTo my mind this should be built into R, as this kind of operational feedback is taken for granted by Stata users. But I guess that‚Äôs the whole point of R being open-source and community-driven in terms of ad-hoc improvements in functionality.\nI don‚Äôt think there‚Äôs really much for me to add that the package author hasn‚Äôt already said here. So please have a look.\nBut to end I will show you a quick before and after. Let‚Äôs use the inbuilt nycflights13 dataset to illustrate what output is returned if you run a bunch of data-wrangling functions.\nWithout tidylog:\n\n\nCode\nlibrary(nycflights13)\nlibrary(tidyverse)\ndat &lt;- flights |&gt;  \n select(year:day, hour, origin, dest, tailnum, carrier) |&gt; \n mutate(month = if_else(nchar(month) == 1, paste0(\"0\",month), as.character(month)),\n day = if_else(nchar(day) == 1, paste0(\"0\",day), as.character(day))) |&gt;  \n unite(\"date\", year:day, sep = \"/\", remove = T) |&gt; \n mutate(date = lubridate::ymd(date)) |&gt; \n filter(hour &gt;= 8) |&gt; \n anti_join(planes, by = \"tailnum\") |&gt; \n count(tailnum, sort = TRUE) \n\n\nNada. Thanks for nothing R!\nWith tidylog:\n\n\nCode\nsuppressMessages(library(tidylog))\ndat &lt;- flights |&gt;  \n select(year:day, hour, origin, dest, tailnum, carrier) |&gt; \n mutate(month = if_else(nchar(month) == 1, paste0(\"0\",month), as.character(month)),\n day = if_else(nchar(day) == 1, paste0(\"0\",day), as.character(day))) |&gt;  \n unite(\"date\", year:day, sep = \"/\", remove = T) |&gt; \n mutate(date = lubridate::ymd(date)) |&gt; \n filter(hour &gt;= 8) |&gt; \n anti_join(planes, by = \"tailnum\") |&gt; \n count(tailnum, sort = TRUE) \n\n\nselect: dropped 11 variables (dep_time, sched_dep_time, dep_delay, arr_time, sched_arr_time, ‚Ä¶)\n\n\nmutate: converted 'month' from integer to character (0 new NA)\n\n\n        converted 'day' from integer to character (0 new NA)\n\n\nmutate: converted 'date' from character to Date (0 new NA)\n\n\nfilter: removed 50,726 rows (15%), 286,050 rows remaining\n\n\nanti_join: added no columns\n\n\n           &gt; rows only in x    45,008\n\n\n           &gt; rows only in y  (     39)\n\n\n           &gt; matched rows    (241,042)\n\n\n           &gt;                 =========\n\n\n           &gt; rows total        45,008\n\n\ncount: now 716 rows and 2 columns, ungrouped\n\n\nNice!\nTill next time - Happy analysing!"
  },
  {
    "objectID": "posts/018_23Aug_2024/index.html",
    "href": "posts/018_23Aug_2024/index.html",
    "title": "Biostats Book Club",
    "section": "",
    "text": "I was playing around with AI image creation this week and asked Microsoft Bing to create an image with ‚Äòbiostatistics book club‚Äô as a prompt - this is what it came up with:\n\n\n\n\n\nHmmm - who would have ever thought talking about biostatistics could be so interesting.\nThen, for even more fun, I asked Bing to create another image using ‚Äòbiostatistics fight club‚Äô as a prompt and it gave me this:\n\n\n\n\n\nYep, just what I imagined a bunch of pugilistic stats-nerds to look like."
  },
  {
    "objectID": "posts/018_23Aug_2024/index.html#essential-medical-statistics",
    "href": "posts/018_23Aug_2024/index.html#essential-medical-statistics",
    "title": "Biostats Book Club",
    "section": "2.1 Essential Medical Statistics",
    "text": "2.1 Essential Medical Statistics\n\n\n\n\n\nI can‚Äôt recommend this book enough. It‚Äôs now over 20 years old but that doesn‚Äôt mean it‚Äôs dated - the ‚Äòessentials‚Äô of statistics, well, haven‚Äôt really changed. Kirkwood‚Äôs book explains statistical concepts in such a clear and concise manner that it makes (for me at least), understanding them much, much easier. It strikes a good balance in covering all the important ideas in enough depth while still maintaining a relative lay language style."
  },
  {
    "objectID": "posts/018_23Aug_2024/index.html#intuitive-biostatistics",
    "href": "posts/018_23Aug_2024/index.html#intuitive-biostatistics",
    "title": "Biostats Book Club",
    "section": "2.2 Intuitive Biostatistics",
    "text": "2.2 Intuitive Biostatistics\n\n\n\n\n\nThe author of Intuitive Biostatistics is also the brains behind the Prism statistical software. You‚Äôll be pleased to know there are almost no formulae written amongst its pages and I think a reasonable summary of the authors intentions is to provide a ‚Äòcommon-sense‚Äô treatment of statistical ideas. The book is littered with teaching examples as well as sections on ‚ÄòQ & A‚Äôs‚Äô and ‚ÄòCommon Mistakes‚Äô and their potential solutions. Get the latest edition."
  },
  {
    "objectID": "posts/018_23Aug_2024/index.html#r-for-data-science",
    "href": "posts/018_23Aug_2024/index.html#r-for-data-science",
    "title": "Biostats Book Club",
    "section": "2.3 R for Data Science",
    "text": "2.3 R for Data Science\n\n\n\n\n\nif you are one of the ‚Äòcool kids‚Äô and use the tidyverse approach to coding in R, then this is probably worthwhile having. There is a free online version as well. R for Data Science is predominantly aimed at data-wrangling and preparing your data for analysis - tidyverse style. I don‚Äôt consider myself a great statistical programmer, so I have found some elements of this a little difficult, but the more basic stuff is really useful (and coding should be a daily journey of self-improvement anyway)."
  },
  {
    "objectID": "posts/018_23Aug_2024/index.html#the-r-book",
    "href": "posts/018_23Aug_2024/index.html#the-r-book",
    "title": "Biostats Book Club",
    "section": "2.4 The R book",
    "text": "2.4 The R book\n\n\n\n\n\nThe R Book differs from R for Data Science in that, yes it‚Äôs a book about coding in R, but the focus isn‚Äôt just on data-wrangling. This book will give you almost any bit of code to run nearly any statistical procedure in R that you could imagine. In that sense it‚Äôs also a worthwhile reference. Mind you, as a result of the breadth of material it covers, this is a BIG book!\nI hope you find these helpful in your statistical learning."
  },
  {
    "objectID": "posts/015_26Jul_2024/index.html",
    "href": "posts/015_26Jul_2024/index.html",
    "title": "Logistic Regression - Under the Hood",
    "section": "",
    "text": "Today‚Äôs post comes from a talk that some of you may have already heard me give in lab meetings, but I thought it could be helpful to have a ‚Äòprint‚Äô copy so I‚Äôm going to do that here. The material in the talk was loosely based on the following paper:\nOphthalmic Statistics Note 11: Logistic Regression\nI‚Äôm going back to basics today. I think too often we use statistical techniques without really understanding what is going on ‚Äòunder the hood‚Äô. While that is ok to some extent, a better appreciation of what you‚Äôre actually doing when you perform a hypothesis test, or run a regression model, lends more robustness to the validity of both your results and your interpretation of them.\nSo let‚Äôs take a more fundamental look at logistic regression - a workhorse statistical model that I‚Äôd be willing to bet most of you have run at some point in your research careers. Now, because I consider myself more an applied rather than theoretical biostatistician, I am going to try and get my main points across to you with as little maths as possible, and hopefully not bore you in the process (but this is statistics, so hey‚Ä¶).\nWhat I hope you can gain from reading this is to think about logistic regression in a new and different way - one that completely illuminates the technique for you."
  },
  {
    "objectID": "posts/015_26Jul_2024/index.html#primary-transformations",
    "href": "posts/015_26Jul_2024/index.html#primary-transformations",
    "title": "Logistic Regression - Under the Hood",
    "section": "3.1 Primary Transformations",
    "text": "3.1 Primary Transformations\nIf we have the odds of an event, then we can transform that to a probability - or a risk, which is another way to refer to the chance of an outcome - by dividing the odds by the odds plus 1.\n\\[\\text{p} = \\frac{\\text{odds}}{1 + \\text{odds}}\\]\nIf instead we have the probability of an event, we can transform that to the equivalent odds of the event, by dividing the probability by its complement (that is 1 minus the probability).\n\\[\\text{odds} = \\frac{\\text{p}}{1 - \\text{p}}\\]\nAnd the log-odds is simply taking the natural logarithm of the odds - there is nothing difficult about that.\n\\[\\text{log-odds} = \\text{log(odds)}\\]\nSo these three are what I consider primary transformations and we can cycle between them quite easily with a calculator."
  },
  {
    "objectID": "posts/015_26Jul_2024/index.html#secondary-transformations",
    "href": "posts/015_26Jul_2024/index.html#secondary-transformations",
    "title": "Logistic Regression - Under the Hood",
    "section": "3.2 Secondary Transformations",
    "text": "3.2 Secondary Transformations\nThere are additional transformations that you can make and while these aren‚Äôt quite as straightforward, they are even more fundamental to logistic regression, which ultimately uses the log-odds as the scale to estimate its parameters on.\nThe first is that we can get to a probability directly from the log-odds by taking the exponent of the log-odds divided by the exponent of the log-odds plus one.\n\\[\\text{p} = \\frac{e^{\\text{log-odds}}}{1 + e^{\\text{log-odds}}}\\]\nAnd as you can see, that‚Äôs really no different to the first equation above because the exponent of the log-odds is simply the odds (the reverse transformation of logging a number is to take the exponent of that logged number). i.e.\n\\[\\text{odds} = e^{\\text{log-odds}}\\]\nNow, the final transformation, and the one that‚Äôs most relevant to logistic regression shows the relationship between the log-odds and probability and is known as the logit function (red box) - and we‚Äôll talk more about this in a minute.\n\\[\\text{log-odds} = \\text{log}\\Bigl(\\frac{\\text{p}}{1 - \\text{p}}\\Bigr)\\]"
  },
  {
    "objectID": "posts/015_26Jul_2024/index.html#probability-vs-odds-and-as-applied-to-the-research-question",
    "href": "posts/015_26Jul_2024/index.html#probability-vs-odds-and-as-applied-to-the-research-question",
    "title": "Logistic Regression - Under the Hood",
    "section": "3.3 Probability vs Odds (and as applied to the Research Question)",
    "text": "3.3 Probability vs Odds (and as applied to the Research Question)\nLet‚Äôs go back now to our original research question and specifically look at the difference between the probability and odds of an event - I want to solidify this concept in your minds. For most of us probability (or chance) is much more intuitive than the notion of odds. If I think about the likelihood of it being a wet day, I think there‚Äôs (for example) an 80% chance of it raining, rather than 4 to 1 odds in favour of it raining. However, if you like to wager on the horses than you may actually be quite comfortable with thinking in terms of odds - as those are the terms bets are placed in.\nIn the following I‚Äôve simulated some data from the original paper (based on the summary statistics and regression model provided). Obviously macular hole size is a continuous variable, but for the sake of this illustration I am going to dichotomise it at it‚Äôs approximate mean value (480 microns). From this we can draw up a 2 x 2 contingency table of the number of successful and failed macular hole repairs as a function of the dichotomised version of macular hole size.\nWhen we talk about probabilities we focus on the proportion of successes out of the total for each row. So, the probability (or risk) of successful repair for those patients with a macular hole size less than 480 microns is therefore 479/487 or about 0.98. That‚Äôs in contrast to the probability of successful repair in those patients with a size greater than 480 microns - where the chance of success is 380/503 or 0.76 - so lower as one would expect. You can then take a ratio of those two probabilities or risks to give the classic risk ratio and that equals 1.3 which tells you that the chance of successful repair is 30% greater if you have a smaller macular hole at initial presentation.\nNow let‚Äôs look at the equivalent odds. When we talk about odds we focus on the ratio of successes to failures and don‚Äôt really worry about the totals. So the odds of successful repair for those patients with a smaller initial hole is 479/8 or 60 to 1. The odds of success with a larger hole are a lot more modest - 380/123 or about 3 to 1. We can then calculate the classic odds ratio which works out at about 20, meaning the odds of successful repair is about 20 times greater in those with thinner initial holes.\nNote that both of these effect measures are commensurate - they point in exactly the same direction - that you fare better with a smaller hole at diagnosis. They are just different ways of expressing the same thing."
  },
  {
    "objectID": "posts/015_26Jul_2024/index.html#the-logit-link-function-and-why-log-odds-are-actually-important",
    "href": "posts/015_26Jul_2024/index.html#the-logit-link-function-and-why-log-odds-are-actually-important",
    "title": "Logistic Regression - Under the Hood",
    "section": "4.1 The Logit Link Function (and why log-odds are actually important)",
    "text": "4.1 The Logit Link Function (and why log-odds are actually important)\nThe log-odds or the logit function is pivotal to how logistic regression works. The logit function is a type of link function and there‚Äôs really nothing mysterious or difficult about this. A link function is just a function of the mean of Y - in other words a transformation of the mean of Y - and we use this link function as the outcome instead of Y itself.\n\\[\\text{logit(p)} = \\text{log}\\Bigl(\\frac{\\text{p}}{1 - \\text{p}}\\Bigr)\\] The logistic model then becomes:\n\\[\\text{Y} = \\text{logit(p)} = \\beta_0 \\; + \\; \\beta_1x\\]\nThis allows the outcome to become continuous and unbounded and the association between Y and X to become linear, therefore satisfying those fundamental assumptions of the linear model that I mentioned earlier.\nThus, the logit is a type of transformation that links or maps probability values from \\((0, 1)\\) to real numbers \\({\\displaystyle (-\\infty ,+\\infty )}\\). Now, on the log-odds/logit scale the effect of a one-unit change in X on Y becomes constant once again."
  },
  {
    "objectID": "posts/015_26Jul_2024/index.html#wth-log-oddsoddsprobabilities---youre-confusing-me---pick-one-please",
    "href": "posts/015_26Jul_2024/index.html#wth-log-oddsoddsprobabilities---youre-confusing-me---pick-one-please",
    "title": "Logistic Regression - Under the Hood",
    "section": "4.2 WTH? log-odds/odds/probabilities - You‚Äôre confusing me - pick one please!!",
    "text": "4.2 WTH? log-odds/odds/probabilities - You‚Äôre confusing me - pick one please!!\nIt‚Äôs important to remember that logistic regression is really about probabilities, not odds. Probabilities are more intuitive than odds (unless you like to put a bet on the horses), so why use odds? Well, model results have historically been expressed in terms of odds and odds ratios for mathematical convenience. As it is for the log-odds scale, the ‚Äòeffect‚Äô of a one-unit change in X on Y is also constant on the odds scale (but as a ratio of two odds, not as a difference in two log-odds). So an odds ratio describes a constant ‚Äòeffect‚Äô of a predictor on the odds of an outcome, irrespective of the value of the predictor. This is not the case on the probability scale, where both differences and ratios vary per unit change depending on the value of the predictor (I will illustrate this later).\nThe TLDR:\n\nModel is estimated (under the hood) on the log-odds scale to satisfy modelling assumptions.\nModel results are displayed on the odds scale (as odds ratios) for their constancy.\nProbabilities are calculated post-hoc with not much extra effort."
  },
  {
    "objectID": "posts/015_26Jul_2024/index.html#back-to-the-research-question",
    "href": "posts/015_26Jul_2024/index.html#back-to-the-research-question",
    "title": "Logistic Regression - Under the Hood",
    "section": "4.3 Back to the Research Question",
    "text": "4.3 Back to the Research Question\nLet‚Äôs go back to the original research question for the final time and now illustrate these concepts in that context. The regression equation for surgical repair success as a function of macular hole size that was estimated in our paper is shown here.\n\\[\\text{logit(p)} = 10.89 - 0.016 \\; \\text{x} \\;\\text{macular hole size}\\;(\\mu m)\\]\nBasically, the log-odds of successful repair can be predicted by calculating the result of 10.89 minus the product of 0.016 and whatever the macular hole size is in microns.\n\n4.3.1 Simulate Some Data\nWe can simulate some data from that regression equation - essentially reverse engineering fake data from the estimated best fit equation on the actual data. Why do this? Well, it gives us some sense of what the actual data may have looked like that the researchers collected empirically. We can then do all sorts of cool things like refit a model, generate model predictions, etc. Data simulation is something I am still learning more about but it opens up a world of possibilities in your analytical work.\nFor this simulation I used a normal distribution with a mean and SD for macular hole size of 486 and 142, respectively (the numbers came from the original research paper). The code to show how to reproduce this in R is included below but you can do something similar in Stata and in most other scriptable statistical software packages.\n\n\nCode\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(emmeans)\nlibrary(gtsummary)\nlibrary(ggmagnify)\nlibrary(ggpubr)\n# Simulation\nn &lt;- 1000                                          # set number of obs to simulate\nset.seed(1234)                                     # set seed for reproducibility\nmac_hole_size  &lt;-  rnorm(n, 486, 142)              # generate macular hole inner opening data with mean 486 and sd = 152\nlogodds_success  &lt;-  10.89 - 0.016 * mac_hole_size # generate variable that is linear combination of intercept = 10.89 and coefficient for macular hole -0.016 (logit scale)\nodds_success  &lt;-  exp(logodds_success)             # exponentiate logodds to get odds\nprob_success  &lt;-  odds_success/(1 + odds_success)  # generate probabilities from this\ny_success  &lt;-  rbinom(n, 1, prob_success)          # generate outcome variable as a function of those probabilities\ndf &lt;-  data.frame(cbind(mac_hole_size,             # combine into dataframe\n                        logodds_success,\n                        odds_success,\n                        prob_success, \n                        y_success))\ndf &lt;- df |&gt; \n  filter(mac_hole_size &gt; 100)                       # only include those with size &gt; 100\n\n\nEssentially, we generate some random values of macular hole size based on the specified distribution and then plug those values in to the regression equation and calculate the various outcome measures. The resultant data look like (first 20 rows shown):\n\n\nCode\nhead(df, 20) |&gt; \n  kable(align = \"c\", digits = 2) |&gt; \n  kable_styling() |&gt; \n  row_spec(1, bold = T, background = \"skyblue1\") |&gt; \n  row_spec(20, bold = T, background = \"lightgoldenrod1\")\n\n\n\n\n\nmac_hole_size\nlogodds_success\nodds_success\nprob_success\ny_success\n\n\n\n\n314.60\n5.86\n349.48\n1.00\n1\n\n\n525.39\n2.48\n11.99\n0.92\n1\n\n\n639.99\n0.65\n1.92\n0.66\n1\n\n\n152.91\n8.44\n4644.44\n1.00\n1\n\n\n546.94\n2.14\n8.49\n0.89\n1\n\n\n557.86\n1.96\n7.13\n0.88\n1\n\n\n404.39\n4.42\n83.08\n0.99\n1\n\n\n408.38\n4.36\n77.94\n0.99\n1\n\n\n405.85\n4.40\n81.16\n0.99\n1\n\n\n359.61\n5.14\n170.06\n0.99\n1\n\n\n418.24\n4.20\n66.57\n0.99\n1\n\n\n344.23\n5.38\n217.53\n1.00\n1\n\n\n375.77\n4.88\n131.32\n0.99\n1\n\n\n495.15\n2.97\n19.44\n0.95\n1\n\n\n622.25\n0.93\n2.54\n0.72\n1\n\n\n470.34\n3.36\n28.92\n0.97\n1\n\n\n413.44\n4.28\n71.88\n0.99\n1\n\n\n356.61\n5.18\n178.44\n0.99\n1\n\n\n367.12\n5.02\n150.82\n0.99\n1\n\n\n829.05\n-2.37\n0.09\n0.09\n0\n\n\n\n\n\n\n\n\nLet‚Äôs pick a couple of data points to examine more closely.\nIf we look at the first row (blue), the macular hole size is about 315 microns and this translates to a log-odds of successful repair of 5.9 (calculated straight from the regression equation), an odds of successful repair of about 350 to 1 (transformation calculation) and a probability of successful repair of 0.997 (transformation calculation). We can then generate the actual outcome status using that probability as a random variable in a binomial distribution. So in this case a relatively low macular hole size translates to a successful repair, as we might have expected.\nNow if we look at the last row of data (yellow), we see something different. Here we have a simulated size of 829 microns and this translates to a log-odds of successful repair of -2.4, an odds of 1 to 10 against and a probability of less than 0.1. Not surprisingly, the outcome status in this case is simulated as a failure.\n\n\n4.3.2 Plot the Observed Data\nWhat if we now plot the observed data, that is, plot the outcome status as a function of macular hole size?\nI‚Äôm a big advocate of always plotting your data first - before you calculate any descriptive statistics and certainly before running any statistical tests. In fact I‚Äôm not being melodramatic when I say that plotting your data can save you from looking like an analytical fool when you‚Äôre potentially ill-founded assumptions aren‚Äôt realised in the actual data. A good visualisation can reveal these kinds of problems instantly.\nAfter that big recommendation, we might be left feeling slightly deflated when we see that plotting binary data doesn‚Äôt appear that informative - I mean where‚Äôs the nice scatter plot and imaginary trend line that we‚Äôre used to thinking about?\n\n\nCode\n# Plot observed data\nggplot(df, aes(mac_hole_size, y_success)) + \n  geom_point(size = 4, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 1)) +\n  xlab(\"Macular hole size\") + ylab(\"Surgical repair success\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\nWell it‚Äôs not entirely helpless - we can see that there‚Äôs more successes skewed towards smaller macular holes and more failures skewed towards larger macular holes, so that does tell us something useful. But there are better ways to plot binary data and I‚Äôll take you through this shortly.\n\n\n4.3.3 Fit Linear Model (Naive Approach)\nWhat if we fit a standard regression line to binary data, effectively treating Y as continuous? Well people have done this in the past, and in some parts of the data analysis world, still do. It kind of works, but it‚Äôs not the best tool for the job and we can certainly do better. The best fit line is negative suggesting that as macular hole size increases, repair success decreases, so it is in line with what we know to be true. Note that I have rescaled macular hole size so that the resulting coefficient isn‚Äôt so small (now 1 ‚Äòunit‚Äô = 100 microns instead of 1 micron).\n\n\nCode\n# Linear model\nmod_linear &lt;- lm(y_success ~ I(mac_hole_size/100), data = df) \nmod_linear |&gt; \n  tbl_regression(intercept = T)\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n1.5\n1.4, 1.5\n&lt;0.001\n\n\nI(mac_hole_size/100)\n-0.12\n-0.14, -0.11\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nCode\n# Plot data with regression line\nggplot(df, aes(mac_hole_size, y_success)) + \n  geom_point(size = 2, alpha = 0.1) +\n  geom_abline(slope = coef(mod_linear)[[\"I(mac_hole_size/100)\"]]/100, \n              intercept = coef(mod_linear)[[\"(Intercept)\"]], \n              color = \"cornflowerblue\", linewidth = 1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 1)) +\n  xlab(\"Macular hole size\") + ylab(\"Surgical repair success\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.4 Problem - Predictions are made Outside the Probability Domain\nThe problem with naively fitting a standard linear model is that because we are know treating Y as unbounded and continuous, the model no longer knows that the outcome has to be constrained between 0 and 1, and predictions can easily fall outside this range. And that‚Äôs exactly what we can see here - a macular hole size of 100 microns (blue) predicts outcome success at 1.35 and a size of 1200 microns (yellow) predicts outcome success at -0.02. So fitting a standard linear model to binary data is really not a good approach.\n\n\nCode\n# Create new df to predict on new values of x\nnew_dat &lt;- data.frame(mac_hole_size = seq(from = 0, to = 1200, by = 50))\n# Predict new fitted values\npred &lt;- cbind(new_dat, prediction = predict(mod_linear, newdata = new_dat))\n# Display predictions\npred |&gt; \n  kable(align = \"c\", digits = 2) |&gt; \n  kable_styling() |&gt; \n  row_spec(3, bold = T, background = \"skyblue1\") |&gt; \n  row_spec(25, bold = T, background = \"lightgoldenrod1\")\n\n\n\n\n\nmac_hole_size\nprediction\n\n\n\n\n0\n1.47\n\n\n50\n1.41\n\n\n100\n1.35\n\n\n150\n1.29\n\n\n200\n1.22\n\n\n250\n1.16\n\n\n300\n1.10\n\n\n350\n1.04\n\n\n400\n0.98\n\n\n450\n0.91\n\n\n500\n0.85\n\n\n550\n0.79\n\n\n600\n0.73\n\n\n650\n0.66\n\n\n700\n0.60\n\n\n750\n0.54\n\n\n800\n0.48\n\n\n850\n0.42\n\n\n900\n0.35\n\n\n950\n0.29\n\n\n1000\n0.23\n\n\n1050\n0.17\n\n\n1100\n0.10\n\n\n1150\n0.04\n\n\n1200\n-0.02\n\n\n\n\n\n\n\n\n\n4.3.5 Plot the ‚ÄòBinned‚Äô Observed Data\nSo I said above that there was a better way to plot binary data in an effort to make the visualisation more information, and there is. Instead of just plotting the raw data - just the 0‚Äòs and 1‚Äôs - you can create ‚Äôbins‚Äô of data based on (some fairly arbitrary) thresholds of the predictor you are plotting against and then plot the proportion of successes within each bin. Here I created ‚Äòbins‚Äô of data for each 50 \\(\\mu m\\)‚Äôs of macular hole size and then calculated the mean success rate (i.e.¬†the proportion of 1‚Äôs) within each bin.\n\n\nCode\n# Create bins of data for each 50 microns of macular hole size\ndf$bins &lt;- cut(df$mac_hole_size, breaks = seq(0, 1000, by = 50))\n# Calculate means (number of successful repairs divided by total number of repairs) in each bin\ndf &lt;- df |&gt; \n  group_by(bins) |&gt; \n  mutate(mean_y = mean(y_success))\n# Plot observed (averaged) data \nggplot(df, aes(as.numeric(bins)*50-25, mean_y)) + \n  geom_point(size = 2) +\n  geom_line(color = \"orange\", linewidth = 1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Proportion\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\nNow we are starting to see the classic sigmoid shape of the relationship between probability and a continuous predictor.\n\n\n4.3.6 Fit a Logistic Model to the Simulated Data\nNow, let‚Äôs finally fit a logistic model to these simulated data.\n\n\nCode\n# Logistic model\nmod_logistic &lt;- glm(y_success ~ I(mac_hole_size/100), data = df, family = \"binomial\")\nmod_logistic |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\nCharacteristic\nlog(OR)1\n95% CI1\np-value\n\n\n\n\nI(mac_hole_size/100)\n-1.5\n-1.8, -1.3\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nOn the model estimation log-odds scale, the parameter estimate for the effect of macular hole size is -1.5, meaning for every 100 micron increase in macular hole size, the log-odds of success decreases by 1.5 units. All stats software will also provide you with parameter estimates as odds ratios, because that‚Äôs how we‚Äôre accustomed to reporting effects. And the equivalent odds ratio in this case is 0.22. In other words, for every 100 micron increase in macular hole size there is about a 78% reduction in the odds of success.\n\n\nCode\nmod_logistic |&gt; \n  tbl_regression(exponentiate = T)\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nI(mac_hole_size/100)\n0.22\n0.17, 0.28\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nNote that the odds ratio coefficient is a simple transformation of the log-odds coefficient (as we described earlier.) That is:\n\\[e^{-1.5} = 0.22\\]\n\n\n4.3.7 Plot Predicted log-odds of Surgical Repair Success\nNow we can plot the predicted log-odds of surgical repair success. Note that this is a straight line effectively fitted to our binary outcome, and the reason again that we can do this is because the binary outcome has been ‚Äòremapped‚Äô to log-odds and the log-odds exist on an unbounded and continuous scale. So this is a valid plot, unlike that before where we plotted the predictions treating Y as if it were a continuous outcome (I‚Äôm not necessarily suggesting this is a useful plot, just that it‚Äôs valid).\n\n\nCode\n# Predictions\n# Emmeans of logodds at 600 and 700 microns\nemmeans_df &lt;- data.frame(emmeans(mod_logistic, ~ mac_hole_size, at = list(mac_hole_size = c(600, 700, 800))))\n# Create df of emmeans and corresponding odds and probs, for plotting\nemmeans_df &lt;- emmeans_df |&gt; \n  select(mac_hole_size, emmean) |&gt; \n  rename(logodds = emmean) |&gt; \n  mutate(odds = round(exp(logodds), 3),\n         probs = round(plogis(logodds), 2),\n         logodds = round(logodds, 3))\n\n# Predictions ----\n# Create new df to predict on new values of x\nnew_dat &lt;- data.frame(mac_hole_size = seq(from = 0, to = 1200, length.out = 100))\n# Predict new fitted values and SE's on logodds scale\npred_logodds &lt;- predict(mod_logistic, newdata = new_dat, type = \"link\", se = TRUE)\nnew_dat &lt;- cbind(new_dat, pred_logodds)\n# Create new df of predictions\npredictions &lt;- new_dat |&gt; \n  rename(pred_logodds_est = fit) |&gt; \n  mutate(pred_logodds_LL = pred_logodds_est - (1.96 * se.fit),\n         pred_logodds_UL = pred_logodds_est + (1.96 * se.fit)) |&gt; \n  select(-c(se.fit, residual.scale))\n# Predict new fitted values and SE's on odds scale\npredictions &lt;- predictions |&gt; \n  mutate(pred_odds_est = exp(pred_logodds_est),\n         pred_odds_LL = exp(pred_logodds_LL),\n         pred_odds_UL = exp(pred_logodds_UL))\n# Predict new fitted values and SE's on probability scale\npred_probs &lt;- predict(mod_logistic, newdata = new_dat, type = \"response\", se = TRUE)\nnew_dat &lt;- cbind(new_dat[1], pred_probs)\nnew_dat &lt;- new_dat |&gt; \n  mutate(pred_probs_LL = fit - (1.96 * se.fit),\n         pred_probs_UL = fit + (1.96 * se.fit))\n# Add predicted probs and CIs to predictions df\npredictions &lt;- cbind(predictions, \n                     pred_probs_est = new_dat$fit, \n                     pred_probs_LL = new_dat$pred_probs_LL,\n                     pred_probs_UL = new_dat$pred_probs_UL)\n# Plot predicted log-odds\nggplot(predictions, aes(x = mac_hole_size, y = pred_logodds_est)) + \n  geom_ribbon(aes(ymin = pred_logodds_LL, ymax = pred_logodds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-100, 100), breaks = seq(-100, 100, by = 2)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(-8, 8)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted log-odds\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.8 Plot Predicted Odds of Surgical Repair Success\nWe can also plot the predicted odds of surgical repair success. Note that this best-fitting line is not linear, but instead looks like an exponential plot. We established previously that odds exist on a scale from 0 to \\(+\\infty\\) and that‚Äôs what we can appeciate here. Remember that this is just another transformation of the log-odds and the same trend is seen - as hole size increases, the odds of success decline.\n\n\nCode\n# Plot predicted odds\nggplot(predictions, aes(x = mac_hole_size, y = pred_odds_est)) + \n  geom_ribbon(aes(ymin = pred_odds_LL, ymax = pred_odds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1000000), breaks = seq(0, 1000000, by = 1000)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(0, 7000)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted odds\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.9 Plot Predicted Probabilities of Surgical Repair Success\nAnd we can also plot the predicted probabilities of surgical repair success. Again this is just another transformation of the log-odds or the odds. Note the classic sigmoid shape which emulates what we saw earlier when plotting the binned raw data. To my mind this plot is more useful than either plots of predicted log-odds or odds, as it tells us the likely chance of successful repair conditional on the initial macular hole size.\n\n\nCode\n# Plot predicted probabilities\nggplot(predictions, aes(x = mac_hole_size, y = pred_probs_est)) + \n  geom_ribbon(aes(ymin = pred_probs_LL, ymax = pred_probs_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted probability\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.10 Plot Observed and Predicted Probabilities of Surgical Repair Success\nAnd in fact what you should ideally find if you plot both your observed and predicted probabilities is that they are quite similar, reflecting that the model actually fits the data quite well. Of course, in this case the convergence of observed and predicted probabilities is unsurprising given that we know the data-generating proces - i.e.¬†we fitted a model to data that came from a model. But, in general, this is a good way to assess your model fit.\n\n\nCode\n# Plot observed and predicted data\nggplot(df, aes(as.numeric(bins)*50-25, mean_y)) + \n  geom_point(size = 2) +\n  geom_line(aes(color = \"observed\"), linewidth = 1) +\n  geom_line(data = predictions, aes(x = mac_hole_size, y = pred_probs_est, color = \"predicted\"), linewidth = 1) +\n  scale_color_manual(name = \"\", values = c(\"predicted\" = \"cornflowerblue\", \"observed\" = \"orange\")) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Probability\") +\n  theme_bw(base_size = 25) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n4.3.11 Putting It All Together\nWhat I want to try and reinforce in the figure below is how the log-odds, the odds and the probability of the outcome are all transformations of each other.\nIn logistic regression we are ultimately interested in the probability of the outcome occurring. But we use the log-odds or logit link function to map probabilities onto a linear real-number range. This means that the change in Y for each unit increase in X remains the same and we can see that in the top plot. So the log-odds coefficient that our stats software gives us is constant across all values of macular hole size. This represents the difference in the log-odds of success for each unit change in X.\nOne unit increase in log-odds:\n\n600 -&gt; 700 \\(\\mu m\\)\n= -0.181 - 1.323 = -1.504\n\n700 -&gt; 800 \\(\\mu m\\)\n= -1.686 - -0.181 = -1.505\n\n\nThe middle plot shows the equivalent odds of success across the same range of macular hole size. Here the difference in Y is no longer constant for each unit increase in X, but the commensurate ratio is constant - and this gives us the odds ratio. This means that the odds ratio is constant across all values of macular hole size.\nOne unit increase in odds:\n\n600 -&gt; 700 \\(\\mu m\\)\n= 0.834/3.756 = 0.22\n\n700 -&gt; 800 \\(\\mu m\\)\n= 0.185/0.834 = 0.22\n\n\nFinally the lower plot shows the equivalent probabilities of success and now there is no constancy in either unit differences or ratios. So, if we are interested in expressing our logistic regression model results as differences or ratios of predicted probabilities, we need to make sure that we qualify the macular hole size that a particular predicted probability corresponds to, as this will change across the range of X.\nOne unit increase in probability:\n\n600 -&gt; 700 \\(\\mu m\\)\n= 0.45 - 0.79 = -0.34\n= 0.45/0.79 = 0.57\n\n700 -&gt; 800 \\(\\mu m\\)\n= 0.16 - 0.45 = -0.29\n= 0.16/0.45 = 0.36\n\n\n\n\nCode\n# Reformat plots slightly for ggarrange ----\np1 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_logodds_est)) + \n  geom_ribbon(aes(ymin = pred_logodds_LL, ymax = pred_logodds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1150, y = 6, label = \"log-odds\", size = 10) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  annotate(\"text\", x = 1110, y = 3.5, label = \"unit differences = constant\", size = 5) +\n  scale_y_continuous(limits = c(-100, 100), breaks = seq(-100, 100, by = 2)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(-8, 8)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, logodds),\n                            label = emmeans_df$logodds, \n                            nudge_x = c(50, 50, 50), nudge_y = c(4, 4, 4),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  theme_bw(base_size = 20) +\n  ylab(\"\") +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\n\np2 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_odds_est)) + \n  geom_ribbon(aes(ymin = pred_odds_LL, ymax = pred_odds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1180, y = 6000, label = \"odds\", size = 10) +\n  annotate(\"text\", x = 1136, y = 5000, label = \"unit ratios = constant\", size = 5) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(seq(0, 1000000, by = 1000))) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(0, 7000)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ylab(\"\") +\n  theme_bw(base_size = 20) +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\np2_inset &lt;- p2 +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(1,2,3,4,5, seq(0, 1000000, by = 1000))) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, odds),\n                            label = emmeans_df$odds, \n                            nudge_x = c(50, 50, 50), nudge_y = c(0.5, 1, 1),\n                            color = \"red\", segment.size = 0.2, size = 5)\np2 &lt;- p2 + geom_magnify(from = c(xmin = 500, xmax = 1000, ymin = 0, ymax = 5), \n                          to = c(xmin = 470, xmax = 1010, ymin = 1000, ymax = 5000), \n                          shadow = T, axes = \"y\", plot = p2_inset)\n\np3 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_probs_est)) + \n  geom_ribbon(aes(ymin = pred_probs_LL, ymax = pred_probs_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1140, y = 0.8, label = \"probability\", size = 10) +\n  annotate(\"text\", x = 1075, y = 0.65, label = \"unit differences/ratios = not constant\", size = 5) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, probs),\n                            label = emmeans_df$probs, \n                            nudge_x = c(50, 50, 50), nudge_y = c(0.1, 0.1, 0.1),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  ylab(\"\") + xlab(\"Macular hole size\") +\n  theme_bw(base_size = 20)\nggarrange(p1, p2, p3, align = \"v\", ncol = 1, heights = c(1,1,1.2))\n\n\n\n\n\n\n\n\n\n\n\n4.3.12 Key points\nThis has been a long post but we are finally at the end. I hope that reading this has demystified what goes on under the hood in logistic regression as it‚Äôs really not that difficult once you understand the purpose of the link function.\nThese are the main summary points:\n\nLogistic regression is a type of generalised linear model that allows the estimation of associations of potential predictors with a binary outcome.\nThe logit link function transforms the probabilities of a binary outcome variable to a continuous, unbounded scale (log-odds). Standard linear modelling methods can then be applied to estimate the model.\nLog-odds, odds and probabilities are all simple transformations of one another.\nThe model is estimated on the log-odds scale where the association between the predictor and the log-odds of the outcome is linear, and the unit difference is constant.\nResults may be reported in terms of odds or probabilities:\n\nOn the odds scale the association between the predictor and the odds of the outcome is non-linear, but the unit ratio is constant.\nOn the probability scale the association between the predictor and the probability of the outcome is non-linear, and the unit difference/ratio is not constant.\nThis means probability estimates (risk differences of ratios) depend on the value of the predictor and this needs to be specified."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#puns-and-plays-on-popular-culture",
    "href": "posts/014_14Jun_2024/index.html#puns-and-plays-on-popular-culture",
    "title": "Academic Research - ‚ÄúWhat‚Äôs in a Title?‚Äù",
    "section": "1 Puns and Plays on Popular Culture",
    "text": "1 Puns and Plays on Popular Culture\nFantastic yeasts and where to find them: the hidden diversity of dimorphic fungal pathogens. For the Harry Potter fans.\nMedical marijuana: can‚Äôt we all just get a bong? This was a conference poster, not a paper.\nmiR miR on the wall, who‚Äôs the most malignant medulloblastoma miR of them all? Sounds like a poisoned apple is the least of anyones worries.\nGut Microbe to Brain Signaling: What Happens in Vagus‚Ä¶ I love this one.\nDie hard: Are cancer stem cells the Bruce Willises of tumor biology? Yippee-ki-yay‚Ä¶\nOne ring to multiplex them all. Well, that‚Äôs just precious.\nLeaf me alone: visual constraints on the ecology of social group formation. How I feel when my kids come up to me and ask for more money.\nHow To Train Your Oncolytic Virus: the Immunological Sequel"
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#just-clever",
    "href": "posts/014_14Jun_2024/index.html#just-clever",
    "title": "Academic Research - ‚ÄúWhat‚Äôs in a Title?‚Äù",
    "section": "2 Just Clever",
    "text": "2 Just Clever\nCan you tell your clunis from your cubitus? A benchmark for functional imaging. Or, can you tell your arse from your elbow?\nYou Probably Think this Paper‚Äôs About You: Narcissists‚Äô Perceptions of their Personality and Reputation. So, it is about me?\nChemical processes in the deep interior of Uranus\nFactitious Diarrhea: A Case of Watery Deception"
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#but-why",
    "href": "posts/014_14Jun_2024/index.html#but-why",
    "title": "Academic Research - ‚ÄúWhat‚Äôs in a Title?‚Äù",
    "section": "3 But Why?",
    "text": "3 But Why?\nOk, perhaps these aren‚Äôt funny titles, but certainly they make for interesting, if in some cases questionable, research.\nAre full or empty beer bottles sturdier and does their fracture-threshold suffice to break the human skull? ‚ÄúNow let‚Äôs get ethics approval for an RCT‚Äù.\nImpact of wet underwear on thermoregulatory responses and thermal comfort in the cold. Just letting you know that wet underwear is not comfortable - tell your friends.\nSword swallowing and its side effects. It turns out that sword swallowing is a hazardous activity (please don‚Äôt distract the next sword swallower you meet).\nRole of Childhood Aerobic Fitness in Successful Street Crossing. No children were actually harmed in the conduct of this study.\nA comparison of jump performances of the dog flea, Ctenocephalides canis (Curtis, 1826) and the cat flea, Ctenocephalides felis felis (Bouch√©, 1835). But cats can jump?!\nChickens prefer beautiful humans. Duh - obviously!\nPigeon‚Äôs discrimination of paintings by Monet and Picasso. Clearly more cultured than me.\nEnriched environment exposure accelerates rodent driving skills. Rat designated-drivers, a market ready to exploit.\nExperimental replication shows knives manufactured from frozen human feces do not work. Science at its best.\nTermination of intractable hiccups with digital rectal massage. We should all keep this in mind at our next dinner party.\nFarting as a defence against unspeakable dread. We‚Äôve all been there."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#not-sure-where-to-put-this-one",
    "href": "posts/014_14Jun_2024/index.html#not-sure-where-to-put-this-one",
    "title": "Academic Research - ‚ÄúWhat‚Äôs in a Title?‚Äù",
    "section": "4 Not Sure Where to Put This One‚Ä¶",
    "text": "4 Not Sure Where to Put This One‚Ä¶\nThe effect of having Christmas dinner with in-laws on gut microbiota composition. Well now you can put some science behind your decision to abstain from visiting the in-laws during the festive season - ‚ÄúIn participants visiting in-laws, there was a significant decrease in all¬†Ruminococcus¬†species, known to be associated with psychological stress and depression.‚Äù"
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#offensive-or-risque",
    "href": "posts/014_14Jun_2024/index.html#offensive-or-risque",
    "title": "Academic Research - ‚ÄúWhat‚Äôs in a Title?‚Äù",
    "section": "5 Offensive or Risque",
    "text": "5 Offensive or Risque\nA couple of papers that may create offence. Click on Details at your peril.\n\nPremature Speculation Concerning Pornography‚Äôs Effects on Relationships. At least read the abstract before coming to your own conclusion.\nGet Me Off Your Fucking Mailing List. This is just awesome.\nStructural and electronic properties of chiral single-wall copper nanotubes. Surely they could have come up with a better abbreviation."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#to-conclude",
    "href": "posts/014_14Jun_2024/index.html#to-conclude",
    "title": "Academic Research - ‚ÄúWhat‚Äôs in a Title?‚Äù",
    "section": "5 To Conclude",
    "text": "5 To Conclude\nI‚Äôm going to end with two papers that I think are highlights.\nThe first is really a tribute to anyone who has gone through the peer-review process and published an academic paper. At some point - if you haven‚Äôt already - you are going to have to deal with Reviewer 2. While this paper provides weak evidence that Reviewer 2 might actually be the victim of Reviewer-Identity-Theft, you can feel rest assured that you are not alone in having an obviously talentless peer-review hack underappreciate your true brilliance and fine work. We‚Äôve all been there.\nDear Reviewer 2: Go F‚Äô Yourself\nThe second is a classic. To the research students out there - don‚Äôt let a lack of words stop you from publishing your best work.\nThe unsuccessful self-treatment of a case of ‚Äúwriter‚Äôs block‚Äù\nI love the review given of it at the time:\n‚ÄúI have studied this manuscript very carefully with lemon juice and X-rays and have not detected a single flaw in either design or writing style. I suggest it be published without revision. Clearly, it is the most concise manuscript I have ever seen¬†‚Äì yet it contains sufficient detail to allow other investigators to replicate Dr.¬†Upper‚Äôs failure. In comparison with the other manuscripts I get from you containing all that complicated detail, this one was a pleasure to examine. Surely we can find a place for this paper in the Journal¬†‚Äì perhaps on the edge of a blank page.‚Äù\nI didn‚Äôt realise this was just the first in a series, and in fact there have been both success and failures in replication of the study. Unfortunately, the more recent meta-analysis still leaves the jury out as far as I‚Äôm concerned‚Ä¶\nThe Unsuccessful Self-Treatment of a Case of ‚ÄúWriter‚Äôs Block‚Äù: A Replication\nUnsuccessful Self-Treatment of a Case of ‚ÄúWriter‚Äôs Block‚Äù: A Partial Failure to Replicate\nUnsuccessful Self-Treatment of ‚ÄúWriter‚Äôs Block‚Äù: A Review of the Literature\nThe Unsuccessful Group-Treatment of ‚ÄúWriter‚Äôs Block‚Äù\nThe Unsuccessful Group Treatment of ‚ÄúWriter‚Äôs Block‚Äù: A Ten-Year Follow-up\nA Multisite Cross-Cultural Replication of Upper‚Äôs (1974) Unsuccessful Self-Treatment of Writer‚Äôs Block\nUnsuccessful Treatments of ‚ÄúWriter‚Äôs Block‚Äù: A Meta-Analysis\nUntil next time‚Ä¶"
  },
  {
    "objectID": "posts/016_09Aug_2024/index.html",
    "href": "posts/016_09Aug_2024/index.html",
    "title": "Logarithms and Why They‚Äôre Important in Statistics",
    "section": "",
    "text": "What do the acidity (pH - power of Hydrogen), sound intensity (dB - decibels) and earthquake intensity (measured on the Richter) scales all have in common?\nThey are all reported on a log scale.\nIn our real-world experience with these scales, I would be willing to bet that you haven‚Äôt put a lot of thought into what the numbers actually mean. Sure, we might remember from high school chemistry that something is more acidic if the pH is lower than 7. We might also know that higher numbers on the decibel scale indicate louder noises, but probably not what sources of sound specific levels relate to. We might also know from it‚Äôs reporting in the news that the most recent (thankfully infrequently occurring) earthquake wasn‚Äôt that severe based on a Richter magnitude of 4. But there‚Äôs actually much more to those numbers than meets the eye.\nLet‚Äôs take a look at each of these scales in a little more detail:\n\n\n\npH (taken from: https://www.pmel.noaa.gov/co2/file/The+pH+scale+by+numbers)\n\n\n\n\n\nSound Intensity\n\n\n\n\n\nRichter (taken from: https://en.m.wikipedia.org/wiki/File:How-the-Richter-Magnitude-Scale-is-determined.jpg)\n\n\nIf you take some time to look at those figures you will realise that there is a commonality among all three of them. In each case, two sets of number scales are presented:\n\nReporting scale\n\nAcidity (0 - 14)\nSound Intensity (0 - 150)\nEarthquake Intensity (0 - 9)\n\nMeasurement scale\n\nAcidity (\\(10^0 - 10^{-14}\\))\nSound Intensity (\\(10^{-12} - 10^{3}\\))\nEarthquake Intensity (\\(10^{-1} - 10^{9}\\))\n\n\nThe reporting scale is the one that we‚Äôre all familiar with, but in each case the actual measurements are recorded on a different scale behind the scenes.\nWhy?\nThe reason is that there is just too much variation on the measurement scale - by orders of magnitude - to make it convenient to also use to describe effects. So we convert the measurement scale to a more interpretable (but somewhat arbitrary) scale for reporting.\nWell hello, logarithms.\nWhen a physical quantity varies over a very large range, it is often convenient to take its logarithm in order to have a more manageable set of numbers (good primers on logarithms and exponents can be found here and here). And that‚Äôs exactly what is happening when we talk about acidity, sound intensity and earthquake intensity.\nThere is a key point to know about logarithms:\nLogarithms convert numbers that are related on a multiplicative (exponential) scale to numbers that are related on an additive (linear) scale.\nYou will see that in each of the above cases, the natural scale that the quantity is measured on is multiplicative in nature. Each ‚Äòunit‚Äô change represents an order of magnitude difference in the quantity. For example, the amplitude of seismic waves (felt as the level of ground shake) in a Richter magnitude 5 earthquake (moderate) are 10 times greater than that of a magnitude 4 earthquake (small). Similarly, a ‚Äòmajor‚Äô earthquake (Richter 7) would be considered 1000 times greater in seismic activity compared to a small earthquake.\nBut when we instead use logarithms, those multiplicative effects are now converted to additive effects. Each one-unit increase in seismic activity on the Richter scale corresponds to a 10 times greater increase in seismic activity on the natural scale.\nSo, how is this relevant in our daily data analysis endeavours?"
  },
  {
    "objectID": "posts/016_09Aug_2024/index.html#simulate-exponential-data",
    "href": "posts/016_09Aug_2024/index.html#simulate-exponential-data",
    "title": "Logarithms and Why They‚Äôre Important in Statistics",
    "section": "2.1 Simulate Exponential Data",
    "text": "2.1 Simulate Exponential Data\nLet‚Äôs first of all visualise my statement regarding the logarithms ability to convert multiplicative effects to additive effects. I‚Äôll create a ‚Äògeometric‚Äô number series of 10 numbers with base 2 - i.e.¬†each subsequent number in the series is double the previous number. In other words, 2 is the multiplying factor in this series. The data looks like:\n\n\nCode\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(gtsummary)\nlibrary(emmeans)\nx &lt;- c(0:10)\ny &lt;- 2^(0:10)\ny2 &lt;- c(paste0(\"1 = 2\\U2070\"),\n        paste0(\"2 = 2\\U00B9\"),\n        paste0(\"2x2 = 2\\U00B2\"),\n        paste0(\"2x2x2 = 2\\U00B3\"),\n        paste0(\"2x2x2x2 = 2\\U2074\"),\n        paste0(\"2x2x2x2x2 = 2\\U2075\"),\n        paste0(\"2x2x2x2x2x2 = 2\\U2076\"),\n        paste0(\"2x2x2x2x2x2x2 = 2\\U2077\"),\n        paste0(\"2x2x2x2x2x2x2x2 = 2\\U2078\"),\n        paste0(\"2x2x2x2x2x2x2x2x2 = 2\\U2079\"),\n        paste0(\"2x2x2x2x2x2x2x2x2x2 = 2\\U00B9\\U2070\"))\ndf &lt;- data.frame(cbind(x = x, y = y, `y_in_exponential_form` = y2))\ndf$x &lt;- as.numeric(df$x); df$y &lt;- as.numeric(df$y)\ndf |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nx\ny\ny_in_exponential_form\n\n\n\n\n0\n1\n1 = 2‚Å∞\n\n\n1\n2\n2 = 2¬π\n\n\n2\n4\n2x2 = 2¬≤\n\n\n3\n8\n2x2x2 = 2¬≥\n\n\n4\n16\n2x2x2x2 = 2‚Å¥\n\n\n5\n32\n2x2x2x2x2 = 2‚Åµ\n\n\n6\n64\n2x2x2x2x2x2 = 2‚Å∂\n\n\n7\n128\n2x2x2x2x2x2x2 = 2‚Å∑\n\n\n8\n256\n2x2x2x2x2x2x2x2 = 2‚Å∏\n\n\n9\n512\n2x2x2x2x2x2x2x2x2 = 2‚Åπ\n\n\n10\n1024\n2x2x2x2x2x2x2x2x2x2 = 2¬π‚Å∞\n\n\n\n\n\nYou can see that the numbers grow large very quickly."
  },
  {
    "objectID": "posts/016_09Aug_2024/index.html#arithmetic-vs-geometric-mean",
    "href": "posts/016_09Aug_2024/index.html#arithmetic-vs-geometric-mean",
    "title": "Logarithms and Why They‚Äôre Important in Statistics",
    "section": "2.2 Arithmetic vs Geometric Mean",
    "text": "2.2 Arithmetic vs Geometric Mean\nIf someone asked you to provide a summary statistic for these data what would you give them? The mean, median or something else? The median is always a good choice when you‚Äôre uncertain about whether your data might conform to parametric distribution assumptions. The median is just the middle value in the series and can be worked out in R as:\n\nmedian(df$y)\n\n[1] 32\n\n\nWhat about the (arithmetic) mean?\n\nmean(df$y)\n\n[1] 186.0909\n\n\nThat seems fairly highly when we see that most values are less than this. But this is symptomatic of data that are related in a multiplicative way - values tend to be condensed towards one end of the scale and skewed towards the other. The fewer, larger values ‚Äòdrag‚Äô the average towards that end of the scale. In these cases, the conventional arithmetic mean is not the best measure of central tendency and instead we should use the geometric mean.\nRemember that the arithmetic mean is calculated as such:\n\\[\\frac{1+2+4+8+16+32+64+128+256+512+1024}{11} = 186.1\\] There are two ways to calculate the geometric mean by hand (but I will also show you how to do it in R as well):\nThe first way is to take the nth root of the product of all the terms:\n\\[\\sqrt[11]{1*2*4*8*16*32*64*128*256*512*1024} = 32\\] and the second way is to take the exponent of the mean of the logged values:\n\\[e\\ ^{\\left( \\frac{log(1)+log(2)+log(4)+log(8)+log(16)+log(32)+log(64)+log(128)+log(256)+log(512)+log(1024)}{11} \\right)} = 32\\]\nIn R:\n\n\nCode\n# nth root method - manual\n(1*2*4*8*16*32*64*128*256*512*1024)^(1/11)\n\n\n[1] 32\n\n\nCode\n# logs method - manual\nexp((log(1)+log(2)+log(4)+log(8)+log(16)+log(32)+log(64)+log(128)+log(256)+log(512)+log(1024))/11)\n\n\n[1] 32\n\n\nCode\n# logs method - quick and easy\nexp(mean(log(df$y)))\n\n\n[1] 32\n\n\nIn a perfectly geometric series the geometric mean will align with the median and is a better measure of central tendency, so keep that in the back of your mind."
  },
  {
    "objectID": "posts/016_09Aug_2024/index.html#plot-data-on-original-scale",
    "href": "posts/016_09Aug_2024/index.html#plot-data-on-original-scale",
    "title": "Logarithms and Why They‚Äôre Important in Statistics",
    "section": "2.3 Plot Data on Original Scale",
    "text": "2.3 Plot Data on Original Scale\nLet‚Äôs now plot this data using a normal linear scale:\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(1, 1050), breaks = c(1,100,200,300,400,500,600,700,800,900,1000)) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nIt is not hard to appreciate the exponential nature of the relationship between X and Y in this plot. As X increases, Y increases at a much faster rate, but it‚Äôs hard to tell by how much."
  },
  {
    "objectID": "posts/016_09Aug_2024/index.html#plot-data-on-original-scale-modified-y-axis",
    "href": "posts/016_09Aug_2024/index.html#plot-data-on-original-scale-modified-y-axis",
    "title": "Logarithms and Why They‚Äôre Important in Statistics",
    "section": "2.4 Plot Data on Original Scale (Modified Y Axis)",
    "text": "2.4 Plot Data on Original Scale (Modified Y Axis)\nWhat does the plot look like if we use the axis tick marks to indicate the actual Y values (keeping the original scale):\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(1, 1050), breaks = c(1,2,4,8,16,32,64,128,256,512,1024)) +\n  geom_segment(aes(x = 0, y = 64, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 6, y = 1, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 128, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 7, y = 1, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 7, ymin = 64, ymax = 128), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 6, xmax = 7, ymin = 1, ymax = 64), fill = \"red\", alpha = 0.02) +\n  geom_segment(aes(x = 0, y = 256, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 8, y = 1, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 512, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 9, y = 1, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 9, ymin = 256, ymax = 512), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 8, xmax = 9, ymin = 1, ymax = 256), fill = \"red\", alpha = 0.02) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nInteresting. Obviously nothing has changed except the values on the Y axis no longer reflect evenly spaced units. In fact if you took a ruler to your screen you would see that the pixel distance between each pair of ascending tick marks is double the previous pair of tick marks. The larger numbers are nicely spread out on the axis, while the smaller numbers are all cramped together.\nWhat is certainly easier to appreciate in this plot compared to the previous one is the doubling of Y for each unit increase in X. We can see for instance that the one-unit increase in X between 6 and 7 corresponds to a doubling of Y from 64 to 128. Similarly, the one-unit increase between 8 and 9 corresponds to a doubling of Y from 256 to 512.\nSo, being good data analysts we always visualise our data before we get too far into analysing it. Although we know the data-generating mechanism for these data (because we simulated it based on what we wanted), we usually don‚Äôt know the data-generating mechanism for most real-world data that we come across. So, if we were in fact naive to the origins of these data an entirely reasonable question we might ask ourselves would be ‚Äúdo these come from an exponential (multiplicative) distribution?‚Äù\nA natural next step would be to see if taking logs of the data linearises (i.e straightens) the association between X and Y. Remember that I mentioned earlier that logs convert numbers that are related on a multiplicative scale to numbers that are related on an additive scale. What this means in practice is that an exponential curve flattens out and becomes linear if the data are truly multiplicative in nature."
  },
  {
    "objectID": "posts/016_09Aug_2024/index.html#plot-data-on-log-scale",
    "href": "posts/016_09Aug_2024/index.html#plot-data-on-log-scale",
    "title": "Logarithms and Why They‚Äôre Important in Statistics",
    "section": "2.5 Plot Data on Log Scale",
    "text": "2.5 Plot Data on Log Scale\nThere are two ways one can plot data on a log scale using ggplot() in R. The first is to log-transform the data and plot it in the normal way; the second is to leave the data as is and use ggplot() in concert with the scales package to log-transform the axis scales. Let‚Äôs consider the second option first.\nHere we specify trans = \"log2\" within the scale_y_continuous() function to transform the Y axis to a base(2) log scale. The result is:\n\n\nCode\nlibrary(scales)\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(trans = \"log2\", breaks = c(1,2,4,8,16,32,64,128,256,512,1024)) +\n  geom_segment(aes(x = 0, y = 64, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 6, y = 1, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 128, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 7, y = 1, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 7, ymin = 64, ymax = 128), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 6, xmax = 7, ymin = 1, ymax = 64), fill = \"red\", alpha = 0.02) +\n  geom_segment(aes(x = 0, y = 256, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 8, y = 1, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 512, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 9, y = 1, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 9, ymin = 256, ymax = 512), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 8, xmax = 9, ymin = 1, ymax = 256), fill = \"red\", alpha = 0.02) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nNow that the Y axis has been rescaled we can easily see that the association between X and Y is in fact linear on this scale. We can also see that where previously the spacing of the ascending tick marks on the Y axis doubled, these now remain the same. Y is still doubling for every unit increase in X, but the Y scale is now considered additive rather than multiplicative in nature (i.e.¬†each doubling is the now the same pixel distance along the axis in the plot).\nI can hopefully consolidate this multiplicative -&gt; additive transformation in your mind by now replacing the raw values on the Y axis with their log-transformed equivalents. If you ignore the base(2) on the ascending Y axis, each exponent is now simply 1 more than the previous value. In other words, on the base(2) log scale, the ‚Äòeffects‚Äô are additive.\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(trans = log2_trans(),\n    breaks = c(1,2,4,8,16,32,64,128,256,512,1024),\n    labels = trans_format(\"log2\", math_format(2^.x))) +\n  geom_segment(aes(x = 0, y = 64, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 6, y = 1, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 128, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 7, y = 1, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 7, ymin = 64, ymax = 128), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 6, xmax = 7, ymin = 1, ymax = 64), fill = \"red\", alpha = 0.02) +\n  geom_segment(aes(x = 0, y = 256, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 8, y = 1, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 512, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 9, y = 1, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 9, ymin = 256, ymax = 512), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 8, xmax = 9, ymin = 1, ymax = 256), fill = \"red\", alpha = 0.02) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nThe other approach to plotting data on a log scale is to actually log-transform the data, and this is not difficult. If you knew that the data series were multiplicative by a factor of 2 you would naturally transform using a base(2) log scale as you would end up with a nice, natural interpretation of the transformed data - each unit increase in X representing a doubling in Y. Often you won‚Äôt know this, but you can still achieve the goal of linearising your data by using either natural (e) or base(10) logs.\nThe plots below show the association between X and log-transformed Y for all three of the common log transformations. Note that they all produce the same effect on the association between X and Y - just the scale differs. The numbers on each Y axis represent the powers that are raised to each base to calculate the value of Y in its original units. So, for example:\n\\[2^{5} \\approx e^{3.46} \\approx 10^{1.51} \\approx 32\\]\n\n\nCode\n# Here I have performed the log-transformation of Y on-the-fly, within the ggplot call, but you can also do this by explicitly creating a new log-transformed variable in the dataset\np1 &lt;- ggplot(df, aes(x, y = log2(y))) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_segment(aes(x = 5, y = 0, xend = 5, yend = 5), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 5, xend = 5, yend = 5), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(0, 10), breaks = c(0,2,4,6,8,10)) +\n  annotate(geom = \"text\", x = 0.8, y = 5.26, label = \"5.00\", color=\"red\", size = 8) +\n  theme_bw(base_size = 20)\np2 &lt;- ggplot(df, aes(x, y = log(y))) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_segment(aes(x = 5, y = 0, xend = 5, yend = 3.46), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 3.46, xend = 5, yend = 3.46), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  annotate(geom = \"text\", x = 0.8, y = 3.65, label = \"3.46\", color=\"red\", size = 8) +\n  theme_bw(base_size = 20)\np3 &lt;- ggplot(df, aes(x, y = log10(y))) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_segment(aes(x = 5, y = 0, xend = 5, yend = 1.51), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 1.51, xend = 5, yend = 1.51), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  annotate(geom = \"text\", x = 0.8, y = 1.6, label = \"1.51\", color=\"red\", size = 8) +\n  theme_bw(base_size = 20)\ncowplot::plot_grid(p1, p2, p3, labels = c('Base(2) log', 'Natural log', 'Base(10) log'), hjust = c(-0.9,-0.7,-0.6), vjust = 4, ncol = 3, label_size = 20)\n\n\n\n\n\n\n\n\n\nSo you might still be wondering where I am headed with all of this."
  },
  {
    "objectID": "posts/016_09Aug_2024/index.html#think-about-modelling-assumptions",
    "href": "posts/016_09Aug_2024/index.html#think-about-modelling-assumptions",
    "title": "Logarithms and Why They‚Äôre Important in Statistics",
    "section": "2.6 Think About Modelling Assumptions",
    "text": "2.6 Think About Modelling Assumptions\nWell, there are several assumptions that the linear model leans on to ensure the parameter estimates it comes up with are valid and these include that the association between a continuous predictor and the outcome is roughly linear and the distribution of the model residuals is roughly normal. Both of these assumptions can fail when you try to model an obviously curved relationship as if it were linear. Certainly if domain-knowledge dictates that the origins of your data come from an exponential distribution (many biological cell process have this basis - e.g.¬†cell division), you need to stop and think about how you will approach your analysis.\nIf you blindly ran a standard linear regression on these data, you would get:\n\n\nCode\nmod_linear &lt;- lm(y ~ x, data = df) \nmod_linear |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nx\n75\n29, 120\n0.005\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_smooth(method = \"lm\", se = F, colour = \"black\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(1, 1050), breaks = c(1,100,200,300,400,500,600,700,800,900,1000)) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nClearly this is not ideal. The model is trying to suggest that for each unit increase in X, Y increases at a constant rate of 75. Let‚Äôs see what the model predicts for the value of Y when X = 5 (when we know the actual value is 32).\n\n\nCode\nemmeans(mod_linear, ~ x, at = (list(x = 5))) |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nx\nemmean\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\n5\n186.09\n64.04\n9\n41.22\n330.97\n\n\n\n\n\nHmmm, further evidence that this is a bad model for these data. Instead, let‚Äôs rerun the regression applying one small change - we will log-transform Y on-the-fly within the model call.\n\n\nCode\nmod_trans &lt;- lm(log(y) ~ x, data = df) \ntbl &lt;- mod_trans |&gt; \n  tbl_regression() # need work around for log transformed response for tbl_regression\ntbl |&gt;\n  # remove character version of 95% CI\n  modify_column_hide(ci) |&gt; \n  # exponentiate the regression estimates\n  modify_table_body(\n    \\(x) x |&gt; mutate(across(c(estimate, conf.low, conf.high), exp))\n  ) |&gt; \n  # merge numeric LB and UB together to display in table\n  modify_column_merge(pattern = \"{conf.low}, {conf.high}\", rows = !is.na(estimate)) |&gt; \n  modify_header(conf.low = \"**95% CI**\") |&gt; \n  as_kable()\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\nx\n2.0\n2.0, 2.0\n&lt;0.001\n\n\n\n\n\nYou will get a warning if you run this model (I have hidden it) as it‚Äôs a perfect fit, because there is no randomness in the data. That doesn‚Äôt really matter though for the sake of the illustration. The Beta value represents the exponentiated coefficient for the association between X and Y and can be considered a ‚Äòresponse ratio‚Äô. This is equivalent to the ratio of each pair of successive values of Y for each unit increase in X. The response ratio of 2 implies that the outcome doubles (or increases by 100%) for each unit increase in the predictor and we know this to be true.\nWhat does this model predict the value of Y at X = 5 should be?\n\n\nCode\nemmeans(mod_trans, ~ x, at = (list(x = 5)), type = \"response\") |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nx\nresponse\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\n5\n32\n0\n9\n32\n32\n\n\n\n\n\nAnd this is what we would expect a good-fitting (perfectly-fitting in this case) model to be able to do - predict values on new data in line with our empirical observations."
  },
  {
    "objectID": "posts/017_23Aug_2024/index.html",
    "href": "posts/017_23Aug_2024/index.html",
    "title": "tidylog - Console Messaging in R",
    "section": "",
    "text": "Today‚Äôs post is really quite short (no thanks needed). It‚Äôs really to point out a super-handy little package that you should load at the beginning of every one of your R scripts (but only useful if you‚Äôre a tidyverse user).\nThe package is called tidylog and it‚Äôs designed to provide immediate feedback about what the data manipulations you make with dplyr and tidyr functions (e.g.¬†filter, select,mutate, group_by, the various join functions, etc) are actually doing to your datasets.\nTo my mind this should be built into R, as this kind of operational feedback is taken for granted by Stata users. But I guess that‚Äôs the whole point of R being open-source and community-driven in terms of ad-hoc improvements in functionality.\nI don‚Äôt think there‚Äôs really much for me to add that the package author hasn‚Äôt already said here. So please have a look.\nBut to end I will show you a quick before and after. Let‚Äôs use the inbuilt nycflights13 dataset to illustrate what output is returned if you run a bunch of data-wrangling functions.\nWithout tidylog:\n\n\nCode\nlibrary(nycflights13)\nlibrary(tidyverse)\ndat &lt;- flights |&gt;  \n select(year:day, hour, origin, dest, tailnum, carrier) |&gt; \n mutate(month = if_else(nchar(month) == 1, paste0(\"0\",month), as.character(month)),\n day = if_else(nchar(day) == 1, paste0(\"0\",day), as.character(day))) |&gt;  \n unite(\"date\", year:day, sep = \"/\", remove = T) |&gt; \n mutate(date = lubridate::ymd(date)) |&gt; \n filter(hour &gt;= 8) |&gt; \n anti_join(planes, by = \"tailnum\") |&gt; \n count(tailnum, sort = TRUE) \n\n\nNada. Thanks for nothing R!\nWith tidylog:\n\n\nCode\nsuppressMessages(library(tidylog))\ndat &lt;- flights |&gt;  \n select(year:day, hour, origin, dest, tailnum, carrier) |&gt; \n mutate(month = if_else(nchar(month) == 1, paste0(\"0\",month), as.character(month)),\n day = if_else(nchar(day) == 1, paste0(\"0\",day), as.character(day))) |&gt;  \n unite(\"date\", year:day, sep = \"/\", remove = T) |&gt; \n mutate(date = lubridate::ymd(date)) |&gt; \n filter(hour &gt;= 8) |&gt; \n anti_join(planes, by = \"tailnum\") |&gt; \n count(tailnum, sort = TRUE) \n\n\nselect: dropped 11 variables (dep_time, sched_dep_time, dep_delay, arr_time, sched_arr_time, ‚Ä¶)\n\n\nmutate: converted 'month' from integer to character (0 new NA)\n\n\n        converted 'day' from integer to character (0 new NA)\n\n\nmutate: converted 'date' from character to Date (0 new NA)\n\n\nfilter: removed 50,726 rows (15%), 286,050 rows remaining\n\n\nanti_join: added no columns\n\n\n           &gt; rows only in x    45,008\n\n\n           &gt; rows only in y  (     39)\n\n\n           &gt; matched rows    (241,042)\n\n\n           &gt;                 =========\n\n\n           &gt; rows total        45,008\n\n\ncount: now 716 rows and 2 columns, ungrouped\n\n\nNice!\nTill next time - Happy analysing!"
  },
  {
    "objectID": "posts/018_06Sep_2024/index.html",
    "href": "posts/018_06Sep_2024/index.html",
    "title": "Biostats Book Club",
    "section": "",
    "text": "I was playing around with AI image creation this week and asked Microsoft Bing to create an image with ‚Äòbiostatistics book club‚Äô as a prompt - this is what it came up with:\n\n\n\n\n\nHmmm - who would have ever thought talking about biostatistics could be so interesting.\nThen, for even more fun, I asked Bing to create another image using ‚Äòbiostatistics fight club‚Äô as a prompt and it gave me this:\n\n\n\n\n\nYep, just what I imagined a bunch of pugilistic stats-nerds to look like."
  },
  {
    "objectID": "posts/018_06Sep_2024/index.html#essential-medical-statistics",
    "href": "posts/018_06Sep_2024/index.html#essential-medical-statistics",
    "title": "Biostats Book Club",
    "section": "2.1 Essential Medical Statistics",
    "text": "2.1 Essential Medical Statistics\n\n\n\n\n\nI can‚Äôt recommend this book enough. It‚Äôs now over 20 years old but that doesn‚Äôt mean it‚Äôs dated - the ‚Äòessentials‚Äô of statistics, well, haven‚Äôt really changed. Kirkwood‚Äôs book explains statistical concepts in such a clear and concise manner that it makes (for me at least), understanding them much, much easier. It strikes a good balance in covering all the important ideas in enough depth while still maintaining a relative lay language style."
  },
  {
    "objectID": "posts/018_06Sep_2024/index.html#intuitive-biostatistics",
    "href": "posts/018_06Sep_2024/index.html#intuitive-biostatistics",
    "title": "Biostats Book Club",
    "section": "2.2 Intuitive Biostatistics",
    "text": "2.2 Intuitive Biostatistics\n\n\n\n\n\nThe author of Intuitive Biostatistics is also the brains behind the Prism statistical software. You‚Äôll be pleased to know there are almost no formulae written amongst its pages and I think a reasonable summary of the authors intentions is to provide a ‚Äòcommon-sense‚Äô treatment of statistical ideas. The book is littered with teaching examples as well as sections on ‚ÄòQ & A‚Äôs‚Äô and ‚ÄòCommon Mistakes‚Äô and their potential solutions. Get the latest edition."
  },
  {
    "objectID": "posts/018_06Sep_2024/index.html#r-for-data-science",
    "href": "posts/018_06Sep_2024/index.html#r-for-data-science",
    "title": "Biostats Book Club",
    "section": "2.3 R for Data Science",
    "text": "2.3 R for Data Science\n\n\n\n\n\nif you are one of the ‚Äòcool kids‚Äô and use the tidyverse approach to coding in R, then this is probably worthwhile having. There is a free online version as well. R for Data Science is predominantly aimed at data-wrangling and preparing your data for analysis - tidyverse style. I don‚Äôt consider myself a great statistical programmer, so I have found some elements of this a little difficult, but the more basic stuff is really useful (and coding should be a daily journey of self-improvement anyway)."
  },
  {
    "objectID": "posts/018_06Sep_2024/index.html#the-r-book",
    "href": "posts/018_06Sep_2024/index.html#the-r-book",
    "title": "Biostats Book Club",
    "section": "2.4 The R book",
    "text": "2.4 The R book\n\n\n\n\n\nThe R Book differs from R for Data Science in that, yes it‚Äôs a book about coding in R, but the focus isn‚Äôt just on data-wrangling. This book will give you almost any bit of code to run nearly any statistical procedure in R that you could imagine. In that sense it‚Äôs also a worthwhile reference. Mind you, as a result of the breadth of material it covers, this is a BIG book!\nI hope you find these helpful in your statistical learning."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#not-sure-where-to-put-these",
    "href": "posts/014_14Jun_2024/index.html#not-sure-where-to-put-these",
    "title": "Academic Research - ‚ÄúWhat‚Äôs in a Title?‚Äù",
    "section": "4 Not Sure Where to Put These‚Ä¶",
    "text": "4 Not Sure Where to Put These‚Ä¶\nThe effect of having Christmas dinner with in-laws on gut microbiota composition. Well now you can put some science behind your decision to abstain from visiting the in-laws during the festive season - ‚ÄúIn participants visiting in-laws, there was a significant decrease in all¬†Ruminococcus¬†species, known to be associated with psychological stress and depression.‚Äù\nGet Me Off Your F‚Äôing Mailing List. This is just awesome."
  },
  {
    "objectID": "posts/019_20Sep_2024/index.html",
    "href": "posts/019_20Sep_2024/index.html",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "",
    "text": "I know I have touched on confounding before but it‚Äôs such an important concept in statistics that I thought I would dedicate a post to it today. There is so much one could talk about on the topic, but my aim is to provide you with some intuition as to what is actually happening when one variable ‚Äòconfuses‚Äô the relationship between two others.\nI‚Äôm going to use a fairly classic example from epidemiology that examines whether there is an association between drinking coffee (full-cream milk only thanks) and developing coronary heart disease (CHD).\n\n\n\n\n\n\nNote\n\n\n\nIn a sense this is a causal question - i.e.¬†‚ÄúDoes drinking coffee cause CHD?‚Äù, but for the sake of the illustration, let‚Äôs excuse ourselves from such a question‚Äôs fairly strict assumptions for the time being.\n\n\nBeing the good researchers that we are, we give considerable thought as to how to answer our question. That means we don‚Äôt forgo our own field-expertise and knowledge at the expense of the data-driven p-value gods. First and foremost, we think about the relationships we are investigating and all the potential factors that could influence those relationships. And that comes from theory, not statistical tests.\nIn examining those knowledge-driven theoretical relationships, we draw a Directed Acyclic Graph (DAG). This is a topic in its own right, but for another post - in the meantime some good primers can be found here and here. A DAG is otherwise more simply known as a causal diagram. It is meant to provide a visual representation of the causal relationships among a set of variables and in doing this forces the researcher to think about their scientific question in a more holistic manner. Indeed, statistical tests aren‚Äôt even involved at this point.\nSo, in thinking about our coffee-CHD question, it occurs to us that smoking is known to cause CHD and we wonder whether smoking might also be related to drinking coffee - i.e.¬†do coffee-drinkers also smoke more (or less) than people who prefer don‚Äôt drink coffee (tea perhaps?). In this case, a simple DAG might look something like:\n\n\n\n\n\nWhat this DAG helps to summarise is what we are now wondering - is smoking confusing or distorting the association between coffee drinking and the development of CHD? In other words, is smoking a confounder in the association between our exposure (coffee-drinking) and outcome (CHD) variables of interest?\nBefore we take a peek at our data, let‚Äôs refresh our memories regarding the three conditions a variable must fulfil to be considered a putative confounder:\n\nThe variable must be associated with both the exposure and the outcome.\nThe variable must be distributed unequally across the exposure (e.g.¬†between treatment groups).\nThe variable must not lie on the causal pathway between the exposure and the outcome (this would be a mediator and requires different treatment)."
  },
  {
    "objectID": "posts/019_20Sep_2024/index.html#background",
    "href": "posts/019_20Sep_2024/index.html#background",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "",
    "text": "I know I have touched on confounding before but it‚Äôs such an important concept in statistics that I thought I would dedicate a post to it today. There is so much one could talk about on the topic, but my aim is to provide you with some intuition as to what is actually happening when one variable ‚Äòconfuses‚Äô the relationship between two others.\nI‚Äôm going to use a fairly classic example from epidemiology that examines whether there is an association between drinking coffee (full-cream milk only thanks) and developing coronary heart disease (CHD).\n\n\n\n\n\n\nNote\n\n\n\nIn a sense this is a causal question - i.e.¬†‚ÄúDoes drinking coffee cause CHD?‚Äù, but for the sake of the illustration, let‚Äôs excuse ourselves from such a question‚Äôs fairly strict assumptions for the time being.\n\n\nBeing the good researchers that we are, we give considerable thought as to how to answer our question. That means we don‚Äôt forgo our own field-expertise and knowledge at the expense of the data-driven p-value gods. First and foremost, we think about the relationships we are investigating and all the potential factors that could influence those relationships. And that comes from theory, not statistical tests.\nIn examining those knowledge-driven theoretical relationships, we draw a Directed Acyclic Graph (DAG). This is a topic in its own right, but for another post - in the meantime some good primers can be found here and here. A DAG is otherwise more simply known as a causal diagram. It is meant to provide a visual representation of the causal relationships among a set of variables and in doing this forces the researcher to think about their scientific question in a more holistic manner. Indeed, statistical tests aren‚Äôt even involved at this point.\nSo, in thinking about our coffee-CHD question, it occurs to us that smoking is known to cause CHD and we wonder whether smoking might also be related to drinking coffee - i.e.¬†do coffee-drinkers also smoke more (or less) than people who prefer don‚Äôt drink coffee (tea perhaps?). In this case, a simple DAG might look something like:\n\n\n\n\n\nWhat this DAG helps to summarise is what we are now wondering - is smoking confusing or distorting the association between coffee drinking and the development of CHD? In other words, is smoking a confounder in the association between our exposure (coffee-drinking) and outcome (CHD) variables of interest?\nBefore we take a peek at our data, let‚Äôs refresh our memories regarding the three conditions a variable must fulfil to be considered a putative confounder:\n\nThe variable must be associated with both the exposure and the outcome.\nThe variable must be distributed unequally across the exposure (e.g.¬†between treatment groups).\nThe variable must not lie on the causal pathway between the exposure and the outcome (this would be a mediator and requires different treatment)."
  },
  {
    "objectID": "posts/019_20Sep_2024/index.html#data",
    "href": "posts/019_20Sep_2024/index.html#data",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "2 Data",
    "text": "2 Data\nOk, let‚Äôs now take a look at the hypothetical retrospective case-control data we‚Äôll be using today. It consists of 40 observations and three variables:\n\noutcome - did the individual have CHD (case) or not (control).\ncoffee-drinker - was the individual a coffee-drinker or not. This is our exposure variable of interest.\nsmoker - was the individual a smoker or not. This is our potential confounder.\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(ggmosaic)\nlibrary(kableExtra)\nlibrary(janitor)\n\n# Hypothetical data\ny &lt;- factor(c(rep(1, 20), rep(0, 20)), levels = c(0, 1), labels = c(\"control\", \"case\"))\nx1 &lt;- factor(c(rep(1, 10), rep(0, 10), rep(1, 5), rep(0, 15)), levels = c(0, 1), labels = c(\"no\", \"yes\"))\nx2 &lt;- factor(c(rep(1, 9), rep(0, 1), rep(1, 3), rep(0, 7), rep(1, 3), rep(0, 2), rep(1, 1), rep(0, 14)), levels = c(0, 1), labels = c(\"no\", \"yes\"))\ndf &lt;- data.frame(\"outcome\" = y, \"coffee_drinker\" = x1, \"smoker\" = x2)\ndf |&gt; \n  kable(align = \"c\")\n\n\n\n\n\noutcome\ncoffee_drinker\nsmoker\n\n\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nno\n\n\ncase\nno\nyes\n\n\ncase\nno\nyes\n\n\ncase\nno\nyes\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncontrol\nyes\nyes\n\n\ncontrol\nyes\nyes\n\n\ncontrol\nyes\nyes\n\n\ncontrol\nyes\nno\n\n\ncontrol\nyes\nno\n\n\ncontrol\nno\nyes\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno"
  },
  {
    "objectID": "posts/019_20Sep_2024/index.html#simple-epidemiological-approach",
    "href": "posts/019_20Sep_2024/index.html#simple-epidemiological-approach",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "3 Simple Epidemiological Approach",
    "text": "3 Simple Epidemiological Approach\nGiven the binary nature of all three variables we can explore the relationships nicely using a basic workhorse of epidemiological analysis - the 2x2 contingency table. In its simplest form this shows the frequency cross-tabulation of the exposure with the outcome. However, I think it‚Äôs also useful to display the conditional row percentages - in other words, the proportions (probabilities) of each outcome (categories as columns) given a particular exposure (categories as rows). In this way it is easy to eyeball whether the exposure is associated with the outcome without doing any specific test simply by looking at whether the row percentages vary greatly. As a good visual accompaniment for each cross-tabulation, I am also going to generate mosaic plots. These can give you an impression of potential associations without using any actual numbers.\n\n3.1 Crude Odds Ratio - Exposure/Outcome\nThe following cross-tabulation and mosaic plot show the potential association between coffee-drinking and CHD. If we look at the proportions of people with CHD in each exposure group we can see that these do in fact differ - 67% of coffee drinkers develop CHD, compared to 40% of non-coffee drinkers. That is a telling sign of an association before we even do anything. The mosaic plot mirrors these numbers graphically.\n\n\n\nCode\ntab &lt;- df |&gt; \n  tabyl(coffee_drinker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    coffee_drinker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    60% (15) \n    40% (10) \n    100% (25) \n  \n  \n    yes \n    33%  (5) \n    67% (10) \n    100% (15) \n  \n  \n    Total \n    50% (20) \n    50% (20) \n    100% (40) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df\ndf2$coffee_drinker &lt;- relevel(df2$coffee_drinker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, coffee_drinker), fill = coffee_drinker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nWorking out the odds ratio (OR) from a 2x2 contingency table is trivial. We want to divide the odds of having CHD given being a coffee-drinker by the odds of having CHD given being a non-coffee-drinker. That is:\n\\[\\text{OR} = \\frac{\\text{10/5}}{10/15}\\]\nOr equivalently:\n\\[\\text{OR} = \\frac{\\text{10 x 15}}{\\text{5 x 10}} = 3\\] The OR is 3 which indicates a 3-fold increase in the odds of CHD among coffee-drinkers compared to their non-coffee-drinking peers. We consider this a crude or unadjusted effect estimate.\n\n\n3.2 Adjusted Odds Ratios - Exposure/Outcome (via stratification)\nTo this point we haven‚Äôt even considered any potential confounding effect of smoking. So how could we incorporate that into the current analysis using 2x2 contingency tables? It‚Äôs actually very easy - we just stratify on smoking and generate two contingency tables - one for the association between coffee drinking and CHD in smokers and one for the association between coffee drinking and CHD in non-smokers. It then follows that within each stratum of smoking the effect of smoking is ‚Äòheld constant‚Äô and therefore cannot confound the association between coffee drinking and CHD. In other words, this becomes an adjusted ‚Äòeffect‚Äô of coffee drinking on CHD and is equivalent to ‚Äòcontrolling‚Äô for smoking in a multivariable statistical model (which I will demonstrate shortly).\n\n3.2.1 Smokers\nSo, the cross-tabulation for the association between coffee-drinking and CHD in smokers, becomes:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(smoker == \"yes\") |&gt; \n  tabyl(coffee_drinker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    coffee_drinker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    25% (1) \n    75%  (3) \n    100%  (4) \n  \n  \n    yes \n    25% (3) \n    75%  (9) \n    100% (12) \n  \n  \n    Total \n    25% (4) \n    75% (12) \n    100% (16) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(smoker == \"yes\")\ndf2$coffee_drinker &lt;- relevel(df2$coffee_drinker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, coffee_drinker), fill = coffee_drinker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nNote how this time the (conditional row) proportions of people with CHD in each exposure group are more similar (in this contrived example they are the same) - in this subgroup of smokers it doesn‚Äôt matter whether you‚Äôre a coffee drinker or not - 75% of people have CHD.\nNow, the OR is calculated as:\n\\[\\text{OR} = \\frac{\\text{9 x 1}}{\\text{3 x 3}} = 1\\]\nThe OR is 1 - in other words there is no longer any association between coffee drinking and CHD. We consider this an adjusted OR as we have removed any potential confounding effect of smoking by holding it at a constant value (i.e.¬†everyone is a smoker).\n\n\n3.2.2 Non-smokers\nNow let‚Äôs consider the non-smokers. The cross-tabulation for the association between coffee-drinking and CHD in non-smokers is:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(smoker == \"no\") |&gt; \n  tabyl(coffee_drinker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    coffee_drinker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (14) \n    33% (7) \n    100% (21) \n  \n  \n    yes \n    67%  (2) \n    33% (1) \n    100%  (3) \n  \n  \n    Total \n    67% (16) \n    33% (8) \n    100% (24) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(smoker == \"no\")\ndf2$coffee_drinker &lt;- relevel(df2$coffee_drinker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, coffee_drinker), fill = coffee_drinker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nDo you see a pattern here? Again, the conditional row percentages are the same across exposure groups (33%) - so, in this subgroup of non-smokers it also doesn‚Äôt matter whether you are a coffee drnker or not, the proportions of CHD are the same.\nThe OR this time is:\n\\[\\text{OR} = \\frac{\\text{1 x 14}}{\\text{7 x 2}} = 1\\]\nThe adjusted OR is again 1 - that is, there is no association between coffee drinking and CHD when we remove any potential confounding effect of smoking by holding it at a (different) constant value (i.e.¬†everyone is a non-smoker).\n\n\n3.2.3 Salient points\nThere are two important points to make in the comparison of the crude and adjusted estimates:\n\nThe adjusted OR is not equal to the crude OR - this suggests confounding is present.\nFurthermore, this represents a special case in which there is complete confounding - the OR reduces to the null value (i.e.¬†1). I will expand on these points in a moment.\n\n\n\n\n3.3 Crude Odds Ratio - Confounder/Outcome\nTo further illuminate the above ideas let‚Äôs repeat the analyses but this time ‚Äòswitch‚Äô the exposure and confounder around (hopefully it will become clear why we are doing this as you continue reading). The cross-tabulation and mosaic plot suggest an association between smoking and CHD - 75% of smokers develop CHD, compared to 33% of non-smokers.\n\n\n\nCode\ntab &lt;- df |&gt; \n  tabyl(smoker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    smoker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (16) \n    33%  (8) \n    100% (24) \n  \n  \n    yes \n    25%  (4) \n    75% (12) \n    100% (16) \n  \n  \n    Total \n    50% (20) \n    50% (20) \n    100% (40) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df\ndf2$smoker &lt;- relevel(df2$smoker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, smoker), fill = smoker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nThe OR for the association between smoking and CHD is:\n\\[\\text{OR} = \\frac{\\text{12 x 16}}{\\text{8 x 4}} = 6\\]\nSo the crude OR is 6, which indicates a 6-fold increase in the odds of CHD in smokers compared to non-smokers (ignoring coffee-drinking status).\n\n\n3.4 Adjusted Odds Ratios - Confounder/Outcome (via stratification)\nLet us know compare the adjusted ‚Äòeffects‚Äô in coffee-drinkers compared to non-coffee-drinkers.\n\n3.4.1 Coffee-drinkers\nSo, the cross-tabulation for the association between coffee-smoking and CHD in coffee-drinkers, becomes:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(coffee_drinker == \"yes\") |&gt; \n  tabyl(smoker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    smoker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (2) \n    33%  (1) \n    100%  (3) \n  \n  \n    yes \n    25% (3) \n    75%  (9) \n    100% (12) \n  \n  \n    Total \n    33% (5) \n    67% (10) \n    100% (15) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(coffee_drinker == \"yes\")\ndf2$smoker &lt;- relevel(df2$smoker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, smoker), fill = smoker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nAnd the adjusted OR is:\n\\[\\text{OR} = \\frac{\\text{9 x 2}}{\\text{1 x 3}} = 6\\]\nThe OR is 6 - in other words the same as the crude estimate.\n\n\n3.4.2 Non-coffee-drinkers\nNow let‚Äôs consider the non-coffee-drinkers. The cross-tabulation for the association between smokers and CHD in non-coffee-drinkers is:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(coffee_drinker == \"no\") |&gt; \n  tabyl(smoker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    smoker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (14) \n    33%  (7) \n    100% (21) \n  \n  \n    yes \n    25%  (1) \n    75%  (3) \n    100%  (4) \n  \n  \n    Total \n    60% (15) \n    40% (10) \n    100% (25) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(coffee_drinker == \"no\")\ndf2$smoker &lt;- relevel(df2$smoker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, smoker), fill = smoker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nAnd the adjusted OR is:\n\\[\\text{OR} = \\frac{\\text{3 x 14}}{\\text{7 x 1}} = 6\\]\nThe OR is again 6! Is this starting to make some sense?\n\n\n3.4.3 Salient points\nAgain, there are two important points to make in the comparison of these crude and adjusted estimates:\n\nIt doesn‚Äôt seem to matter whether we adjust for coffee-drinking or not, the association between smoking and CHD remains the same.\nIt then follows that coffee-drinking is not a confounder in the smoking-CHD relationship.\n\n\n\n\n3.5 Adjusted Odds Ratio - Overall\nIt just so happens that the adjusted OR‚Äôs from Section¬†3.2 are the same in each subgroup, but this is an exception rather than a rule. Normally, in the presence of confounding, effect estimates will differ in each subgroup to the crude estimate but will not be equal to each other. What do we do with separate effect estimates from each subgroup - this makes reporting somewhat painful, surely? Well, if the adjusted estimates aren‚Äôt too different from each other, they can be combined in a weighted manner to provide an overall summary estimate and this can be done using the Cochran-Mantel-Haenszel (CMH) test. I won‚Äôt illustrate this here as in the modern computing age there is really no need to be crunching this statistic anymore, and we use a statistical model instead.\nAs I alluded to above, we should only attempt to combine individual estimates when they are ‚Äòsimilar‚Äô. But what does that mean? How similar should they be and how different can they be? There are no hard and fast rules but I will outline a couple of guidelines at the end of this post. The important thing is to realise that when individual estimates are different, this actually represents an effect modification/interaction effect - that is the strength of the association between the exposure and the outcome depends on the level of the third variable. Interaction effects are of interest (scientifically and clinically) and we shouldn‚Äôt try to cancel them out by averaging over them."
  },
  {
    "objectID": "posts/019_20Sep_2024/index.html#simple-epidemiological-analysis",
    "href": "posts/019_20Sep_2024/index.html#simple-epidemiological-analysis",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "3 Simple Epidemiological Analysis",
    "text": "3 Simple Epidemiological Analysis\nGiven the binary nature of all three variables ‚Ä¶\n\n\nCode\n# Crude OR for effect of coffee drinking\nmod &lt;- glm(outcome ~ coffee_drinker, data = df, family = \"binomial\")\nexp(mod$coef) # 3\n\n\n      (Intercept) coffee_drinkeryes \n        0.6666667         3.0000000 \n\n\nCode\n# Adjusted OR for effect of coffee drinking (stratified by smoking)\nmod &lt;- glm(outcome ~ coffee_drinker, data = subset(df, smoker == \"no\"), family = \"binomial\")\nexp(mod$coef) # 1\n\n\n      (Intercept) coffee_drinkeryes \n              0.5               1.0 \n\n\nCode\nmod &lt;- glm(outcome ~ coffee_drinker, data = subset(df, smoker == \"yes\"), family = \"binomial\")\nexp(mod$coef) # 1\n\n\n      (Intercept) coffee_drinkeryes \n                3                 1 \n\n\nCode\n# Crude OR for effect of smoking\nmod &lt;- glm(outcome ~ smoker, data = df, family = \"binomial\")\nexp(mod$coef) # 6\n\n\n(Intercept)   smokeryes \n        0.5         6.0 \n\n\nCode\n# Adjusted OR for effect of smoking (stratified by coffee drinking)\nmod &lt;- glm(outcome ~ smoker, data = subset(df, coffee_drinker == \"no\"), family = \"binomial\")\nexp(mod$coef) # 6\n\n\n(Intercept)   smokeryes \n        0.5         6.0 \n\n\nCode\nmod &lt;- glm(outcome ~ smoker, data = subset(df, coffee_drinker == \"yes\"), family = \"binomial\")\nexp(mod$coef) # 6\n\n\n(Intercept)   smokeryes \n        0.5         6.0 \n\n\nCode\n# Adjusted OR's for both (statistical)\nsummary(mod &lt;- glm(outcome ~ coffee_drinker + smoker, data = df, family = \"binomial\"))\n\n\n\nCall:\nglm(formula = outcome ~ coffee_drinker + smoker, family = \"binomial\", \n    data = df)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)       -6.931e-01  4.485e-01  -1.546   0.1222  \ncoffee_drinkeryes -8.192e-16  9.342e-01   0.000   1.0000  \nsmokeryes          1.792e+00  9.283e-01   1.930   0.0536 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 55.452  on 39  degrees of freedom\nResidual deviance: 48.547  on 37  degrees of freedom\nAIC: 54.547\n\nNumber of Fisher Scoring iterations: 4\n\n\nCode\nexp(mod$coef)\n\n\n      (Intercept) coffee_drinkeryes         smokeryes \n              0.5               1.0               6.0 \n\n\nCode\ntbl_regression(mod, exponentiate = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\ncoffee_drinker\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n\n\n\n¬†¬†¬†¬†yes\n1.00\n0.13, 5.84\n&gt;0.9\n\n\nsmoker\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n\n\n\n¬†¬†¬†¬†yes\n6.00\n1.08, 48.2\n0.054\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nCode\ntable(df$coffee_drinker, df$outcome)\n\n\n     \n      control case\n  no       15   10\n  yes       5   10\n\n\nCode\nggplot(data = df) +\n geom_mosaic(aes(x = product(coffee_drinker, outcome)), fill = \"blue\") +\n  theme_bw(base_size = 15)\n\n\nWarning: The `scale_name` argument of `continuous_scale()` is deprecated as of ggplot2\n3.5.0.\n\n\nWarning: The `trans` argument of `continuous_scale()` is deprecated as of ggplot2 3.5.0.\n‚Ñπ Please use the `transform` argument instead.\n\n\nWarning: `unite_()` was deprecated in tidyr 1.2.0.\n‚Ñπ Please use `unite()` instead.\n‚Ñπ The deprecated feature was likely used in the ggmosaic package.\n  Please report the issue at &lt;https://github.com/haleyjeppson/ggmosaic&gt;.\n\n\n\n\n\n\n\n\n\nCode\nggplot(data = df) +\n geom_mosaic(aes(x = product(coffee_drinker, outcome), fill = smoker)) +\n  theme_bw(base_size = 15)\n\n\n\n\n\n\n\n\n\nMention that stratification stops being effective when you have too many categories (or continuous variables) to stratify on. The limitations visualisation found at the bottom of https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704-ep713_confounding-em/BS704-EP713_Confounding-EM7.html could be worthwhile including.\ngood info in: Attia, J. R., Jones, M. P., & Hure, A. (2017). Deconfounding confounding part 1: traditional explanations. Med J Aust, 206(6), 244-245. Richardson, A. M., & Joshy, G. (2020). Deconfounding confounding part 3: controlling for confounding in statistical analyses. Medical Journal of Australia.\nMaybe plan for this is: - Show contingency tables for: 1. coffee vs outcome (crude) 2. coffee vs outcome stratified by smokers (adjusted) 3. coffee vs outcome stratified by non-smokers (adjusted) Note that these are both different to the crude estimate. Also the adjusted estimates are = 1 which means complete confounding of the association between coffee and CHD by smoking 4. smoking vs outcome (crude) 5. smoking vs outcome stratified by coffee drinking (adjusted) 6. smoking vs outcome stratified by non-coffee drinking (adjusted) Note that these are all = 6 which means coffee drinking doesn‚Äôt affect the crude estimate and is not a confounder\nThen I could show the disproportionate distribution table and explain why smoking is a confounder (accompany by mosaic plot with smoking as a fill colour)\nThen replicate the 2 x 2 tables with the above models\nThen give tips on diffs between confounding and effect modification and whether to combine estimates or not from (Confounding vs Effect Modification Notes.md)\nFrom LMR Module 3 Stratification utilises the above concepts by assessing the association between the risk factor of interest and the outcome variable within homogeneous strata or cate- gories of the confounding variable. Within each stratum the confounding factor is ‚Äúheld constant‚Äù and therefore cannot confound the association between risk factor and outcome.\nThe key practical assessment of confounding involves comparing the ‚Äúcrude‚Äù and ‚Äúadjusted‚Äù measures of association. When they differ by a meaningfully important amount, confounding is considered to exist. Unfortunately there is no benchmark nor statistical test for the amount of discrepancy needed to rule confounding present, and indeed its determination is necessarily context-specific‚Äîwhat may be an important discrepancy between crude and adjusted measures in one application may be of far less importance in another.\nHow should we combine these two unconfounded estimates of the as- sumed common gender difference in SBP? Simply take the average, or create a weighted average, based on sample size, or on precision? Each of these options can be expressed in terms of a weighted average of the two w b(1) + w b(0) estimates, in the form of b‚àó1 = 1 1 0 1 , where the stratum-specific w1 + w0 estimates of the gender effect on SBP are denoted by b(1) and b(0) with 11 superscripts (1) for age ‚â• 40, and (0) for age &lt; 40, and similarly for the weights w1 and w0. We will see in fact that a multiple regression model produces just such a weighted average of stratum-specific effects, with weights that are sensi- ble and meaningful. We first present the general linear model with two covariates, and then examine this special case"
  },
  {
    "objectID": "posts/019_20Sep_2024/index.html#statistical-modelling-approach",
    "href": "posts/019_20Sep_2024/index.html#statistical-modelling-approach",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "4 Statistical Modelling Approach",
    "text": "4 Statistical Modelling Approach\nThe cross-tabulation approach is great for expository purposes but is limiting in the types of relationships you can practically explore - things just become unwieldy with variables that contain more than two categories and impossible with continuous variables.\nSo, we tend to use statistical models instead. These make confounder control/adjustment easy - we just need to include the potential confounder in the model along with our exposure of interest. Let‚Äôs now replicate everything we have done thus far, but in a modelling-paradigm.\n\n4.1 Crude Odds Ratio - Exposure/Outcome\n\n\nCode\nglm(outcome ~ coffee_drinker, data = df, family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n3.00\n0.81, 12.2\n0.11\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nWe get the same OR as in Section¬†3.1 in addition to a 95% C.I. and p-value.\n\n\n4.2 Adjusted Odds Ratios - Exposure/Outcome (via stratification)\nWe can replicate a stratified effect in our modelling by subsetting the data to select only those people in each smoking subgroup.\n\n4.2.1 Smokers\n\n\nCode\nglm(outcome ~ coffee_drinker, data = subset(df, smoker == \"yes\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n1.00\n0.04, 12.2\n&gt;0.9\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 1 (compare to Section¬†3.2.1)\n\n\n4.2.2 Non-smokers\n\n\nCode\nglm(outcome ~ coffee_drinker, data = subset(df, smoker == \"no\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n1.00\n0.04, 12.3\n&gt;0.9\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 1 (compare to Section¬†3.2.2)\n\n\n\n4.3 Crude Odds Ratio - Confounder/Outcome\n\n\nCode\nglm(outcome ~ smoker, data = df, family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    smoker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n6.00\n1.55, 27.5\n0.013\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 6 (compare to Section¬†3.3)\n\n\n4.4 Adjusted Odds Ratios - Confounder/Outcome (via stratification)\n\n4.4.1 Coffee-drinkers\n\n\nCode\nglm(outcome ~ smoker, data = subset(df, coffee_drinker == \"yes\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    smoker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n6.00\n0.43, 161\n0.2\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 6 (compare to Section¬†3.4.1)\n\n\n4.4.2 Non-coffee-drinkers\n\n\nCode\nglm(outcome ~ smoker, data = subset(df, coffee_drinker == \"no\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    smoker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n6.00\n0.64, 134\n0.15\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 6 (compare to Section¬†3.4.2)\n\n\n\n4.5 Adjusted Odds Ratio - Overall\nThe above models are all considered univariable in that one predictor only is specified in each model. In controlling or adjusting for a third variable we now produce a multivariable model where both the exposure and potential confounder are specified as predictors. This automatically produces an adjusted ‚Äòeffect‚Äô of coffee drinking on CHD, controlling for smoking (and conversely an adjusted ‚Äòeffect‚Äô of smoking on CHD, controlling for coffee drinking). We run these kinds of models all the time without thinking, but what the model is doing ‚Äòunder the hood‚Äô is calculating a weighted average of the individual subgroup estimates in producing a single coefficient that becomes our effect/s of interest.\n\n\nCode\nglm(outcome ~ coffee_drinker + smoker, data = df, family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n1.00\n0.13, 5.84\n&gt;0.9\n    smoker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n6.00\n1.08, 48.2\n0.054\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "posts/019_20Sep_2024/index.html#some-guidelines",
    "href": "posts/019_20Sep_2024/index.html#some-guidelines",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "5 Some Guidelines‚Ä¶",
    "text": "5 Some Guidelines‚Ä¶\nTo consolidate the ideas that we have explored today I want to lay out some very broad guidelines for how to interpret and compare crude and adjusted effects.\n\n5.1 No Confounding or Effect Modification Present\nIf there is neither confounding nor effect modification, the crude estimate of association and the stratum-specific estimates will be similar (converging to being the same in the ideal context). This is reflected in Sections¬†3.3 and Section¬†3.4 above.\n\n\n5.2 Only Confounding Present\nIf there is only confounding, the stratum-specific measures of association will be similar to one another, but they will be different from the overall crude estimate (by ~ 10% or more). In this situation, one can use CMH methods to calculate a weighted estimate and p-value, or even easier is to run a statistical model including the confounder as just another covariate. i.e.¬†(in R)\nmod1 &lt;- lm(outcome ~ exposure + confounder, data = dat)\nThis is reflected in Sections¬†3.1 and Section¬†3.2 above.\n\n\n5.3 Confounding and/or Effect Modification Present\nIn this case the stratum-specific estimates will differ from one another significantly and these will also differ from the overall crude estimate. These effects should be reported as they are and not weighted and combined (i.e.¬†averaged over) as this is of scientific and clinical interest in its own right. In practical terms in a statistical model, an interaction term should be specified. This changes the coding from:\nmod1 &lt;- lm(outcome ~ exposure + confounder, data = dat)\nto\nmod2 &lt;- lm(outcome ~ exposure * confounder, data = dat)\nAs the two models are ‚Äònested‚Äô, an assessment of whether the interaction term is necessary or not can be performed using a likelihood ratio test:\nanova(mod2, mod1)\nIf the p-value is significant you can conclude that the interaction term increases the explanatory power of the model and should be retained."
  },
  {
    "objectID": "posts/019_20Sep_2024/index.html#the-end",
    "href": "posts/019_20Sep_2024/index.html#the-end",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "6 The End!",
    "text": "6 The End!\nAnother post that has gone on longer than I had anticipated - there is much more I could talk about on the topic but my goal is not to make you fall asleep on your keyboard. Hopefully this has helped to make the concept of confounding just that little bit clearer in your mind. Until next time‚Ä¶"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Paul Sanfilippo",
    "section": "",
    "text": "I am an applied biostatistician currently assisting researchers working in the field of multiple sclerosis research at Monash University. I like playing with data, my miniature poodle and my family - not necessarily in that order."
  },
  {
    "objectID": "posts/020_04Oct_2024/index.html",
    "href": "posts/020_04Oct_2024/index.html",
    "title": "Breaking Free From ‚ÄòThe Cult of P‚Äô",
    "section": "",
    "text": "Apologies to all - I know I have already preached about this in the first WhatsApp message I sent out on ‚ÄòWeekly Stats Tips‚Äô about 12 months ago. But given the idea is so important, there‚Äôs no permanent and easily-accessible record of that message, and also that in hindsight I think I told you what not to do, but didn‚Äôt really suggest what you could/should do - I‚Äôm going to make it a blog post here as well."
  },
  {
    "objectID": "posts/020_04Oct_2024/index.html#background",
    "href": "posts/020_04Oct_2024/index.html#background",
    "title": "Breaking Free From The Cult of P",
    "section": "",
    "text": "I know I have touched on confounding before but it‚Äôs such an important concept in statistics that I thought I would dedicate a post to it today. There is so much one could talk about on the topic, but my aim is to provide you with some intuition as to what is actually happening when one variable ‚Äòconfuses‚Äô the relationship between two others.\nI‚Äôm going to use a fairly classic example from epidemiology that examines whether there is an association between drinking coffee (full-cream milk only thanks) and developing coronary heart disease (CHD).\n\n\n\n\n\n\nNote\n\n\n\nIn a sense this is a causal question - i.e.¬†‚ÄúDoes drinking coffee cause CHD?‚Äù, but for the sake of the illustration, let‚Äôs excuse ourselves from such a question‚Äôs fairly strict assumptions for the time being.\n\n\nBeing the good researchers that we are, we give considerable thought as to how to answer our question. That means we don‚Äôt forgo our own field-expertise and knowledge at the expense of the data-driven p-value gods. First and foremost, we think about the relationships we are investigating and all the potential factors that could influence those relationships. And that comes from theory, not statistical tests.\nIn examining those knowledge-driven theoretical relationships, we draw a Directed Acyclic Graph (DAG). This is a topic in its own right, but for another post - in the meantime some good primers can be found here and here. A DAG is otherwise more simply known as a causal diagram. It is meant to provide a visual representation of the causal relationships among a set of variables and in doing this forces the researcher to think about their scientific question in a more holistic manner. Indeed, statistical tests aren‚Äôt even involved at this point.\nSo, in thinking about our coffee-CHD question, it occurs to us that smoking is known to cause CHD and we wonder whether smoking might also be related to drinking coffee - i.e.¬†do coffee-drinkers also smoke more (or less) than people who prefer don‚Äôt drink coffee (tea perhaps?). In this case, a simple DAG might look something like:\n\n\n\n\n\nWhat this DAG helps to summarise is what we are now wondering - is smoking confusing or distorting the association between coffee drinking and the development of CHD? In other words, is smoking a confounder in the association between our exposure (coffee-drinking) and outcome (CHD) variables of interest?\nBefore we take a peek at our data, let‚Äôs refresh our memories regarding the three conditions a variable must fulfil to be considered a putative confounder:\n\nThe variable must be associated with both the exposure and the outcome.\nThe variable must be distributed unequally across the exposure (e.g.¬†between treatment groups).\nThe variable must not lie on the causal pathway between the exposure and the outcome (this would be a mediator and requires different treatment)."
  },
  {
    "objectID": "posts/020_04Oct_2024/index.html#data",
    "href": "posts/020_04Oct_2024/index.html#data",
    "title": "Breaking Free From The Cult of P",
    "section": "2 Data",
    "text": "2 Data\nOk, let‚Äôs now take a look at the hypothetical retrospective case-control data we‚Äôll be using today. It consists of 40 observations and three variables:\n\noutcome - did the individual have CHD (case) or not (control).\ncoffee-drinker - was the individual a coffee-drinker or not. This is our exposure variable of interest.\nsmoker - was the individual a smoker or not. This is our potential confounder.\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(ggmosaic)\nlibrary(kableExtra)\nlibrary(janitor)\n\n# Hypothetical data\ny &lt;- factor(c(rep(1, 20), rep(0, 20)), levels = c(0, 1), labels = c(\"control\", \"case\"))\nx1 &lt;- factor(c(rep(1, 10), rep(0, 10), rep(1, 5), rep(0, 15)), levels = c(0, 1), labels = c(\"no\", \"yes\"))\nx2 &lt;- factor(c(rep(1, 9), rep(0, 1), rep(1, 3), rep(0, 7), rep(1, 3), rep(0, 2), rep(1, 1), rep(0, 14)), levels = c(0, 1), labels = c(\"no\", \"yes\"))\ndf &lt;- data.frame(\"outcome\" = y, \"coffee_drinker\" = x1, \"smoker\" = x2)\ndf |&gt; \n  kable(align = \"c\")\n\n\n\n\n\noutcome\ncoffee_drinker\nsmoker\n\n\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nno\n\n\ncase\nno\nyes\n\n\ncase\nno\nyes\n\n\ncase\nno\nyes\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncontrol\nyes\nyes\n\n\ncontrol\nyes\nyes\n\n\ncontrol\nyes\nyes\n\n\ncontrol\nyes\nno\n\n\ncontrol\nyes\nno\n\n\ncontrol\nno\nyes\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno"
  },
  {
    "objectID": "posts/020_04Oct_2024/index.html#simple-epidemiological-approach",
    "href": "posts/020_04Oct_2024/index.html#simple-epidemiological-approach",
    "title": "Breaking Free From The Cult of P",
    "section": "3 Simple Epidemiological Approach",
    "text": "3 Simple Epidemiological Approach\nGiven the binary nature of all three variables we can explore the relationships nicely using a basic workhorse of epidemiological analysis - the 2x2 contingency table. In its simplest form this shows the frequency cross-tabulation of the exposure with the outcome. However, I think it‚Äôs also useful to display the conditional row percentages - in other words, the proportions (probabilities) of each outcome (categories as columns) given a particular exposure (categories as rows). In this way it is easy to eyeball whether the exposure is associated with the outcome without doing any specific test simply by looking at whether the row percentages vary greatly. As a good visual accompaniment for each cross-tabulation, I am also going to generate mosaic plots. These can give you an impression of potential associations without using any actual numbers.\n\n3.1 Crude Odds Ratio - Exposure/Outcome\nThe following cross-tabulation and mosaic plot show the potential association between coffee-drinking and CHD. If we look at the proportions of people with CHD in each exposure group we can see that these do in fact differ - 67% of coffee drinkers develop CHD, compared to 40% of non-coffee drinkers. That is a telling sign of an association before we even do anything. The mosaic plot mirrors these numbers graphically.\n\n\n\nCode\ntab &lt;- df |&gt; \n  tabyl(coffee_drinker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    coffee_drinker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    60% (15) \n    40% (10) \n    100% (25) \n  \n  \n    yes \n    33%  (5) \n    67% (10) \n    100% (15) \n  \n  \n    Total \n    50% (20) \n    50% (20) \n    100% (40) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df\ndf2$coffee_drinker &lt;- relevel(df2$coffee_drinker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, coffee_drinker), fill = coffee_drinker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nWorking out the odds ratio (OR) from a 2x2 contingency table is trivial. We want to divide the odds of having CHD given being a coffee-drinker by the odds of having CHD given being a non-coffee-drinker. That is:\n\\[\\text{OR} = \\frac{\\text{10/5}}{10/15}\\]\nOr equivalently:\n\\[\\text{OR} = \\frac{\\text{10 x 15}}{\\text{5 x 10}} = 3\\] The OR is 3 which indicates a 3-fold increase in the odds of CHD among coffee-drinkers compared to their non-coffee-drinking peers. We consider this a crude or unadjusted effect estimate.\n\n\n3.2 Adjusted Odds Ratios - Exposure/Outcome (via stratification)\nTo this point we haven‚Äôt even considered any potential confounding effect of smoking. So how could we incorporate that into the current analysis using 2x2 contingency tables? It‚Äôs actually very easy - we just stratify on smoking and generate two contingency tables - one for the association between coffee drinking and CHD in smokers and one for the association between coffee drinking and CHD in non-smokers. It then follows that within each stratum of smoking the effect of smoking is ‚Äòheld constant‚Äô and therefore cannot confound the association between coffee drinking and CHD. In other words, this becomes an adjusted ‚Äòeffect‚Äô of coffee drinking on CHD and is equivalent to ‚Äòcontrolling‚Äô for smoking in a multivariable statistical model (which I will demonstrate shortly).\n\n3.2.1 Smokers\nSo, the cross-tabulation for the association between coffee-drinking and CHD in smokers, becomes:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(smoker == \"yes\") |&gt; \n  tabyl(coffee_drinker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    coffee_drinker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    25% (1) \n    75%  (3) \n    100%  (4) \n  \n  \n    yes \n    25% (3) \n    75%  (9) \n    100% (12) \n  \n  \n    Total \n    25% (4) \n    75% (12) \n    100% (16) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(smoker == \"yes\")\ndf2$coffee_drinker &lt;- relevel(df2$coffee_drinker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, coffee_drinker), fill = coffee_drinker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nNote how this time the (conditional row) proportions of people with CHD in each exposure group are more similar (in this contrived example they are the same) - in this subgroup of smokers it doesn‚Äôt matter whether you‚Äôre a coffee drinker or not - 75% of people have CHD.\nNow, the OR is calculated as:\n\\[\\text{OR} = \\frac{\\text{9 x 1}}{\\text{3 x 3}} = 1\\]\nThe OR is 1 - in other words there is no longer any association between coffee drinking and CHD. We consider this an adjusted OR as we have removed any potential confounding effect of smoking by holding it at a constant value (i.e.¬†everyone is a smoker).\n\n\n3.2.2 Non-smokers\nNow let‚Äôs consider the non-smokers. The cross-tabulation for the association between coffee-drinking and CHD in non-smokers is:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(smoker == \"no\") |&gt; \n  tabyl(coffee_drinker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    coffee_drinker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (14) \n    33% (7) \n    100% (21) \n  \n  \n    yes \n    67%  (2) \n    33% (1) \n    100%  (3) \n  \n  \n    Total \n    67% (16) \n    33% (8) \n    100% (24) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(smoker == \"no\")\ndf2$coffee_drinker &lt;- relevel(df2$coffee_drinker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, coffee_drinker), fill = coffee_drinker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nDo you see a pattern here? Again, the conditional row percentages are the same across exposure groups (33%) - so, in this subgroup of non-smokers it also doesn‚Äôt matter whether you are a coffee drnker or not, the proportions of CHD are the same.\nThe OR this time is:\n\\[\\text{OR} = \\frac{\\text{1 x 14}}{\\text{7 x 2}} = 1\\]\nThe adjusted OR is again 1 - that is, there is no association between coffee drinking and CHD when we remove any potential confounding effect of smoking by holding it at a (different) constant value (i.e.¬†everyone is a non-smoker).\n\n\n3.2.3 Salient points\nThere are two important points to make in the comparison of the crude and adjusted estimates:\n\nThe adjusted OR is not equal to the crude OR - this suggests confounding is present.\nFurthermore, this represents a special case in which there is complete confounding - the OR reduces to the null value (i.e.¬†1). I will expand on these points in a moment.\n\n\n\n\n3.3 Crude Odds Ratio - Confounder/Outcome\nTo further illuminate the above ideas let‚Äôs repeat the analyses but this time ‚Äòswitch‚Äô the exposure and confounder around (hopefully it will become clear why we are doing this as you continue reading). The cross-tabulation and mosaic plot suggest an association between smoking and CHD - 75% of smokers develop CHD, compared to 33% of non-smokers.\n\n\n\nCode\ntab &lt;- df |&gt; \n  tabyl(smoker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    smoker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (16) \n    33%  (8) \n    100% (24) \n  \n  \n    yes \n    25%  (4) \n    75% (12) \n    100% (16) \n  \n  \n    Total \n    50% (20) \n    50% (20) \n    100% (40) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df\ndf2$smoker &lt;- relevel(df2$smoker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, smoker), fill = smoker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nThe OR for the association between smoking and CHD is:\n\\[\\text{OR} = \\frac{\\text{12 x 16}}{\\text{8 x 4}} = 6\\]\nSo the crude OR is 6, which indicates a 6-fold increase in the odds of CHD in smokers compared to non-smokers (ignoring coffee-drinking status).\n\n\n3.4 Adjusted Odds Ratios - Confounder/Outcome (via stratification)\nLet us know compare the adjusted ‚Äòeffects‚Äô in coffee-drinkers compared to non-coffee-drinkers.\n\n3.4.1 Coffee-drinkers\nSo, the cross-tabulation for the association between coffee-smoking and CHD in coffee-drinkers, becomes:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(coffee_drinker == \"yes\") |&gt; \n  tabyl(smoker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    smoker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (2) \n    33%  (1) \n    100%  (3) \n  \n  \n    yes \n    25% (3) \n    75%  (9) \n    100% (12) \n  \n  \n    Total \n    33% (5) \n    67% (10) \n    100% (15) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(coffee_drinker == \"yes\")\ndf2$smoker &lt;- relevel(df2$smoker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, smoker), fill = smoker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nAnd the adjusted OR is:\n\\[\\text{OR} = \\frac{\\text{9 x 2}}{\\text{1 x 3}} = 6\\]\nThe OR is 6 - in other words the same as the crude estimate.\n\n\n3.4.2 Non-coffee-drinkers\nNow let‚Äôs consider the non-coffee-drinkers. The cross-tabulation for the association between smokers and CHD in non-coffee-drinkers is:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(coffee_drinker == \"no\") |&gt; \n  tabyl(smoker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    smoker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (14) \n    33%  (7) \n    100% (21) \n  \n  \n    yes \n    25%  (1) \n    75%  (3) \n    100%  (4) \n  \n  \n    Total \n    60% (15) \n    40% (10) \n    100% (25) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(coffee_drinker == \"no\")\ndf2$smoker &lt;- relevel(df2$smoker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, smoker), fill = smoker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nAnd the adjusted OR is:\n\\[\\text{OR} = \\frac{\\text{3 x 14}}{\\text{7 x 1}} = 6\\]\nThe OR is again 6! Is this starting to make some sense?\n\n\n3.4.3 Salient points\nAgain, there are two important points to make in the comparison of these crude and adjusted estimates:\n\nIt doesn‚Äôt seem to matter whether we adjust for coffee-drinking or not, the association between smoking and CHD remains the same.\nIt then follows that coffee-drinking is not a confounder in the smoking-CHD relationship.\n\n\n\n\n3.5 Adjusted Odds Ratio - Overall\nIt just so happens that the adjusted OR‚Äôs from Section¬†3.2 are the same in each subgroup, but this is an exception rather than a rule. Normally, in the presence of confounding, effect estimates will differ in each subgroup to the crude estimate but will not be equal to each other. What do we do with separate effect estimates from each subgroup - this makes reporting somewhat painful, surely? Well, if the adjusted estimates aren‚Äôt too different from each other, they can be combined in a weighted manner to provide an overall summary estimate and this can be done using the Cochran-Mantel-Haenszel (CMH) test. I won‚Äôt illustrate this here as in the modern computing age there is really no need to be crunching this statistic anymore, and we use a statistical model instead.\nAs I alluded to above, we should only attempt to combine individual estimates when they are ‚Äòsimilar‚Äô. But what does that mean? How similar should they be and how different can they be? There are no hard and fast rules but I will outline a couple of guidelines at the end of this post. The important thing is to realise that when individual estimates are different, this actually represents an effect modification/interaction effect - that is the strength of the association between the exposure and the outcome depends on the level of the third variable. Interaction effects are of interest (scientifically and clinically) and we shouldn‚Äôt try to cancel them out by averaging over them."
  },
  {
    "objectID": "posts/020_04Oct_2024/index.html#statistical-modelling-approach",
    "href": "posts/020_04Oct_2024/index.html#statistical-modelling-approach",
    "title": "Breaking Free From The Cult of P",
    "section": "4 Statistical Modelling Approach",
    "text": "4 Statistical Modelling Approach\nThe cross-tabulation approach is great for expository purposes but is limiting in the types of relationships you can practically explore - things just become unwieldy with variables that contain more than two categories and impossible with continuous variables.\nSo, we tend to use statistical models instead. These make confounder control/adjustment easy - we just need to include the potential confounder in the model along with our exposure of interest. Let‚Äôs now replicate everything we have done thus far, but in a modelling-paradigm.\n\n4.1 Crude Odds Ratio - Exposure/Outcome\n\n\nCode\nglm(outcome ~ coffee_drinker, data = df, family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n3.00\n0.81, 12.2\n0.11\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nWe get the same OR as in Section¬†3.1 in addition to a 95% C.I. and p-value.\n\n\n4.2 Adjusted Odds Ratios - Exposure/Outcome (via stratification)\nWe can replicate a stratified effect in our modelling by subsetting the data to select only those people in each smoking subgroup.\n\n4.2.1 Smokers\n\n\nCode\nglm(outcome ~ coffee_drinker, data = subset(df, smoker == \"yes\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n1.00\n0.04, 12.2\n&gt;0.9\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 1 (compare to Section¬†3.2.1)\n\n\n4.2.2 Non-smokers\n\n\nCode\nglm(outcome ~ coffee_drinker, data = subset(df, smoker == \"no\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n1.00\n0.04, 12.3\n&gt;0.9\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 1 (compare to Section¬†3.2.2)\n\n\n\n4.3 Crude Odds Ratio - Confounder/Outcome\n\n\nCode\nglm(outcome ~ smoker, data = df, family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    smoker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n6.00\n1.55, 27.5\n0.013\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 6 (compare to Section¬†3.3)\n\n\n4.4 Adjusted Odds Ratios - Confounder/Outcome (via stratification)\n\n4.4.1 Coffee-drinkers\n\n\nCode\nglm(outcome ~ smoker, data = subset(df, coffee_drinker == \"yes\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    smoker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n6.00\n0.43, 161\n0.2\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 6 (compare to Section¬†3.4.1)\n\n\n4.4.2 Non-coffee-drinkers\n\n\nCode\nglm(outcome ~ smoker, data = subset(df, coffee_drinker == \"no\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    smoker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n6.00\n0.64, 134\n0.15\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 6 (compare to Section¬†3.4.2)\n\n\n\n4.5 Adjusted Odds Ratio - Overall\nThe above models are all considered univariable in that one predictor only is specified in each model. In controlling or adjusting for a third variable we now produce a multivariable model where both the exposure and potential confounder are specified as predictors. This automatically produces an adjusted ‚Äòeffect‚Äô of coffee drinking on CHD, controlling for smoking (and conversely an adjusted ‚Äòeffect‚Äô of smoking on CHD, controlling for coffee drinking). We run these kinds of models all the time without thinking, but what the model is doing ‚Äòunder the hood‚Äô is calculating a weighted average of the individual subgroup estimates in producing a single coefficient that becomes our effect/s of interest.\n\n\nCode\nglm(outcome ~ coffee_drinker + smoker, data = df, family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n1.00\n0.13, 5.84\n&gt;0.9\n    smoker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n6.00\n1.08, 48.2\n0.054\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "posts/020_04Oct_2024/index.html#some-guidelines",
    "href": "posts/020_04Oct_2024/index.html#some-guidelines",
    "title": "Breaking Free From The Cult of P",
    "section": "5 Some Guidelines‚Ä¶",
    "text": "5 Some Guidelines‚Ä¶\nTo consolidate the ideas that we have explored today I want to lay out some very broad guidelines for how to interpret and compare crude and adjusted effects.\n\n5.1 No Confounding or Effect Modification Present\nIf there is neither confounding nor effect modification, the crude estimate of association and the stratum-specific estimates will be similar (converging to being the same in the ideal context). This is reflected in Sections¬†3.3 and Section¬†3.4 above.\n\n\n5.2 Only Confounding Present\nIf there is only confounding, the stratum-specific measures of association will be similar to one another, but they will be different from the overall crude estimate (by ~ 10% or more). In this situation, one can use CMH methods to calculate a weighted estimate and p-value, or even easier is to run a statistical model including the confounder as just another covariate. i.e.¬†(in R)\nmod1 &lt;- lm(outcome ~ exposure + confounder, data = dat)\nThis is reflected in Sections¬†3.1 and Section¬†3.2 above.\n\n\n5.3 Confounding and/or Effect Modification Present\nIn this case the stratum-specific estimates will differ from one another significantly and these will also differ from the overall crude estimate. These effects should be reported as they are and not weighted and combined (i.e.¬†averaged over) as this is of scientific and clinical interest in its own right. In practical terms in a statistical model, an interaction term should be specified. This changes the coding from:\nmod1 &lt;- lm(outcome ~ exposure + confounder, data = dat)\nto\nmod2 &lt;- lm(outcome ~ exposure * confounder, data = dat)\nAs the two models are ‚Äònested‚Äô, an assessment of whether the interaction term is necessary or not can be performed using a likelihood ratio test:\nanova(mod2, mod1)\nIf the p-value is significant you can conclude that the interaction term increases the explanatory power of the model and should be retained."
  },
  {
    "objectID": "posts/020_04Oct_2024/index.html#the-end",
    "href": "posts/020_04Oct_2024/index.html#the-end",
    "title": "Breaking Free From The Cult of P",
    "section": "6 The End!",
    "text": "6 The End!\nAnother post that has gone on longer than I had anticipated - there is much more I could talk about on the topic but my goal is not to make you fall asleep on your keyboard. Hopefully this has helped to make the concept of confounding just that little bit clearer in your mind. Until next time‚Ä¶"
  },
  {
    "objectID": "posts/020_04Oct_2024/index.html#the-issue",
    "href": "posts/020_04Oct_2024/index.html#the-issue",
    "title": "Breaking Free From ‚ÄòThe Cult of P‚Äô",
    "section": "1 The Issue",
    "text": "1 The Issue\nSo, as researchers how many of you can honestly say that you have never used language around p-values in a way reminiscent of this xkcd comic (or one of these 509 variations on the same theme):\n\n\n\n\n\nI know I can‚Äôt. Certainly in my early research career and before changing paths to become a biostatistician, I was guilty of this sort of thing. All in the name of some misguided sense of wanting to achieve research glory by getting that p-value to fall under the all-important threshold of 0.05 (my research glory did not ever materialise by the way).\nWell can I suggest, it just doesn‚Äôt matter. And can I also suggest to stop thinking about p-values in this way.\nIt‚Äôs generally accepted that Sir Ronald Fisher was the guy who at least formalised the ideas of Null Hypothesis Significance Testing (NHST) and the p-value in the 1920‚Äôs. He never intended for the p-value of 0.05 to be set in stone as an arbiter of the value of a piece of research in a statistical sense, and certainly not in a scientific sense. The intention was to use it to guide decision making, not make the actual decision. But, in the past 100 years this arbitrary threshold of p = 0.05 has not only stuck, but taken on an almost mythic status in the research community, in part it seems because humans are lazy and like to avoid decision-making where possible. The natural and silly extension to this entrenched notion of ‚Äúsignificance‚Äù is that researchers may feel elated if their statistical test returns p = 0.049 and despondent if it returns p = 0.051. On reflection, any rational person can see that‚Äôs crazy, but it doesn‚Äôt seem to give us pause for thought when we have our results in front of us."
  },
  {
    "objectID": "posts/020_04Oct_2024/index.html#what-not-to-do",
    "href": "posts/020_04Oct_2024/index.html#what-not-to-do",
    "title": "Breaking Free From ‚ÄòThe Cult of P‚Äô",
    "section": "2 What Not To Do",
    "text": "2 What Not To Do\nYou may be aware that there has been a push in recent years by the statistical community to abandon the p-value all together, set off in large part by a statement published by the American Statistical Association in 2016. There is nothing new in any of this - the same issues have been of concern for decades and a Pubmed (or even Google) search will reveal a large literature on the topic. In 2019, The American Statistician dedicated an entire special issue to statistical inference entitled ‚ÄúStatistical Inference in the 21st Century: A World Beyond¬†p¬†&lt; 0.05.‚Äù If you scan down those titles you‚Äôll see that not too many are in favour of retaining the 0.05 dichotomy we seem to have imposed on ourselves. The Editorial in that edition doesn‚Äôt pull any punches in terms of how to use p-values in your research:\n\nDon‚Äôt base your conclusions solely on whether an association or effect was found to be ‚Äústatistically significant‚Äù (i.e., the p-value passed some arbitrary threshold such as p &lt; 0.05).\nDon‚Äôt believe that an association or effect exists just because it was statistically significant.\nDon‚Äôt believe that an association or effect is absent just because it was not statistically significant.\nDon‚Äôt believe that your p-value gives the probability that chance alone produced the observed association or effect or the probability that your test hypothesis is true.\nDon‚Äôt conclude anything about scientific or practical importance based on statistical significance (or lack thereof).\n\nAnd the authors go further to say:\nThe ASA Statement on P-Values and Statistical Significance stopped just short of recommending that declarations of ‚Äústatistical significance‚Äù be abandoned. We take that step here. We conclude, based on our review of the articles in this special issue and the broader literature, that it is time to stop using the term ‚Äústatistically significant‚Äù entirely. Nor should variants such as ‚Äúsignificantly different,‚Äù ‚Äúp &lt; 0.05,‚Äù and ‚Äúnonsignificant‚Äù survive, whether expressed in words, by asterisks in a table, or in some other way.\nPretty damning!"
  },
  {
    "objectID": "posts/020_04Oct_2024/index.html#what-you-couldshould-do",
    "href": "posts/020_04Oct_2024/index.html#what-you-couldshould-do",
    "title": "Breaking Free From ‚ÄòThe Cult of P‚Äô",
    "section": "3 What You Could/Should Do",
    "text": "3 What You Could/Should Do\nIf you look at some of the papers in that special issue, you will find a wealth of suggestions about how to approach a ‚ÄúPost ‚Äòp &lt; 0.05‚Äô world‚Äù, including that we should all become Bayesians (not meaning to toot my own trumpet, but I have also written around this in one of my not-so-highly-cited papers several years ago - I‚Äôm sure it will gain traction in years to come!).\nSo I‚Äôm not going to reiterate all of those suggestions here. Instead, for what my opinion is worth, I have two practical tips that I think would make interpreting and reporting our research more robust and less reliant on the p-value.\n\n3.1 Use A Language Of ‚ÄòEvidence‚Äô\nWe can still calculate p-values as we currently do - nothing needs to change. But instead of referring to the p-value itself, let‚Äôs start discussing our results in the context of ‚Äòevidence‚Äô (i.e.¬†against the null hypothesis of no effect). The following table from this paper sums it up beautifully and so I am not going to reinvent the wheel (in fact I can thoroughly recommend the other 3 papers in the series, so do give them a look if you have time).\n\n\n\n\n\nNow, I have used my own subtle variation on the terminology over the years and replaced ‚Äúinsufficient‚Äù with ‚Äúweak‚Äù, ‚Äúsome‚Äù with ‚Äúmoderate‚Äù and ‚Äúoverwhelming‚Äù with ‚Äúvery strong‚Äù. The point is, it doesn‚Äôt really matter what words you use to describe your ‚Äòeffect‚Äô with - this kind of language immediately frees you from the confines of an arbitrary dichotomisation that forces you to either value or devalue all of your hard work in one fell swoop. Instead of saying ‚Äòthere was a trend towards statistical significance for the association between x and y‚Äô, if your p-value turns out to be 0.051, you can now simply say there was ‚Äòweak evidence for an association between x and y‚Äô, instead.\n\n\n3.2 Be Consistent In p-Value Reporting\nIf I had a dollar for every time a research student has sent me results that contain p-values between 0.045 and 0.05 and reported that to 3 decimal places, I‚Äôd be a little less poor than I am now. Seriously, there is no need to do this. Please adhere to the following reporting guide and don‚Äôt try to put lipstick on a pig by suggesting that your p-value of 0.049 is any different to 0.05.\n\n\n\n\n\nThese are a couple of small changes we can make in our research reporting practice that will help to break our self-imposed binds to the p-value gods. Until next time‚Ä¶"
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html",
    "href": "posts/021_18Oct_2024/index.html",
    "title": "Survival Analysis - Under the Hood",
    "section": "",
    "text": "Welcome to today‚Äôs post which is based on a talk that some of you may have heard me give in lab meetings. A warning in advance - it‚Äôs a bit of a lengthy one, so maybe down a couple of reds to steel yourself before you start. We‚Äôre going to discuss survival analysis, but in a similar spirit to how I presented the post on logistic regression a couple of months ago - nothing too fancy, but with an aim that you understand what is happening ‚Äòunder the hood‚Äô when you next fire up your stats software to plot a Kaplan-Meier curve or run a Cox model.\nSurvival analysis is a BIG topic and in university statistics courses, it tends to have an entire subject dedicted just to it, so it‚Äôs not just taught as a part of regression modelling, for example. I also don‚Äôt find it particularly easy, even now - there are a lot of challenging concepts to understand, and that is in large part because time is such an integral component of everything you do in this area of biostatistics. So, what I hope I can do today is to take a broad-brush approach to survival methods, touch on the concepts that I think are most important in establishing a good base understanding, build in a little intuition for these, and also give you some practical tips along the way.\nLet‚Äôs get started."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#primary-transformations",
    "href": "posts/021_18Oct_2024/index.html#primary-transformations",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.1 Primary Transformations",
    "text": "3.1 Primary Transformations\nIf we have the odds of an event, then we can transform that to a probability - or a risk, which is another way to refer to the chance of an outcome - by dividing the odds by the odds plus 1.\n\\[\\text{p} = \\frac{\\text{odds}}{1 + \\text{odds}}\\]\nIf instead we have the probability of an event, we can transform that to the equivalent odds of the event, by dividing the probability by its complement (that is 1 minus the probability).\n\\[\\text{odds} = \\frac{\\text{p}}{1 - \\text{p}}\\]\nAnd the log-odds is simply taking the natural logarithm of the odds - there is nothing difficult about that.\n\\[\\text{log-odds} = \\text{log(odds)}\\]\nSo these three are what I consider primary transformations and we can cycle between them quite easily with a calculator."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#secondary-transformations",
    "href": "posts/021_18Oct_2024/index.html#secondary-transformations",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.2 Secondary Transformations",
    "text": "3.2 Secondary Transformations\nThere are additional transformations that you can make and while these aren‚Äôt quite as straightforward, they are even more fundamental to logistic regression, which ultimately uses the log-odds as the scale to estimate its parameters on.\nThe first is that we can get to a probability directly from the log-odds by taking the exponent of the log-odds divided by the exponent of the log-odds plus one.\n\\[\\text{p} = \\frac{e^{\\text{log-odds}}}{1 + e^{\\text{log-odds}}}\\]\nAnd as you can see, that‚Äôs really no different to the first equation above because the exponent of the log-odds is simply the odds (the reverse transformation of logging a number is to take the exponent of that logged number). i.e.\n\\[\\text{odds} = e^{\\text{log-odds}}\\]\nNow, the final transformation, and the one that‚Äôs most relevant to logistic regression shows the relationship between the log-odds and probability and is known as the logit function (red box) - and we‚Äôll talk more about this in a minute.\n\\[\\text{log-odds} = \\text{log}\\Bigl(\\frac{\\text{p}}{1 - \\text{p}}\\Bigr)\\]"
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#section",
    "href": "posts/021_18Oct_2024/index.html#section",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.3 ",
    "text": "3.3"
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#what-is-survival-analysis",
    "href": "posts/021_18Oct_2024/index.html#what-is-survival-analysis",
    "title": "Survival Analysis - Under the Hood",
    "section": "0.1 What is survival analysis?",
    "text": "0.1 What is survival analysis?\nToday‚Äôs post comes from a talk that some of you may have already heard me give in lab meetings, but I thought it could be helpful to have a ‚Äòprint‚Äô copy so I‚Äôm going to do that here. The material in the talk was loosely based on the following paper:\nOphthalmic Statistics Note 11: Logistic Regression\nI‚Äôm going back to basics today. I think too often we use statistical techniques without really understanding what is going on ‚Äòunder the hood‚Äô. While that is ok to some extent, a better appreciation of what you‚Äôre actually doing when you perform a hypothesis test, or run a regression model, lends more robustness to the validity of both your results and your interpretation of them.\nSo let‚Äôs take a more fundamental look at logistic regression - a workhorse statistical model that I‚Äôd be willing to bet most of you have run at some point in your research careers. Now, because I consider myself more an applied rather than theoretical biostatistician, I am going to try and get my main points across to you with as little maths as possible, and hopefully not bore you in the process (but this is statistics, so hey‚Ä¶).\nWhat I hope you can gain from reading this is to think about logistic regression in a new and different way - one that completely illuminates the technique for you."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#calendar-time-format",
    "href": "posts/021_18Oct_2024/index.html#calendar-time-format",
    "title": "Survival Analysis - Under the Hood",
    "section": "2.1 Calendar-time format",
    "text": "2.1 Calendar-time format\nWhat does survival data look like? Well - in the case of a prospective study, subjects are usually recruited into the study at different times. To illustrate this, let‚Äôs consider this hypothetical dataset of patients diagnosed with some disease that we follow until they either die or are right-censored. The study was planned to recruit for 4 months from Dec 2015 to Mar 2016, and during this period 5 subjects were entered into the study, all starting at various times, but importantly, the exact dates of their start were recorded. All subjects were then followed at regular intervals, and the study was then ended, as planned, in Jan 2017.\nNow, this is showing the observation times for each subject as they occurred in calendar-time format. While this reflects the reality of how survival data are collected, it doesn‚Äôt really inform you with an intuitive sense of time because you can‚Äôt easily make comparisons across subjects."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#time-on-study-format",
    "href": "posts/021_18Oct_2024/index.html#time-on-study-format",
    "title": "Survival Analysis - Under the Hood",
    "section": "2.2 Time-on-study format",
    "text": "2.2 Time-on-study format\nSo, for easier interpretation we can align the start time of all subjects like so - and we call this format, ‚Äòtime-on-study‚Äô. We can see that Subject 1 dropped out because they moved interstate and couldn‚Äôt attend clinic appointments anymore, and so they are considered censored. Their follow-up time was 6 months. Subjects 2 and 4 are also censored, but for a different reason - they were still alive when we decided to end the study and had the study gone on we may have been able to follow them for longer. In any case they were each observed for 12 months. Unfortunately, the other two subjects weren‚Äôt so lucky - Subject 3 died after 7 months and Subject 5 died after 4 months of being in the study.\n\n\n\n\n\nNow, what if we were interested in working out the one-year survival for this sample of patients?\nHow would you do that?"
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#this-bed-is-too-soft",
    "href": "posts/021_18Oct_2024/index.html#this-bed-is-too-soft",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.1 This bed is too soft",
    "text": "3.1 This bed is too soft\nThis can commonly lead us to assume that the subject survived the full year - and if I‚Äôm to borrow a metaphor from a children‚Äôs fable, it would have to in this case be Goldilocks. So what we have here is the equivalent of Goldilock‚Äôs bed being too soft.\n\n\n\n\n\nUnder this assumption we would include Subject 1 in the proportion numerator and calculate the one-year survival as 3/5 or 60% (we could also get that from a logistic regression model).\n\\[\n\\text{Survival (one-year)} = \\frac{\\text{3}}{\\text{5}} = 60\\%\n\\]\nBut there‚Äôs something not quite right about that.\nUnfortunately, Goldilocks doesn‚Äôt sleep well."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#this-bed-is-too-hard",
    "href": "posts/021_18Oct_2024/index.html#this-bed-is-too-hard",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.2 This bed is too hard",
    "text": "3.2 This bed is too hard\nWell then, let‚Äôs be conservative you might say, and assume Subject 1 has in fact died. In this case we would exclude them from the proportion numerator and calculate the one-year survival as 2/5 or 40%. Can I suggest this is now the equivalent of Goldilock‚Äôs bed being too hard, because we know Subject 1 was followed for a full 6 months - that must count for something surely?\n\n\n\n\n\n\\[\n\\text{Survival (one-year)} = \\frac{\\text{2}}{\\text{5}} = 40\\%\n\\]\nPoor Goldilocks still doesn‚Äôt sleep well."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#ahhh-just-right.",
    "href": "posts/021_18Oct_2024/index.html#ahhh-just-right.",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.3 Ahhh, just right.",
    "text": "3.3 Ahhh, just right.\nWhen you use survival methods, it in fact turns out that the probability of surviving in the entire year, taking into account censoring is (4/5)x(2/3) = 53%, somewhere in between 40 and 60% - ahhh, Goldilocks has finally found the right bed.\n\n\n\n\n\n\\[\n\\text{Survival (one-year)} = \\frac{\\text{4}}{\\text{5}} \\times \\frac{\\text{2}}{\\text{3}} = 53\\%\n\\] I will show you how to get to that number shortly."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#the-take-home-message",
    "href": "posts/021_18Oct_2024/index.html#the-take-home-message",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.4 The take-home message",
    "text": "3.4 The take-home message\nIf everybody is observed for at least the same length of time of interest (e.g.¬†1 year), logistic regression could be used‚Ä¶\n‚Ä¶ but not if the duration of observation is less for some people."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#ahhh-just-right",
    "href": "posts/021_18Oct_2024/index.html#ahhh-just-right",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.3 Ahhh, just right",
    "text": "3.3 Ahhh, just right\nWhen you use survival methods, it in fact turns out that the probability of surviving in the entire year, taking into account censoring is (4/5)x(2/3) = 53%, somewhere in between 40 and 60% - ahhh, Goldilocks has finally found the right bed.\n\n\n\n\n\n\\[\n\\text{Survival (one-year)} = \\frac{\\text{4}}{\\text{5}} \\times \\frac{\\text{2}}{\\text{3}} = 53\\%\n\\] I will show you how to get to that number shortly."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#questions---so-many-questions",
    "href": "posts/021_18Oct_2024/index.html#questions---so-many-questions",
    "title": "Survival Analysis - Under the Hood",
    "section": "4.1 Questions - so many questions‚Ä¶",
    "text": "4.1 Questions - so many questions‚Ä¶\nSo, let me get you thinking about this by way of a thought exercise.\nIf I were to ask you about human mortality; then as a function of age, how would you draw or describe:\n\nThe probability of death?\nThe hazard of death?\nSurvival?\n\nFollowing from that:\n\nIs the probability of death greater at 80 or 100?\nIs the hazard of death greater at 80 or 100?\n\nAnd finally:\nWhat‚Äôs the difference!?"
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#some-answers",
    "href": "posts/021_18Oct_2024/index.html#some-answers",
    "title": "Survival Analysis - Under the Hood",
    "section": "4.2 Some answers",
    "text": "4.2 Some answers\n(Most figures in today‚Äôs post created with¬†BioRender.com)."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#some-answershopefully",
    "href": "posts/021_18Oct_2024/index.html#some-answershopefully",
    "title": "Survival Analysis - Under the Hood",
    "section": "4.2 Some answers‚Ä¶hopefully",
    "text": "4.2 Some answers‚Ä¶hopefully\nOk, this is a little bit of a trick question, because it really hangs on semantics.\n\n4.2.1 Marginal probabilities of event\nWhen we talk about the probability of death we are referring to a probability density for death (as a function of age). This is where the area under the density curve (and while this involves a bit of calculus, you don‚Äôt need to concern yourself with the details) adds up to 1 - that is everyone is certain to die eventually. In other words the sums of all possible probabilities of dying across all ages equals 1 - and that‚Äôs what essentially defines a probability density function. So if you look at the following plot you will appreciate that the overall probability of death gradually increases until about 80 years than actually declines after that. So, in fact the most likely age to die at is about 80, not 100, and that‚Äôs simply because fewer people are alive at 100 in the first place. This is what is known as a ‚Äòmarginal‚Äô probability of death.\n\n\n\n\n\n\n\n4.2.2 Conditional probabilities of event\nNow, the marginal probability of death is a different concept to the hazard of death. The hazard represents an instantaneous or conditional risk of the event happening. Looking at the commensurate hazard function, you will see that it increases exponentially. What this is saying is that the probability or risk that you die in the next year, increases the longer you remain alive. So, the risk of dying in the next year, given that you survived until 100 years is far, far, greater than than case for an 80 year old. These are conditional probabilities. In other words, the risk in the next instant of experiencing the event given you haven‚Äôt yet experienced it.\n\n\n\n\n\n\n\n4.2.3 Survival\nAnd then finally the survival. This isn‚Äôt difficult. We all die (don‚Äôt thank me for cheering you up) - so the survival function eventually reaches 0."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#definition",
    "href": "posts/021_18Oct_2024/index.html#definition",
    "title": "Survival Analysis - Under the Hood",
    "section": "5.1 Definition",
    "text": "5.1 Definition\nOk, so having some understanding of the hazard rate is really important in survival analysis. At a basic level, the hazard rate is like an incident rate - the number of new events that occur in a period of time divided by the number of people at risk of that event at the beginning of that period. But it gets a little more complicated because the hazard rate is actually the limiting case of that - the instantaneous event rate. In other words the probability that, given that a subject has survived up to some time t, that he or she has the event in the next infinitesimally small time interval, divided by the length of that interval - and we need to use calculus to be able to compute that. The hazard rate ranges from 0 (no risk of the event) to infinity (certain to experience the event) and over time, it can be constant, increase, decrease, or take some other non-monotonic pattern, which I‚Äôll show you in a moment.\nThe non-complicated, non-calculus formula for the hazard rate is shown below: Basically the hazard rate can be thought of as the number of events between time \\(t\\) and \\(\\delta t\\), divided by the number of people at risk of the event at time \\(t\\) - that gives a conditional probability of the event - and then we divide that by the length of the interval.\n\\[\nh(t) = \\lim_{\\delta \\text{t} \\to 0} \\frac{\\left( \\cfrac{ \\text{number of events between time t and } (\\text{t} + \\delta \\text{t})}{\\text{number of people at risk at time t }} \\right) } {\\delta \\text{t}}\n\\]\n\\[\n= \\lim_{\\delta \\text{t} \\to 0} \\frac{\\text{conditional probability of event between time t and } (\\text{t} + \\delta \\text{t})} {\\delta \\text{t}}\n\\]"
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#the-hazard-rate-is-related-to-the-incident-rate",
    "href": "posts/021_18Oct_2024/index.html#the-hazard-rate-is-related-to-the-incident-rate",
    "title": "Survival Analysis - Under the Hood",
    "section": "5.2 The hazard rate is related to the incident rate",
    "text": "5.2 The hazard rate is related to the incident rate\nLet me use a silly example to illustrate this idea of how the hazard rate is related to your run-of-the-mill incident rate. Let‚Äôs think about the hazard of getting married at 20 years of age - perilous I might suggest. But jokes aside, how would you work this out? Well we could start by thinking in terms of the incident rate - in a nutshell, count up the number of people married between the ages of 20 and 21 and divide that by the number NOT already married at age 20.\nSo, let‚Äôs say we have 1000 people in a population of interest who still aren‚Äôt married at age 20 and we follow them forward and see that 100 get married in the next year. The incident rate is then 100/1000 per year or 0.1 marriages per year.\n\n\n\n\n\nWe could have actually considered a shorter time interval to work out the incident rate - let‚Äôs now say 6 months instead of one year. Assuming marriages are evenly distributed then the incident rate is now 50/1000 per 6 months or 0.05 marriages per 1/2 year.\n\n\n\n\n\nAnd we could even consider the incident rate in the forthcoming day, which would be 0.274/1000 per day or 0.000274 marriages per day.\n\n\n\n\n\nSo the incident rate differs in these three cases only because we are using different units of time. If we normalised the time period to one year in the second and third scenarios, then we would expect the same number of marriages over the year.\nThe point of all of this is really to show you that the hazard rate is a limiting case of the incident rate. As the time interval approaches zero, we essentially have an instantaneous estimate of the risk of the event at any point in time and this is what the hazard rate represents. It‚Äôs not hard when you think about it."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#some-common-hazard-function-shapes",
    "href": "posts/021_18Oct_2024/index.html#some-common-hazard-function-shapes",
    "title": "Survival Analysis - Under the Hood",
    "section": "5.3 Some common hazard function shapes",
    "text": "5.3 Some common hazard function shapes\nLet‚Äôs have a look at some of the more common hazard function shapes that we might encounter. The shape of the hazard function is determined by the underlying disease, physiological or physical process - so some of these are more realistic in biological systems and some more realistic in mechanical systems.\n\n5.3.1 Constant (exponential)\nThe first example is of a constant hazard - so the risk of failure at any point in time is the same. This could apply to a light bulb and it could also apply to a healthy individual in a study. When the hazard function is constant we say the survival model is exponential and memoryless, in that the age of the subject has no bearing on the instantaneous risk of failure. Now, this is not the most realistic distribution for biological systems, because - taking a well-known quote from a textbook - ‚ÄúIf human lifetimes were exponential there wouldn‚Äôt be old or young people, just lucky or unlucky ones‚Äù.\n\n\n\n\n\n\n\n5.3.2 Increasing (Weibull)\nThis is a monotone increasing hazard function. This is one of the more realistic shapes that we expect in the real world - basically as things age, they wear out and are more likely to fail. In industry, cars and batteries are good examples of this. Most biological systems also follow this profile - as humans it‚Äôs why our life insurance premiums go up as we age. At 60 you‚Äôve done well to have lived this long, but your short term prospects aren‚Äôt as optimistic as they were at 20, and the actuaries build this in to their company‚Äôs insurance premiums.\nAn example in a medical study might be the hazard of metastatic cancer patients, where the event of interest is death. As survival time increases for such a patient, and as the prognosis accordingly worsens, the patient‚Äôs potential for dying of the disease also increases.\n\n\n\n\n\n\n\n5.3.3 Decreasing (Weibull)\nBut not everything wears out over time. This is an example of a monotone decreasing hazard. Many electronic systems actually become more robust over time - believe it or not. In other words, the instantaneous risk of failure tends to be higher earlier on and then subsides with time. So manufacturers can utilise that to their advantage by running the system for a period before shipping and if it passes, it‚Äôs likely to be ok for the customer.\nIn medicine we might see this in patients recovering from major surgery, because the potential for dying after surgery usually decreases as the time after surgery increases.\n\n\n\n\n\n\n\n5.3.4 Increasing/Decreasing (lognormal)\nNow we‚Äôre getting even more complex with a hazard function that can both increase and decrease. These kinds of shapes become increasingly difficult to generalise, but a specific example might be the hazard for tuberculosis patients since their potential for dying from the disease initially increases and then subsides.\n\n\n\n\n\n\n\n5.3.5 Bathtub\nAnd then finally the ‚Äòbathtub‚Äô shaped hazard function, which I introduced to you before when talking about the risks of humans dying with age. In a general sense, this hazard function reflects the fact that some of the riskiest days of your life are those following birth, and then things start to settle down through and until mid-life.\nBut gradually, as you age, parts of your body start to wear out and your short-term risk begins to increase again - and if you manage to get to 80 for example, then your short-term risk of dying might be as high as it was during your first months of life."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#time-on-study",
    "href": "posts/021_18Oct_2024/index.html#time-on-study",
    "title": "Survival Analysis - Under the Hood",
    "section": "6.1 Time-on-study",
    "text": "6.1 Time-on-study\nI want to illustrate these ideas about time scales with a figure and a very simplified example. Let‚Äôs assume this is some hazard function for death related to some fictitious disease. The risk of death related to this disease initially increases and then decreases over time. \nWhen we look at the hazard, it‚Äôs a fairly safe bet that we would use time-on-study in this case, because the hazard is going to primarily be a function of time since diagnosis more than a function of age. And that‚Äôs because the subject becomes at risk of the outcome from the point of diagnosis. So in this case we can expect subjects of any age to have a similar hazard. If we were to estimate a Cox model for example, the hazards are fairly consistent in shape across subjects, regardless of age (note that the intercepts might actually be different for different ages - gives rise to proportional hazards - but the shapes are consistent). In other words, an apples-to-apples comparison of the hazard function at any point in observation time."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#age",
    "href": "posts/021_18Oct_2024/index.html#age",
    "title": "Survival Analysis - Under the Hood",
    "section": "6.2 Age",
    "text": "6.2 Age\nBut now let‚Äôs assume that the same hazard is instead a function of age instead of time-on-study - for the sake of the exercise let‚Äôs say the outcome is now a type of cancer and you become ‚Äòat risk‚Äô of this cancer from birth. So the risk of the outcome this time relates more to just getting older than it does to any other precursor event.\n\n\n\n\n\nIn this case, if we were to use time-on-study as the time scale, we end up trying to compare hazard functions among age groups that are not consistent in shape on that time scale. So, the hazard for a 20 year old entering the study might be different to the hazard for a 60 year old entering the study. Now we are attempting an apples-to-oranges comparison of the hazard function at any point in observation time and that is less than ideal - instead, we‚Äôre better off using age as the time scale on which to base our computations.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen using age as the time scale for the analysis, age is automatically corrected for in the analysis and does not require specific covariate adjustment."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#non-parametric-kaplan-meier",
    "href": "posts/021_18Oct_2024/index.html#non-parametric-kaplan-meier",
    "title": "Survival Analysis - Under the Hood",
    "section": "7.1 Non-parametric (Kaplan-Meier)",
    "text": "7.1 Non-parametric (Kaplan-Meier)\nSo, in statistics, what does non-parametric actually mean? Basically, we do not rely on any assumptions that the data are drawn from a given parametric family of probability distributions. So we‚Äôre less interested or able to make inferences about the population and we‚Äôre really only talking about the data that we actually have - the sample at hand.\nNow, in survival analysis, the Kaplan-Meier estimator is one such non-parametric approach to calculating the survival function. We don‚Äôt make any assumptions about the distribution of survival times in the population from which the sample was drawn. So, it‚Äôs just the empirical probability of surviving past certain times in the sample - and to that end it‚Äôs main purpose is descriptive.\nThe Kaplan-Meier approach is NOT a modelling method, and it‚Äôs really about visualising survival - and we can do this overall or commonly by splitting based on one important grouping variable, whether that be treatment or some other exposure variable (e.g.¬†sex). Usually this a starting point in your survival analyses, and you‚Äôll go on and do some modelling, but sometimes a simple Kaplan-Meier curve is enough to do what you want.\nLet‚Äôs revisit that first example I showed you with the 5 patients, because it‚Äôs a worthwhile exercise in understanding how the Kaplan-Meier estimator works out a probability of survival taking into account, censoring. It‚Äôs a simple exercise with these data but with more complex datasets you obviously wouldn‚Äôt do this manually.\nSo, we have our 5 patients and we want to plot the proportion surviving at every step over the study duration.\nRemember that if we‚Äôd used logistic regression, then depending on how we defined our censored subject we could have ended up with a one-year survival of either 40 or 60%, and in fact the correct one-year survival estimate is 53%. How do we get that?\n\n\n\n\n\nSo, we start at time 0 with 100% survival and with all subjects at risk of the event.\nWe know that survival remains at 100% until at least month 4 because everyone remains alive during that time.\nBut then Subject 5 dies at 4 months. So, this is the event we‚Äôre interested in and the survival probability is re-estimated each time there is an event. How do we do that? Well, we multiply the cumulative survival proportion before the event by the conditional survival proportion just after the event - conditional because a subject must have survived at least until the event, to remain in the study after it.\nAnd if we do that, this time around it‚Äôs a fairly straightforward multiplication of 1 x 4/5, which gives 80%. Note that censoring has not even entered into this calculation yet.\nSo the proportion surviving immediately after 4 months is 80% and the number of people at risk of dying at this point is 4.\n\n\n\n\n\nNow as we go along in the time line we see that Subject 1 is censored at 6 months and we indicate that with a mark at that time point. Note that the survival estimate doesn‚Äôt change after 6 months, but the number of people at risk of dying now reduces by 1 to 3.\n\n\n\n\n\nNow Subject 3 dies at 7 months. To re-estimate the survival this time we need to take just a little bit of extra care. The cumulative survival proportion immediately before 7 months is still 80% but when we work out the conditional survival immediately after 7 months, the censoring of Subject 1 means that we now only have 2 people remaining alive (that we are certain about) out of the 3 potentially at risk, just prior to the death.\nWe then multiply those two survival proportions to give us the overall survival after 7 months. And that is 4 people out of 5 (80%) alive immediately prior to and 2 people out of 3 (67%) alive immediately after. The overall estimate of survival at 7 months is then 4/5 times 2/3 which gives us the 53% we stated earlier.\nClearly this is a very simple case and you can appreciate how complicated this can get in larger datasets, but standard functions will take care of the calculations for you in whatever statistical package you‚Äôre using.\n\n\n\n\n\n(Most figures in today‚Äôs post created with¬†BioRender.com)."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#non-parametric-kaplan-meier-estimator",
    "href": "posts/021_18Oct_2024/index.html#non-parametric-kaplan-meier-estimator",
    "title": "Survival Analysis - Under the Hood",
    "section": "7.1 Non-parametric (Kaplan-Meier estimator)",
    "text": "7.1 Non-parametric (Kaplan-Meier estimator)\nSo, in statistics, what does non-parametric actually mean? Basically, we do not rely on any assumptions that the data are drawn from a given parametric family of probability distributions. So we‚Äôre less interested or able to make inferences about the population and we‚Äôre really only talking about the data that we actually have - the sample at hand.\nNow, in survival analysis, the Kaplan-Meier estimator is one such non-parametric approach to calculating the survival function. We don‚Äôt make any assumptions about the distribution of survival times in the population from which the sample was drawn. So, it‚Äôs just the empirical probability of surviving past certain times in the sample - and to that end it‚Äôs main purpose is descriptive.\nThe Kaplan-Meier approach is NOT a modelling method, and it‚Äôs really about visualising survival - and we can do this overall or commonly by splitting based on one important grouping variable, whether that be treatment or some other exposure variable (e.g.¬†sex). Usually this a starting point in your survival analyses, and you‚Äôll go on and do some modelling, but sometimes a simple Kaplan-Meier curve is enough to do what you want.\nLet‚Äôs revisit that first example I showed you with the 5 patients, because it‚Äôs a worthwhile exercise in understanding how the Kaplan-Meier estimator works out a probability of survival taking into account, censoring. It‚Äôs a simple exercise with these data but with more complex datasets you obviously wouldn‚Äôt do this manually.\nSo, we have our 5 patients and we want to plot the proportion surviving at every step over the study duration.\nRemember that if we‚Äôd used logistic regression, then depending on how we defined our censored subject we could have ended up with a one-year survival of either 40 or 60%, and in fact the correct one-year survival estimate is 53%. How do we get that?\n\n\n\n\n\nSo, we start at time 0 with 100% survival and with all subjects at risk of the event.\nWe know that survival remains at 100% until at least month 4 because everyone remains alive during that time.\nBut then Subject 5 dies at 4 months. So, this is the event we‚Äôre interested in and the survival probability is re-estimated each time there is an event. How do we do that? Well, we multiply the cumulative survival proportion before the event by the conditional survival proportion just after the event - conditional because a subject must have survived at least until the event, to remain in the study after it.\nAnd if we do that, this time around it‚Äôs a fairly straightforward multiplication of 1 x 4/5, which gives 80%. Note that censoring has not even entered into this calculation yet.\nSo the proportion surviving immediately after 4 months is 80% and the number of people at risk of dying at this point is 4.\n\n\n\n\n\nNow as we go along in the time line we see that Subject 1 is censored at 6 months and we indicate that with a mark at that time point. Note that the survival estimate doesn‚Äôt change after 6 months, but the number of people at risk of dying now reduces by 1 to 3.\n\n\n\n\n\nNow Subject 3 dies at 7 months. To re-estimate the survival this time we need to take just a little bit of extra care. The cumulative survival proportion immediately before 7 months is still 80% but when we work out the conditional survival immediately after 7 months, the censoring of Subject 1 means that we now only have 2 people remaining alive (that we are certain about) out of the 3 potentially at risk, just prior to the death.\nWe then multiply those two survival proportions to give us the overall survival after 7 months. And that is 4 people out of 5 (80%) alive immediately prior to and 2 people out of 3 (67%) alive immediately after. The overall estimate of survival at 7 months is then 4/5 times 2/3 which gives us the 53% we stated earlier.\nClearly this is a very simple case and you can appreciate how complicated this can get in larger datasets, but standard functions will take care of the calculations for you in whatever statistical package you‚Äôre using."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#semi-parametric-cox-model",
    "href": "posts/021_18Oct_2024/index.html#semi-parametric-cox-model",
    "title": "Survival Analysis - Under the Hood",
    "section": "7.2 Semi-parametric (Cox model)",
    "text": "7.2 Semi-parametric (Cox model)\nIf you want to estimate survival as a function of more than just one grouping variable, then you really need to look further afield than the Kaplan-Meier estimator and this is usually where people will turn to a Cox model.\nThe Cox model is considered semi-parametric because there is both a non-parametric component and a fully parametric component. The baseline hazard function is non-parametric and is estimated directly from the sample data without any assumptions about the distribution of survival times. Because of this the hazard function in a Cox model can take on any shape you can imagine. This is both an advantage and a disadvantage - an advantage because the shape can be highly flexible, but a disadvantage because we don‚Äôt actually get any information returned in terms of parameter estimates that tell us anything about what that shape looks like.\nThe parametric part of the Cox model is to do with the effects of the predictors on the hazard function - and these are assumed to be linearly related to the log of the hazard. The coefficient estimates we get out of a Cox model are exponentiated to give a hazard ratio which is just the ratio of two hazard rates.\nBecause we don‚Äôt get any parameter estimates for the baseline hazard function, we aren‚Äôt in a position to easily predict the absolute risk of an event, and so the strength of the Cox model is really in providing information about relative rather than absolute risks of the event occurring.\nAnd finally because the hazard is empirical, you really shouldn‚Äôt try to predict from a Cox model beyond the range of observable data that you have.\nOk, before we leave the Cox model, let‚Äôs talk about The Elephant in the room - proportional hazards. This is probably the most important assumption that you need to keep in mind and one that you should really test for each time you run a Cox model.\nThe proportional hazards assumption basically requires that the hazard ratio is constant over time, or equivalently, that the hazard for one individual is proportional to the hazard for any other individual, and where the proportionality constant doesn‚Äôt depend on time.\nSo if we look at the figure below - I have plotted hypothetical hazard functions for males (in blue) and females (in red) for the risk of some event. You‚Äôll note that the shapes are essentially the same but are scaled versions of one another. When proportional hazards hold, then we can take a ratio of the two hazard rates at any time point and that ratio should remain constant. And I‚Äôve illustrated that here at two time points - \\(t1\\) and \\(t2\\). The difference in the hazards obviously varies but the ratio of the hazards at those two time points is the same. At time 1 a hazard rate of 3 for males and 2 for females gives a hazard ratio of 1.5 and similarly, at time 2, a hazard rate of 0.75 for males and 0.5 for females, again gives a hazard ratio of 1.5.\nIn reality, hazard rates are probably not going to be exactly proportional the entire time, but the aim is to make sure any departures from proportionality are minimal - otherwise this means the hazard ratio itself varies with time and this adds further complexity to your model."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#parametric-exponential-weibull-log-normal-etc",
    "href": "posts/021_18Oct_2024/index.html#parametric-exponential-weibull-log-normal-etc",
    "title": "Survival Analysis - Under the Hood",
    "section": "7.3 Parametric (exponential, Weibull, log-normal, etc)",
    "text": "7.3 Parametric (exponential, Weibull, log-normal, etc)\nFully parametric survival models are essentially the same in concept and interpretation to the Cox model, barring a few structural differences. The main difference to the Cox model is that the baseline hazard function is fully specified in these models. In other words, the hazard function is assumed to follow a particular statistical distribution, and we looked at some of these before - monotone increasing, decreasing, lognormal, etc. And this then becomes a modelling choice that you have to make. You can never be certain that you‚Äôve chosen the correct distribution but there are tools that you can use to guide your choice - theory should always be at the top of the list, but you can also do things like generate empirical hazards to see what their shape might look like, compare model fits via some fitting criteria like the AIC, and so on. If you get the distribution about right, these models are more powerful than their Cox counterparts.\nLike Cox models, you get adjusted estimates of risk/survival. Unlike Cox models, you can estimate absolute risk of the event of interest, because you get a model parameter or parameters returned for that. Additionally, you‚Äôre in a better position with these models to predict beyond the maximum follow-up time, because there is a specified distribution of survival times that is expected to follow that of the population. But that comes with the usual caveat of always be careful about extrapolating beyond the range of your data."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#extensions",
    "href": "posts/021_18Oct_2024/index.html#extensions",
    "title": "Survival Analysis - Under the Hood",
    "section": "7.4 Extensions",
    "text": "7.4 Extensions\n\nTime-dependent covariates:\n\nThe value of the covariate (e.g.¬†treatment, weight, etc) changes over time, but it‚Äôs effect remains constant (i.e.¬†the HR doesn‚Äôt change).\n\nTime-dependent coefficients:\n\nThe value of the covariate doesn‚Äôt change over time, but it‚Äôs effect does (i.e.¬†the HR varies over time).\n\nFlexible parametric models:\n\nInclude splines on the log baseline hazard to allow even more flexibility in hazard function shapes.\n\nCompeting risks:\n\nExplicitly account for other events that ‚Äòcompete‚Äô with our primary event of interest.\n\nRecurrent events/frailties (clustering)\n\nMore than one event per person/groups of people with shared characteristics\n\n\n(Most figures in today‚Äôs post created with¬†BioRender.com)."
  },
  {
    "objectID": "posts/019_04Oct_2024/index.html",
    "href": "posts/019_04Oct_2024/index.html",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "",
    "text": "I know I have touched on confounding before but it‚Äôs such an important concept in statistics that I thought I would dedicate a post to it today. There is so much one could talk about on the topic, but my aim is to provide you with some intuition as to what is actually happening when one variable ‚Äòconfuses‚Äô the relationship between two others.\nI‚Äôm going to use a fairly classic example from epidemiology that examines whether there is an association between drinking coffee (full-cream milk only thanks) and developing coronary heart disease (CHD).\n\n\n\n\n\n\nNote\n\n\n\nIn a sense this is a causal question - i.e.¬†‚ÄúDoes drinking coffee cause CHD?‚Äù, but for the sake of the illustration, let‚Äôs excuse ourselves from such a question‚Äôs fairly strict assumptions for the time being.\n\n\nBeing the good researchers that we are, we give considerable thought as to how to answer our question. That means we don‚Äôt forgo our own field-expertise and knowledge at the expense of the data-driven p-value gods. First and foremost, we think about the relationships we are investigating and all the potential factors that could influence those relationships. And that comes from theory, not statistical tests.\nIn examining those knowledge-driven theoretical relationships, we draw a Directed Acyclic Graph (DAG). This is a topic in its own right, but for another post - in the meantime some good primers can be found here and here. A DAG is otherwise more simply known as a causal diagram. It is meant to provide a visual representation of the causal relationships among a set of variables and in doing this forces the researcher to think about their scientific question in a more holistic manner. Indeed, statistical tests aren‚Äôt even involved at this point.\nSo, in thinking about our coffee-CHD question, it occurs to us that smoking is known to cause CHD and we wonder whether smoking might also be related to drinking coffee - i.e.¬†do coffee-drinkers also smoke more (or less) than people who prefer don‚Äôt drink coffee (tea perhaps?). In this case, a simple DAG might look something like:\n\n\n\n\n\nWhat this DAG helps to summarise is what we are now wondering - is smoking confusing or distorting the association between coffee drinking and the development of CHD? In other words, is smoking a confounder in the association between our exposure (coffee-drinking) and outcome (CHD) variables of interest?\nBefore we take a peek at our data, let‚Äôs refresh our memories regarding the three conditions a variable must fulfil to be considered a putative confounder:\n\nThe variable must be associated with both the exposure and the outcome.\nThe variable must be distributed unequally across the exposure (e.g.¬†between treatment groups).\nThe variable must not lie on the causal pathway between the exposure and the outcome (this would be a mediator and requires different treatment)."
  },
  {
    "objectID": "posts/019_04Oct_2024/index.html#background",
    "href": "posts/019_04Oct_2024/index.html#background",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "",
    "text": "I know I have touched on confounding before but it‚Äôs such an important concept in statistics that I thought I would dedicate a post to it today. There is so much one could talk about on the topic, but my aim is to provide you with some intuition as to what is actually happening when one variable ‚Äòconfuses‚Äô the relationship between two others.\nI‚Äôm going to use a fairly classic example from epidemiology that examines whether there is an association between drinking coffee (full-cream milk only thanks) and developing coronary heart disease (CHD).\n\n\n\n\n\n\nNote\n\n\n\nIn a sense this is a causal question - i.e.¬†‚ÄúDoes drinking coffee cause CHD?‚Äù, but for the sake of the illustration, let‚Äôs excuse ourselves from such a question‚Äôs fairly strict assumptions for the time being.\n\n\nBeing the good researchers that we are, we give considerable thought as to how to answer our question. That means we don‚Äôt forgo our own field-expertise and knowledge at the expense of the data-driven p-value gods. First and foremost, we think about the relationships we are investigating and all the potential factors that could influence those relationships. And that comes from theory, not statistical tests.\nIn examining those knowledge-driven theoretical relationships, we draw a Directed Acyclic Graph (DAG). This is a topic in its own right, but for another post - in the meantime some good primers can be found here and here. A DAG is otherwise more simply known as a causal diagram. It is meant to provide a visual representation of the causal relationships among a set of variables and in doing this forces the researcher to think about their scientific question in a more holistic manner. Indeed, statistical tests aren‚Äôt even involved at this point.\nSo, in thinking about our coffee-CHD question, it occurs to us that smoking is known to cause CHD and we wonder whether smoking might also be related to drinking coffee - i.e.¬†do coffee-drinkers also smoke more (or less) than people who prefer don‚Äôt drink coffee (tea perhaps?). In this case, a simple DAG might look something like:\n\n\n\n\n\nWhat this DAG helps to summarise is what we are now wondering - is smoking confusing or distorting the association between coffee drinking and the development of CHD? In other words, is smoking a confounder in the association between our exposure (coffee-drinking) and outcome (CHD) variables of interest?\nBefore we take a peek at our data, let‚Äôs refresh our memories regarding the three conditions a variable must fulfil to be considered a putative confounder:\n\nThe variable must be associated with both the exposure and the outcome.\nThe variable must be distributed unequally across the exposure (e.g.¬†between treatment groups).\nThe variable must not lie on the causal pathway between the exposure and the outcome (this would be a mediator and requires different treatment)."
  },
  {
    "objectID": "posts/019_04Oct_2024/index.html#data",
    "href": "posts/019_04Oct_2024/index.html#data",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "2 Data",
    "text": "2 Data\nOk, let‚Äôs now take a look at the hypothetical retrospective case-control data we‚Äôll be using today. It consists of 40 observations and three variables:\n\noutcome - did the individual have CHD (case) or not (control).\ncoffee-drinker - was the individual a coffee-drinker or not. This is our exposure variable of interest.\nsmoker - was the individual a smoker or not. This is our potential confounder.\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(ggmosaic)\nlibrary(kableExtra)\nlibrary(janitor)\n\n# Hypothetical data\ny &lt;- factor(c(rep(1, 20), rep(0, 20)), levels = c(0, 1), labels = c(\"control\", \"case\"))\nx1 &lt;- factor(c(rep(1, 10), rep(0, 10), rep(1, 5), rep(0, 15)), levels = c(0, 1), labels = c(\"no\", \"yes\"))\nx2 &lt;- factor(c(rep(1, 9), rep(0, 1), rep(1, 3), rep(0, 7), rep(1, 3), rep(0, 2), rep(1, 1), rep(0, 14)), levels = c(0, 1), labels = c(\"no\", \"yes\"))\ndf &lt;- data.frame(\"outcome\" = y, \"coffee_drinker\" = x1, \"smoker\" = x2)\ndf |&gt; \n  kable(align = \"c\")\n\n\n\n\n\noutcome\ncoffee_drinker\nsmoker\n\n\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nno\n\n\ncase\nno\nyes\n\n\ncase\nno\nyes\n\n\ncase\nno\nyes\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncontrol\nyes\nyes\n\n\ncontrol\nyes\nyes\n\n\ncontrol\nyes\nyes\n\n\ncontrol\nyes\nno\n\n\ncontrol\nyes\nno\n\n\ncontrol\nno\nyes\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno"
  },
  {
    "objectID": "posts/019_04Oct_2024/index.html#simple-epidemiological-approach",
    "href": "posts/019_04Oct_2024/index.html#simple-epidemiological-approach",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "3 Simple Epidemiological Approach",
    "text": "3 Simple Epidemiological Approach\nGiven the binary nature of all three variables we can explore the relationships nicely using a basic workhorse of epidemiological analysis - the 2x2 contingency table. In its simplest form this shows the frequency cross-tabulation of the exposure with the outcome. However, I think it‚Äôs also useful to display the conditional row percentages - in other words, the proportions (probabilities) of each outcome (categories as columns) given a particular exposure (categories as rows). In this way it is easy to eyeball whether the exposure is associated with the outcome without doing any specific test simply by looking at whether the row percentages vary greatly. As a good visual accompaniment for each cross-tabulation, I am also going to generate mosaic plots. These can give you an impression of potential associations without using any actual numbers.\n\n3.1 Crude Odds Ratio - Exposure/Outcome\nThe following cross-tabulation and mosaic plot show the potential association between coffee-drinking and CHD. If we look at the proportions of people with CHD in each exposure group we can see that these do in fact differ - 67% of coffee drinkers develop CHD, compared to 40% of non-coffee drinkers. That is a telling sign of an association before we even do anything. The mosaic plot mirrors these numbers graphically.\n\n\n\nCode\ntab &lt;- df |&gt; \n  tabyl(coffee_drinker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    coffee_drinker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    60% (15) \n    40% (10) \n    100% (25) \n  \n  \n    yes \n    33%  (5) \n    67% (10) \n    100% (15) \n  \n  \n    Total \n    50% (20) \n    50% (20) \n    100% (40) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df\ndf2$coffee_drinker &lt;- relevel(df2$coffee_drinker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, coffee_drinker), fill = coffee_drinker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nWorking out the odds ratio (OR) from a 2x2 contingency table is trivial. We want to divide the odds of having CHD given being a coffee-drinker by the odds of having CHD given being a non-coffee-drinker. That is:\n\\[\\text{OR} = \\frac{\\text{10/5}}{10/15}\\]\nOr equivalently:\n\\[\\text{OR} = \\frac{\\text{10 x 15}}{\\text{5 x 10}} = 3\\] The OR is 3 which indicates a 3-fold increase in the odds of CHD among coffee-drinkers compared to their non-coffee-drinking peers. We consider this a crude or unadjusted effect estimate.\n\n\n3.2 Adjusted Odds Ratios - Exposure/Outcome (via stratification)\nTo this point we haven‚Äôt even considered any potential confounding effect of smoking. So how could we incorporate that into the current analysis using 2x2 contingency tables? It‚Äôs actually very easy - we just stratify on smoking and generate two contingency tables - one for the association between coffee drinking and CHD in smokers and one for the association between coffee drinking and CHD in non-smokers. It then follows that within each stratum of smoking the effect of smoking is ‚Äòheld constant‚Äô and therefore cannot confound the association between coffee drinking and CHD. In other words, this becomes an adjusted ‚Äòeffect‚Äô of coffee drinking on CHD and is equivalent to ‚Äòcontrolling‚Äô for smoking in a multivariable statistical model (which I will demonstrate shortly).\n\n3.2.1 Smokers\nSo, the cross-tabulation for the association between coffee-drinking and CHD in smokers, becomes:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(smoker == \"yes\") |&gt; \n  tabyl(coffee_drinker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    coffee_drinker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    25% (1) \n    75%  (3) \n    100%  (4) \n  \n  \n    yes \n    25% (3) \n    75%  (9) \n    100% (12) \n  \n  \n    Total \n    25% (4) \n    75% (12) \n    100% (16) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(smoker == \"yes\")\ndf2$coffee_drinker &lt;- relevel(df2$coffee_drinker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, coffee_drinker), fill = coffee_drinker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nNote how this time the (conditional row) proportions of people with CHD in each exposure group are more similar (in this contrived example they are the same) - in this subgroup of smokers it doesn‚Äôt matter whether you‚Äôre a coffee drinker or not - 75% of people have CHD.\nNow, the OR is calculated as:\n\\[\\text{OR} = \\frac{\\text{9 x 1}}{\\text{3 x 3}} = 1\\]\nThe OR is 1 - in other words there is no longer any association between coffee drinking and CHD. We consider this an adjusted OR as we have removed any potential confounding effect of smoking by holding it at a constant value (i.e.¬†everyone is a smoker).\n\n\n3.2.2 Non-smokers\nNow let‚Äôs consider the non-smokers. The cross-tabulation for the association between coffee-drinking and CHD in non-smokers is:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(smoker == \"no\") |&gt; \n  tabyl(coffee_drinker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    coffee_drinker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (14) \n    33% (7) \n    100% (21) \n  \n  \n    yes \n    67%  (2) \n    33% (1) \n    100%  (3) \n  \n  \n    Total \n    67% (16) \n    33% (8) \n    100% (24) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(smoker == \"no\")\ndf2$coffee_drinker &lt;- relevel(df2$coffee_drinker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, coffee_drinker), fill = coffee_drinker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nDo you see a pattern here? Again, the conditional row percentages are the same across exposure groups (33%) - so, in this subgroup of non-smokers it also doesn‚Äôt matter whether you are a coffee drnker or not, the proportions of CHD are the same.\nThe OR this time is:\n\\[\\text{OR} = \\frac{\\text{1 x 14}}{\\text{7 x 2}} = 1\\]\nThe adjusted OR is again 1 - that is, there is no association between coffee drinking and CHD when we remove any potential confounding effect of smoking by holding it at a (different) constant value (i.e.¬†everyone is a non-smoker).\n\n\n3.2.3 Salient points\nThere are two important points to make in the comparison of the crude and adjusted estimates:\n\nThe adjusted OR is not equal to the crude OR - this suggests confounding is present.\nFurthermore, this represents a special case in which there is complete confounding - the OR reduces to the null value (i.e.¬†1). I will expand on these points in a moment.\n\n\n\n\n3.3 Crude Odds Ratio - Confounder/Outcome\nTo further illuminate the above ideas let‚Äôs repeat the analyses but this time ‚Äòswitch‚Äô the exposure and confounder around (hopefully it will become clear why we are doing this as you continue reading). The cross-tabulation and mosaic plot suggest an association between smoking and CHD - 75% of smokers develop CHD, compared to 33% of non-smokers.\n\n\n\nCode\ntab &lt;- df |&gt; \n  tabyl(smoker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    smoker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (16) \n    33%  (8) \n    100% (24) \n  \n  \n    yes \n    25%  (4) \n    75% (12) \n    100% (16) \n  \n  \n    Total \n    50% (20) \n    50% (20) \n    100% (40) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df\ndf2$smoker &lt;- relevel(df2$smoker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, smoker), fill = smoker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nThe OR for the association between smoking and CHD is:\n\\[\\text{OR} = \\frac{\\text{12 x 16}}{\\text{8 x 4}} = 6\\]\nSo the crude OR is 6, which indicates a 6-fold increase in the odds of CHD in smokers compared to non-smokers (ignoring coffee-drinking status).\n\n\n3.4 Adjusted Odds Ratios - Confounder/Outcome (via stratification)\nLet us know compare the adjusted ‚Äòeffects‚Äô in coffee-drinkers compared to non-coffee-drinkers.\n\n3.4.1 Coffee-drinkers\nSo, the cross-tabulation for the association between coffee-smoking and CHD in coffee-drinkers, becomes:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(coffee_drinker == \"yes\") |&gt; \n  tabyl(smoker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    smoker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (2) \n    33%  (1) \n    100%  (3) \n  \n  \n    yes \n    25% (3) \n    75%  (9) \n    100% (12) \n  \n  \n    Total \n    33% (5) \n    67% (10) \n    100% (15) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(coffee_drinker == \"yes\")\ndf2$smoker &lt;- relevel(df2$smoker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, smoker), fill = smoker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nAnd the adjusted OR is:\n\\[\\text{OR} = \\frac{\\text{9 x 2}}{\\text{1 x 3}} = 6\\]\nThe OR is 6 - in other words the same as the crude estimate.\n\n\n3.4.2 Non-coffee-drinkers\nNow let‚Äôs consider the non-coffee-drinkers. The cross-tabulation for the association between smokers and CHD in non-coffee-drinkers is:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(coffee_drinker == \"no\") |&gt; \n  tabyl(smoker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    smoker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (14) \n    33%  (7) \n    100% (21) \n  \n  \n    yes \n    25%  (1) \n    75%  (3) \n    100%  (4) \n  \n  \n    Total \n    60% (15) \n    40% (10) \n    100% (25) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(coffee_drinker == \"no\")\ndf2$smoker &lt;- relevel(df2$smoker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, smoker), fill = smoker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nAnd the adjusted OR is:\n\\[\\text{OR} = \\frac{\\text{3 x 14}}{\\text{7 x 1}} = 6\\]\nThe OR is again 6! Is this starting to make some sense?\n\n\n3.4.3 Salient points\nAgain, there are two important points to make in the comparison of these crude and adjusted estimates:\n\nIt doesn‚Äôt seem to matter whether we adjust for coffee-drinking or not, the association between smoking and CHD remains the same.\nIt then follows that coffee-drinking is not a confounder in the smoking-CHD relationship.\n\n\n\n\n3.5 Adjusted Odds Ratio - Overall\nIt just so happens that the adjusted OR‚Äôs from Section¬†3.2 are the same in each subgroup, but this is an exception rather than a rule. Normally, in the presence of confounding, effect estimates will differ in each subgroup to the crude estimate but will not be equal to each other. What do we do with separate effect estimates from each subgroup - this makes reporting somewhat painful, surely? Well, if the adjusted estimates aren‚Äôt too different from each other, they can be combined in a weighted manner to provide an overall summary estimate and this can be done using the Cochran-Mantel-Haenszel (CMH) test. I won‚Äôt illustrate this here as in the modern computing age there is really no need to be crunching this statistic anymore, and we use a statistical model instead.\nAs I alluded to above, we should only attempt to combine individual estimates when they are ‚Äòsimilar‚Äô. But what does that mean? How similar should they be and how different can they be? There are no hard and fast rules but I will outline a couple of guidelines at the end of this post. The important thing is to realise that when individual estimates are different, this actually represents an effect modification/interaction effect - that is the strength of the association between the exposure and the outcome depends on the level of the third variable. Interaction effects are of interest (scientifically and clinically) and we shouldn‚Äôt try to cancel them out by averaging over them."
  },
  {
    "objectID": "posts/019_04Oct_2024/index.html#statistical-modelling-approach",
    "href": "posts/019_04Oct_2024/index.html#statistical-modelling-approach",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "4 Statistical Modelling Approach",
    "text": "4 Statistical Modelling Approach\nThe cross-tabulation approach is great for expository purposes but is limiting in the types of relationships you can practically explore - things just become unwieldy with variables that contain more than two categories and impossible with continuous variables.\nSo, we tend to use statistical models instead. These make confounder control/adjustment easy - we just need to include the potential confounder in the model along with our exposure of interest. Let‚Äôs now replicate everything we have done thus far, but in a modelling-paradigm.\n\n4.1 Crude Odds Ratio - Exposure/Outcome\n\n\nCode\nglm(outcome ~ coffee_drinker, data = df, family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n3.00\n0.81, 12.2\n0.11\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nWe get the same OR as in Section¬†3.1 in addition to a 95% C.I. and p-value.\n\n\n4.2 Adjusted Odds Ratios - Exposure/Outcome (via stratification)\nWe can replicate a stratified effect in our modelling by subsetting the data to select only those people in each smoking subgroup.\n\n4.2.1 Smokers\n\n\nCode\nglm(outcome ~ coffee_drinker, data = subset(df, smoker == \"yes\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n1.00\n0.04, 12.2\n&gt;0.9\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 1 (compare to Section¬†3.2.1)\n\n\n4.2.2 Non-smokers\n\n\nCode\nglm(outcome ~ coffee_drinker, data = subset(df, smoker == \"no\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n1.00\n0.04, 12.3\n&gt;0.9\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 1 (compare to Section¬†3.2.2)\n\n\n\n4.3 Crude Odds Ratio - Confounder/Outcome\n\n\nCode\nglm(outcome ~ smoker, data = df, family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    smoker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n6.00\n1.55, 27.5\n0.013\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 6 (compare to Section¬†3.3)\n\n\n4.4 Adjusted Odds Ratios - Confounder/Outcome (via stratification)\n\n4.4.1 Coffee-drinkers\n\n\nCode\nglm(outcome ~ smoker, data = subset(df, coffee_drinker == \"yes\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    smoker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n6.00\n0.43, 161\n0.2\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 6 (compare to Section¬†3.4.1)\n\n\n4.4.2 Non-coffee-drinkers\n\n\nCode\nglm(outcome ~ smoker, data = subset(df, coffee_drinker == \"no\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    smoker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n6.00\n0.64, 134\n0.15\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 6 (compare to Section¬†3.4.2)\n\n\n\n4.5 Adjusted Odds Ratio - Overall\nThe above models are all considered univariable in that one predictor only is specified in each model. In controlling or adjusting for a third variable we now produce a multivariable model where both the exposure and potential confounder are specified as predictors. This automatically produces an adjusted ‚Äòeffect‚Äô of coffee drinking on CHD, controlling for smoking (and conversely an adjusted ‚Äòeffect‚Äô of smoking on CHD, controlling for coffee drinking). We run these kinds of models all the time without thinking, but what the model is doing ‚Äòunder the hood‚Äô is calculating a weighted average of the individual subgroup estimates in producing a single coefficient that becomes our effect/s of interest.\n\n\nCode\nglm(outcome ~ coffee_drinker + smoker, data = df, family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n1.00\n0.13, 5.84\n&gt;0.9\n    smoker\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n6.00\n1.08, 48.2\n0.054\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "posts/019_04Oct_2024/index.html#some-guidelines",
    "href": "posts/019_04Oct_2024/index.html#some-guidelines",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "5 Some Guidelines‚Ä¶",
    "text": "5 Some Guidelines‚Ä¶\nTo consolidate the ideas that we have explored today I want to lay out some very broad guidelines for how to interpret and compare crude and adjusted effects.\n\n5.1 No Confounding or Effect Modification Present\nIf there is neither confounding nor effect modification, the crude estimate of association and the stratum-specific estimates will be similar (converging to being the same in the ideal context). This is reflected in Sections¬†3.3 and Section¬†3.4 above.\n\n\n5.2 Only Confounding Present\nIf there is only confounding, the stratum-specific measures of association will be similar to one another, but they will be different from the overall crude estimate (by ~ 10% or more). In this situation, one can use CMH methods to calculate a weighted estimate and p-value, or even easier is to run a statistical model including the confounder as just another covariate. i.e.¬†(in R)\nmod1 &lt;- lm(outcome ~ exposure + confounder, data = dat)\nThis is reflected in Sections¬†3.1 and Section¬†3.2 above.\n\n\n5.3 Confounding and/or Effect Modification Present\nIn this case the stratum-specific estimates will differ from one another significantly and these will also differ from the overall crude estimate. These effects should be reported as they are and not weighted and combined (i.e.¬†averaged over) as this is of scientific and clinical interest in its own right. In practical terms in a statistical model, an interaction term should be specified. This changes the coding from:\nmod1 &lt;- lm(outcome ~ exposure + confounder, data = dat)\nto\nmod2 &lt;- lm(outcome ~ exposure * confounder, data = dat)\nAs the two models are ‚Äònested‚Äô, an assessment of whether the interaction term is necessary or not can be performed using a likelihood ratio test:\nanova(mod2, mod1)\nIf the p-value is significant you can conclude that the interaction term increases the explanatory power of the model and should be retained."
  },
  {
    "objectID": "posts/019_04Oct_2024/index.html#the-end",
    "href": "posts/019_04Oct_2024/index.html#the-end",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "6 The End!",
    "text": "6 The End!\nAnother post that has gone on longer than I had anticipated - there is much more I could talk about on the topic but my goal is not to make you fall asleep on your keyboard. Hopefully this has helped to make the concept of confounding just that little bit clearer in your mind. Until next time‚Ä¶"
  },
  {
    "objectID": "posts/020_18Oct_2024/index.html",
    "href": "posts/020_18Oct_2024/index.html",
    "title": "Breaking Free From ‚ÄòThe Cult of P‚Äô",
    "section": "",
    "text": "Apologies to all - I know I have already preached about this in the first WhatsApp message I sent out on ‚ÄòWeekly Stats Tips‚Äô about 12 months ago. But given the idea is so important, there‚Äôs no permanent and easily-accessible record of that message, and also that in hindsight I think I told you what not to do, but didn‚Äôt really suggest what you could/should do - I‚Äôm going to make it a blog post here as well."
  },
  {
    "objectID": "posts/020_18Oct_2024/index.html#the-issue",
    "href": "posts/020_18Oct_2024/index.html#the-issue",
    "title": "Breaking Free From ‚ÄòThe Cult of P‚Äô",
    "section": "1 The Issue",
    "text": "1 The Issue\nSo, as researchers how many of you can honestly say that you have never used language around p-values in a way reminiscent of this xkcd comic (or one of these 509 variations on the same theme):\n\n\n\n\n\nI know I can‚Äôt. Certainly in my early research career and before changing paths to become a biostatistician, I was guilty of this sort of thing. All in the name of some misguided sense of wanting to achieve research glory by getting that p-value to fall under the all-important threshold of 0.05 (my research glory did not ever materialise by the way).\nWell can I suggest, it just doesn‚Äôt matter. And can I also suggest to stop thinking about p-values in this way.\nIt‚Äôs generally accepted that Sir Ronald Fisher was the guy who at least formalised the ideas of Null Hypothesis Significance Testing (NHST) and the p-value in the 1920‚Äôs. He never intended for the p-value of 0.05 to be set in stone as an arbiter of the value of a piece of research in a statistical sense, and certainly not in a scientific sense. The intention was to use it to guide decision making, not make the actual decision. But, in the past 100 years this arbitrary threshold of p = 0.05 has not only stuck, but taken on an almost mythic status in the research community, in part it seems because humans are lazy and like to avoid decision-making where possible. The natural and silly extension to this entrenched notion of ‚Äúsignificance‚Äù is that researchers may feel elated if their statistical test returns p = 0.049 and despondent if it returns p = 0.051. On reflection, any rational person can see that‚Äôs crazy, but it doesn‚Äôt seem to give us pause for thought when we have our results in front of us."
  },
  {
    "objectID": "posts/020_18Oct_2024/index.html#what-not-to-do",
    "href": "posts/020_18Oct_2024/index.html#what-not-to-do",
    "title": "Breaking Free From ‚ÄòThe Cult of P‚Äô",
    "section": "2 What Not To Do",
    "text": "2 What Not To Do\nYou may be aware that there has been a push in recent years by the statistical community to abandon the p-value all together, set off in large part by a statement published by the American Statistical Association in 2016. There is nothing new in any of this - the same issues have been of concern for decades and a Pubmed (or even Google) search will reveal a large literature on the topic. In 2019, The American Statistician dedicated an entire special issue to statistical inference entitled ‚ÄúStatistical Inference in the 21st Century: A World Beyond¬†p¬†&lt; 0.05.‚Äù If you scan down those titles you‚Äôll see that not too many are in favour of retaining the 0.05 dichotomy we seem to have imposed on ourselves. The Editorial in that edition doesn‚Äôt pull any punches in terms of how to use p-values in your research:\n\nDon‚Äôt base your conclusions solely on whether an association or effect was found to be ‚Äústatistically significant‚Äù (i.e., the p-value passed some arbitrary threshold such as p &lt; 0.05).\nDon‚Äôt believe that an association or effect exists just because it was statistically significant.\nDon‚Äôt believe that an association or effect is absent just because it was not statistically significant.\nDon‚Äôt believe that your p-value gives the probability that chance alone produced the observed association or effect or the probability that your test hypothesis is true.\nDon‚Äôt conclude anything about scientific or practical importance based on statistical significance (or lack thereof).\n\nAnd the authors go further to say:\nThe ASA Statement on P-Values and Statistical Significance stopped just short of recommending that declarations of ‚Äústatistical significance‚Äù be abandoned. We take that step here. We conclude, based on our review of the articles in this special issue and the broader literature, that it is time to stop using the term ‚Äústatistically significant‚Äù entirely. Nor should variants such as ‚Äúsignificantly different,‚Äù ‚Äúp &lt; 0.05,‚Äù and ‚Äúnonsignificant‚Äù survive, whether expressed in words, by asterisks in a table, or in some other way.\nPretty damning!"
  },
  {
    "objectID": "posts/020_18Oct_2024/index.html#what-you-couldshould-do",
    "href": "posts/020_18Oct_2024/index.html#what-you-couldshould-do",
    "title": "Breaking Free From ‚ÄòThe Cult of P‚Äô",
    "section": "3 What You Could/Should Do",
    "text": "3 What You Could/Should Do\nIf you look at some of the papers in that special issue, you will find a wealth of suggestions about how to approach a ‚ÄúPost ‚Äòp &lt; 0.05‚Äô world‚Äù, including that we should all become Bayesians (not meaning to toot my own trumpet, but I have also written around this in one of my not-so-highly-cited papers several years ago - I‚Äôm sure it will gain traction in years to come!).\nSo I‚Äôm not going to reiterate all of those suggestions here. Instead, for what my opinion is worth, I have two practical tips that I think would make interpreting and reporting our research more robust and less reliant on the p-value.\n\n3.1 Use A Language Of ‚ÄòEvidence‚Äô\nWe can still calculate p-values as we currently do - nothing needs to change. But instead of referring to the p-value itself, let‚Äôs start discussing our results in the context of ‚Äòevidence‚Äô (i.e.¬†against the null hypothesis of no effect). The following table from this paper sums it up beautifully and so I am not going to reinvent the wheel (in fact I can thoroughly recommend the other 3 papers in the series, so do give them a look if you have time).\n\n\n\n\n\nNow, I have used my own subtle variation on the terminology over the years and replaced ‚Äúinsufficient‚Äù with ‚Äúweak‚Äù, ‚Äúsome‚Äù with ‚Äúmoderate‚Äù and ‚Äúoverwhelming‚Äù with ‚Äúvery strong‚Äù. The point is, it doesn‚Äôt really matter what words you use to describe your ‚Äòeffect‚Äô with - this kind of language immediately frees you from the confines of an arbitrary dichotomisation that forces you to either value or devalue all of your hard work in one fell swoop. Instead of saying ‚Äòthere was a trend towards statistical significance for the association between x and y‚Äô, if your p-value turns out to be 0.051, you can now simply say there was ‚Äòweak evidence for an association between x and y‚Äô, instead.\n\n\n3.2 Be Consistent In p-Value Reporting\nIf I had a dollar for every time a research student has sent me results that contain p-values between 0.045 and 0.05 and reported that to 3 decimal places, I‚Äôd be a little less poor than I am now. Seriously, there is no need to do this. Please adhere to the following reporting guide and don‚Äôt try to put lipstick on a pig by suggesting that your p-value of 0.049 is any different to 0.05.\n\n\n\n\n\nThese are a couple of small changes we can make in our research reporting practice that will help to break our self-imposed binds to the p-value gods. Until next time‚Ä¶"
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html",
    "href": "posts/021_01Nov_2024/index.html",
    "title": "Survival Analysis - Under the Hood",
    "section": "",
    "text": "Welcome to today‚Äôs post which is based on a talk that some of you may have heard me give in lab meetings. A warning in advance - it‚Äôs a bit of a lengthy one, so maybe down a couple of reds to steel yourself before you start. We‚Äôre going to discuss survival analysis, but in a similar spirit to how I presented the post on logistic regression a couple of months ago - nothing too fancy, but with an aim that you understand what is happening ‚Äòunder the hood‚Äô when you next fire up your stats software to plot a Kaplan-Meier curve or run a Cox model.\nSurvival analysis is a BIG topic and in university statistics courses, it tends to have an entire subject dedicated just to it, so it‚Äôs not just taught as a part of regression modelling, for example. I also don‚Äôt find it particularly easy, even now - there are a lot of challenging concepts to understand, and that is in large part because time is such an integral component of everything you do in this area of biostatistics. So, what I hope I can do today is to take a broad-brush approach to survival methods, touch on the concepts that I think are most important in establishing a good base understanding, build in a little intuition for these, and also give you some practical tips along the way.\nLet‚Äôs get started."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#calendar-time-format",
    "href": "posts/021_01Nov_2024/index.html#calendar-time-format",
    "title": "Survival Analysis - Under the Hood",
    "section": "2.1 Calendar-time format",
    "text": "2.1 Calendar-time format\nWhat does survival data look like? Well - in the case of a prospective study, subjects are usually recruited into the study at different times. To illustrate this, let‚Äôs consider this hypothetical dataset of patients diagnosed with some disease that we follow until they either die or are right-censored. The study was planned to recruit for 4 months from Dec 2015 to Mar 2016, and during this period 5 subjects were entered into the study, all starting at various times, but importantly, the exact dates of their start were recorded. All subjects were then followed at regular intervals, and the study was then ended, as planned, in Jan 2017.\nNow, this is showing the observation times for each subject as they occurred in calendar-time format. While this reflects the reality of how survival data are collected, it doesn‚Äôt really inform you with an intuitive sense of time because you can‚Äôt easily make comparisons across subjects."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#time-on-study-format",
    "href": "posts/021_01Nov_2024/index.html#time-on-study-format",
    "title": "Survival Analysis - Under the Hood",
    "section": "2.2 Time-on-study format",
    "text": "2.2 Time-on-study format\nSo, for easier interpretation we can align the start time of all subjects like so - and we call this format, ‚Äòtime-on-study‚Äô. We can see that Subject 1 dropped out because they moved interstate and couldn‚Äôt attend clinic appointments anymore, and so they are considered censored. Their follow-up time was 6 months. Subjects 2 and 4 are also censored, but for a different reason - they were still alive when we decided to end the study and had the study gone on we may have been able to follow them for longer. In any case they were each observed for 12 months. Unfortunately, the other two subjects weren‚Äôt so lucky - Subject 3 died after 7 months and Subject 5 died after 4 months of being in the study.\n\n\n\n\n\nNow, what if we were interested in working out the one-year survival for this sample of patients?\nHow would you do that?"
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#this-bed-is-too-soft",
    "href": "posts/021_01Nov_2024/index.html#this-bed-is-too-soft",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.1 This bed is too soft",
    "text": "3.1 This bed is too soft\nThis can commonly lead us to assume that the subject survived the full year - and if I‚Äôm to borrow a metaphor from a children‚Äôs fable, it would have to in this case be Goldilocks. So what we have here is the equivalent of Goldilock‚Äôs bed being too soft.\n\n\n\n\n\nUnder this assumption we would include Subject 1 in the proportion numerator and calculate the one-year survival as 3/5 or 60% (we could also get that from a logistic regression model).\n\\[\n\\text{Survival (one-year)} = \\frac{\\text{3}}{\\text{5}} = 60\\%\n\\]\nBut there‚Äôs something not quite right about that.\nUnfortunately, Goldilocks doesn‚Äôt sleep well."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#this-bed-is-too-hard",
    "href": "posts/021_01Nov_2024/index.html#this-bed-is-too-hard",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.2 This bed is too hard",
    "text": "3.2 This bed is too hard\nWell then, let‚Äôs be conservative you might say, and assume Subject 1 has in fact died. In this case we would exclude them from the proportion numerator and calculate the one-year survival as 2/5 or 40%. Can I suggest this is now the equivalent of Goldilock‚Äôs bed being too hard, because we know Subject 1 was followed for a full 6 months - that must count for something surely?\n\n\n\n\n\n\\[\n\\text{Survival (one-year)} = \\frac{\\text{2}}{\\text{5}} = 40\\%\n\\]\nPoor Goldilocks still doesn‚Äôt sleep well."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#ahhh-just-right",
    "href": "posts/021_01Nov_2024/index.html#ahhh-just-right",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.3 Ahhh, just right",
    "text": "3.3 Ahhh, just right\nWhen you use survival methods, it in fact turns out that the probability of surviving in the entire year, taking into account censoring is (4/5)x(2/3) = 53%, somewhere in between 40 and 60% - ahhh, Goldilocks has finally found the right bed.\n\n\n\n\n\n\\[\n\\text{Survival (one-year)} = \\frac{\\text{4}}{\\text{5}} \\times \\frac{\\text{2}}{\\text{3}} = 53\\%\n\\] I will show you how to get to that number shortly."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#the-take-home-message",
    "href": "posts/021_01Nov_2024/index.html#the-take-home-message",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.4 The take-home message",
    "text": "3.4 The take-home message\nIf everybody is observed for at least the same length of time of interest (e.g.¬†1 year), logistic regression could be used‚Ä¶\n‚Ä¶ but not if the duration of observation is less for some people."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#questions---so-many-questions",
    "href": "posts/021_01Nov_2024/index.html#questions---so-many-questions",
    "title": "Survival Analysis - Under the Hood",
    "section": "4.1 Questions - so many questions‚Ä¶",
    "text": "4.1 Questions - so many questions‚Ä¶\nSo, let me get you thinking about this by way of a thought exercise.\nIf I were to ask you about human mortality; then as a function of age, how would you draw or describe:\n\nThe probability of death?\nThe hazard of death?\nSurvival?\n\nFollowing from that:\n\nIs the probability of death greater at 80 or 100?\nIs the hazard of death greater at 80 or 100?\n\nAnd finally:\nWhat‚Äôs the difference!?"
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#some-answershopefully",
    "href": "posts/021_01Nov_2024/index.html#some-answershopefully",
    "title": "Survival Analysis - Under the Hood",
    "section": "4.2 Some answers‚Ä¶hopefully",
    "text": "4.2 Some answers‚Ä¶hopefully\nOk, this is a little bit of a trick question, because it really hangs on semantics.\n\n4.2.1 Marginal probabilities of event\nWhen we talk about the probability of death we are referring to a probability density for death (as a function of age). This is where the area under the density curve (and while this involves a bit of calculus, you don‚Äôt need to concern yourself with the details) adds up to 1 - that is everyone is certain to die eventually. In other words the sums of all possible probabilities of dying across all ages equals 1 - and that‚Äôs what essentially defines a probability density function. So if you look at the following plot you will appreciate that the overall probability of death gradually increases until about 80 years than actually declines after that. So, in fact the most likely age to die at is about 80, not 100, and that‚Äôs simply because fewer people are alive at 100 in the first place. This is what is known as a ‚Äòmarginal‚Äô probability of death.\n\n\n\n\n\n\n\n4.2.2 Conditional probabilities of event\nNow, the marginal probability of death is a different concept to the hazard of death. The hazard represents an instantaneous or conditional risk of the event happening. Looking at the commensurate hazard function, you will see that it increases exponentially. What this is saying is that the probability or risk that you die in the next year, increases the longer you remain alive. So, the risk of dying in the next year, given that you survived until 100 years is far, far, greater than than case for an 80 year old. These are conditional probabilities. In other words, the risk in the next instant of experiencing the event given you haven‚Äôt yet experienced it.\n\n\n\n\n\n\n\n4.2.3 Survival\nAnd then finally the survival. This isn‚Äôt difficult. We all die (don‚Äôt thank me for cheering you up) - so the survival function eventually reaches 0."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#definition",
    "href": "posts/021_01Nov_2024/index.html#definition",
    "title": "Survival Analysis - Under the Hood",
    "section": "5.1 Definition",
    "text": "5.1 Definition\nOk, so having some understanding of the hazard rate is really important in survival analysis. At a basic level, the hazard rate is like an incident rate - the number of new events that occur in a period of time divided by the number of people at risk of that event at the beginning of that period. But it gets a little more complicated because the hazard rate is actually the limiting case of that - the instantaneous event rate. In other words the probability that, given that a subject has survived up to some time t, that he or she has the event in the next infinitesimally small time interval, divided by the length of that interval - and we need to use calculus to be able to compute that. The hazard rate ranges from 0 (no risk of the event) to infinity (certain to experience the event) and over time, it can be constant, increase, decrease, or take some other non-monotonic pattern, which I‚Äôll show you in a moment.\nThe non-complicated, non-calculus formula for the hazard rate is shown below: Basically the hazard rate can be thought of as the number of events between time \\(t\\) and \\(\\delta t\\), divided by the number of people at risk of the event at time \\(t\\) - that gives a conditional probability of the event - and then we divide that by the length of the interval.\n\\[\nh(t) = \\lim_{\\delta \\text{t} \\to 0} \\frac{\\left( \\cfrac{ \\text{number of events between time t and } (\\text{t} + \\delta \\text{t})}{\\text{number of people at risk at time t }} \\right) } {\\delta \\text{t}}\n\\]\n\\[\n= \\lim_{\\delta \\text{t} \\to 0} \\frac{\\text{conditional probability of event between time t and } (\\text{t} + \\delta \\text{t})} {\\delta \\text{t}}\n\\]"
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#the-hazard-rate-is-related-to-the-incident-rate",
    "href": "posts/021_01Nov_2024/index.html#the-hazard-rate-is-related-to-the-incident-rate",
    "title": "Survival Analysis - Under the Hood",
    "section": "5.2 The hazard rate is related to the incident rate",
    "text": "5.2 The hazard rate is related to the incident rate\nLet me use a silly example to illustrate this idea of how the hazard rate is related to your run-of-the-mill incident rate. Let‚Äôs think about the hazard of getting married at 20 years of age - perilous I might suggest. But jokes aside, how would you work this out? Well we could start by thinking in terms of the incident rate - in a nutshell, count up the number of people married between the ages of 20 and 21 and divide that by the number NOT already married at age 20.\nSo, let‚Äôs say we have 1000 people in a population of interest who still aren‚Äôt married at age 20 and we follow them forward and see that 100 get married in the next year. The incident rate is then 100/1000 per year or 0.1 marriages per year.\n\n\n\n\n\nWe could have actually considered a shorter time interval to work out the incident rate - let‚Äôs now say 6 months instead of one year. Assuming marriages are evenly distributed then the incident rate is now 50/1000 per 6 months or 0.05 marriages per 1/2 year.\n\n\n\n\n\nAnd we could even consider the incident rate in the forthcoming day, which would be 0.274/1000 per day or 0.000274 marriages per day.\n\n\n\n\n\nSo the incident rate differs in these three cases only because we are using different units of time. If we normalised the time period to one year in the second and third scenarios, then we would expect the same number of marriages over the year.\nThe point of all of this is really to show you that the hazard rate is a limiting case of the incident rate. As the time interval approaches zero, we essentially have an instantaneous estimate of the risk of the event at any point in time and this is what the hazard rate represents. It‚Äôs not hard when you think about it."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#some-common-hazard-function-shapes",
    "href": "posts/021_01Nov_2024/index.html#some-common-hazard-function-shapes",
    "title": "Survival Analysis - Under the Hood",
    "section": "5.3 Some common hazard function shapes",
    "text": "5.3 Some common hazard function shapes\nLet‚Äôs have a look at some of the more common hazard function shapes that we might encounter. The shape of the hazard function is determined by the underlying disease, physiological or physical process - so some of these are more realistic in biological systems and some more realistic in mechanical systems.\n\n5.3.1 Constant (exponential)\nThe first example is of a constant hazard - so the risk of failure at any point in time is the same. This could apply to a light bulb and it could also apply to a healthy individual in a study. When the hazard function is constant we say the survival model is exponential and memoryless, in that the age of the subject has no bearing on the instantaneous risk of failure. Now, this is not the most realistic distribution for biological systems, because - taking a well-known quote from a textbook - ‚ÄúIf human lifetimes were exponential there wouldn‚Äôt be old or young people, just lucky or unlucky ones‚Äù.\n\n\n\n\n\n\n\n5.3.2 Increasing (Weibull)\nThis is a monotone increasing hazard function. This is one of the more realistic shapes that we expect in the real world - basically as things age, they wear out and are more likely to fail. In industry, cars and batteries are good examples of this. Most biological systems also follow this profile - as humans it‚Äôs why our life insurance premiums go up as we age. At 60 you‚Äôve done well to have lived this long, but your short term prospects aren‚Äôt as optimistic as they were at 20, and the actuaries build this in to their company‚Äôs insurance premiums.\nAn example in a medical study might be the hazard of metastatic cancer patients, where the event of interest is death. As survival time increases for such a patient, and as the prognosis accordingly worsens, the patient‚Äôs potential for dying of the disease also increases.\n\n\n\n\n\n\n\n5.3.3 Decreasing (Weibull)\nBut not everything wears out over time. This is an example of a monotone decreasing hazard. Many electronic systems actually become more robust over time - believe it or not. In other words, the instantaneous risk of failure tends to be higher earlier on and then subsides with time. So manufacturers can utilise that to their advantage by running the system for a period before shipping and if it passes, it‚Äôs likely to be ok for the customer.\nIn medicine we might see this in patients recovering from major surgery, because the potential for dying after surgery usually decreases as the time after surgery increases.\n\n\n\n\n\n\n\n5.3.4 Increasing/Decreasing (lognormal)\nNow we‚Äôre getting even more complex with a hazard function that can both increase and decrease. These kinds of shapes become increasingly difficult to generalise, but a specific example might be the hazard for tuberculosis patients since their potential for dying from the disease initially increases and then subsides.\n\n\n\n\n\n\n\n5.3.5 Bathtub\nAnd then finally the ‚Äòbathtub‚Äô shaped hazard function, which I introduced to you before when talking about the risks of humans dying with age. In a general sense, this hazard function reflects the fact that some of the riskiest days of your life are those following birth, and then things start to settle down through and until mid-life.\nBut gradually, as you age, parts of your body start to wear out and your short-term risk begins to increase again - and if you manage to get to 80 for example, then your short-term risk of dying might be as high as it was during your first months of life."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#time-on-study",
    "href": "posts/021_01Nov_2024/index.html#time-on-study",
    "title": "Survival Analysis - Under the Hood",
    "section": "6.1 Time-on-study",
    "text": "6.1 Time-on-study\nI want to illustrate these ideas about time scales with a figure and a very simplified example. Let‚Äôs assume this is some hazard function for death related to some fictitious disease. The risk of death related to this disease initially increases and then decreases over time. \nWhen we look at the hazard, it‚Äôs a fairly safe bet that we would use time-on-study in this case, because the hazard is going to primarily be a function of time since diagnosis more than a function of age. And that‚Äôs because the subject becomes at risk of the outcome from the point of diagnosis. So in this case we can expect subjects of any age to have a similar hazard. If we were to estimate a Cox model for example, the hazards are fairly consistent in shape across subjects, regardless of age (note that the intercepts might actually be different for different ages - gives rise to proportional hazards - but the shapes are consistent). In other words, an apples-to-apples comparison of the hazard function at any point in observation time."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#age",
    "href": "posts/021_01Nov_2024/index.html#age",
    "title": "Survival Analysis - Under the Hood",
    "section": "6.2 Age",
    "text": "6.2 Age\nBut now let‚Äôs assume that the same hazard is instead a function of age instead of time-on-study - for the sake of the exercise let‚Äôs say the outcome is now a type of cancer and you become ‚Äòat risk‚Äô of this cancer from birth. So the risk of the outcome this time relates more to just getting older than it does to any other precursor event.\n\n\n\n\n\nIn this case, if we were to use time-on-study as the time scale, we end up trying to compare hazard functions among age groups that are not consistent in shape on that time scale. So, the hazard for a 20 year old entering the study might be different to the hazard for a 60 year old entering the study. Now we are attempting an apples-to-oranges comparison of the hazard function at any point in observation time and that is less than ideal - instead, we‚Äôre better off using age as the time scale on which to base our computations.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen using age as the time scale for the analysis, age is automatically corrected for in the analysis and does not require specific covariate adjustment."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#non-parametric-kaplan-meier-estimator",
    "href": "posts/021_01Nov_2024/index.html#non-parametric-kaplan-meier-estimator",
    "title": "Survival Analysis - Under the Hood",
    "section": "7.1 Non-parametric (Kaplan-Meier estimator)",
    "text": "7.1 Non-parametric (Kaplan-Meier estimator)\nSo, in statistics, what does non-parametric actually mean? Basically, we do not rely on any assumptions that the data are drawn from a given parametric family of probability distributions. So we‚Äôre less interested or able to make inferences about the population and we‚Äôre really only talking about the data that we actually have - the sample at hand.\nNow, in survival analysis, the Kaplan-Meier estimator is one such non-parametric approach to calculating the survival function. We don‚Äôt make any assumptions about the distribution of survival times in the population from which the sample was drawn. So, it‚Äôs just the empirical probability of surviving past certain times in the sample - and to that end it‚Äôs main purpose is descriptive.\nThe Kaplan-Meier approach is NOT a modelling method, and it‚Äôs really about visualising survival - and we can do this overall or commonly by splitting based on one important grouping variable, whether that be treatment or some other exposure variable (e.g.¬†sex). Usually this a starting point in your survival analyses, and you‚Äôll go on and do some modelling, but sometimes a simple Kaplan-Meier curve is enough to do what you want.\nLet‚Äôs revisit that first example I showed you with the 5 patients, because it‚Äôs a worthwhile exercise in understanding how the Kaplan-Meier estimator works out a probability of survival taking into account, censoring. It‚Äôs a simple exercise with these data but with more complex datasets you obviously wouldn‚Äôt do this manually.\nSo, we have our 5 patients and we want to plot the proportion surviving at every step over the study duration.\nRemember that if we‚Äôd used logistic regression, then depending on how we defined our censored subject we could have ended up with a one-year survival of either 40 or 60%, and in fact the correct one-year survival estimate is 53%. How do we get that?\n\n\n\n\n\nSo, we start at time 0 with 100% survival and with all subjects at risk of the event.\nWe know that survival remains at 100% until at least month 4 because everyone remains alive during that time.\nBut then Subject 5 dies at 4 months. So, this is the event we‚Äôre interested in and the survival probability is re-estimated each time there is an event. How do we do that? Well, we multiply the cumulative survival proportion before the event by the conditional survival proportion just after the event - conditional because a subject must have survived at least until the event, to remain in the study after it.\nAnd if we do that, this time around it‚Äôs a fairly straightforward multiplication of 1 x 4/5, which gives 80%. Note that censoring has not even entered into this calculation yet.\nSo the proportion surviving immediately after 4 months is 80% and the number of people at risk of dying at this point is 4.\n\n\n\n\n\nNow as we go along in the time line we see that Subject 1 is censored at 6 months and we indicate that with a mark at that time point. Note that the survival estimate doesn‚Äôt change after 6 months, but the number of people at risk of dying now reduces by 1 to 3.\n\n\n\n\n\nNow Subject 3 dies at 7 months. To re-estimate the survival this time we need to take just a little bit of extra care. The cumulative survival proportion immediately before 7 months is still 80% but when we work out the conditional survival immediately after 7 months, the censoring of Subject 1 means that we now only have 2 people remaining alive (that we are certain about) out of the 3 potentially at risk, just prior to the death.\nWe then multiply those two survival proportions to give us the overall survival after 7 months. And that is 4 people out of 5 (80%) alive immediately prior to and 2 people out of 3 (67%) alive immediately after. The overall estimate of survival at 7 months is then 4/5 times 2/3 which gives us the 53% we stated earlier.\nClearly this is a very simple case and you can appreciate how complicated this can get in larger datasets, but standard functions will take care of the calculations for you in whatever statistical package you‚Äôre using."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#semi-parametric-cox-model",
    "href": "posts/021_01Nov_2024/index.html#semi-parametric-cox-model",
    "title": "Survival Analysis - Under the Hood",
    "section": "7.2 Semi-parametric (Cox model)",
    "text": "7.2 Semi-parametric (Cox model)\nIf you want to estimate survival as a function of more than just one grouping variable, then you really need to look further afield than the Kaplan-Meier estimator and this is usually where people will turn to a Cox model.\nThe Cox model is considered semi-parametric because there is both a non-parametric component and a fully parametric component. The baseline hazard function is non-parametric and is estimated directly from the sample data without any assumptions about the distribution of survival times. Because of this the hazard function in a Cox model can take on any shape you can imagine. This is both an advantage and a disadvantage - an advantage because the shape can be highly flexible, but a disadvantage because we don‚Äôt actually get any information returned in terms of parameter estimates that tell us anything about what that shape looks like.\nThe parametric part of the Cox model is to do with the effects of the predictors on the hazard function - and these are assumed to be linearly related to the log of the hazard. The coefficient estimates we get out of a Cox model are exponentiated to give a hazard ratio which is just the ratio of two hazard rates.\nBecause we don‚Äôt get any parameter estimates for the baseline hazard function, we aren‚Äôt in a position to easily predict the absolute risk of an event, and so the strength of the Cox model is really in providing information about relative rather than absolute risks of the event occurring.\nAnd finally because the hazard is empirical, you really shouldn‚Äôt try to predict from a Cox model beyond the range of observable data that you have.\nOk, before we leave the Cox model, let‚Äôs talk about The Elephant in the room - proportional hazards. This is probably the most important assumption that you need to keep in mind and one that you should really test for each time you run a Cox model.\nThe proportional hazards assumption basically requires that the hazard ratio is constant over time, or equivalently, that the hazard for one individual is proportional to the hazard for any other individual, and where the proportionality constant doesn‚Äôt depend on time.\nSo if we look at the figure below - I have plotted hypothetical hazard functions for males (in blue) and females (in red) for the risk of some event. You‚Äôll note that the shapes are essentially the same but are scaled versions of one another. When proportional hazards hold, then we can take a ratio of the two hazard rates at any time point and that ratio should remain constant. And I‚Äôve illustrated that here at two time points - \\(t1\\) and \\(t2\\). The difference in the hazards obviously varies but the ratio of the hazards at those two time points is the same. At time 1 a hazard rate of 3 for males and 2 for females gives a hazard ratio of 1.5 and similarly, at time 2, a hazard rate of 0.75 for males and 0.5 for females, again gives a hazard ratio of 1.5.\nIn reality, hazard rates are probably not going to be exactly proportional the entire time, but the aim is to make sure any departures from proportionality are minimal - otherwise this means the hazard ratio itself varies with time and this adds further complexity to your model."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#parametric-exponential-weibull-log-normal-etc",
    "href": "posts/021_01Nov_2024/index.html#parametric-exponential-weibull-log-normal-etc",
    "title": "Survival Analysis - Under the Hood",
    "section": "7.3 Parametric (exponential, Weibull, log-normal, etc)",
    "text": "7.3 Parametric (exponential, Weibull, log-normal, etc)\nFully parametric survival models are essentially the same in concept and interpretation to the Cox model, barring a few structural differences. The main difference to the Cox model is that the baseline hazard function is fully specified in these models. In other words, the hazard function is assumed to follow a particular statistical distribution, and we looked at some of these before - monotone increasing, decreasing, lognormal, etc. And this then becomes a modelling choice that you have to make. You can never be certain that you‚Äôve chosen the correct distribution but there are tools that you can use to guide your choice - theory should always be at the top of the list, but you can also do things like generate empirical hazards to see what their shape might look like, compare model fits via some fitting criteria like the AIC, and so on. If you get the distribution about right, these models are more powerful than their Cox counterparts.\nLike Cox models, you get adjusted estimates of risk/survival. Unlike Cox models, you can estimate absolute risk of the event of interest, because you get a model parameter or parameters returned for that. Additionally, you‚Äôre in a better position with these models to predict beyond the maximum follow-up time, because there is a specified distribution of survival times that is expected to follow that of the population. But that comes with the usual caveat of always be careful about extrapolating beyond the range of your data."
  },
  {
    "objectID": "posts/022_15Nov_2024/index.html",
    "href": "posts/022_15Nov_2024/index.html",
    "title": "R Programming - Try for DRY",
    "section": "",
    "text": "Let‚Äôs face it - we could all become better R coders. Although I have been using R for quite a few years, there is still so much I could learn to make my code-writing life easier. The problem is that I consider myself a statistician before a programmer, and if I‚Äôm going to spend some time in learning something new, I tend to focus more on the statistics side of the job, rather than the syntax. A side-effect of this is that while I can no doubt code, I don‚Äôt think I code as efficiently as I could. Rather, I think I have become somewhat of a ‚Äòlazy‚Äô coder. What do I mean by that? Well, I can always write code to get achieve a result - there‚Äôs no issue with that - but I find that I can repeat myself a lot. And I tend to do that because, in the moment I think that‚Äôs easier than investing a little more time to work out a way to avoid that repetition. Consequently, I find that I may save time initially but this sometimes comes back to bite me later, when I discover either an error or am requested to update the existing code. Doing either of these things means that I have to change every bit of duplicated code, instead of doing it (ideally) just once.\nSo this is where the idea of DRY (Don‚Äôt Repeat Yourself) comes in. DRY is a general principle in software development, but applies equally in coding in R. Undoubtedly the most common realisation of the DRY principle in coding is the creation of a function for re-used logic. This is summed up in the book R for Data Science as:\n‚ÄúA good rule of thumb is to consider writing a function whenever you‚Äôve copied and pasted a block of code more than twice (i.e.¬†you now have three copies of the same code).‚Äù\nIn providing some foundation for how one might become more DRY in their daily work, I‚Äôm going to share with you some thoughts for how I approached a recent coding problem I was presented with (indulge my use of the acronym in this way). This will be somewhat of a holistic overview - I am going to focus less on the detail of the actual code and more on the thought process of taking the overall task and formulating a way to break that task down into smaller steps that can then be operated on in a way which avoids all of that nasty repetition."
  },
  {
    "objectID": "posts/022_15Nov_2024/index.html#the-longlazy-way---definitely-not-dry",
    "href": "posts/022_15Nov_2024/index.html#the-longlazy-way---definitely-not-dry",
    "title": "R Programming - Try for DRY",
    "section": "3.1 The long/lazy way - definitely not DRY",
    "text": "3.1 The long/lazy way - definitely not DRY\nIn this approach we are going to incorporate the above 3 steps into a single workflow. In doing this we will create 6 temporary dataframes to store the converted contents of each original variable. We will then join the resulting dataframes together. We can actually make this a little more succinct by combining Steps 1 and 2 in a single call, but even so, this approach leads to a LOT of repetition - 55 lines of code - to be exact.\n\n\nCode\n# Create individual dataframes with each original column split into multiple columns and converted to long format\ntemp1 &lt;-  dat |&gt; \n  select(id, var_1) |&gt; \n  separate_wider_delim(var_1, \",\", too_few = \"align_start\",\n                       names = c(\"var_1_base\", \"var_1_3mo\", \"var_1_6mo\", \"var_1_12mo\")) |&gt; \n  pivot_longer(2:5,\n               names_to = \"timepoint\",\n               values_to = \"var_1\",\n               names_prefix = \"var_1_\")\ntemp2 &lt;-  dat |&gt; \n  select(id, var_2) |&gt; \n  separate_wider_delim(var_2, \",\", too_few = \"align_start\",\n                       names = c(\"var_2_base\", \"var_2_3mo\", \"var_2_6mo\", \"var_2_12mo\")) |&gt; \n  pivot_longer(2:5,\n               names_to = \"timepoint\",\n               values_to = \"var_2\",\n               names_prefix = \"var_2_\")\ntemp3 &lt;-  dat |&gt; \n  select(id, var_3) |&gt; \n  separate_wider_delim(var_3, \",\", too_few = \"align_start\",\n                       names = c(\"var_3_base\", \"var_3_3mo\", \"var_3_6mo\", \"var_3_12mo\")) |&gt; \n  pivot_longer(2:5,\n               names_to = \"timepoint\",\n               values_to = \"var_3\",\n               names_prefix = \"var_3_\")\ntemp4 &lt;-  dat |&gt; \n  select(id, var_4) |&gt; \n  separate_wider_delim(var_4, \",\", too_few = \"align_start\",\n                       names = c(\"var_4_base\", \"var_4_3mo\", \"var_4_6mo\", \"var_4_12mo\")) |&gt; \n  pivot_longer(2:5,\n               names_to = \"timepoint\",\n               values_to = \"var_4\",\n               names_prefix = \"var_4_\")\ntemp5 &lt;-  dat |&gt; \n  select(id, var_5) |&gt; \n  separate_wider_delim(var_5, \",\", too_few = \"align_start\",\n                       names = c(\"var_5_base\", \"var_5_3mo\", \"var_5_6mo\", \"var_5_12mo\")) |&gt; \n  pivot_longer(2:5,\n               names_to = \"timepoint\",\n               values_to = \"var_5\",\n               names_prefix = \"var_5_\")\ntemp6 &lt;-  dat |&gt; \n  select(id, var_6) |&gt; \n  separate_wider_delim(var_6, \",\", too_few = \"align_start\",\n                       names = c(\"var_6_base\", \"var_6_3mo\", \"var_6_6mo\", \"var_6_12mo\")) |&gt; \n  pivot_longer(2:5,\n               names_to = \"timepoint\",\n               values_to = \"var_6\",\n               names_prefix = \"var_6_\")\n# Merge all dataframes together\ndat_converted &lt;-  left_join(temp1, temp2)\ndat_converted &lt;-  left_join(dat_converted, temp3)\ndat_converted &lt;-  left_join(dat_converted, temp4)\ndat_converted &lt;-  left_join(dat_converted, temp5)\ndat_converted &lt;-  left_join(dat_converted, temp6)\ndat_converted |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nid\ntimepoint\nvar_1\nvar_2\nvar_3\nvar_4\nvar_5\nvar_6\n\n\n\n\n1\nbase\n1\n0\n0\n38\n2\n4\n\n\n1\n3mo\n1\n8\n0\n30\n4\n10\n\n\n1\n6mo\n1\n0\n0\n38\n1\n1\n\n\n1\n12mo\n1\n0\n0\n30\n3\n1\n\n\n2\nbase\n1\n0\n0\n24\n1\n1\n\n\n2\n3mo\n1\n0\n0\n34\n1\n2\n\n\n2\n6mo\n1\n0\n1\n28\n1\n1\n\n\n2\n12mo\n1\n0\n2\n30\n3\n3\n\n\n3\nbase\n0\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n3\n3mo\n0\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n3\n6mo\n0\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n3\n12mo\n0\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n4\nbase\n1\n0\n0\n50\n0\n0\n\n\n4\n3mo\n1\n0\n0\n60\n3\n1\n\n\n4\n6mo\n1\n0\n0\n60\n3\n0\n\n\n4\n12mo\n1\n0\n0\n61\n0\n0\n\n\n5\nbase\n1\n0\n0\n40\n4\n6\n\n\n5\n3mo\n1\n0\n0\n16\n6\n8\n\n\n5\n6mo\n1\n20\n0\n20\n7\n4\n\n\n5\n12mo\n-11000\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n\n\n\nWhat‚Äôs wrong with this approach? Nothing really. The main criticism is that in creating each temporary dataframe we are copying and pasting the relevant code block 5 times. We then need to change the names of variables in the copies - each block has 8 places that a variable name appears, so that means 40 changes across the workflow. That‚Äôs a lot of places where we could potentially make an error by simply hitting the wrong key. But even if you don‚Äôt make mistakes, consider the all too common scenario where an updated dataset is sent to you at a later date for re-analysis. Sometimes, variable names are inadvertently changed between datasets - that also means many changes in your code.\nThere must be more robust approaches to this task."
  },
  {
    "objectID": "posts/022_15Nov_2024/index.html#write-a-function",
    "href": "posts/022_15Nov_2024/index.html#write-a-function",
    "title": "R Programming - Try for DRY",
    "section": "3.3 Write a function",
    "text": "3.3 Write a function\nA function is a good way to encapsulate some code that you might want to use more than once. We know what code we want to re-use - it‚Äôs essentially the following block, but we want to generalise it to be usable with any of the 6 variables in the original dataframe, not just var_1.\n\ntemp1 &lt;-  dat |&gt; \n  select(id, var_1) |&gt; \n  separate_wider_delim(var_1, \",\", too_few = \"align_start\",\n                       names = c(\"var_1_base\", \"var_1_3mo\", \"var_1_6mo\", \"var_1_12mo\")) |&gt; \n  pivot_longer(2:5,\n               names_to = \"timepoint\",\n               values_to = \"var_1\",\n               names_prefix = \"var_1_\")\n\nOne version of that code block wrapped up in a function - which I‚Äôve called split_to_long() - is shown below.\n\nsplit_to_long &lt;-  function(col){\n  i &lt;-  substr(col, 5, 5)\n  temp &lt;-  dat |&gt;\n    select(\"id\", all_of(col)) |&gt;\n    separate_wider_delim(all_of(col), \n                         \",\", \n                         too_few = \"align_start\", \n                         names = c(paste0(col,\"_\", c(\"base\", \"3mo\", \"6mo\", \"12mo\")))) |&gt;\n    pivot_longer(2:5,\n                 names_to = \"timepoint\",\n                 values_to = paste0(\"var_\", i),\n                 names_prefix = paste0(\"var_\", i, \"_\"))\n  if (i &gt; 1){\n    temp &lt;-  temp[3]\n  }\n  temp\n}\n\nWhat does split_to_long() do?\n\nLine 1 declares the function and specifies the single argument it accepts - col (i.e.¬†the name of the column we are interested in splitting and reshaping).\nLine 2 gets the variable number belonging to col and assigns it to i. This works here because all variable names follow the same format with the number being at the 5th position in the name.\nLines 3 - 12 take the original dataframe and selects out two variables - the id variable and col. It then splits and reshapes col into long format and saves it to temp.\nLines 13 - 15 tell the function that for any variable other than the first, we don‚Äôt need to retain the id and timepoint variables when creating temp.\nLine 16 ‚Äòreturns‚Äô temp. This is essential to ensure the function captures the result of interest."
  },
  {
    "objectID": "posts/022_15Nov_2024/index.html#chunk-1",
    "href": "posts/022_15Nov_2024/index.html#chunk-1",
    "title": "R Programming - Try for DRY",
    "section": "2.1 Chunk 1",
    "text": "2.1 Chunk 1\nWhat is the most obvious thing we can initially do? An easy suggestion is to split each column that contains multiple values into multiple columns each containing one value. separate_wider_delim() is a tidyr function that will allow us to do that and the code and output when applied to var_1 is shown below.\n\n\nCode\ndat |&gt; \n  select(id, var_1) |&gt; \n  separate_wider_delim(var_1, \",\", too_few = \"align_start\",\n                       names = c(\"var_1_base\", \"var_1_3mo\", \"var_1_6mo\", \"var_1_12mo\")) |&gt;  \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nid\nvar_1_base\nvar_1_3mo\nvar_1_6mo\nvar_1_12mo\n\n\n\n\n1\n1\n1\n1\n1\n\n\n2\n1\n1\n1\n1\n\n\n3\n0\n0\n0\n0\n\n\n4\n1\n1\n1\n1\n\n\n5\n1\n1\n1\n-11000"
  },
  {
    "objectID": "posts/022_15Nov_2024/index.html#step-1",
    "href": "posts/022_15Nov_2024/index.html#step-1",
    "title": "R Programming - Try for DRY",
    "section": "2.1 Step 1",
    "text": "2.1 Step 1\nWell the most obvious first thing we could do is to split each column that contains multiple values into multiple columns each containing one value. separate_wider_delim() is a tidyr function that will allow us to do that and the code and output when applied to var_1 is shown below (note that we would need to repeat this for every variable).\n\ntemp &lt;-  dat |&gt; \n  select(id, var_1) |&gt; \n  separate_wider_delim(var_1, \",\", too_few = \"align_start\",\n                       names = c(\"var_1_base\", \"var_1_3mo\", \"var_1_6mo\", \"var_1_12mo\"))\ntemp |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\nid\nvar_1_base\nvar_1_3mo\nvar_1_6mo\nvar_1_12mo\n\n\n\n\n1\n1\n1\n1\n1\n\n\n2\n1\n1\n1\n1\n\n\n3\n0\n0\n0\n0\n\n\n4\n1\n1\n1\n1\n\n\n5\n1\n1\n1\n-11000\n\n\n\n\n\nThat seems to have worked quite well - we now have each value in its own cell!"
  },
  {
    "objectID": "posts/022_15Nov_2024/index.html#step-2",
    "href": "posts/022_15Nov_2024/index.html#step-2",
    "title": "R Programming - Try for DRY",
    "section": "2.2 Step 2",
    "text": "2.2 Step 2\nNow that we‚Äôve found a way to split a column, we can take that output and convert it to long format. We can use pivot_longer to do that very easily (note that we would need to repeat this for every variable).\n\ntemp_long &lt;-  temp |&gt; \n  pivot_longer(2:5,\n               names_to = \"timepoint\",\n               values_to = \"var_1\",\n               names_prefix = \"var_1_\")\ntemp_long |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\nid\ntimepoint\nvar_1\n\n\n\n\n1\nbase\n1\n\n\n1\n3mo\n1\n\n\n1\n6mo\n1\n\n\n1\n12mo\n1\n\n\n2\nbase\n1\n\n\n2\n3mo\n1\n\n\n2\n6mo\n1\n\n\n2\n12mo\n1\n\n\n3\nbase\n0\n\n\n3\n3mo\n0\n\n\n3\n6mo\n0\n\n\n3\n12mo\n0\n\n\n4\nbase\n1\n\n\n4\n3mo\n1\n\n\n4\n6mo\n1\n\n\n4\n12mo\n1\n\n\n5\nbase\n1\n\n\n5\n3mo\n1\n\n\n5\n6mo\n1\n\n\n5\n12mo\n-11000"
  },
  {
    "objectID": "posts/022_15Nov_2024/index.html#step-3",
    "href": "posts/022_15Nov_2024/index.html#step-3",
    "title": "R Programming - Try for DRY",
    "section": "2.3 Step 3",
    "text": "2.3 Step 3\nReally, we‚Äôre nearly there. All that‚Äôs left to do at this point is to join the resulting columns together from performing Steps 1 and 2 on each of the remaining variables in the dataset. There are multiple ways you can do that in R."
  },
  {
    "objectID": "posts/022_15Nov_2024/index.html#not-dry",
    "href": "posts/022_15Nov_2024/index.html#not-dry",
    "title": "R Programming - Try for DRY",
    "section": "3.1 Not DRY",
    "text": "3.1 Not DRY\nIn this approach we are going to incorporate the above 3 steps into a single workflow. In doing this we will create 6 temporary dataframes to store the converted contents of each original variable. We will then join the resulting dataframes together. We can actually make this a little more succinct by combining Steps 1 and 2 in a single call, but even so, this approach leads to a LOT of repetition - 55 lines of code to be exact.\n\n# Create individual dataframes with each original column split into multiple columns and converted to long format\ntemp1 &lt;-  dat |&gt; \n  select(id, var_1) |&gt; \n  separate_wider_delim(var_1, \",\", too_few = \"align_start\",\n                       names = c(\"var_1_base\", \"var_1_3mo\", \"var_1_6mo\", \"var_1_12mo\")) |&gt; \n  pivot_longer(2:5,\n               names_to = \"timepoint\",\n               values_to = \"var_1\",\n               names_prefix = \"var_1_\")\ntemp2 &lt;-  dat |&gt; \n  select(id, var_2) |&gt; \n  separate_wider_delim(var_2, \",\", too_few = \"align_start\",\n                       names = c(\"var_2_base\", \"var_2_3mo\", \"var_2_6mo\", \"var_2_12mo\")) |&gt; \n  pivot_longer(2:5,\n               names_to = \"timepoint\",\n               values_to = \"var_2\",\n               names_prefix = \"var_2_\")\ntemp3 &lt;-  dat |&gt; \n  select(id, var_3) |&gt; \n  separate_wider_delim(var_3, \",\", too_few = \"align_start\",\n                       names = c(\"var_3_base\", \"var_3_3mo\", \"var_3_6mo\", \"var_3_12mo\")) |&gt; \n  pivot_longer(2:5,\n               names_to = \"timepoint\",\n               values_to = \"var_3\",\n               names_prefix = \"var_3_\")\ntemp4 &lt;-  dat |&gt; \n  select(id, var_4) |&gt; \n  separate_wider_delim(var_4, \",\", too_few = \"align_start\",\n                       names = c(\"var_4_base\", \"var_4_3mo\", \"var_4_6mo\", \"var_4_12mo\")) |&gt; \n  pivot_longer(2:5,\n               names_to = \"timepoint\",\n               values_to = \"var_4\",\n               names_prefix = \"var_4_\")\ntemp5 &lt;-  dat |&gt; \n  select(id, var_5) |&gt; \n  separate_wider_delim(var_5, \",\", too_few = \"align_start\",\n                       names = c(\"var_5_base\", \"var_5_3mo\", \"var_5_6mo\", \"var_5_12mo\")) |&gt; \n  pivot_longer(2:5,\n               names_to = \"timepoint\",\n               values_to = \"var_5\",\n               names_prefix = \"var_5_\")\ntemp6 &lt;-  dat |&gt; \n  select(id, var_6) |&gt; \n  separate_wider_delim(var_6, \",\", too_few = \"align_start\",\n                       names = c(\"var_6_base\", \"var_6_3mo\", \"var_6_6mo\", \"var_6_12mo\")) |&gt; \n  pivot_longer(2:5,\n               names_to = \"timepoint\",\n               values_to = \"var_6\",\n               names_prefix = \"var_6_\")\n# Merge all dataframes together\ndat_converted &lt;-  left_join(temp1, temp2)\ndat_converted &lt;-  left_join(dat_converted, temp3)\ndat_converted &lt;-  left_join(dat_converted, temp4)\ndat_converted &lt;-  left_join(dat_converted, temp5)\ndat_converted &lt;-  left_join(dat_converted, temp6)\ndat_converted |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\nid\ntimepoint\nvar_1\nvar_2\nvar_3\nvar_4\nvar_5\nvar_6\n\n\n\n\n1\nbase\n1\n0\n0\n38\n2\n4\n\n\n1\n3mo\n1\n8\n0\n30\n4\n10\n\n\n1\n6mo\n1\n0\n0\n38\n1\n1\n\n\n1\n12mo\n1\n0\n0\n30\n3\n1\n\n\n2\nbase\n1\n0\n0\n24\n1\n1\n\n\n2\n3mo\n1\n0\n0\n34\n1\n2\n\n\n2\n6mo\n1\n0\n1\n28\n1\n1\n\n\n2\n12mo\n1\n0\n2\n30\n3\n3\n\n\n3\nbase\n0\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n3\n3mo\n0\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n3\n6mo\n0\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n3\n12mo\n0\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n4\nbase\n1\n0\n0\n50\n0\n0\n\n\n4\n3mo\n1\n0\n0\n60\n3\n1\n\n\n4\n6mo\n1\n0\n0\n60\n3\n0\n\n\n4\n12mo\n1\n0\n0\n61\n0\n0\n\n\n5\nbase\n1\n0\n0\n40\n4\n6\n\n\n5\n3mo\n1\n0\n0\n16\n6\n8\n\n\n5\n6mo\n1\n20\n0\n20\n7\n4\n\n\n5\n12mo\n-11000\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n\n\n\n\nWhat‚Äôs wrong with this approach? Nothing, really. The main criticism is that in creating each temporary dataframe we are copying and pasting the relevant code block 5 times. We then need to change the names of variables in the copies - each block has 8 places that a variable name appears, so that means 40 changes across the workflow. That‚Äôs a lot of places where we could potentially make an error by simply hitting the wrong key. But even if you don‚Äôt make mistakes, consider the all too common scenario where an updated dataset is sent to you at a later date for re-analysis. Sometimes, variable names are inadvertently changed between datasets - that also means many changes in your code.\nThere must be more robust approaches to this task."
  },
  {
    "objectID": "posts/022_15Nov_2024/index.html#dry",
    "href": "posts/022_15Nov_2024/index.html#dry",
    "title": "R Programming - Try for DRY",
    "section": "3.2 DRY",
    "text": "3.2 DRY\nWhatever approach we take from here, there should be one coding concept that we incorporate as a foundational commonality - and that is the function. I‚Äôm not going to teach you how to write functions here - there are plenty of resources online and R for Data Science is a good place to start."
  },
  {
    "objectID": "posts/022_15Nov_2024/index.html#becoming-dry",
    "href": "posts/022_15Nov_2024/index.html#becoming-dry",
    "title": "R Programming - Try for DRY",
    "section": "3.2 Becoming DRY",
    "text": "3.2 Becoming DRY\nWhatever approach we take from here, there should be one coding concept that we incorporate as a foundational commonality - and that is the function. I‚Äôm not going to teach you how to write functions here - there are plenty of resources online and R for Data Science is a good place to start.\n\n3.2.1 Write a function\nA function is a good way to encapsulate some code that you might want to use more than once. We know what code we want to re-use - it‚Äôs essentially the following block, but we want to generalise it to be usable with any of the 6 variables in the original dataframe, not just var_1.\n\ntemp1 &lt;-  dat |&gt; \n  select(id, var_1) |&gt; \n  separate_wider_delim(var_1, \",\", too_few = \"align_start\",\n                       names = c(\"var_1_base\", \"var_1_3mo\", \"var_1_6mo\", \"var_1_12mo\")) |&gt; \n  pivot_longer(2:5,\n               names_to = \"timepoint\",\n               values_to = \"var_1\",\n               names_prefix = \"var_1_\")\n\n\nOne version of that code block wrapped up in a function - which I‚Äôve called split_to_long() - is shown below.\n\nsplit_to_long &lt;-  function(col){\n  i &lt;-  substr(col, 5, 5)\n  temp &lt;-  dat |&gt;\n    select(id, all_of(col)) |&gt;\n    separate_wider_delim(all_of(col), \n                         \",\", \n                         too_few = \"align_start\", \n                         names = c(paste0(col,\"_\", c(\"base\", \"3mo\", \"6mo\", \"12mo\")))) |&gt;\n    pivot_longer(2:5,\n                 names_to = \"timepoint\",\n                 values_to = paste0(\"var_\", i),\n                 names_prefix = paste0(\"var_\", i, \"_\"))\n  if (i &gt; 1){\n    temp &lt;-  temp[3]\n  }\n  temp\n}\n\n\nWhat does split_to_long() do?\n\nLine 1 declares the function and specifies the single argument it accepts - col (i.e.¬†the name of the column we are interested in splitting and reshaping).\nLine 2 gets the variable number belonging to col and assigns it to i. This works here because all variable names follow the same format with the number being at the 5th position in the name.\nLines 3 - 12 take the original dataframe and selects out two variables - the id variable and col. It then splits and reshapes col into long format and saves it to temp - a dataframe.\nLines 13 - 15 tell the function that for any variable other than the first, we don‚Äôt need to retain the id and timepoint variables when creating temp.\nLine 16 ‚Äòreturns‚Äô temp. This is essential to ensure the function captures the result of interest.\n\nHooray!\nWe now have a function which is generalisable to any column of the original dataframe. If we call split_to_long(\"var_1\"), we get:\n\n\nCode\nsplit_to_long(\"var_1\") |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nid\ntimepoint\nvar_1\n\n\n\n\n1\nbase\n1\n\n\n1\n3mo\n1\n\n\n1\n6mo\n1\n\n\n1\n12mo\n1\n\n\n2\nbase\n1\n\n\n2\n3mo\n1\n\n\n2\n6mo\n1\n\n\n2\n12mo\n1\n\n\n3\nbase\n0\n\n\n3\n3mo\n0\n\n\n3\n6mo\n0\n\n\n3\n12mo\n0\n\n\n4\nbase\n1\n\n\n4\n3mo\n1\n\n\n4\n6mo\n1\n\n\n4\n12mo\n1\n\n\n5\nbase\n1\n\n\n5\n3mo\n1\n\n\n5\n6mo\n1\n\n\n5\n12mo\n-11000\n\n\n\n\n\n\nSimilarly, we could call split_to_long(\"var_6\"), and get:\n\n\nCode\nsplit_to_long(\"var_6\") |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nvar_6\n\n\n\n\n4\n\n\n10\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n-13000\n\n\n-13000\n\n\n-13000\n\n\n-13000\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n6\n\n\n8\n\n\n4\n\n\n-13000\n\n\n\n\n\n\nRemember that in this case we asked the function not to keep the id and timeline variables.\nThe important point to take away from this is that in each case, we have reduced the code from 8 lines to just 1 line, and been able to produce the same result.\n\n\n3.2.2 Recycle that function\nAt this juncture, you may be thinking that we‚Äôve done all we can do to be DRY and the only thing left is to call our function on every column in our original dataset. Well, yes, that would work as advertised.\n\ndat_converted &lt;-  cbind(\n  split_to_long(\"var_1\"),\n  split_to_long(\"var_2\"),\n  split_to_long(\"var_3\"),\n  split_to_long(\"var_4\"),\n  split_to_long(\"var_5\"),\n  split_to_long(\"var_6\"))\ndat_converted |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\nid\ntimepoint\nvar_1\nvar_2\nvar_3\nvar_4\nvar_5\nvar_6\n\n\n\n\n1\nbase\n1\n0\n0\n38\n2\n4\n\n\n1\n3mo\n1\n8\n0\n30\n4\n10\n\n\n1\n6mo\n1\n0\n0\n38\n1\n1\n\n\n1\n12mo\n1\n0\n0\n30\n3\n1\n\n\n2\nbase\n1\n0\n0\n24\n1\n1\n\n\n2\n3mo\n1\n0\n0\n34\n1\n2\n\n\n2\n6mo\n1\n0\n1\n28\n1\n1\n\n\n2\n12mo\n1\n0\n2\n30\n3\n3\n\n\n3\nbase\n0\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n3\n3mo\n0\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n3\n6mo\n0\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n3\n12mo\n0\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n4\nbase\n1\n0\n0\n50\n0\n0\n\n\n4\n3mo\n1\n0\n0\n60\n3\n1\n\n\n4\n6mo\n1\n0\n0\n60\n3\n0\n\n\n4\n12mo\n1\n0\n0\n61\n0\n0\n\n\n5\nbase\n1\n0\n0\n40\n4\n6\n\n\n5\n3mo\n1\n0\n0\n16\n6\n8\n\n\n5\n6mo\n1\n20\n0\n20\n7\n4\n\n\n5\n12mo\n-11000\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n\n\n\n\nBut do you notice that we have inadvertently written code that is again repetitive? It might not seem like much, but we‚Äôve called split_to_long() 6 times - one for each column that we want to convert. Is there a way to be even more DRY than this? Well it turns out that there is, and the general idea is that we will take our function and iterate over the columns in the data frame. In other words, automatically apply the function without having to manually type the function out multiple times. There are two primary ways to do this - let‚Äôs consider them now.\n\n\n3.2.3 Function + for-loop\nThe for-loop is one of the main control-flow constructs in almost any programming language, including R. It can be used to iterate over a group of objects (e.g.¬†a vector, a list, a dataframe), applying the same manipulations to each object. In our specific case we are going to use a for-loop to iterate over the variable columns of our original dataframe. The for-loop I have written is fairly simple - the basic idea is to cycle over each of the relevant variables, applying our function to each variable in term. A dataframe is created and appended to in each cycle.\n\ndat_converted &lt;-  data.frame(matrix(ncol = 0, nrow = 20))\nfor (name in names(dat[2:7])){\n  dat_converted &lt;-  cbind(dat_converted, split_to_long(name))\n}\ndat_converted |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\nid\ntimepoint\nvar_1\nvar_2\nvar_3\nvar_4\nvar_5\nvar_6\n\n\n\n\n1\nbase\n1\n0\n0\n38\n2\n4\n\n\n1\n3mo\n1\n8\n0\n30\n4\n10\n\n\n1\n6mo\n1\n0\n0\n38\n1\n1\n\n\n1\n12mo\n1\n0\n0\n30\n3\n1\n\n\n2\nbase\n1\n0\n0\n24\n1\n1\n\n\n2\n3mo\n1\n0\n0\n34\n1\n2\n\n\n2\n6mo\n1\n0\n1\n28\n1\n1\n\n\n2\n12mo\n1\n0\n2\n30\n3\n3\n\n\n3\nbase\n0\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n3\n3mo\n0\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n3\n6mo\n0\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n3\n12mo\n0\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n4\nbase\n1\n0\n0\n50\n0\n0\n\n\n4\n3mo\n1\n0\n0\n60\n3\n1\n\n\n4\n6mo\n1\n0\n0\n60\n3\n0\n\n\n4\n12mo\n1\n0\n0\n61\n0\n0\n\n\n5\nbase\n1\n0\n0\n40\n4\n6\n\n\n5\n3mo\n1\n0\n0\n16\n6\n8\n\n\n5\n6mo\n1\n20\n0\n20\n7\n4\n\n\n5\n12mo\n-11000\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n\n\n\n\nAgain, this gives us the desired result and combined with our function, this approach uses 21 lines of code, and importantly there is no repetition.\n\n\n3.2.4 Function + purrr::map\nThe map family of functions within the purrr package are the tidyverse answer to base Rs apply family of functions. They essentially do the same thing, but map tend to be more consistent in their application, and are of course, designed for use with other tidyverse functions.\nAt its core, R is a functional programming language, which essentially means that it lends itself to using functions to solve complex problems. In this way it has the ability ‚Äúto wrap up for loops in a function, and call that function instead of using the for loop directly‚Äù. In the current context, we could then imagine a way to replace our for-loop with another function - i.e.¬†we could feed our function to another function that does the job of the for-loop. Why would you do this? Well, it is usually the case that map (or apply) not only requires less code to achieve the same result, but makes your code more readable, opens your code to parallelisation, and is slightly faster than a comparable for-loop (it‚Äôs a misconception that for-loops are slow if you set them up correctly - e.g.¬†you should always pre-allocate a vector length rather than growing the vector in the loop).\nHere we use the map function to apply our split_to_long function to each variable in the original dataframe. Because map returns a list, we take one extra step to reformat the list to a dataframe with list_cbind(). This approach uses 19 lines of code, and again, importantly, there is no repetition.\n\ndat_converted &lt;-  map(names(dat)[2:7], split_to_long) |&gt; \n  list_cbind()\ndat_converted |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\nid\ntimepoint\nvar_1\nvar_2\nvar_3\nvar_4\nvar_5\nvar_6\n\n\n\n\n1\nbase\n1\n0\n0\n38\n2\n4\n\n\n1\n3mo\n1\n8\n0\n30\n4\n10\n\n\n1\n6mo\n1\n0\n0\n38\n1\n1\n\n\n1\n12mo\n1\n0\n0\n30\n3\n1\n\n\n2\nbase\n1\n0\n0\n24\n1\n1\n\n\n2\n3mo\n1\n0\n0\n34\n1\n2\n\n\n2\n6mo\n1\n0\n1\n28\n1\n1\n\n\n2\n12mo\n1\n0\n2\n30\n3\n3\n\n\n3\nbase\n0\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n3\n3mo\n0\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n3\n6mo\n0\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n3\n12mo\n0\n-13000\n-13000\n-13000\n-13000\n-13000\n\n\n4\nbase\n1\n0\n0\n50\n0\n0\n\n\n4\n3mo\n1\n0\n0\n60\n3\n1\n\n\n4\n6mo\n1\n0\n0\n60\n3\n0\n\n\n4\n12mo\n1\n0\n0\n61\n0\n0\n\n\n5\nbase\n1\n0\n0\n40\n4\n6\n\n\n5\n3mo\n1\n0\n0\n16\n6\n8\n\n\n5\n6mo\n1\n20\n0\n20\n7\n4\n\n\n5\n12mo\n-11000\n-13000\n-13000\n-13000\n-13000\n-13000"
  },
  {
    "objectID": "posts/023_29Nov_2024/index.html#on-the-first-day-of-christmas-a-statistician-sent-to-me",
    "href": "posts/023_29Nov_2024/index.html#on-the-first-day-of-christmas-a-statistician-sent-to-me",
    "title": "On the 12th Day of Christmas, a Statistician Sent to Me‚Ä¶",
    "section": "",
    "text": "Shmueli, G. (2010). To Explain or to Predict. Statistical Science, 25(3), 289-310.\n\n\nWhat is your research question?\nWhat are your hypotheses and aims?\nIs your research meant to :\n\nDescribe\n\nFocus on descriptive (i.e.¬†summary), rather than inferential statistics.\n\nExplain\n\nAssociations.\nCausation.\nModelling and inferential statistics.\n\nPredict\n\nPredict outcome from set of covariates.\nFocus on maximising predictive power at expense of explanation.\nModelling ¬± inferential statistics.\n\n\nWhat is the outcome (described statistically as the ‚Äòestimand‚Äô)?\nDoes knowing the outcome address your research question.\n\ni.e.¬†is the answer aligned with the question?"
  },
  {
    "objectID": "posts/023_29Nov_2024/index.html",
    "href": "posts/023_29Nov_2024/index.html",
    "title": "On the 12th Day of Christmas, a Statistician Sent to Me‚Ä¶",
    "section": "",
    "text": "For the last post of this year,\nHere‚Äôs some Xmas-themed researcher cheer,\nEach ‚Äòday‚Äô is a statistical pearl,\nFor your next manuscript give them a whirl,\nTake heed of the 12 recommendations made,\nAnd you will make my life a lot easier.\n\nHmm, I just realised that last line might not completely rhyme - I‚Äôll work on something better, but in the meantime please accept that as a placeholder‚Ä¶\nIn your downtime over the upcoming break I can thoroughly recommend a read of the following 2022 BMJ Christmas edition paper:\nOn the 12th Day of Christmas, a Statistician Sent to Me . . .\nBeing not only aware of, but proactive about these common statistical faux pas will, I believe, help you in your chances of future successful submissions - as well as becoming better clinical researchers.\nThe slides I used in recent lab meetings relating to this talk are included below.\nMerry Xmas and Happy New Year to all! See you in 2025.\n(BTW - the home page image was created in ChatGPT with the prompt ‚ÄúCreate a christmas-themed image of a statistician wearing a santa hat‚Äù. I guess you get what you ask for‚Ä¶)\n    View slides in full screen"
  },
  {
    "objectID": "posts/024_07Feb_2025/index.html",
    "href": "posts/024_07Feb_2025/index.html",
    "title": "Missing Data and Multiple Imputation for Dummies  (Part 1)",
    "section": "",
    "text": "Welcome back to Stats Tips 2025!\nWe‚Äôre going to hit the ground running today given you should all be revitalised from a hopefully restful break, and talk about a pervasive problem in data analyses that can be quite frustrating to deal with - missing data. But don‚Äôt fret! There is a solution and we‚Äôll talk about that as well - multiple imputation. However, as a single post I fear the lengthy nature of what we need to cover on this topic will cause you to fall back asleep face-first into your morning porridge, so I‚Äôm going to split this over two posts. This week we‚Äôll focus on the missing data problem itself and in the next post I will then go on to introduce multiple imputation and provide a practical example of its application."
  },
  {
    "objectID": "posts/024_07Feb_2025/index.html#missing-completely-at-random-mcar",
    "href": "posts/024_07Feb_2025/index.html#missing-completely-at-random-mcar",
    "title": "Missing Data and Multiple Imputation for Dummies  (Part 1)",
    "section": "2.1 Missing Completely at Random (MCAR)",
    "text": "2.1 Missing Completely at Random (MCAR)\nData are considered to be Missing Completely at Random (MCAR) if the probability of a missing value neither depends on any observed or unobserved data. Let‚Äôs consider an example where we survey individuals about their bodyweight and sex (amongst other questions). In this case missingness on bodyweight would be considered MCAR if some individuals just didn‚Äôt see that particular question. Participants with missing data are then a simple random subsample of all available participants, independent of whether they were male or female, obese or non-obese. As a result, the exclusion of such individuals from any analyses maintains the representativeness of the original sample and aside from compromising power, does not bias the results.\nUnfortunately, MCAR is both the most restrictive and most unrealistic mechanism to assume one‚Äôs data obeys."
  },
  {
    "objectID": "posts/024_07Feb_2025/index.html#missing-at-random-mar",
    "href": "posts/024_07Feb_2025/index.html#missing-at-random-mar",
    "title": "Missing Data and Multiple Imputation for Dummies  (Part 1)",
    "section": "2.2 Missing at Random (MAR)",
    "text": "2.2 Missing at Random (MAR)\nData are considered to be Missing at Random (MAR) if the probability of a missing value depends (or is conditional) on observed data, and within categories of the observed data, the distribution of missing and non-missing values is the same (i.e.¬†MCAR within categories). Confusing, I know, but let me try and solidify this idea using the example above. In this context, missingness on bodyweight would be considered MAR if:\n\nFemales, in general, were less willing to reveal their bodyweight than males (i.e.¬†missingness is dependent on sex).\nFurthermore, if we looked at males and females separately, and if we knew the missing bodyweight values, we should find similar average observed and unobserved (missing) bodyweight values (i.e.¬†MCAR within categories).\n\nNow, we can test for the first assumption, but we can‚Äôt really test for the second - because we don‚Äôt know what we don‚Äôt know. I will give you an example of this shortly.\nThe assumptions underpinning the MAR mechanism are more general and more realistic than those for MCAR. If data are MCAR than they are also considered MAR, and therefore most modern missing data methods generally start from the MAR assumption."
  },
  {
    "objectID": "posts/024_07Feb_2025/index.html#missing-not-at-random-mar",
    "href": "posts/024_07Feb_2025/index.html#missing-not-at-random-mar",
    "title": "Missing Data and Multiple Imputation for Dummies (Part 1)",
    "section": "2.3 Missing Not at Random (MAR)",
    "text": "2.3 Missing Not at Random (MAR)\nData are considered to be Missing Not at Random (MNAR) if the probability of a missing value depends on unobserved data. In other words, the probability of missingness is related to the missing values themselves (had we been able to actually observe them). In the context of our example, missingness on bodyweight would be considered MNAR if heavier individuals, in general, were less willing to reveal their bodyweight than lighter individuals (i.e.¬†missingness is dependent on bodyweight itself).\nMNAR is the most complex missing data mechanism to deal with, and there really is no good solution to the problem, because as is the case for the second MAR assumption described above - we simply don‚Äôt know what we don‚Äôt know. MNAR is also referred to as ‚Äúnon-ignorable‚Äù because the missing data mechanism itself has to be modelled if you assume your data fall into this category - pattern mixture models are typically employed for this purpose. In contrast, MCAR and MAR mechanisms are both considered ‚Äúignorable‚Äù because we don‚Äôt have to include any information about the missing data itself when we make these assumptions about our data.\n‚Ä¶refer back to why we can‚Äôt identify the mechanism as significant test could be MAR or NMAR and a N.S. test could be MCAR or NMAR\n‚Ä¶when is CCA ok - less than 5% missingness according to this https://www.annualreviews.org/content/journals/10.1146/annurev.psych.58.110405.085530\nunbiased when MCAR"
  },
  {
    "objectID": "posts/024_07Feb_2025/index.html#missing-not-at-random-mnar",
    "href": "posts/024_07Feb_2025/index.html#missing-not-at-random-mnar",
    "title": "Missing Data and Multiple Imputation for Dummies  (Part 1)",
    "section": "2.3 Missing Not at Random (MNAR)",
    "text": "2.3 Missing Not at Random (MNAR)\nData are considered to be Missing Not at Random (MNAR) if the probability of a missing value depends on unobserved data. In other words, the probability of missingness is related to the missing values themselves (had we been able to actually observe them). In the context of our example, missingness on bodyweight would be considered MNAR if heavier individuals, in general, were less willing to reveal their bodyweight than lighter individuals (i.e.¬†missingness is dependent on bodyweight itself).\nMNAR is the most complex missing data mechanism to deal with, and there really is no good solution to the problem, because as is the case for the second MAR assumption described above - we simply don‚Äôt know what we don‚Äôt know. MNAR is also referred to as ‚Äúnon-ignorable‚Äù because the missing data mechanism itself has to be modelled if you assume your data fall into this category - pattern mixture models are typically employed for this purpose. In contrast, MCAR and MAR mechanisms are both considered ‚Äúignorable‚Äù because we don‚Äôt have to include any information about the missingness itself when we make these assumptions about our data."
  },
  {
    "objectID": "posts/024_07Feb_2025/index.html#complete-case-analysis-cca-aka---listwise-deletion",
    "href": "posts/024_07Feb_2025/index.html#complete-case-analysis-cca-aka---listwise-deletion",
    "title": "Missing Data and Multiple Imputation for Dummies  (Part 1)",
    "section": "4.1 Complete Case Analysis (CCA) [AKA - Listwise Deletion]",
    "text": "4.1 Complete Case Analysis (CCA) [AKA - Listwise Deletion]\nThis is the default solution that we all use, even when we don‚Äôt realise we‚Äôre choosing to use it, and that‚Äôs because our software tools make the decision on our behalf. Essentially, observations in our dataset that contain missingness in any variables involved in an analysis are discarded. So, if you are running a regression model with 9 predictors and an individual has a missing value on one of those predictors, all of that individuals data will be ignored (i.e.¬†they will be dropped from the analysis). It‚Äôs important then to consider here how the pattern of missingness across both observations (rows) and variables (columns) can affect the amount of data discarded and the subsequent power. For example, if you were to have 10 missing values, you would be much better off having one individual missing all 10 values than 10 individuals missing each of one 1 different value on the 10 different variables specified in the model. In the former case, your sample size decreases by 1, whereas in the latter it decreases by 10.\nCCA is only considered to produce unbiased results when the assumption of MCAR holds. When the amount of missing data is small (say less than 5%) than it may not matter all that much even if we assume incorrectly, but with larger amounts of missingness it may be prudent to consider an alternative method in attempting to deal with the problem. This paper goes into some of these issues in more detail and is certainly worth a read."
  },
  {
    "objectID": "posts/024_07Feb_2025/index.html#single-imputation-methods",
    "href": "posts/024_07Feb_2025/index.html#single-imputation-methods",
    "title": "Missing Data and Multiple Imputation for Dummies  (Part 1)",
    "section": "4.2 Single Imputation Methods",
    "text": "4.2 Single Imputation Methods\nAnother relatively quick fix is to impute the missing values with some single ‚Äúfixed‚Äù value - typically the mean or median is used. Unfortunately, while single imputation methods solve the problem of information wastefulness and loss of power, they produce potentially more biased results. Consequently CCA is still regarded as a better choice in handling missingness between the two."
  },
  {
    "objectID": "posts/024_07Feb_2025/index.html#multiple-imputation",
    "href": "posts/024_07Feb_2025/index.html#multiple-imputation",
    "title": "Missing Data and Multiple Imputation for Dummies (Part 1)",
    "section": "4.3 Multiple Imputation",
    "text": "4.3 Multiple Imputation"
  },
  {
    "objectID": "posts/024_07Feb_2025/index.html#simulating-mar-data",
    "href": "posts/024_07Feb_2025/index.html#simulating-mar-data",
    "title": "Missing Data and Multiple Imputation for Dummies  (Part 1)",
    "section": "3.1 Simulating MAR data",
    "text": "3.1 Simulating MAR data\nThe idea of this simulation is to show you how one would hypothetically test the null hypothesis that missingness on a particular variable were MCAR. If the test were statistically significant, that would provide evidence against the null and allow us to say that the missingness were (at least) not MCAR. To deduce the mechanism beyond that requires knowledge of the missing values themselves - which, with being simulated data, we are lucky enough to have. So we can actually take the diagnostic exercise to its conclusion with these fake data.\n\n3.1.1 Simulation\nFor this example, I will simulate 100 observations, making males, on average, 5 kg heavier than females. I will then create a missingness indicator variable such that the probability of missingness is ~ 12% for males and 73% for females (i.e.¬†as per above - females, in general, were less willing to reveal their bodyweight than males). The first 20 rows of the resulting dataframe look like:\n\n\nCode\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(gtsummary)\n\n# Simulation\nn &lt;- 100                   # set number of obs to simulate\nset.seed(1234)             # set seed for reproducibility\nsex &lt;-  rbinom(n, 1, 0.5)  # generate sex variable ~ 50% females (0) and 50% males (1)\n# Generate bodyweight from regression model based on additive effect of sex - on average, males are 5 kg heavier than females\nbodyweight &lt;-  5 + 5 * sex + rnorm(n, 0, 1)\n# Now, let's create some missingness in bodyweight at random (where the missingness in bodyweight depends on sex)\n# To do this we will leave the actual bodyweight values intact (as we will need them later) and create a separate missing variable to indicate which values would be absent in a real dataset.\n# The model intercept and coefficient have been selected so that the probability of missingness will be ~ 12% for males and ~ 73% for females\nlogodds_missing &lt;-  1 - 3 * sex                      \nodds_missing &lt;-  exp(logodds_missing)               # calculate odds of missingness\nprob_missing  &lt;-  odds_missing/(1 + odds_missing)   # calculate probability of missingness\n# Assemble dataframe\ndat &lt;-  data.frame(sex = factor(sex, levels = c(\"0\", \"1\"), labels = c(\"females\", \"males\")),\n                   bodyweight = bodyweight,\n                   odds_missing = odds_missing,\n                   prob_missing = prob_missing,\n                   missing = factor(rbinom(n, 1, prob_missing), levels = c(\"0\", \"1\"), labels = c(\"no\", \"yes\")))\nhead(dat, 20) |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nsex\nbodyweight\nodds_missing\nprob_missing\nmissing\n\n\n\n\nfemales\n3.19\n2.72\n0.73\nyes\n\n\nmales\n9.42\n0.14\n0.12\nyes\n\n\nmales\n8.89\n0.14\n0.12\nno\n\n\nmales\n8.99\n0.14\n0.12\nno\n\n\nmales\n9.84\n0.14\n0.12\nyes\n\n\nmales\n10.56\n0.14\n0.12\nno\n\n\nfemales\n6.65\n2.72\n0.73\nyes\n\n\nfemales\n4.23\n2.72\n0.73\nno\n\n\nmales\n11.61\n0.14\n0.12\nno\n\n\nmales\n8.84\n0.14\n0.12\nno\n\n\nmales\n10.66\n0.14\n0.12\nyes\n\n\nmales\n12.55\n0.14\n0.12\nno\n\n\nfemales\n4.97\n2.72\n0.73\nyes\n\n\nmales\n9.33\n0.14\n0.12\nno\n\n\nfemales\n4.99\n2.72\n0.73\nyes\n\n\nmales\n11.78\n0.14\n0.12\nno\n\n\nfemales\n3.86\n2.72\n0.73\nyes\n\n\nfemales\n6.37\n2.72\n0.73\nyes\n\n\nfemales\n6.33\n2.72\n0.73\nyes\n\n\nfemales\n5.34\n2.72\n0.73\nno\n\n\n\n\n\nNow, let‚Äôs check that we recover the original estimates that we specified; both the intercept and coefficient for sex should be about 5.\n\n\nCode\n# Check that we recover the original coefficients:\nlm(bodyweight ~ sex , data = dat) |&gt; \n  tbl_regression(intercept = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n5.0\n4.8, 5.3\n&lt;0.001\n\n\nsex\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†females\n‚Äî\n‚Äî\n\n\n\n\n¬†¬†¬†¬†males\n5.1\n4.7, 5.5\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nThat‚Äôs good - we are on track. Finally let‚Äôs check that the simulated missing values are in proportion with what we asked.\n\n\nCode\n# Check expected missing proportions with tabulation\ntable(round(dat$prob, 2), dat$missing)\n\n\n      \n       no yes\n  0.12 35  10\n  0.73 17  38\n\n\nCode\nround(0.12*45, 0) # gives expectation 5/45 males with missingness\n\n\n[1] 5\n\n\nCode\nround(0.73*55, 0) # gives expectation 40/55 females with missingness\n\n\n[1] 40\n\n\nWe expected about 5 males and 40 females to have missing values and ended up with 10 and 38, respectively. Closer for females than males but that‚Äôs the nature of random sampling.\n\n\n3.1.2 Step 1 - Test whether missingness depends on sex\nIn assessing the MCAR assumption, the first step we can take is to simply regress the missingness indicator on sex. This provides a statistical test for whether missing values in bodyweight depend on sex. Note that we don‚Äôt need the missing values themselves as this point in the diagnostic process. The logistic regression model results give:\n\n\nCode\nglm(missing ~ sex, data = dat, family = \"binomial\") |&gt; \n  tbl_regression(intercept = TRUE, exp = TRUE) |&gt; \n  modify_caption(\"**Missing (outcome) vs Sex (predictor)**\")\n\n\n\n\n\n\nMissing (outcome) vs Sex (predictor)\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\n(Intercept)\n2.24\n1.28, 4.06\n0.006\n\n\nsex\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†females\n‚Äî\n‚Äî\n\n\n\n\n¬†¬†¬†¬†males\n0.13\n0.05, 0.31\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nThis suggests that there is an ~ 87% reduction in the odds of missingness for males relative to be females and this is statistically significant at the 5% level. Knowing this piece of information rules out MCAR as the mechanism for missingness in our bodyweight variable and we can now say that MAR or MNAR causes are potential culprits. With real-world data we would normally stop at this point, as we don‚Äôt have any more information to go on. But as alluded to earlier, given these are simulated data, we know the actual missing values and can therefore continue our forensic investigations.\n\n\n3.1.3 Step 2 - Test whether missingness depends on bodyweight within each category of sex\n\n\n\n\n\n\nImportant\n\n\n\nRemember, this is generally considered an untestable assumption as we typically don‚Äôt have these data.\n\n\nTo perform this test we will regress bodyweight on the missingness indicator for males and females, separately, using a standard linear regression approach.\n\n\nCode\n# Females\nlm(bodyweight ~ missing, data = subset(dat, sex == \"females\")) |&gt; \n  tbl_regression() |&gt; \n  modify_caption(\"**Females**\")\n\n\n\n\n\n\nFemales\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nmissing\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n\n\n\n¬†¬†¬†¬†yes\n0.04\n-0.56, 0.63\n&gt;0.9\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nCode\n# Males\nlm(bodyweight ~ missing, data = subset(dat, sex == \"males\")) |&gt; \n  tbl_regression() |&gt; \n  modify_caption(\"**Males**\")\n\n\n\n\n\n\nMales\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nmissing\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n\n\n\n¬†¬†¬†¬†yes\n0.02\n-0.63, 0.67\n&gt;0.9\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nThe coefficient for missingness is not statistically significant in either subgroup which lends weight to our assertion that the missingness in bodyweight has a potential MAR cause. It‚Äôs always helpful to visualise the data as well, so let‚Äôs do that by making some boxplots:\n\n\nCode\n# Boxplots of distributions by sex\nggplot(dat, aes(x = sex, y = bodyweight, fill = missing)) +\n  geom_boxplot() +\n  geom_dotplot(binaxis = 'y', stackdir = 'center', dotsize = 0.5, position = position_dodge(0.75)) +\n  theme_bw(base_size = 20)\n\n\n\n\n\n\n\n\n\nI will wrap up this section with the suggestion that there‚Äôs actually a more succinct (if not slightly more complex), equivalent approach that integrates both Steps 1 and 2 within a single model framework. Here we modify the model slightly to include an interaction term between the missingness indicator and sex (but again this requires knowledge of the missing values).\n\n\nCode\nlm(bodyweight ~ missing * sex, data = dat) |&gt; \n  tbl_regression() |&gt; \n  modify_caption(\"**Interaction Model**\")\n\n\n\n\n\n\nInteraction Model\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nmissing\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n\n\n\n¬†¬†¬†¬†yes\n0.04\n-0.52, 0.59\n0.9\n\n\nsex\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†females\n‚Äî\n‚Äî\n\n\n\n\n¬†¬†¬†¬†males\n5.1\n4.5, 5.7\n&lt;0.001\n\n\nmissing * sex\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†yes * males\n-0.02\n-0.90, 0.87\n&gt;0.9\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nIn this framework we can cut straight to the chase in the expectation that if the MAR assumption were true for these data, than the statistical tests for both the coefficients on missingness and the interaction between missingness and sex would not be statistically significant - in other words there would be no evidence against the null hypotheses that both of these coefficients were zero (i.e.¬†they are probably zero!)"
  },
  {
    "objectID": "posts/024_07Feb_2025/index.html#multiple-imputation-mi",
    "href": "posts/024_07Feb_2025/index.html#multiple-imputation-mi",
    "title": "Missing Data and Multiple Imputation for Dummies  (Part 1)",
    "section": "4.3 Multiple Imputation (MI)",
    "text": "4.3 Multiple Imputation (MI)\nI will end today‚Äôs post by introducing the idea of multiple imputation. There is no doubt that this approach requires more work on the analyst‚Äôs part than the other two options, but the rewards are likely worth it. In contrast to single imputation where we consider the substituted value ‚Äúfixed‚Äù, MI uses the distribution of the observed data to estimate multiple possible ‚Äúplaceholder‚Äù values. Ultimately, this allows us to account for the uncertainty around the true value, and obtain approximately unbiased estimates (under the assumption of MAR).\nMI can be thought of in broad terms as a three-step procedure:\n\nMultiple plausible ‚Äúcomplete‚Äù versions of the original incomplete dataset are created by sampling values based on a statistical model that accurately describes the data (plus a random error component).\nEach imputed dataset is analysed using standard statistical methods, resulting in multiple (m) parameter estimates (due only to the differences in the imputed values).\nThe estimates are pooled in an overall statistical analysis in which the uncertainty in the imputed missing values are incorporated in the standard errors and significance tests.\n\nIn this last step, the m parameter estimates are pooled, by simply averaging them, into one estimate, and the variance of the estimate calculated. The estimate pooling by simple averaging is the conceptually simple part. How we arrive at the variance of that estimate is the part that may be a bit more complex to grasp. In computing the overall variance, ‚Äúwithin‚Äù and ‚Äúbetween‚Äù variance estimates need to be combined. The within-variance represents the usual ‚Äúnoise‚Äù in an estimate (within a dataset) that is always present independent of whether one is peforming MI or not. The between-variance, however, represents the variance in an estimate (across datasets) and addresses how much uncertainty is due to the ‚Äúguessing‚Äù done to fill in the missing values during the MI procedure.\nAs mentioned above, under the right conditions, the pooled estimates are unbiased and have the correct statistical properties. It‚Äôs for this reason (in addition to others) that MI is widely regarded as the gold standard method for handling missing data in clinical research.\nAnd I think that‚Äôs a good place to take a pause and let you catch your breath. Next time I will introduce MI to you by way of example, so hopefully that will be a little less dry than what we have discussed (but needed to discuss), today. Do stay tuned‚Ä¶"
  },
  {
    "objectID": "posts/025_21Feb_2025/index.html",
    "href": "posts/025_21Feb_2025/index.html",
    "title": "Missing Data and Multiple Imputation for Dummies  (Part 2)",
    "section": "",
    "text": "Welcome to Part 2 of our tutorial on missing data and multiple imputation. If you recall, we ended the last post with a brief introduction to MI and so today I am going to extend that discussion further with some specific, but important detail in how MI actually works via its implementation in R. Having a better understanding of the algorithmic process involved is invaluable in contextualising what is actually happening to your data when you incorporate MI into your analysis. As an expository aid, we will also apply MI to a small dataset which I hope will further build your practical intuition of this fairly complex statistical topic."
  },
  {
    "objectID": "posts/025_21Feb_2025/index.html#overview",
    "href": "posts/025_21Feb_2025/index.html#overview",
    "title": "Missing Data and Multiple Imputation for Dummies  (Part 2)",
    "section": "2.2 Overview",
    "text": "2.2 Overview\nIn the last post I suggested that MI can be thought of in broad terms as a three-step procedure: imputation, analysis, and pooling. Using the mice package, each of these steps can be equated with a function (blue) and resulting object (red) in your R workspace - in short:\n\nmice() imputes the data, producing a mids object\nwith() analyses the data, producing a mira object\npool() combines the data, producing a mipo object\n\n\n\n\nMain Steps in MI\n\n\nSo, imagine that we have our nhanes2 dataset and we want to explore the relationship between bmi (as the outcome) and age, hyp and chl (as predictors). If we just go ahead and run a linear regression with the data as is, we get:\n\n\nCode\nlm(bmi ~ age + hyp + chl, data = nhanes2) |&gt; \n  tbl_regression() |&gt; \n  add_glance_source_note()\n\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    age\n\n\n\n    ¬†¬†¬†¬†20-39\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†40-59\n-6.6\n-12, -1.3\n0.021\n    ¬†¬†¬†¬†60-99\n-11\n-19, -3.4\n0.010\n    hyp\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n2.4\n-3.5, 8.2\n0.4\n    chl\n0.08\n0.02, 0.14\n0.012\n  \n  \n    \n      R¬≤ = 0.670; Adjusted R¬≤ = 0.505; Sigma = 3.23; Statistic = 4.06; p-value = 0.044; df = 4; Log-likelihood = -30.6; AIC = 73.1; BIC = 76.5; Deviance = 83.7; Residual df = 8; No. Obs. = 13\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nEverything seems to look ok - regression coefficients seem plausible and standard errors haven‚Äôt blown up. But if you look at the table footer you will see that R‚Äôs default setting of listwise deletion, means that only 13 observations have been able to be used. The model has discarded 48% of the data. ‚ÄúUnacceptable!‚Äù you say. Not only have we lost a decent amount of power (and maybe that wouldn‚Äôt concern you so much with a larger dataset), but we are also potentially at risk of bias if our missing data mechanism isn‚Äôt MCAR (which is what complete case analysis assumes).\nGiven I want to show you how to use MI, I am going to do that regardless of whether these data are MCAR or not. So, I will leave as an exercise for you to test the MCAR assumption based on what I have shown you in the previous post, if you are interested (I actually haven‚Äôt looked at this so I don‚Äôt know whether MCAR may be considered reasonable in this case).\nSo let‚Äôs run through the steps that mice uses if we were to impute these data.\nAs the first step, the mice() function creates several complete datasets (in the above we have set this to n = 3). It considers the missingness in each variable to adhere to a particular distribution, drawing plausible values from this distribution as ‚Äúplaceholders‚Äù for the missing values. These ‚Äúcomplete‚Äù datasets are then stored in an R object class called mids (which is an abbreviation of ‚Äúmultiply imputed dataset‚Äù). Each dataset is a copy of the original dataframe, except that the missing values are now replaced by those generated by mice().\n\n\n\n\n\n\nNote\n\n\n\nI call these values placeholders as they are not fixed or actually observed, but merely serve to allow estimation of the parameter and its variance.\n\n\nIn the second step, we now analyse each of our three complete datasets. Using the with() function, we run the same linear regression model as above on each dataframe of 25 observations, obtaining three different regression coefficients for each parameter. These coefficients only differ due to the differences in the imputed values across each dataset. The analysis results are then stored in another R object class called mira (‚Äúmultiply imputed repeated analysis‚Äù).\nIn the third and final step, we pool the three coefficients from separate models into one overall regression coefficient for each parameter, and estimate its variance using the pool() function. The final coefficient is the simple average of the three separate coefficients. As described in the last post, the variance estimate combines both ‚Äúwithin‚Äù (dataset) and ‚Äúbetween‚Äù (dataset) variance components. The pooled results are stored in a mipo - ‚Äúmultiple imputation pooled object‚Äù - object class in your R environment. This becomes the object that you are interested in reporting the results from.\nThere is one other function in the mice package that is worth a mention - complete(). While this function isn‚Äôt essential in the MI pipeline it is nonetheless a very helpful tool in examining the imputed datasets, and I will show you its usage, along with the other primary functions, shortly."
  },
  {
    "objectID": "posts/025_21Feb_2025/index.html#example-data",
    "href": "posts/025_21Feb_2025/index.html#example-data",
    "title": "Missing Data and Multiple Imputation for Dummies  (Part 2)",
    "section": "2.1 Example Data",
    "text": "2.1 Example Data\nBefore we go any further, let me introduce a small dataset included in the mice package that we will use to illustrate conditional MI in action. The nhanes2 dataset contains sample data from the US National Health and Nutrition Examination Study. It consists of 25 observations of 4 variables:\n\nage - Age group (1=20-39, 2=40-59, 3=60+)\nbmi - Body mass index (kg/m^2)\nhyp - Hypertensive (1=no,2=yes)\nchl - Total serum cholesterol (mg/dL)\n\nage and hyp are formatted as factors and bmi and chl as numeric. The data look like:\n\n\nCode\nlibrary(tidyverse)\nlibrary(mice)\nlibrary(gtsummary)\ndata(nhanes2)\nnhanes2 |&gt; tibble() |&gt; print(n = Inf)\n\n\n# A tibble: 25 √ó 4\n   age     bmi hyp     chl\n   &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1 20-39  NA   &lt;NA&gt;     NA\n 2 40-59  22.7 no      187\n 3 20-39  NA   no      187\n 4 60-99  NA   &lt;NA&gt;     NA\n 5 20-39  20.4 no      113\n 6 60-99  NA   &lt;NA&gt;    184\n 7 20-39  22.5 no      118\n 8 20-39  30.1 no      187\n 9 40-59  22   no      238\n10 40-59  NA   &lt;NA&gt;     NA\n11 20-39  NA   &lt;NA&gt;     NA\n12 40-59  NA   &lt;NA&gt;     NA\n13 60-99  21.7 no      206\n14 40-59  28.7 yes     204\n15 20-39  29.6 no       NA\n16 20-39  NA   &lt;NA&gt;     NA\n17 60-99  27.2 yes     284\n18 40-59  26.3 yes     199\n19 20-39  35.3 no      218\n20 60-99  25.5 yes      NA\n21 20-39  NA   &lt;NA&gt;     NA\n22 20-39  33.2 no      229\n23 20-39  27.5 no      131\n24 60-99  24.9 no       NA\n25 40-59  27.4 no      186\n\n\nNote there is some missingness on all variables except age."
  },
  {
    "objectID": "posts/025_21Feb_2025/index.html#iterations-and-imputations",
    "href": "posts/025_21Feb_2025/index.html#iterations-and-imputations",
    "title": "Missing Data and Multiple Imputation for Dummies (Part 2)",
    "section": "2.3 Iterations and Imputations",
    "text": "2.3 Iterations and Imputations"
  },
  {
    "objectID": "posts/025_21Feb_2025/index.html#iterations-within-imputations",
    "href": "posts/025_21Feb_2025/index.html#iterations-within-imputations",
    "title": "Missing Data and Multiple Imputation for Dummies  (Part 2)",
    "section": "2.3 Iterations Within Imputations",
    "text": "2.3 Iterations Within Imputations\nThere is one last bit of theory we will cover before we get to the practical example. In my understanding of MI, I found it super-helpful to have a cursory working knowledge of the chained equation equation approach. As I think this will also help you, we‚Äôll take a few minutes to flesh out some of the detail in what the procedure is actually doing in the background when you call mice() on your data.\nRecall that in essence the mice procedure runs a series of regression models whereby each variable with missing data is regressed conditional on the other variables present. In this way, variables that differ in their distribution can be modelled as such - for example, logistic regression for binary variables or linear regression for continuous. More explicitly, the chained equation process can be broken down into one ‚Äúcycle‚Äù or iteration, consisting of five steps:\nIteration start -\n\nStep 1: Every missing value in the dataset is replaced with a simple (single) imputation, such as the mean or median.\nStep 2: For one variable - let‚Äôs call this ‚Äúmi_var‚Äù - the simple imputations are set back to missing.\nStep 3: The observed values from ‚Äúmi_var‚Äù in Step 2 are regressed (as the outcome) on the other variables (as predictors) in the imputation model, which may or may not consist of all remaining variables in the dataset. The distribution of ‚Äúmi_var‚Äù is selected as appropriate for that variable.\nStep 4: The missing values for ‚Äúmi_var‚Äù are then replaced with predictions (imputations) from the regression model.\nStep 5: Repeat Steps 2-4 for each variable that has missing data. Note that at the end of Step 4 ‚Äúmi_var‚Äù may serve as a predictor for any other variable with missing data, whereby both the observed and imputed values will be used to predict that variable‚Äôs missing values.\n\n- Iteration end\nAt the end of one iteration, all variables with missing data will have been replaced with predictions (imputations) from regression models that represent the associations observed in the data. Usually multiple iterations (ten is considered standard) are specified with the imputations being updated at the end of each cycle. Enough iterations should be specified to establish with confidence that the imputation parameter estimates (regression coefficients) should ‚Äúconverge‚Äù so they are no longer changing, within some tolerance, in a new cycle.\nSo, when we think of Inception as a movie about layers (dreams within dreams), it can be helpful to think of MI in a similar way, as also consisting of layers (iterations within imputations). Each call to mice() produces multiple imputed datasets where each imputed dataset is itself the result of multiple iterations. Hopefully your brain doesn‚Äôt hurt like mine did when I watched Inception!"
  },
  {
    "objectID": "posts/025_21Feb_2025/index.html#inspect",
    "href": "posts/025_21Feb_2025/index.html#inspect",
    "title": "Missing Data and Multiple Imputation for Dummies  (Part 2)",
    "section": "3.1 Inspect",
    "text": "3.1 Inspect\nIt‚Äôs always a good idea to first take a couple of minutes to inspect and/or visualise the missing values and any potential patterns in the data. Let‚Äôs first check the number and proportion of missing values for each variable. We could code this ourselves, but there‚Äôs actually a handy little function especially for this in the questionr package:\n\n\nCode\nlibrary(questionr)\nfreq.na(nhanes2)\n\n\n    missing  %\nchl      10 40\nbmi       9 36\nhyp       8 32\nage       0  0\n\n\nWe can extend this further to look for patterns of missing values and mice provides the md.pattern() function for this. An equivalent plot and tabulation are supplied by default but it is possible to suppress the plot if one wishes.\n\n\nCode\nmd.pattern(nhanes2)\n\n\n\n\n\n\n\n\n\n   age hyp bmi chl   \n13   1   1   1   1  0\n3    1   1   1   0  1\n1    1   1   0   1  1\n1    1   0   0   1  2\n7    1   0   0   0  3\n     0   8   9  10 27\n\n\nValues are either 0 (missing) or 1 (observed). From top to bottom, we read this as:\n\n13 people have no missing data.\n3 people have missingness on chl only.\n1 person has missingness on bmi only.\n1 person has missingness on hyp and bmi only.\n7 people have missingness on hyp, bmi and chl.\n\nThe numbers in the bottom row represent the frequencies of missingness on each variable and the resulting marginal total (far right). The numbers in the right-most column represent the number of variables with missingness in each pattern."
  },
  {
    "objectID": "posts/025_21Feb_2025/index.html#mice",
    "href": "posts/025_21Feb_2025/index.html#mice",
    "title": "Missing Data and Multiple Imputation for Dummies  (Part 2)",
    "section": "3.2 mice()",
    "text": "3.2 mice()\nNow, let‚Äôs do the imputation. It‚Äôs as simple as calling the mice() function on our dataset, but here I will specify 3 imputations rather than the default of 5.\n\n\nCode\nset.seed(1234)\nimp &lt;-  mice(nhanes2, m = 3, print = FALSE)\nimp\n\n\nClass: mids\nNumber of multiple imputations:  3 \nImputation methods:\n     age      bmi      hyp      chl \n      \"\"    \"pmm\" \"logreg\"    \"pmm\" \nPredictorMatrix:\n    age bmi hyp chl\nage   0   1   1   1\nbmi   1   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\n\nBy inspecting the resulting mids object, we can see that mice() used pmm (predictive mean matching) to impute all the variables except hyp which used logistic regression (logreg) instead, given this is a binary variable. Note that variables with no missing values will have no method (‚Äú‚Äú) specified because no imputation is actually needed.\n\n\n\n\n\n\nNote\n\n\n\npmm is the recommended and default method for the imputation of continuous data in mice()\n\n\nThe output also shows the predictorMatrix which determines what variables are used in the imputation of another. It is useful to familiarise yourself with this matrix as you may want to customise imputations at some point and amending the predictorMatrix provides such functionality. You can interpret the predictorMatrix as rows corresponding to incomplete target variables and columns as complete predictor variables. A value of 1 indicates that the column variable acts as a predictor to impute the target (row) variable, and a 0 means that it is not used. Thus, in our example, hyp is predicted from age, bmi and chl. Note that the diagonal is set to 0 since variables cannot predict themselves. The default setting of the predictorMatrix specifies that every variable predicts all others.\nAs a relatively quick diagnostic exercise, we can easily examine the correspondence between observed and imputed values (for the continuous variables at least) by using the stripplot() function. Note that mice provides other graphical tools for this that are also worth exploring.\n\n\nCode\nstripplot(imp, col = c(\"red\", \"blue\"), pch = c(1, 20), cex = c(2, 2))\n\n\n\n\n\n\n\n\n\nThe plots show relatively good overlap of the imputed values (in blue) with the original non-missing data (red)."
  },
  {
    "objectID": "posts/025_21Feb_2025/index.html#complete",
    "href": "posts/025_21Feb_2025/index.html#complete",
    "title": "Missing Data and Multiple Imputation for Dummies  (Part 2)",
    "section": "3.3 complete()",
    "text": "3.3 complete()\nAt this point you may actually want to look at the imputed values and datasets themselves. mice makes this easy with the complete() function and while this isn‚Äôt integral to the workflow, it can be useful as another plausiblity check that the imputations are working as they should. There are multiple formats that we can view the imputed data in, and I will show you what I consider are the three main ways. In all cases, each observation will be labelled with its imputation number and I will also include the original data (imputation = 0) for comparison.\n\n3.3.1 ‚Äúall‚Äù\nUsing ‚Äúall‚Äù produces a list, where each imputed dataframe becomes an element of that list.\n\n\nCode\nimp_all &lt;-  complete(imp, \"all\", include = T)\nimp_all\n\n\n$`0`\n     age  bmi  hyp chl\n1  20-39   NA &lt;NA&gt;  NA\n2  40-59 22.7   no 187\n3  20-39   NA   no 187\n4  60-99   NA &lt;NA&gt;  NA\n5  20-39 20.4   no 113\n6  60-99   NA &lt;NA&gt; 184\n7  20-39 22.5   no 118\n8  20-39 30.1   no 187\n9  40-59 22.0   no 238\n10 40-59   NA &lt;NA&gt;  NA\n11 20-39   NA &lt;NA&gt;  NA\n12 40-59   NA &lt;NA&gt;  NA\n13 60-99 21.7   no 206\n14 40-59 28.7  yes 204\n15 20-39 29.6   no  NA\n16 20-39   NA &lt;NA&gt;  NA\n17 60-99 27.2  yes 284\n18 40-59 26.3  yes 199\n19 20-39 35.3   no 218\n20 60-99 25.5  yes  NA\n21 20-39   NA &lt;NA&gt;  NA\n22 20-39 33.2   no 229\n23 20-39 27.5   no 131\n24 60-99 24.9   no  NA\n25 40-59 27.4   no 186\n\n$`1`\n     age  bmi hyp chl\n1  20-39 30.1 yes 131\n2  40-59 22.7  no 187\n3  20-39 22.0  no 187\n4  60-99 22.7 yes 187\n5  20-39 20.4  no 113\n6  60-99 25.5 yes 184\n7  20-39 22.5  no 118\n8  20-39 30.1  no 187\n9  40-59 22.0  no 238\n10 40-59 27.4 yes 204\n11 20-39 33.2  no 199\n12 40-59 22.7 yes 187\n13 60-99 21.7  no 206\n14 40-59 28.7 yes 204\n15 20-39 29.6  no 199\n16 20-39 27.2  no 238\n17 60-99 27.2 yes 284\n18 40-59 26.3 yes 199\n19 20-39 35.3  no 218\n20 60-99 25.5 yes 131\n21 20-39 22.0  no 131\n22 20-39 33.2  no 229\n23 20-39 27.5  no 131\n24 60-99 24.9  no 131\n25 40-59 27.4  no 186\n\n$`2`\n     age  bmi hyp chl\n1  20-39 22.5  no 187\n2  40-59 22.7  no 187\n3  20-39 25.5  no 187\n4  60-99 27.4 yes 204\n5  20-39 20.4  no 113\n6  60-99 21.7  no 184\n7  20-39 22.5  no 118\n8  20-39 30.1  no 187\n9  40-59 22.0  no 238\n10 40-59 22.7 yes 187\n11 20-39 22.5  no 187\n12 40-59 25.5 yes 184\n13 60-99 21.7  no 206\n14 40-59 28.7 yes 204\n15 20-39 29.6  no 199\n16 20-39 22.7  no 131\n17 60-99 27.2 yes 284\n18 40-59 26.3 yes 199\n19 20-39 35.3  no 218\n20 60-99 25.5 yes 206\n21 20-39 26.3  no 187\n22 20-39 33.2  no 229\n23 20-39 27.5  no 131\n24 60-99 24.9  no 186\n25 40-59 27.4  no 186\n\n$`3`\n     age  bmi hyp chl\n1  20-39 27.2  no 187\n2  40-59 22.7  no 187\n3  20-39 26.3  no 187\n4  60-99 21.7  no 186\n5  20-39 20.4  no 113\n6  60-99 21.7  no 184\n7  20-39 22.5  no 118\n8  20-39 30.1  no 187\n9  40-59 22.0  no 238\n10 40-59 25.5  no 187\n11 20-39 28.7 yes 238\n12 40-59 27.5 yes 206\n13 60-99 21.7  no 206\n14 40-59 28.7 yes 204\n15 20-39 29.6  no 131\n16 20-39 30.1  no 238\n17 60-99 27.2 yes 284\n18 40-59 26.3 yes 199\n19 20-39 35.3  no 218\n20 60-99 25.5 yes 218\n21 20-39 20.4  no 238\n22 20-39 33.2  no 229\n23 20-39 27.5  no 131\n24 60-99 24.9  no 206\n25 40-59 27.4  no 186\n\nattr(,\"class\")\n[1] \"mild\" \"list\"\n\n\n\n\n3.3.2 ‚Äúlong‚Äù\nUsing ‚Äúlong‚Äù produces a dataframe of imputations stacked on top of each other. Imputation and ID numbers are provided as variables to identify which observations belong to which imputations.\n\n\nCode\nimp_long &lt;-  complete(imp, \"long\", include = T)\nimp_long |&gt; tibble() |&gt; print(n = Inf)\n\n\n# A tibble: 100 √ó 6\n     .imp   .id age     bmi hyp     chl\n    &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n  1     0     1 20-39  NA   &lt;NA&gt;     NA\n  2     0     2 40-59  22.7 no      187\n  3     0     3 20-39  NA   no      187\n  4     0     4 60-99  NA   &lt;NA&gt;     NA\n  5     0     5 20-39  20.4 no      113\n  6     0     6 60-99  NA   &lt;NA&gt;    184\n  7     0     7 20-39  22.5 no      118\n  8     0     8 20-39  30.1 no      187\n  9     0     9 40-59  22   no      238\n 10     0    10 40-59  NA   &lt;NA&gt;     NA\n 11     0    11 20-39  NA   &lt;NA&gt;     NA\n 12     0    12 40-59  NA   &lt;NA&gt;     NA\n 13     0    13 60-99  21.7 no      206\n 14     0    14 40-59  28.7 yes     204\n 15     0    15 20-39  29.6 no       NA\n 16     0    16 20-39  NA   &lt;NA&gt;     NA\n 17     0    17 60-99  27.2 yes     284\n 18     0    18 40-59  26.3 yes     199\n 19     0    19 20-39  35.3 no      218\n 20     0    20 60-99  25.5 yes      NA\n 21     0    21 20-39  NA   &lt;NA&gt;     NA\n 22     0    22 20-39  33.2 no      229\n 23     0    23 20-39  27.5 no      131\n 24     0    24 60-99  24.9 no       NA\n 25     0    25 40-59  27.4 no      186\n 26     1     1 20-39  30.1 yes     131\n 27     1     2 40-59  22.7 no      187\n 28     1     3 20-39  22   no      187\n 29     1     4 60-99  22.7 yes     187\n 30     1     5 20-39  20.4 no      113\n 31     1     6 60-99  25.5 yes     184\n 32     1     7 20-39  22.5 no      118\n 33     1     8 20-39  30.1 no      187\n 34     1     9 40-59  22   no      238\n 35     1    10 40-59  27.4 yes     204\n 36     1    11 20-39  33.2 no      199\n 37     1    12 40-59  22.7 yes     187\n 38     1    13 60-99  21.7 no      206\n 39     1    14 40-59  28.7 yes     204\n 40     1    15 20-39  29.6 no      199\n 41     1    16 20-39  27.2 no      238\n 42     1    17 60-99  27.2 yes     284\n 43     1    18 40-59  26.3 yes     199\n 44     1    19 20-39  35.3 no      218\n 45     1    20 60-99  25.5 yes     131\n 46     1    21 20-39  22   no      131\n 47     1    22 20-39  33.2 no      229\n 48     1    23 20-39  27.5 no      131\n 49     1    24 60-99  24.9 no      131\n 50     1    25 40-59  27.4 no      186\n 51     2     1 20-39  22.5 no      187\n 52     2     2 40-59  22.7 no      187\n 53     2     3 20-39  25.5 no      187\n 54     2     4 60-99  27.4 yes     204\n 55     2     5 20-39  20.4 no      113\n 56     2     6 60-99  21.7 no      184\n 57     2     7 20-39  22.5 no      118\n 58     2     8 20-39  30.1 no      187\n 59     2     9 40-59  22   no      238\n 60     2    10 40-59  22.7 yes     187\n 61     2    11 20-39  22.5 no      187\n 62     2    12 40-59  25.5 yes     184\n 63     2    13 60-99  21.7 no      206\n 64     2    14 40-59  28.7 yes     204\n 65     2    15 20-39  29.6 no      199\n 66     2    16 20-39  22.7 no      131\n 67     2    17 60-99  27.2 yes     284\n 68     2    18 40-59  26.3 yes     199\n 69     2    19 20-39  35.3 no      218\n 70     2    20 60-99  25.5 yes     206\n 71     2    21 20-39  26.3 no      187\n 72     2    22 20-39  33.2 no      229\n 73     2    23 20-39  27.5 no      131\n 74     2    24 60-99  24.9 no      186\n 75     2    25 40-59  27.4 no      186\n 76     3     1 20-39  27.2 no      187\n 77     3     2 40-59  22.7 no      187\n 78     3     3 20-39  26.3 no      187\n 79     3     4 60-99  21.7 no      186\n 80     3     5 20-39  20.4 no      113\n 81     3     6 60-99  21.7 no      184\n 82     3     7 20-39  22.5 no      118\n 83     3     8 20-39  30.1 no      187\n 84     3     9 40-59  22   no      238\n 85     3    10 40-59  25.5 no      187\n 86     3    11 20-39  28.7 yes     238\n 87     3    12 40-59  27.5 yes     206\n 88     3    13 60-99  21.7 no      206\n 89     3    14 40-59  28.7 yes     204\n 90     3    15 20-39  29.6 no      131\n 91     3    16 20-39  30.1 no      238\n 92     3    17 60-99  27.2 yes     284\n 93     3    18 40-59  26.3 yes     199\n 94     3    19 20-39  35.3 no      218\n 95     3    20 60-99  25.5 yes     218\n 96     3    21 20-39  20.4 no      238\n 97     3    22 20-39  33.2 no      229\n 98     3    23 20-39  27.5 no      131\n 99     3    24 60-99  24.9 no      206\n100     3    25 40-59  27.4 no      186\n\n\n\n\n3.3.3 ‚Äúbroad‚Äù\nIn contrast, the ‚Äúbroad‚Äù or wide format produces a dataframe of imputations stacked side-by-side. Imputation numbers are included in the variable names.\n\n\nCode\nimp_wide &lt;-  complete(imp, \"broad\", include = T)\n\n\nNew names:\n‚Ä¢ `age` -&gt; `age...1`\n‚Ä¢ `bmi` -&gt; `bmi...2`\n‚Ä¢ `hyp` -&gt; `hyp...3`\n‚Ä¢ `chl` -&gt; `chl...4`\n‚Ä¢ `age` -&gt; `age...5`\n‚Ä¢ `bmi` -&gt; `bmi...6`\n‚Ä¢ `hyp` -&gt; `hyp...7`\n‚Ä¢ `chl` -&gt; `chl...8`\n‚Ä¢ `age` -&gt; `age...9`\n‚Ä¢ `bmi` -&gt; `bmi...10`\n‚Ä¢ `hyp` -&gt; `hyp...11`\n‚Ä¢ `chl` -&gt; `chl...12`\n‚Ä¢ `age` -&gt; `age...13`\n‚Ä¢ `bmi` -&gt; `bmi...14`\n‚Ä¢ `hyp` -&gt; `hyp...15`\n‚Ä¢ `chl` -&gt; `chl...16`\n\n\nCode\nimp_wide |&gt; tibble() |&gt; print(n = Inf)\n\n\n# A tibble: 25 √ó 16\n   age.0 bmi.0 hyp.0 chl.0 age.1 bmi.1 hyp.1 chl.1 age.2 bmi.2 hyp.2 chl.2 age.3\n   &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt;\n 1 20-39  NA   &lt;NA&gt;     NA 20-39  30.1 yes     131 20-39  22.5 no      187 20-39\n 2 40-59  22.7 no      187 40-59  22.7 no      187 40-59  22.7 no      187 40-59\n 3 20-39  NA   no      187 20-39  22   no      187 20-39  25.5 no      187 20-39\n 4 60-99  NA   &lt;NA&gt;     NA 60-99  22.7 yes     187 60-99  27.4 yes     204 60-99\n 5 20-39  20.4 no      113 20-39  20.4 no      113 20-39  20.4 no      113 20-39\n 6 60-99  NA   &lt;NA&gt;    184 60-99  25.5 yes     184 60-99  21.7 no      184 60-99\n 7 20-39  22.5 no      118 20-39  22.5 no      118 20-39  22.5 no      118 20-39\n 8 20-39  30.1 no      187 20-39  30.1 no      187 20-39  30.1 no      187 20-39\n 9 40-59  22   no      238 40-59  22   no      238 40-59  22   no      238 40-59\n10 40-59  NA   &lt;NA&gt;     NA 40-59  27.4 yes     204 40-59  22.7 yes     187 40-59\n11 20-39  NA   &lt;NA&gt;     NA 20-39  33.2 no      199 20-39  22.5 no      187 20-39\n12 40-59  NA   &lt;NA&gt;     NA 40-59  22.7 yes     187 40-59  25.5 yes     184 40-59\n13 60-99  21.7 no      206 60-99  21.7 no      206 60-99  21.7 no      206 60-99\n14 40-59  28.7 yes     204 40-59  28.7 yes     204 40-59  28.7 yes     204 40-59\n15 20-39  29.6 no       NA 20-39  29.6 no      199 20-39  29.6 no      199 20-39\n16 20-39  NA   &lt;NA&gt;     NA 20-39  27.2 no      238 20-39  22.7 no      131 20-39\n17 60-99  27.2 yes     284 60-99  27.2 yes     284 60-99  27.2 yes     284 60-99\n18 40-59  26.3 yes     199 40-59  26.3 yes     199 40-59  26.3 yes     199 40-59\n19 20-39  35.3 no      218 20-39  35.3 no      218 20-39  35.3 no      218 20-39\n20 60-99  25.5 yes      NA 60-99  25.5 yes     131 60-99  25.5 yes     206 60-99\n21 20-39  NA   &lt;NA&gt;     NA 20-39  22   no      131 20-39  26.3 no      187 20-39\n22 20-39  33.2 no      229 20-39  33.2 no      229 20-39  33.2 no      229 20-39\n23 20-39  27.5 no      131 20-39  27.5 no      131 20-39  27.5 no      131 20-39\n24 60-99  24.9 no       NA 60-99  24.9 no      131 60-99  24.9 no      186 60-99\n25 40-59  27.4 no      186 40-59  27.4 no      186 40-59  27.4 no      186 40-59\n# ‚Ñπ 3 more variables: bmi.3 &lt;dbl&gt;, hyp.3 &lt;fct&gt;, chl.3 &lt;dbl&gt;\n\n\n\n\n3.3.4 View just one imputation\nAll good you say, but what if I want to view just one imputation. Well you can do that quite easily in the call to complete(). Here we will ask for just the second imputation.\n\n\nCode\ncomplete(imp, 2) |&gt; tibble() |&gt; print(n = Inf)\n\n\n# A tibble: 25 √ó 4\n   age     bmi hyp     chl\n   &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1 20-39  22.5 no      187\n 2 40-59  22.7 no      187\n 3 20-39  25.5 no      187\n 4 60-99  27.4 yes     204\n 5 20-39  20.4 no      113\n 6 60-99  21.7 no      184\n 7 20-39  22.5 no      118\n 8 20-39  30.1 no      187\n 9 40-59  22   no      238\n10 40-59  22.7 yes     187\n11 20-39  22.5 no      187\n12 40-59  25.5 yes     184\n13 60-99  21.7 no      206\n14 40-59  28.7 yes     204\n15 20-39  29.6 no      199\n16 20-39  22.7 no      131\n17 60-99  27.2 yes     284\n18 40-59  26.3 yes     199\n19 20-39  35.3 no      218\n20 60-99  25.5 yes     206\n21 20-39  26.3 no      187\n22 20-39  33.2 no      229\n23 20-39  27.5 no      131\n24 60-99  24.9 no      186\n25 40-59  27.4 no      186\n\n\n\n\n3.3.5 Compare imputed values\nIf you want to compare the imputed values in a more direct way, we can use the broad format to do that on a variable by variable basis. Here I will ask to look at just bmi.\n\n\nCode\ncomplete(imp, \"broad\", include = T) |&gt; \n  select(contains(\"bmi\")) |&gt; tibble() |&gt; print(n = Inf)\n\n\nNew names:\n‚Ä¢ `age` -&gt; `age...1`\n‚Ä¢ `bmi` -&gt; `bmi...2`\n‚Ä¢ `hyp` -&gt; `hyp...3`\n‚Ä¢ `chl` -&gt; `chl...4`\n‚Ä¢ `age` -&gt; `age...5`\n‚Ä¢ `bmi` -&gt; `bmi...6`\n‚Ä¢ `hyp` -&gt; `hyp...7`\n‚Ä¢ `chl` -&gt; `chl...8`\n‚Ä¢ `age` -&gt; `age...9`\n‚Ä¢ `bmi` -&gt; `bmi...10`\n‚Ä¢ `hyp` -&gt; `hyp...11`\n‚Ä¢ `chl` -&gt; `chl...12`\n‚Ä¢ `age` -&gt; `age...13`\n‚Ä¢ `bmi` -&gt; `bmi...14`\n‚Ä¢ `hyp` -&gt; `hyp...15`\n‚Ä¢ `chl` -&gt; `chl...16`\n\n\n# A tibble: 25 √ó 4\n   bmi.0 bmi.1 bmi.2 bmi.3\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  NA    30.1  22.5  27.2\n 2  22.7  22.7  22.7  22.7\n 3  NA    22    25.5  26.3\n 4  NA    22.7  27.4  21.7\n 5  20.4  20.4  20.4  20.4\n 6  NA    25.5  21.7  21.7\n 7  22.5  22.5  22.5  22.5\n 8  30.1  30.1  30.1  30.1\n 9  22    22    22    22  \n10  NA    27.4  22.7  25.5\n11  NA    33.2  22.5  28.7\n12  NA    22.7  25.5  27.5\n13  21.7  21.7  21.7  21.7\n14  28.7  28.7  28.7  28.7\n15  29.6  29.6  29.6  29.6\n16  NA    27.2  22.7  30.1\n17  27.2  27.2  27.2  27.2\n18  26.3  26.3  26.3  26.3\n19  35.3  35.3  35.3  35.3\n20  25.5  25.5  25.5  25.5\n21  NA    22    26.3  20.4\n22  33.2  33.2  33.2  33.2\n23  27.5  27.5  27.5  27.5\n24  24.9  24.9  24.9  24.9\n25  27.4  27.4  27.4  27.4\n\n\nbmi.0 is the original bmi data and the columns to its right are each of the three imputations for that variable. This provides an easy way to scan across and examine what mice has given us."
  },
  {
    "objectID": "posts/025_21Feb_2025/index.html#with",
    "href": "posts/025_21Feb_2025/index.html#with",
    "title": "Missing Data and Multiple Imputation for Dummies  (Part 2)",
    "section": "3.4 with()",
    "text": "3.4 with()\nNow that we have imputed the data, we can use with() to analyse the data. Generally there is not much need to inspect the output of this step and we usually go straight onto pooling the results, but for the sake of the exercise we will take a closer look at the mira object that is generated. So, let‚Äôs run the regression model that we specified earlier, with bmi as the outcome and age, hyp and chl as predictors.\n\n\nCode\nfit_imp &lt;- with(imp, lm(bmi ~ age + hyp + chl))\nsummary(fit_imp)\n\n\n# A tibble: 15 √ó 6\n   term        estimate std.error statistic    p.value  nobs\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n 1 (Intercept)  20.3       3.22        6.29 0.00000386    25\n 2 age40-59     -4.74      1.97       -2.41 0.0260        25\n 3 age60-99     -5.13      2.08       -2.46 0.0229        25\n 4 hypyes        2.36      1.79        1.32 0.203         25\n 5 chl           0.0420    0.0176      2.39 0.0266        25\n 6 (Intercept)  16.1       3.57        4.51 0.000214      25\n 7 age40-59     -4.11      1.86       -2.22 0.0385        25\n 8 age60-99     -5.11      1.93       -2.64 0.0156        25\n 9 hypyes        2.00      1.80        1.11 0.279         25\n10 chl           0.0602    0.0200      3.01 0.00687       25\n11 (Intercept)  21.8       3.74        5.84 0.0000104     25\n12 age40-59     -3.03      1.77       -1.71 0.103         25\n13 age60-99     -5.19      1.85       -2.81 0.0108        25\n14 hypyes        1.88      1.88        1.00 0.328         25\n15 chl           0.0306    0.0198      1.55 0.138         25\n\n\nSee how we obtain the regression output for each imputed dataset stacked one on top of the other in a single dataframe. This allows us to quickly compare the estimated regression parameters across imputations."
  },
  {
    "objectID": "posts/025_21Feb_2025/index.html#pool",
    "href": "posts/025_21Feb_2025/index.html#pool",
    "title": "Missing Data and Multiple Imputation for Dummies  (Part 2)",
    "section": "3.5 pool()",
    "text": "3.5 pool()\nWe can now call pool() to pool the results of the separate regressions based on Rubin‚Äôs rules.\n\n\nCode\nsummary(pool(fit_imp), conf.int = TRUE)\n\n\n# A tibble: 5 √ó 8\n  term        estimate std.error statistic    df p.value `2.5 %` `97.5 %`\n  &lt;fct&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  19.4       4.90        3.96  4.48  0.0134  6.35     32.4  \n2 age40-59     -3.96      2.12       -1.87 10.5   0.0895 -8.65      0.728\n3 age60-99     -5.14      1.96       -2.63 18.2   0.0169 -9.25     -1.04 \n4 hypyes        2.08      1.84        1.13 17.7   0.274  -1.80      5.95 \n5 chl           0.0443    0.0258      1.72  5.01  0.146  -0.0219    0.110\n\n\nNote that there is also a ‚Äútidy‚Äù way to format this output directly, using the tbl_regression() function, which provides for much nicer viewing.\n\n\nCode\nfit_imp |&gt; \n  tbl_regression() |&gt; \n  add_glance_source_note()\n\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    age\n\n\n\n    ¬†¬†¬†¬†20-39\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†40-59\n-4.0\n-8.7, 0.73\n0.090\n    ¬†¬†¬†¬†60-99\n-5.1\n-9.3, -1.0\n0.017\n    hyp\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n2.1\n-1.8, 6.0\n0.3\n    chl\n0.04\n-0.02, 0.11\n0.15\n  \n  \n    \n      nimp = 3.00; No. Obs. = 25; R¬≤ = 0.364; Adjusted R¬≤ = 0.236\n    \n  \n  \n    \n      1 CI = Confidence Interval"
  },
  {
    "objectID": "posts/025_21Feb_2025/index.html#compare-with-cca",
    "href": "posts/025_21Feb_2025/index.html#compare-with-cca",
    "title": "Missing Data and Multiple Imputation for Dummies  (Part 2)",
    "section": "3.6 Compare with CCA",
    "text": "3.6 Compare with CCA\nI will re-print the model output from the CCA so that we can compare it to the MI results:\n\n\nCode\nlm(bmi ~ age + hyp + chl, data = nhanes2) |&gt; \n  tbl_regression() |&gt; \n  add_glance_source_note()\n\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    age\n\n\n\n    ¬†¬†¬†¬†20-39\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†40-59\n-6.6\n-12, -1.3\n0.021\n    ¬†¬†¬†¬†60-99\n-11\n-19, -3.4\n0.010\n    hyp\n\n\n\n    ¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n    ¬†¬†¬†¬†yes\n2.4\n-3.5, 8.2\n0.4\n    chl\n0.08\n0.02, 0.14\n0.012\n  \n  \n    \n      R¬≤ = 0.670; Adjusted R¬≤ = 0.505; Sigma = 3.23; Statistic = 4.06; p-value = 0.044; df = 4; Log-likelihood = -30.6; AIC = 73.1; BIC = 76.5; Deviance = 83.7; Residual df = 8; No. Obs. = 13\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nThere are certainly some differences in the regression coefficients - most notable for age. Interestingly age had no missingness in the original data, so MI is likely facilitating more accurate estimation of the age parameters given we are able to utilise all of the available data by not throwing almost half of it away (as in the CCA). Obviously we cannot say with certainty what the true population parameters are, as these are not simulated data, but it would be reasonable to assume that MI provides a more valid (less biased) analysis."
  },
  {
    "objectID": "posts/026_07Mar_2025/index.html",
    "href": "posts/026_07Mar_2025/index.html",
    "title": "janitor - Your local R handypackage",
    "section": "",
    "text": "A (fairly) short post today. I want to introduce you to a great little R package that I load in every one of my scripts, because I never know when I might need to use it. janitor contains several functions, but there are two primary ones that I want to draw your attention to - clean_names and tabyl."
  },
  {
    "objectID": "posts/026_07Mar_2025/index.html#one-way-tabulation",
    "href": "posts/026_07Mar_2025/index.html#one-way-tabulation",
    "title": "janitor - Your local R handypackage",
    "section": "2.1 One-way Tabulation",
    "text": "2.1 One-way Tabulation\nBase R‚Äôs tabulation of eye-colour gives:\n\n\nCode\n# Load data\nhumans &lt;- starwars |&gt; \n  filter(species == \"Human\")\n# Base R table\ntable(humans$eye_color)\n\n\n\n     blue blue-gray     brown      dark     hazel   unknown    yellow \n       12         1        16         1         2         1         2 \n\n\nYou can imagine that with a factor that contains more categories, the listing starts to spill out over another row (sometimes more) in your console. It‚Äôs just not pleasant to quickly look at and interpret. In comparison, in its most basic call, tabyl() gives:\n\n\nCode\nhumans |&gt; \n  tabyl(eye_color, show_na = T)\n\n\n# A tibble: 7 √ó 3\n  eye_color     n percent\n  &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt;\n1 blue         12  0.343 \n2 blue-gray     1  0.0286\n3 brown        16  0.457 \n4 dark          1  0.0286\n5 hazel         2  0.0571\n6 unknown       1  0.0286\n7 yellow        2  0.0571\n\n\nIt prints in a much more readable column format, and provides percentages, as well as frequencies, by default.\nTo save keystrokes, I have written a function for myself with a few further embellishments - marginal totals as well as makeshift borders (note you will need to have installed the knitr package if you want to use this). My aim with this and the subsequent function I am going to share with you was to emulate Stata‚Äôs tabulate command as much as possible.\n\n\nCode\ntab1 &lt;- function(df = dat, var, show.miss = T) {\n  df |&gt; \n    tabyl({{var}}, show_na = show.miss) |&gt; \n    adorn_totals(\"row\") |&gt; \n    adorn_pct_formatting(rounding = \"half up\", digits = 2) |&gt; \n    knitr::kable()\n}\n\n\nThen, with a simple tab1(humans, eye_color, show.miss = T) I can get:\n\n\nCode\ntab1(humans, eye_color, T)\n\n\n\n\n\n\n\nThe show.miss = T will include any missing data as a separate category. You can set this to F if you prefer."
  },
  {
    "objectID": "posts/026_07Mar_2025/index.html#two-way-tabulation",
    "href": "posts/026_07Mar_2025/index.html#two-way-tabulation",
    "title": "janitor - Your local R handypackage",
    "section": "2.2 Two-way Tabulation",
    "text": "2.2 Two-way Tabulation\nLet‚Äôs extend the example further by now cross-tabulating eye-colour with gender. The Base R version gives:\n\n\nCode\ntable(humans$eye_color, humans$gender)\n\n\n           \n            feminine masculine\n  blue             3         9\n  blue-gray        0         1\n  brown            4        12\n  dark             0         1\n  hazel            1         1\n  unknown          1         0\n  yellow           0         2\n\n\nThis is actually more readable than the one-way tabulation. In comparison, tabyl() produces:\n\n\nCode\nhumans |&gt;\n  tabyl(eye_color, gender, show_na = T)\n\n\n# A tibble: 7 √ó 3\n  eye_color feminine masculine\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 blue             3         9\n2 blue-gray        0         1\n3 brown            4        12\n4 dark             0         1\n5 hazel            1         1\n6 unknown          1         0\n7 yellow           0         2\n\n\nIn its default state, not that different I agree. However, to save some time, and also include a couple of useful additional summaries, I have written myself a function for cross-tabulations as well. Again, I have customised this to my liking, by including marginal totals and bordering. Note that as I have specified row percentages, the marginal row totals all add up to 100%.\n\n\nCode\ntab2 &lt;- function(df = dat, var1, var2, show.miss = T) {\n  df |&gt;\n    tabyl({{var1}}, {{var2}}, show_na = show.miss) |&gt;\n    adorn_totals(\"row\") |&gt;\n    adorn_totals(\"col\") |&gt;\n    adorn_percentages(\"row\") |&gt;\n    adorn_pct_formatting(rounding = \"half up\", digits = 2) |&gt;\n    adorn_ns() |&gt;\n    knitr::kable()\n}\n\n\nNow I simply type tab2(humans, eye_color, gender, show.miss = T) to get:\n\n\nCode\ntab2(humans, eye_color, gender, show.miss = T)\n\n\n\n\n\n\n\nI better stop there. How is it that my ‚Äòshort‚Äô posts even somehow become long? Until next time‚Ä¶"
  },
  {
    "objectID": "posts/027_21Mar_2025/index.html",
    "href": "posts/027_21Mar_2025/index.html",
    "title": "Model Prediction/Visualisation  1. The Basics",
    "section": "",
    "text": "Most of you will be familiar with how to create and run a statistical model in R. But what do you then do with your model results? I bet you take R‚Äôs rather bland console output, convert that to a formatted table in your manuscript, then based on the regression coefficient values, go on and talk about associations of your exposure/s with your outcome.\nAnd leave it at that.\nWell, the old adage of ‚Äúa picture is worth a thousand words‚Äù has never been truer in the context of model prediction and how much additional information value you can obtain if you take your analyses a step further. Our reliance on just looking at the numbers (i.e.¬†the regression coefficients) is sufficient a lot of the time in simple cases, but when models become a more complex - for example, by the specification of non-linear covariate functional forms via splines - regression coefficients on their own are essentially useless in aiding our interpretation. Plotting the model predictions, however, can put us back on track for understanding the important relationships in our data. Even a complex model can make more sense when we can visualise its predictions. These are skills worth learning and adding to your data analyst toolkit.\nSo, I thought I would put together a series of posts on predicting from and visualising your model, where in each post we cover a different area related to the subject. This week it‚Äôs just about the basics of model prediction and visualisation, but in future posts I hope to extend those ideas to more advanced topics including non-linear functional forms, glm‚Äôs, mixed-models, etc (let‚Äôs see how we go). But first, let‚Äôs start at the start‚Ä¶"
  },
  {
    "objectID": "posts/027_21Mar_2025/index.html#birthweight-as-a-function-of-maternal-age",
    "href": "posts/027_21Mar_2025/index.html#birthweight-as-a-function-of-maternal-age",
    "title": "Model Prediction/Visualisation  1. The Basics",
    "section": "2.1 Birthweight as a Function of Maternal Age",
    "text": "2.1 Birthweight as a Function of Maternal Age\nLet‚Äôs initially consider the possible association between baby birthweight (bwt - outcome) and mother‚Äôs age (exposure). We will run the model as follows:\n\n\nCode\nmod &lt;-  lm(bwt ~ age, data = birthwt)\ntbl_regression(mod, intercept = T)\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n2,656\n2,185, 3,127\n&lt;0.001\n\n\nage\n12\n-7.3, 32\n0.2\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nThe model seems to suggest that increasing maternal age is associated with increasing birthweight. Keep in mind that this is unlikely to represent reality and I would guess that the relationship between maternal age and birthweight is non-linear (i.e.¬†at some maternal age, birthweight is more likely to start decreasing with increasing maternal age). But for the sake of today‚Äôs exercise let‚Äôs just assume the relationship is linear.\n\n2.1.1 Predict\nCalculating predictions from the model is as simple as using the predict() function following your model call, adding the predictions as a new column to the original dataframe (or creating a new dataframe if you desire). Here I will just add the new column containing the predictions to a new dataframe containing only the relevant variables, and print the first 10 rows:\n\n\nCode\nbirthwt2 &lt;-  birthwt |&gt; \n  select(bwt, age) |&gt; \n  mutate(pred = predict(mod))\nhead(birthwt2, 10) |&gt; \n  kable(align = \"c\", digits = 1)\n\n\n\n\n\n\nbwt\nage\npred\n\n\n\n\n85\n2523\n19\n2891.9\n\n\n86\n2551\n33\n3065.9\n\n\n87\n2557\n20\n2904.3\n\n\n88\n2594\n21\n2916.8\n\n\n89\n2600\n18\n2879.5\n\n\n91\n2622\n21\n2916.8\n\n\n92\n2637\n22\n2929.2\n\n\n93\n2637\n17\n2867.0\n\n\n94\n2663\n29\n3016.2\n\n\n95\n2665\n26\n2978.9\n\n\n\n\n\n\n\n2.1.2 Visualise\nPlotting the model predictions is simple using ggplot2.\n\n\nCode\nggplot(birthwt2, aes(x = age, y = pred)) +\n  geom_line(linewidth = 1)\n\n\n\n\n\n\n\n\n\nIt can be useful to add in the raw datapoints, so let‚Äôs do that, and I‚Äôll add a couple of plotting embellishments as well.\n\n\nCode\nggplot(data = birthwt2, aes(x = age, y = pred)) +\n  geom_line(linewidth = 1) +\n  geom_point(data = birthwt2, aes(x = age, y = bwt)) +\n  xlab(\"Maternal Age\") + ylab(\"Predicted Birthweight\") +\n  theme_bw(base_size = 20)\n\n\n\n\n\n\n\n\n\nGreat! We‚Äôve generated predictions from our model and visualised the results. That wasn‚Äôt too hard."
  },
  {
    "objectID": "posts/027_21Mar_2025/index.html#birthweight-as-a-function-of-maternal-age-and-smoking",
    "href": "posts/027_21Mar_2025/index.html#birthweight-as-a-function-of-maternal-age-and-smoking",
    "title": "Model Prediction/Visualisation  1. The Basics",
    "section": "2.2 Birthweight as a Function of Maternal Age and Smoking",
    "text": "2.2 Birthweight as a Function of Maternal Age and Smoking\nLet‚Äôs take it up a step by now including maternal smoking (smoke) as another exposure in our model:\n\n\nCode\nmod2 &lt;-  lm(bwt ~ age + smoke, data = birthwt)\ntbl_regression(mod2, intercept = T)\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n2,791\n2,316, 3,267\n&lt;0.001\n\n\nage\n11\n-8.2, 31\n0.3\n\n\nsmoke\n-278\n-489, -67\n0.010\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nSo, the coefficient for age hasn‚Äôt really changed that much, but we can see that smoking seems to have a significant impact, decreasing birthweight on average by 278 grams.\nLet‚Äôs predict birthweight from this model as we did before:\n\n\nCode\nbirthwt2 &lt;-  birthwt |&gt; \n  select(bwt, age, smoke) |&gt; \n  mutate(pred = predict(mod2))\nhead(birthwt2, 10) |&gt; \n  kable(align = \"c\", digits = 1)\n\n\n\n\n\n\nbwt\nage\nsmoke\npred\n\n\n\n\n85\n2523\n19\n0\n3005.7\n\n\n86\n2551\n33\n0\n3163.8\n\n\n87\n2557\n20\n1\n2738.7\n\n\n88\n2594\n21\n1\n2749.9\n\n\n89\n2600\n18\n1\n2716.1\n\n\n91\n2622\n21\n0\n3028.3\n\n\n92\n2637\n22\n0\n3039.6\n\n\n93\n2637\n17\n0\n2983.1\n\n\n94\n2663\n29\n1\n2840.3\n\n\n95\n2665\n26\n1\n2806.4\n\n\n\n\n\nAnd plot the predictions as we did before:\n\n\nCode\nggplot(data = birthwt2, aes(x = age, y = pred)) +\n  geom_line(linewidth = 1) +\n  geom_point(data = birthwt2, aes(x = age, y = bwt)) +\n  xlab(\"Maternal Age\") + ylab(\"Predicted Birthweight\") +\n  theme_bw(base_size = 20)\n\n\n\n\n\n\n\n\n\nUh-oh! Something has obviously gone badly wrong.\nThe problem is that we have predicted on the original data and the predictions are the result of the combination of each woman‚Äôs unique values of both maternal age and smoking status. In other words, we are not just plotting the predicted value of birthweight as a function of maternal age alone, but rather as a function of both maternal age and smoking status. We are then trying to visualise those predictions as if they were generated from a function of maternal age alone.\nAs you can then see - ‚ÄúComputer says no‚Äù.\nTo properly visualise predictions when more than one predictor are involved, requires that we find a way to generate the adjusted predictions I mentioned earlier. And, of course, there‚Äôs a fairly easy way to do that in R.\n\n2.2.1 Predict\nWhen you have more than one covariate in your model, predicting on new data will allow you to generate the adjusted predictions that facilitate sensible visualisation. The predict() function actually contains a newdata argument that you can point towards a ‚Äògrid‚Äô of specified covariate values to predict on. This ‚Äògrid‚Äô is essentially a new dataframe that contains all possible combinations of covariates at salient values (typically a range of values at set intervals for numeric variables and every category/level for factor variables). In this way (adjusted) predictions can be made for one variable while keeping the other variable/s at fixed values.\nTo construct the grid of new data, we will use the expand.grid() function. I will specify a maternal age range from 15 to 45 in 5 year intervals and a smoking status variable equal to either 0 or 1. This grid of new data (which I have called newdf) looks like:\n\n\nCode\nnewdf &lt;-  expand.grid(age = seq(15, 45, by = 5),\n                      smoke = c(0,1))\nnewdf |&gt; \n  kable(align = \"c\", digits = 1)\n\n\n\n\n\nage\nsmoke\n\n\n\n\n15\n0\n\n\n20\n0\n\n\n25\n0\n\n\n30\n0\n\n\n35\n0\n\n\n40\n0\n\n\n45\n0\n\n\n15\n1\n\n\n20\n1\n\n\n25\n1\n\n\n30\n1\n\n\n35\n1\n\n\n40\n1\n\n\n45\n1\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote two conditions in generating your grid of new data:\n1. You MUST include every variable that is specified in your model;\n2. The variable names MUST match the names of the corresponding variables in your original dataset.\nFailing these, predict() will become upset with you and refuse to do any more work.\n\n\nNow we are in a position to generate the predictions that we need. Nothing changes compared to before, except we now specify a new dataframe to predict upon using the newdata argument:\n\n\nCode\nnewdf &lt;-  newdf |&gt; \n  mutate(pred = predict(mod2, newdata = newdf))\nnewdf |&gt; \n  kable(align = \"c\", digits = 1)\n\n\n\n\n\nage\nsmoke\npred\n\n\n\n\n15\n0\n2960.6\n\n\n20\n0\n3017.0\n\n\n25\n0\n3073.5\n\n\n30\n0\n3129.9\n\n\n35\n0\n3186.4\n\n\n40\n0\n3242.8\n\n\n45\n0\n3299.3\n\n\n15\n1\n2682.2\n\n\n20\n1\n2738.7\n\n\n25\n1\n2795.1\n\n\n30\n1\n2851.6\n\n\n35\n1\n2908.0\n\n\n40\n1\n2964.5\n\n\n45\n1\n3020.9\n\n\n\n\n\n\n\n2.2.2 Visualise\nThe key when we come to plotting the predictions is to now realise that because we are representing age on the x-axis, we need to use some other means to represent smoking status. In this case we can do that quite easily by using the colour aesthetic in our ggplot call:\n\n\nCode\nggplot(data = newdf, aes(x = age, y = pred, color = factor(smoke))) +\n  geom_line(linewidth = 1) +\n  geom_point(data = birthwt2, aes(x = age, y = bwt), size = 2) +\n  xlab(\"Maternal Age\") + ylab(\"Predicted Birthweight\") + labs(color = \"Smoking Status\") +\n    scale_y_continuous(limits = c(0, 5000), breaks = seq(0, 5000, by = 1000)) +\n  theme_bw(base_size = 20) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nNote that I converted smoking status to a factor variable on the fly so that ggplot knows that it only has two levels (otherwise it assumes it‚Äôs a continuous variable).\nSo, what do we actually have here, when I call this a plot of adjusted predictions? We have two parallel regression lines - one for smokers (blue) and one for non-smokers (red). They are parallel because ‚Äòeffects‚Äô are additive - we did not specify an interaction between maternal age and smoking status. The red line is the predicted birthweight as a function of maternal age for non-smoking mothers and the blue line is the predicted birthweight as a function of maternal age for smoking mothers. In other words, predictions of the association between maternal age and newborn birthweight, adjusted for maternal smoking status (we can flip this around and also say that these are predictions of the association between smoking status and newborn birthweight, adjusted for maternal age). The regression line/s no longer zig-zag, like they did in the earlier plot, because we have separated out the ‚Äòeffect‚Äô of the second exposure from the first allowing a ‚Äòpure‚Äô association between maternal age and newborn birthweight at each level of smoking status to be observed.\nThese same principles of prediction and visualisation can be applied in nearly all model plotting endeavours. There does, however, become a limit in the number of covariates that can practically be visualised and that is something we will look at in more detail in the next post.\nI will conclude by saying that for this series I am going to illustrate all concepts using what I consider to be ‚Äòfirst principles‚Äô - that is, applying the predict() and expand.grid() functions for predicting, and ggplot2 for visualising. But there may be better/faster ways to do this and a package I came across recently that I think would be worthwhile exploring is ggeffects. I haven‚Äôt used this myself but a quick scan of the package website gives me the impression this will make even shorter work of my ‚Äòfirst principles‚Äô approach. It is a package that I will keep in mind as we go forward on this topic.\nCatch you in a couple of weeks‚Ä¶"
  },
  {
    "objectID": "posts/028_04Apr_2025/index.html",
    "href": "posts/028_04Apr_2025/index.html",
    "title": "Model Prediction/Visualisation  2. How Many Dimensions?",
    "section": "",
    "text": "Last time I introduced you to the idea of model prediction and visualisation and hopefully equipped you with the both the know-how and tools to attempt this on your own. Before we move on to more advanced concepts in this space though, it‚Äôs important that we clear up a pragmatic issue that will strike you at some point in your model visualisation travels. And that issue is about how many predictors one can visualise in a sensible way.\nOK you say, but I don‚Äôt really understand what you mean.\nWell think about it what we are attempting to do in model visualisation in terms of ‚Äòdimensions‚Äô. While we physically inhabit a three-dimensional where we can appreciate height, length and width (depth), our data-analysis world is largely confined to just the first two of those dimensions. Putting aside advances in virtual reality technology, when we work with our data it is typically on a standard computer screen which is an inherently two-dimensional surface (yes, I accept our computer will allow simulated three-dimensional viewing - more on that in a moment). The point is that we fast run out of dimension-viewing capacity. Let‚Äôs consider what we can effectively view as the number of predictors increases. Keep in mind that the dependent variable in our model is already using up one dimension (height - assigned to the y-axis), so if our model includes:\n\nOne predictor. The regression solution is a line of best fit.\nConsequently, this is simple to visualise. Our independent variable is assigned to the x-axis (length). But note - we have now already used up our two dimensions!\nTwo predictors. The regression solution is a plane of best fit.\nWell, this is immediately more challenging. But it is doable. Here we can leverage our computer‚Äôs graphical capabilities to reproduce depth and plot our predictions in a simulated three-dimensional environment residing on a two-dimensional surface. We keep our first independent variable as is and assign our second independent variable to the z-axis. There are packages in R that provide this kind of flexibility.\nThree predictors (or more). The regression solution is a hyperplane of best fit.\nA What?! It doesn‚Äôt matter - you can‚Äôt practically visualise it.\n\nIt‚Äôs not all doom and gloom, however. There are alternative ways around this apparent roadblock - at least to be able to visualise three, four and even five predictors (beyond that I have no idea). An effective approach that I alluded to in the last post, and that will be the focus of the remainder of this one is to think in terms of ‚Äòcollapsing‚Äô each of multiple dimensions into two-dimensional surfaces. In this way we can still view a multi-dimensional space on our height/length limited computer screen. Let‚Äôs look at some examples now - we‚Äôll use the same birthwt dataset as in the last post to illustrate these.\n\n1 Two Predictors\nWe have essentially already made this plot in the last post. In our model we are regressing newborn birthweight on maternal age and smoking status, and if you recall we managed the visualisation of our second predictor (smoking status) by assigning it to the colour aesthetic in ggplot().\nA couple of notes: In this and all subsequent models I will specify interaction terms between all predictors just to make the output a little more interesting (otherwise you will only see parallel regression lines). Don‚Äôt get hung up on this - maintain focus on the plotting devices that I will discuss. I have also converted categorical variables to factors in advance, rather than on the fly (as in the last post). This just makes some aspects of the plotting (especially labelling) easier.\nSo, our model is:\nmod2 &lt;-  lm(bwt ~ age * smoke, data = birthwt)\n\n\nCode\nlibrary(tidyverse)\ndata(birthwt, package = \"MASS\")\n# Relabel smoking\nbirthwt$smoke &lt;-  factor(birthwt$smoke, levels = c(0,1), labels = c(\"No\", \"Yes\"))\n# Model\nmod2 &lt;-  lm(bwt ~ age * smoke, data = birthwt)\n# Create new data\nnewdf &lt;-  expand.grid(age = seq(15, 45, by = 5),\n                      smoke = levels(birthwt$smoke))\n# Predict\nnewdf &lt;-  newdf |&gt; \n  mutate(pred = predict(mod2, newdata = newdf))\n# Visualise\nggplot(data = newdf, aes(x = age, y = pred, color = smoke)) +\n  geom_line(linewidth = 1) +\n  xlab(\"Maternal Age\") + ylab(\"Predicted Birthweight\") + labs(color = \"Smoking Status\") +\n  scale_y_continuous(limits = c(0, 5000), breaks = seq(0, 5000, by = 1000)) +\n  theme_bw(base_size = 20) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n2 Three Predictors\nLet‚Äôs now include mother‚Äôs race as a third predictor in the model:\nmod3 &lt;-  lm(bwt ~ age * smoke * race, data = birthwt)\nTo get over this hurdle, we can take advantage of ggplot()‚Äôs facetting functionality to create separate side-by-side plots for each mother‚Äôs race. And that could look something like:\n\n\nCode\n# Relabel race\nbirthwt$race &lt;-  factor(birthwt$race, levels = c(1,2,3), labels = c(\"White\", \"Black\", \"Other\"))\n# Model\nmod3 &lt;-  lm(bwt ~ age * smoke * race, data = birthwt)\n# Create new data\nnewdf &lt;-  expand.grid(age = seq(15, 45, by = 5),\n                      smoke = levels(birthwt$smoke),\n                      race = levels(birthwt$race))\n# Predict\nnewdf &lt;-  newdf |&gt; \n  mutate(pred = predict(mod3, newdata = newdf))\n# Create labels for plotting race\nlabel_race &lt;- c(\"Race: White\", \"Race: Black\", \"Race: Other\")\nnames(label_race) &lt;- c(\"White\", \"Black\", \"Other\")\n# Visualise\nggplot(data = newdf, aes(x = age, y = pred, color = smoke)) +\n  geom_line(linewidth = 1) +\n  xlab(\"Maternal Age\") + ylab(\"Predicted Birthweight\") + labs(color = \"Smoking Status\") +\n  scale_y_continuous(limits = c(0, 5000), breaks = seq(0, 5000, by = 1000)) +\n  facet_grid(~ race, labeller = labeller(race = label_race)) +\n  theme_bw(base_size = 20) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n3 Four Predictors\nOk, let‚Äôs add in presence of uterine irritability as a fourth predictor.\nmod4 &lt;-  lm(bwt ~ age * smoke * race * ui, data = birthwt)\nNow we just facet vertically in addition to horizontally:\n\n\nCode\n# Relabel uterine irritability\nbirthwt$ui &lt;-  factor(birthwt$ui, levels = c(0, 1), labels = c(\"No\", \"Yes\"))\n# Model\nmod4 &lt;-  lm(bwt ~ age * smoke * race * ui, data = birthwt)\n# Create new data\nnewdf &lt;-  expand.grid(age = seq(15, 45, by = 5),\n                      smoke = levels(birthwt$smoke),\n                      race = levels(birthwt$race),\n                      ui = levels(birthwt$ui))\n# Predict\nnewdf &lt;-  newdf |&gt; \n  mutate(pred = predict(mod4, newdata = newdf))\n# Create labels for plotting ui\nlabel_ui &lt;- c(\"Uterine Irritability: No\", \"Uterine Irritability: Yes\")\nnames(label_ui) &lt;- c(\"No\", \"Yes\")\n# Visualise\nggplot(data = newdf, aes(x = age, y = pred, color = smoke)) +\n  geom_line(linewidth = 1) +\n  xlab(\"Maternal Age\") + ylab(\"Predicted Birthweight\") + labs(color = \"Smoking Status\") +\n  scale_y_continuous(limits = c(0, 5000), breaks = seq(0, 5000, by = 1000)) +\n  facet_grid(ui ~ race, labeller = labeller(race = label_race, ui = label_ui)) +\n  theme_bw(base_size = 20) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n4 Five Predictors\nNow we‚Äôre getting to the limits of what I think is possible. And honestly, I think even this is potentially too much in terms information overload - there is a fine line between visualisations that are pleasantly informative and those that are just confusing. But, for the sake of the exercise, this is one way that I‚Äôm aware of that you could incorporate the plotting of predictions from a model with five predictors. In this case, we specify a ‚Äògraphical interaction‚Äô between two variables (here I have selected smoking status and history of hypertension), allowing those predictions to be distinguishable by not only color, but by specifying a second plotting aesthetic - linetype. Adjusted predictions for birthweight conditional on no history of hypertension are assigned a solid line and adjusted predictions for birthweight conditional on a history of hypertension are assigned a dashed line.\n\n\nCode\n# Relabel history of hypertension\nbirthwt$ht &lt;-  factor(birthwt$ht, levels = c(0, 1), labels = c(\"No\", \"Yes\"))\n# Model\nmod5 &lt;-  lm(bwt ~ age * smoke * race * ui * ht, data = birthwt)\n# Create new data\nnewdf &lt;-  expand.grid(age = seq(15, 45, by = 5),\n                      smoke = levels(birthwt$smoke),\n                      race = levels(birthwt$race),\n                      ui = levels(birthwt$ui),\n                      ht = levels(birthwt$ht))\n# Predict\nnewdf &lt;-  newdf |&gt; \n  mutate(pred = predict(mod5, newdata = newdf))\n# Visualise\nggplot(data = newdf, aes(x = age, y = pred, group = interaction(smoke, ht), color = smoke, linetype = ht)) +\n  geom_line(linewidth = 1) +\n  xlab(\"Maternal Age\") + ylab(\"Predicted Birthweight\") + labs(color = \"Smoking Status\", linetype = \"History of Hypertension\") +\n  scale_y_continuous(limits = c(0, 5000), breaks = seq(0, 5000, by = 1000)) +\n  facet_grid(ui ~ race, labeller = labeller(race = label_race, ui = label_ui)) +\n  theme_bw(base_size = 20) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n(Note that some regression lines appear to be missing because we have just run out of data for that particular combination of covariate values).\n\n\n5 ggeffects\nRemember how I mentioned the ggeffects package in the last post? Well now may be a good time to visit that packages website. While everything I have shown you in this post is the result of combined base R and ggplot2 functionality, there are prediction and visualisation options in ggeffects that you may find more intuitive and/or easier to use. Especially for the five-predictor case, ggeffects essentially allows nested facetting, which is an interesting proposition and arguably more pleasing to the brain than what I have shown you above.\nUntil next time‚Ä¶"
  },
  {
    "objectID": "posts/028_04Apr_2025/index.html#two-predictors",
    "href": "posts/028_04Apr_2025/index.html#two-predictors",
    "title": "Model Prediction/Visualisation  2. How Many Dimensions?",
    "section": "1 Two Predictors",
    "text": "1 Two Predictors\nWe have essentially already made this plot in the last post. In our model we are regressing newborn birthweight on maternal age and smoking status, and if you recall we managed the visualisation of our second predictor (smoking status) by assigning it to the colour aesthetic in ggplot().\nA couple of notes: In this and all subsequent models I will specify interaction terms between all predictors just to make the output a little more interesting (otherwise you will only see parallel regression lines). Don‚Äôt get hung up on this - maintain focus on the plotting devices that I will discuss. I have also converted categorical variables to factors in advance, rather than on the fly (as in the last post). This just makes some aspects of the plotting (especially labelling) easier.\nSo, our model is:\nmod2 &lt;-  lm(bwt ~ age * smoke, data = birthwt)\n\n\nCode\nlibrary(tidyverse)\ndata(birthwt, package = \"MASS\")\n# Relabel smoking\nbirthwt$smoke &lt;-  factor(birthwt$smoke, levels = c(0,1), labels = c(\"No\", \"Yes\"))\n# Model\nmod2 &lt;-  lm(bwt ~ age * smoke, data = birthwt)\n# Create new data\nnewdf &lt;-  expand.grid(age = seq(15, 45, by = 5),\n                      smoke = levels(birthwt$smoke))\n# Predict\nnewdf &lt;-  newdf |&gt; \n  mutate(pred = predict(mod2, newdata = newdf))\n# Visualise\nggplot(data = newdf, aes(x = age, y = pred, color = smoke)) +\n  geom_line(linewidth = 1) +\n  xlab(\"Maternal Age\") + ylab(\"Predicted Birthweight\") + labs(color = \"Smoking Status\") +\n  scale_y_continuous(limits = c(0, 5000), breaks = seq(0, 5000, by = 1000)) +\n  theme_bw(base_size = 20) +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/028_04Apr_2025/index.html#three-predictors",
    "href": "posts/028_04Apr_2025/index.html#three-predictors",
    "title": "Model Prediction/Visualisation  2. How Many Dimensions?",
    "section": "2 Three Predictors",
    "text": "2 Three Predictors\nLet‚Äôs now include mother‚Äôs race as a third predictor in the model:\nmod3 &lt;-  lm(bwt ~ age * smoke * race, data = birthwt)\nTo get over this hurdle, we can take advantage of ggplot()‚Äôs facetting functionality to create separate side-by-side plots for each mother‚Äôs race. And that could look something like:\n\n\nCode\n# Relabel race\nbirthwt$race &lt;-  factor(birthwt$race, levels = c(1,2,3), labels = c(\"White\", \"Black\", \"Other\"))\n# Model\nmod3 &lt;-  lm(bwt ~ age * smoke * race, data = birthwt)\n# Create new data\nnewdf &lt;-  expand.grid(age = seq(15, 45, by = 5),\n                      smoke = levels(birthwt$smoke),\n                      race = levels(birthwt$race))\n# Predict\nnewdf &lt;-  newdf |&gt; \n  mutate(pred = predict(mod3, newdata = newdf))\n# Create labels for plotting race\nlabel_race &lt;- c(\"Race: White\", \"Race: Black\", \"Race: Other\")\nnames(label_race) &lt;- c(\"White\", \"Black\", \"Other\")\n# Visualise\nggplot(data = newdf, aes(x = age, y = pred, color = smoke)) +\n  geom_line(linewidth = 1) +\n  xlab(\"Maternal Age\") + ylab(\"Predicted Birthweight\") + labs(color = \"Smoking Status\") +\n  scale_y_continuous(limits = c(0, 5000), breaks = seq(0, 5000, by = 1000)) +\n  facet_grid(~ race, labeller = labeller(race = label_race)) +\n  theme_bw(base_size = 20) +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/028_04Apr_2025/index.html#four-predictors",
    "href": "posts/028_04Apr_2025/index.html#four-predictors",
    "title": "Model Prediction/Visualisation  2. How Many Dimensions?",
    "section": "3 Four Predictors",
    "text": "3 Four Predictors\nOk, let‚Äôs add in presence of uterine irritability as a fourth predictor.\nmod4 &lt;-  lm(bwt ~ age * smoke * race * ui, data = birthwt)\nNow we just facet vertically in addition to horizontally:\n\n\nCode\n# Relabel uterine irritability\nbirthwt$ui &lt;-  factor(birthwt$ui, levels = c(0, 1), labels = c(\"No\", \"Yes\"))\n# Model\nmod4 &lt;-  lm(bwt ~ age * smoke * race * ui, data = birthwt)\n# Create new data\nnewdf &lt;-  expand.grid(age = seq(15, 45, by = 5),\n                      smoke = levels(birthwt$smoke),\n                      race = levels(birthwt$race),\n                      ui = levels(birthwt$ui))\n# Predict\nnewdf &lt;-  newdf |&gt; \n  mutate(pred = predict(mod4, newdata = newdf))\n# Create labels for plotting ui\nlabel_ui &lt;- c(\"Uterine Irritability: No\", \"Uterine Irritability: Yes\")\nnames(label_ui) &lt;- c(\"No\", \"Yes\")\n# Visualise\nggplot(data = newdf, aes(x = age, y = pred, color = smoke)) +\n  geom_line(linewidth = 1) +\n  xlab(\"Maternal Age\") + ylab(\"Predicted Birthweight\") + labs(color = \"Smoking Status\") +\n  scale_y_continuous(limits = c(0, 5000), breaks = seq(0, 5000, by = 1000)) +\n  facet_grid(ui ~ race, labeller = labeller(race = label_race, ui = label_ui)) +\n  theme_bw(base_size = 20) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Relabel history of hypertension\nbirthwt$ht &lt;-  factor(birthwt$ht, levels = c(0, 1), labels = c(\"No\", \"Yes\"))\n# Model\nmod5 &lt;-  lm(bwt ~ age * smoke * race * ui * ht, data = birthwt)\n# Create new data\nnewdf &lt;-  expand.grid(age = seq(15, 45, by = 5),\n                      smoke = levels(birthwt$smoke),\n                      race = levels(birthwt$race),\n                      ui = levels(birthwt$ui),\n                      ht = levels(birthwt$ht))\n# Predict\nnewdf &lt;-  newdf |&gt; \n  mutate(pred = predict(mod5, newdata = newdf))\n# Visualise\nggplot(data = newdf, aes(x = age, y = pred, group = interaction(smoke, ht), color = smoke, linetype = ht)) +\n  geom_line(linewidth = 1) +\n  xlab(\"Maternal Age\") + ylab(\"Predicted Birthweight\") + labs(color = \"Smoking Status\", linetype = \"History of Hypertension\") +\n  scale_y_continuous(limits = c(0, 5000), breaks = seq(0, 5000, by = 1000)) +\n  facet_grid(ui ~ race, labeller = labeller(race = label_race, ui = label_ui)) +\n  theme_bw(base_size = 20) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nggeffects‚Ä¶"
  },
  {
    "objectID": "posts/029_02May_2025/index.html",
    "href": "posts/029_02May_2025/index.html",
    "title": "Model Prediction/Visualisation  3. Non-Linear Functional Forms",
    "section": "",
    "text": "You‚Äôll be glad to know this is going to be a relatively short post. In fact, in discussing aspects of model prediction and visualisation, the non-linear functional form is almost not worthy of a separate post in itself. BUT, I‚Äôm going to walk you through an example anyway - mainly to highlight that the process is no more difficult, nor different, to that of predicting and visualising the linear relationship.\nTo illustrate this, we‚Äôll again use the birthwt dataset. If you recall from the first post in this series, when we considered the modelling of birthweight as a function of maternal age (and later, sex), we made the somewhat unrealistic assumption that maternal age was linearly associated with birthweight. With this constraint in place, the model suggested to us that increasing maternal age was associated with increasing birthweight. At the time I suggested this was biologically implausible and guessed that the association would be more of an inverted U-shape - i.e.¬†birthweight increasing with maternal age to a certain point and then decreasing thereafter.\nSo, let‚Äôs relax the assumption of linearity by including a restricted cubic spline term on maternal age and see what the simplest model with that as the sole predictor shows.\n\n\n\n\n\n\nNote\n\n\n\nNote that to better capture potential non-linearity, I have increased the ‚Äòresolution‚Äô of maternal age when creating the new data grid. I originally used 5 year intervals which was more than enough when we assumed the relationship was linear, but have now amended that to 1 year intervals.\n\n\nOur model then becomes:\nmod &lt;-  lm(bwt ~ ns(age, 3), data = birthwt)\nAnd the visualisation of the model prediction is:\n\n\nCode\nlibrary(tidyverse)\nlibrary(splines)\ndata(birthwt, package = \"MASS\")\n# Model\nmod &lt;-  lm(bwt ~ ns(age, 3), data = birthwt)\n# Create new data\nnewdf &lt;-  expand.grid(age = seq(15, 45, by = 1))\n# Predict\nnewdf &lt;-  newdf |&gt; \n  mutate(pred = predict(mod, newdata = newdf))\n# Visualise\nggplot(data = newdf, aes(x = age, y = pred)) +\n  geom_line(linewidth = 1) +\n  geom_point(data = birthwt, aes(x = age, y = bwt)) +\n  xlab(\"Maternal Age\") + ylab(\"Predicted Birthweight\") +\n  scale_y_continuous(limits = c(0, 5000), breaks = seq(0, 5000, by = 1000)) +\n  theme_bw(base_size = 20) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nOk, so clearly there‚Äôs more to this relationship than I had anticipated, but we already have more information than when we modelled the association assuming it was linear. Based on this prediction, birthweight remains relatively stable until the mother‚Äôs age approaches 30 years, then increases thereafter. This still seems somewhat counter-intuitive to my mind, and in fact we may have good reason for not paying too much attention to the model prediction at older maternal ages. It turns out that only 20 out of 189 (i.e ~ 10% of mothers) are older than 30 years - and only 1 mother is older than 40 years. In addition, that single data point appears, on cursory examination, to exert considerable influence on the model fit.\nWhat happens if we refit the model without that observation?\n\n\nCode\nlibrary(tidyverse)\nlibrary(splines)\ndata(birthwt, package = \"MASS\")\n# Model\nmod &lt;-  lm(bwt ~ ns(age, 3), data = subset(birthwt, age &lt; 40))\n# Create new data\nnewdf &lt;-  expand.grid(age = seq(15, 40, by = 1))\n# Predict\nnewdf &lt;-  newdf |&gt; \n  mutate(pred = predict(mod, newdata = newdf))\n# Visualise\nggplot(data = newdf, aes(x = age, y = pred)) +\n  geom_line(linewidth = 1) +\n  geom_point(data = birthwt, aes(x = age, y = bwt)) +\n  xlab(\"Maternal Age\") + ylab(\"Predicted Birthweight\") +\n  scale_y_continuous(limits = c(0, 5000), breaks = seq(0, 5000, by = 1000)) +\n  theme_bw(base_size = 20) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nThis appears a little more sensible than with the inclusion of the last data point, so we‚Äôll leave that observation out of all further calculations. Now let‚Äôs also consider adjusting the prediction for potentially confounding factors that could be distorting the relationship as it currently stands. We‚Äôll start by first considering maternal smoking.\nThe association between maternal age and birthweight adjusted for maternal smoking can be visualised as:\n\n\nCode\n# Relabel smoking\nbirthwt$smoke &lt;-  factor(birthwt$smoke, levels = c(0,1), labels = c(\"No\", \"Yes\"))\n# Model\nmod2 &lt;-  lm(bwt ~ ns(age, 3) * smoke, data = subset(birthwt, age &lt; 40))\n# Create new data\nnewdf &lt;-  expand.grid(age = seq(15, 40, by = 1),\n                      smoke = levels(birthwt$smoke))\n# Predict\nnewdf &lt;-  newdf |&gt;\n  mutate(pred = predict(mod2, newdata = newdf))\n# Visualise\nggplot(data = newdf, aes(x = age, y = pred, color = smoke)) +\n  geom_line(linewidth = 1) +\n  geom_point(data = birthwt, aes(x = age, y = bwt)) +\n  xlab(\"Maternal Age\") + ylab(\"Predicted Birthweight\") + labs(color = \"Smoking Status\") +\n  scale_y_continuous(limits = c(0, 5000), breaks = seq(0, 5000, by = 1000)) +\n  theme_bw(base_size = 20) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nNow we can see that amongst smokers, birthweight declines with maternal age. However, we still observe that same oddly increasing profile of birthweight with maternal age amongst the non-smokers. Perhaps other factors are also at play? Let‚Äôs just adjust for the full complement as we did in the last post (at least that we can visualise).\nThus, the association between maternal age and birthweight adjusted for maternal smoking, history of hypertension, race and uterine irritability, can be visualised as:\n\n\nCode\n# Relabel history of hypertension\nbirthwt$ht &lt;-  factor(birthwt$ht, levels = c(0, 1), labels = c(\"No\", \"Yes\"))\nbirthwt$race &lt;-  factor(birthwt$race, levels = c(1,2,3), labels = c(\"White\", \"Black\", \"Other\"))\nbirthwt$ui &lt;-  factor(birthwt$ui, levels = c(0, 1), labels = c(\"No\", \"Yes\"))\n# Model\nmod3 &lt;-  lm(bwt ~ ns(age,3) * smoke * race * ui * ht, data = subset(birthwt, age &lt; 40))\n# Create new data\nnewdf &lt;-  expand.grid(age = seq(15, 40, by = 1),\n                      smoke = levels(birthwt$smoke),\n                      race = levels(birthwt$race),\n                      ui = levels(birthwt$ui),\n                      ht = levels(birthwt$ht))\n# Create labels for plotting race\nlabel_race &lt;- c(\"Race: White\", \"Race: Black\", \"Race: Other\")\nnames(label_race) &lt;- c(\"White\", \"Black\", \"Other\")\n# Create labels for plotting ui\nlabel_ui &lt;- c(\"Uterine Irritability: No\", \"Uterine Irritability: Yes\")\nnames(label_ui) &lt;- c(\"No\", \"Yes\")\n# Predict\nnewdf &lt;-  newdf |&gt; \n  mutate(pred = predict(mod3, newdata = newdf))\n# Visualise\nggplot(data = newdf, aes(x = age, y = pred, group = interaction(smoke, ht), color = smoke, linetype = ht)) +\n  geom_line(linewidth = 1) +\n  geom_point(data = birthwt, aes(x = age, y = bwt)) +\n  xlab(\"Maternal Age\") + ylab(\"Predicted Birthweight\") + labs(color = \"Smoking Status\", linetype = \"History of Hypertension\") +\n  scale_y_continuous(limits = c(0, 5000), breaks = seq(0, 5000, by = 1000)) +\n  facet_grid(ui ~ race, labeller = labeller(race = label_race, ui = label_ui)) +\n  theme_bw(base_size = 20) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nWe‚Äôre now seeing that the predictions further depend on the factors adjusted for. I am not going to attempt to explain any of this - my feeling is that there probably isn‚Äôt enough data at older maternal ages to come to any firm conclusions about the nature of the true relationship. Furthermore, this is NOT a good model - there is not enough data to support the estimation of all the parameters involved when you interact a spline term with multiple covariates. But that‚Äôs ok - ensuring model validity wasn‚Äôt really the point of this post. Instead it was to show you the simplicity of predicting and visualising non-linear functional forms. And I hope that I have managed to do that.\nUntil next time‚Ä¶"
  },
  {
    "objectID": "posts/030_16May_2025/index.html",
    "href": "posts/030_16May_2025/index.html",
    "title": "Model Prediction/Visualisation  4. Generalised Linear Models (GLMs)",
    "section": "",
    "text": "The examples that I‚Äôve used so far in this series of posts on model prediction and visualisation have all involved a continuous outcome measure (birthweight measured in grams), and so accordingly we have used a simple linear (regression) model. Nothing difficult there. But does anything change in the visualisation process when the models become a little more advanced?\nWell, the answer is both yes and no (sorry!). While the coding fundamentals largely remain the same, there are some additional considerations in the visualisation of GLM‚Äôs (as well as mixed and survival models which we will discuss in future posts) that you need to be aware of and better still, understand.\nSo let‚Äôs get to it. We‚Äôll again use the familiar birthwt dataset, this time to predict from, and visualise a logistic regression model. Our outcome variable (low) is now a dichotomised version of birthweight, with 1 assigned to values less than 2500 grams and 0 to values that are greater."
  },
  {
    "objectID": "posts/030_16May_2025/index.html#assuming-linear-in-the-log-odds",
    "href": "posts/030_16May_2025/index.html#assuming-linear-in-the-log-odds",
    "title": "Model Prediction/Visualisation  4. Generalised Linear Models (GLMs)",
    "section": "1.1 Assuming Linear in the Log-Odds",
    "text": "1.1 Assuming Linear in the Log-Odds\nIn the first instance we‚Äôll assume a linear relationship of maternal age with the log-odds of low birthweight and our model may be written as:\nmod &lt;-  glm(low ~ age, family = \"binomial\", data = birthwt)\nIf you recall in the last post, we excluded the observation for the oldest mother having the heaviest baby, and we will do that again here.\n\n\nCode\nlibrary(tidyverse)\nlibrary(splines)\nlibrary(gtsummary)\ndata(birthwt, package = \"MASS\")\n# Model\nmod &lt;-  glm(low ~ age, family = \"binomial\", data = subset(birthwt, age &lt; 40))\ntbl_regression(mod, intercept = T)\n\n\n\n\n\n\n\n\nCharacteristic\nlog(OR)1\n95% CI1\np-value\n\n\n\n\n(Intercept)\n0.32\n-1.1, 1.8\n0.7\n\n\nage\n-0.05\n-0.11, 0.01\n0.14\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nThe model suggests that there is an ~ 0.05 reduction in the log-odds of being low birthweight with each year increase in maternal age.\nBut how do we visualise the relationship between maternal age and the log-odds of low birthweight? And then compare the model fit? Well, the first part of that is not as straightforward as creating a simple scatterplot of the association between a continuous covariate and a continuous outcome, but it‚Äôs certainly not intractable. We need to do a little bit of legwork to get what we want, and in this case the simplest solution is to calculate the observed proportions of low birthweight aggregated across deciles of maternal age, then convert those proportions to log-odds using the qlogis() function. The code to do this is shown below.\n\n\nCode\n# Categorise maternal age into deciles\nbirthwt &lt;- birthwt |&gt; \n    mutate(age_10 = ntile(age, 10))\n\n# Proportion of women with low birthweight babies in each decile of maternal age\ngrouped_dat &lt;- birthwt |&gt; \n  group_by(age_10) |&gt; \n  count(low) |&gt; \n  pivot_wider(names_from = low,\n              values_from = n) |&gt; \n  mutate(prop = `1`/(`0` + `1`),\n         logodds = qlogis(prop))\n\n\nWe can then plot that data as:\n\n\nCode\nggplot(data = grouped_dat, aes(x = age_10, y = logodds)) +\n  geom_point(size = 3) +\n  scale_y_continuous(limits = c(-2, 0), breaks = seq(-2, 0, by = 0.2)) + \n  scale_x_continuous(limits = c(1, 10), breaks = seq(1, 10, by = 1)) +\n  xlab(\"Maternal Age Decile\") + ylab(\"Log-odds of Low Birthweight\") +\n  theme_bw(base_size = 20)"
  },
  {
    "objectID": "posts/030_16May_2025/index.html#allowing-non-linear-in-the-log-odds",
    "href": "posts/030_16May_2025/index.html#allowing-non-linear-in-the-log-odds",
    "title": "Model Prediction/Visualisation  4. Generalised Linear Models (GLMs)",
    "section": "1.2 Allowing Non-Linear in the Log-Odds",
    "text": "1.2 Allowing Non-Linear in the Log-Odds\nPaul to do - - Calculate the proportions in each decile (look up ‚Äòlog-odds linearity assumption‚Äô in Quiver for code). Convert each proportion to a log-odds. Superimpose the prediction from the model and see it fits a linear association. - Whether to also plot predicted odds and probabilities? - Repeat above with spline on log-odds to see if it fits better. - The point is to highlight that the visualisation is only linear in the log-odds and non-linear for odds and probs‚Ä¶and also non-linear for spline log-odds if assuming the association is not in fact linear."
  },
  {
    "objectID": "posts/030_16May_2025/index.html#assuming-linear-relationship-in-the-log-odds",
    "href": "posts/030_16May_2025/index.html#assuming-linear-relationship-in-the-log-odds",
    "title": "Model Prediction/Visualisation  4. Generalised Linear Models (GLMs)",
    "section": "1.1 Assuming Linear Relationship in the Log-Odds",
    "text": "1.1 Assuming Linear Relationship in the Log-Odds\nIn the first instance we‚Äôll assume a linear relationship of maternal age with the log-odds of low birthweight and our model may be written as:\nmod &lt;-  glm(low ~ age, family = \"binomial\", data = birthwt)\nIf you recall in the last post, we excluded the observation for the oldest mother having the heaviest baby, and we will do that again here.\n\n\nCode\nlibrary(tidyverse)\nlibrary(splines)\nlibrary(gtsummary)\ndata(birthwt, package = \"MASS\")\n# Model\nmod &lt;-  glm(low ~ age, family = \"binomial\", data = subset(birthwt, age &lt; 40))\ntbl_regression(mod, intercept = T)\n\n\n\n\n\n\n\n\nCharacteristic\nlog(OR)1\n95% CI1\np-value\n\n\n\n\n(Intercept)\n0.32\n-1.1, 1.8\n0.7\n\n\nage\n-0.05\n-0.11, 0.01\n0.14\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nThe model suggests that there is an ~ 0.05 reduction in the log-odds of being low birthweight with each year increase in maternal age.\nBut how do we visualise the relationship between maternal age and the log-odds of low birthweight? And then compare the model fit? Well, the first part of that is not as straightforward as creating a simple scatterplot of the association between a continuous covariate and a continuous outcome, but it‚Äôs certainly not intractable. We need to do a little bit of legwork to get what we want, and in this case the simplest solution is to calculate the observed proportions of low birthweight aggregated across quantiles of maternal age, then convert those proportions to log-odds using the qlogis() function. The code to do this is shown below.\n\n\nCode\n# Categorise maternal age into 5 quantiles (could make this more or less)\nbirthwt &lt;- birthwt |&gt;\n    mutate(age_5 = cut_number(age, 5))\n# Proportion of women with low birthweight babies in each quantile of maternal age\ngrouped_dat &lt;- birthwt |&gt; \n  group_by(age_5) |&gt; \n  count(low) |&gt; \n  pivot_wider(names_from = low,\n              values_from = n) |&gt; \n  mutate(prop = `1`/(`0` + `1`),\n         logodds = qlogis(prop))\n# Take the middle age of each quantile range and bind to data\nvec = c(16.5, 20, 22.5, 26, 36.5)\ngrouped_dat &lt;-  cbind(\"age_5_mid\" = vec, grouped_dat)\n\n\nWe can then plot that data as:\n\n\nCode\nggplot(data = grouped_dat, aes(x = age_5_mid, y = logodds)) +\n  geom_point(size = 3) +\n  scale_y_continuous(limits = c(-2, 0), breaks = seq(-2, 0, by = 0.2)) + \n  scale_x_continuous(limits = c(15, 45), breaks = seq(15, 45, by = 5)) +\n  xlab(\"Maternal Age\") + ylab(\"Log-odds of Low Birthweight\") +\n  theme_bw(base_size = 20)\n\n\n\n\n\n\n\n\n\nAnd superimpose the model predictions as:\n\n\nCode\nnewdf &lt;-  expand.grid(age = seq(15, 45, by = 5))\nnewdf &lt;-  newdf |&gt; \n    mutate(pred = predict(mod, newdata = newdf))\nggplot(data = grouped_dat, aes(x = age_5_mid, y = logodds)) +\n  geom_point(size = 3) +\n  geom_line(data = newdf, aes(x = age, y = pred)) +\n  scale_y_continuous(limits = c(-2, 0), breaks = seq(-2, 0, by = 0.2)) + \n  xlab(\"Maternal Age (yrs)\") + ylab(\"Predicted Log-odds of Low Birthweight\") +\n  theme_bw(base_size = 20)"
  },
  {
    "objectID": "posts/030_16May_2025/index.html#allowing-non-linear-relationship-in-the-log-odds",
    "href": "posts/030_16May_2025/index.html#allowing-non-linear-relationship-in-the-log-odds",
    "title": "Model Prediction/Visualisation  4. Generalised Linear Models (GLMs)",
    "section": "1.3 Allowing Non-Linear Relationship in the Log-Odds",
    "text": "1.3 Allowing Non-Linear Relationship in the Log-Odds"
  },
  {
    "objectID": "posts/030_16May_2025/index.html#visualising-log-odds-of-the-outcome",
    "href": "posts/030_16May_2025/index.html#visualising-log-odds-of-the-outcome",
    "title": "Model Prediction/Visualisation  4. Generalised Linear Models (GLMs)",
    "section": "1.1 Visualising Log-Odds of the Outcome",
    "text": "1.1 Visualising Log-Odds of the Outcome\nBut how do we visualise the relationship between maternal age and the log-odds of low birthweight? And then compare the model fit? Well, the first part of that is not as straightforward as creating a simple scatterplot of the association between a continuous covariate and a continuous outcome, but it‚Äôs certainly not intractable. We need to do a little bit of legwork to get what we want, and in this case the simplest solution is to calculate the observed proportions of low birthweight aggregated across quantiles of maternal age, then convert those proportions to log-odds using the qlogis() function. The code to do this is shown below.\n\n\nCode\n# Categorise maternal age into 5 quantiles (could make this more or less)\nbirthwt &lt;- birthwt |&gt;\n    mutate(age_5 = cut_number(age, 5))\n# Proportion of women with low birthweight babies in each quantile of maternal age\ngrouped_dat &lt;- birthwt |&gt; \n  group_by(age_5) |&gt; \n  count(low) |&gt; \n  pivot_wider(names_from = low,\n              values_from = n) |&gt; \n  mutate(obs_props = `1`/(`0` + `1`),\n         obs_logodds = qlogis(obs_props))\n# Take the middle age of each quantile range and bind to data\nvec = c(16.5, 20, 22.5, 26, 36.5)\ngrouped_dat &lt;-  cbind(\"age_5_mid\" = vec, grouped_dat)\n\n\nWe can then plot that data as:\n\n\nCode\nggplot(data = grouped_dat, aes(x = age_5_mid, y = obs_logodds)) +\n  geom_point(size = 3) +\n  scale_y_continuous(limits = c(-2, 0), breaks = seq(-2, 0, by = 0.2)) + \n  scale_x_continuous(limits = c(15, 45), breaks = seq(15, 45, by = 5)) +\n  xlab(\"Maternal Age\") + ylab(\"Log-odds of Low Birthweight\") +\n  theme_bw(base_size = 20)\n\n\n\n\n\n\n\n\n\nAnd superimpose the model predictions as:\n\n\nCode\nnewdf &lt;-  expand.grid(age = seq(15, 45, by = 5))\nnewdf &lt;-  newdf |&gt; \n    mutate(pred_logodds = predict(mod, newdata = newdf))\nggplot(data = grouped_dat, aes(x = age_5_mid, y = obs_logodds)) +\n  geom_point(size = 3) +\n  geom_line(data = newdf, aes(x = age, y = pred_logodds)) +\n  scale_y_continuous(limits = c(-2, 0), breaks = seq(-2, 0, by = 0.2)) + \n  scale_x_continuous(limits = c(15, 45), breaks = seq(15, 45, by = 5)) +\n  xlab(\"Maternal Age (yrs)\") + ylab(\"Predicted Log-odds of Low Birthweight\") +\n  theme_bw(base_size = 20)\n\n\n\n\n\n\n\n\n\nLet‚Äôs recap what we have done, and why, here. We‚Äôve taken the observed proportions of low birthweight babies, converted those values to equivalent observed log-odds and plotted those alongside the model predicted log-odds as a visual aid to assess model fit. Now, I‚Äôm not suggesting that this is something you do every time you fit a logistic regression model, but it gives you some idea of the basis in the model fitting - which is always in the log-odds of the outcome. And in its default state, this assumes a linear association of the covariate with that outcome form."
  },
  {
    "objectID": "posts/030_16May_2025/index.html#visualising-probability-of-the-outcome",
    "href": "posts/030_16May_2025/index.html#visualising-probability-of-the-outcome",
    "title": "Model Prediction/Visualisation  4. Generalised Linear Models (GLMs)",
    "section": "1.2 Visualising Probability of the Outcome",
    "text": "1.2 Visualising Probability of the Outcome\nI would like to point out at this stage that it‚Äôs not much extra work to plot the predicted probabilities (of low birthweight) - the outcome that you are as a general rule, more interested in knowing about. We already calculated the observed proportions, so the only small tweak we now need to make in predicting from our model to ensure we are generating predicted probabilities, is to include type = \"response\" in our prediction call:\n\n\nCode\nnewdf &lt;-  expand.grid(age = seq(15, 45, by = 5))\nnewdf &lt;-  newdf |&gt; \n    mutate(pred_probs = predict(mod, newdata = newdf, type = \"response\"))\nggplot(data = grouped_dat, aes(x = age_5_mid, y = obs_props)) +\n  geom_point(size = 3) +\n  geom_line(data = newdf, aes(x = age, y = pred_probs)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) + \n  scale_x_continuous(limits = c(15, 45), breaks = seq(15, 45, by = 5)) +\n  xlab(\"Maternal Age (yrs)\") + ylab(\"Predicted Probability of Low Birthweight\") +\n  theme_bw(base_size = 20)\n\n\n\n\n\n\n\n\n\nNote that in contrast to the (assumed linear) log-odds, the association between a continuous covariate and the predicted probability is now expected to be non-linear."
  },
  {
    "objectID": "posts/030_16May_2025/index.html#visualising-log-odds-of-the-outcome-1",
    "href": "posts/030_16May_2025/index.html#visualising-log-odds-of-the-outcome-1",
    "title": "Model Prediction/Visualisation  4. Generalised Linear Models (GLMs)",
    "section": "2.1 Visualising Log-Odds of the Outcome",
    "text": "2.1 Visualising Log-Odds of the Outcome\n\n\nCode\nnewdf2 &lt;-  expand.grid(age = seq(15, 45, by = 0.5))\nnewdf2 &lt;-  newdf2 |&gt; \n    mutate(pred_logodds = predict(mod2, newdata = newdf2))\nggplot(data = grouped_dat, aes(x = age_5_mid, y = obs_logodds)) +\n  geom_point(size = 3) +\n  geom_line(data = newdf2, aes(x = age, y = pred_logodds)) +\n  scale_y_continuous(limits = c(-2, 0), breaks = seq(-2, 0, by = 0.2)) + \n  scale_x_continuous(limits = c(15, 45), breaks = seq(15, 45, by = 5)) +\n  xlab(\"Maternal Age (yrs)\") + ylab(\"Predicted Log-odds of Low Birthweight\") +\n  theme_bw(base_size = 20)"
  },
  {
    "objectID": "posts/030_16May_2025/index.html#visualising-probability-of-the-outcome-1",
    "href": "posts/030_16May_2025/index.html#visualising-probability-of-the-outcome-1",
    "title": "Model Prediction/Visualisation  4. Generalised Linear Models (GLMs)",
    "section": "2.2 Visualising Probability of the Outcome",
    "text": "2.2 Visualising Probability of the Outcome\n\n\nCode\nnewdf2 &lt;-  expand.grid(age = seq(15, 45, by = 0.5))\nnewdf2 &lt;-  newdf2 |&gt; \n    mutate(pred_probs = predict(mod2, newdata = newdf2, type = \"response\"))\nggplot(data = grouped_dat, aes(x = age_5_mid, y = obs_props)) +\n  geom_point(size = 3) +\n  geom_line(data = newdf2, aes(x = age, y = pred_probs)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) + \n  scale_x_continuous(limits = c(15, 45), breaks = seq(15, 45, by = 5)) +\n  xlab(\"Maternal Age (yrs)\") + ylab(\"Predicted Probability of Low Birthweight\") +\n  theme_bw(base_size = 20)\n\n\n\n\n\n\n\n\n\nA likelihood ratio test of the two models suggests the spline term doesn‚Äôt actually add any statistical value and can be removed. In any case, the takeaway here is to note that visualising the non-linear functional form is no more difficult than the default linear case. Also take note that with the introduction of a spline term, both the predicted log-odds and probabilities no longer assume a linear association with the covariate of interest.\nThat‚Äôs probably enough on this particular topic - I will hopefully see you again in two weeks for more prediction/visualisation tips."
  },
  {
    "objectID": "posts/031_30May_2025/index.html",
    "href": "posts/031_30May_2025/index.html",
    "title": "Model Prediction/Visualisation  5. Linear Mixed Models (LMMs)",
    "section": "",
    "text": "This week we are continuing our series on predicting and visualising models in R by focussing on linear mixed models (LMM‚Äôs). Like GLM‚Äôs, the fundamentals of the process remain the same bar a few tweaks specific to LMM‚Äôs that you need to be aware of. Here we are back to dealing with a continuous covariate and outcome - the new development to note is the distinction between fixed and random effects and how to handle each appropriately in the predict() function.\n\n1 Predicting\nTo illustrate these ideas we‚Äôll revisit the sleepstudy dataset which is available in the lme4 package (I have used this for a similar purpose in one of my previous posts). The sleepstudy data looks at reaction times over time in sleep-deprived individuals. For the sake of the exercise we will fit a mixed model with reaction time (ms) as the outcome, time (days) as a fixed-effect and time (days) and individual as random-effects. So this is a random slopes model allowing the ‚Äòeffect‚Äô of sleep-deprivation on reaction time to vary over time for each individual. We fit the model and view a few lines of the dataframe which now contains the fixed (mod_pred_fix) and random (mod_pred_ran) predictions. Obtaining the predictions is very easy - we create the two new variables by simply calling:\npredict(mod) for the random-effects, and:\npredict(mod, re.form = NA) for the fixed-effects.\nNote the re.form = NA sets all random-effects to 0.\n\n\nCode\nlibrary(lme4)\nlibrary(ggplot2)\nlibrary(gtsummary)\nlibrary(kableExtra)\n# Load data\ndata(\"sleepstudy\")\n# Model\nmod &lt;- lmer(Reaction ~ Days + (Days | Subject), data = sleepstudy)\n# Predict\nsleepstudy$mod_pred_fix &lt;- predict(mod, re.form = NA) # predict fixed effects\nsleepstudy$mod_pred_ran &lt;- predict(mod) # predict random effects\n# View data\nhead(sleepstudy, 10) |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nReaction\nDays\nSubject\nmod_pred_fix\nmod_pred_ran\n\n\n\n\n249.56\n0\n308\n251.41\n253.66\n\n\n258.70\n1\n308\n261.87\n273.33\n\n\n250.80\n2\n308\n272.34\n293.00\n\n\n321.44\n3\n308\n282.81\n312.66\n\n\n356.85\n4\n308\n293.27\n332.33\n\n\n414.69\n5\n308\n303.74\n351.99\n\n\n382.20\n6\n308\n314.21\n371.66\n\n\n290.15\n7\n308\n324.68\n391.33\n\n\n430.59\n8\n308\n335.14\n410.99\n\n\n466.35\n9\n308\n345.61\n430.66\n\n\n\n\n\n\nSo, mod_pred_fix represents the group-averaged prediction of reaction time (outcome) as a function of time (covariate) and similarly mod_pred_ran represents the individual predictions of the same.\n\n\n2 One-Plot Visualisation\nPlotting the results is then quite straightforward with ggplot2:\n\n\nCode\n# Plot all data\nsleepstudy |&gt;\n    ggplot(aes(x = Days, y = Reaction, color = factor(Subject))) +\n    geom_line(aes(x = Days, y = mod_pred_ran)) +\n    geom_line(aes(x = Days, y = mod_pred_fix), linewidth = 2, color = \"blue\") +\n    scale_y_continuous(limits = c(200, 500), breaks = seq(200, 500, by = 50)) +\n    scale_x_continuous(limits = c(0, 9), breaks = seq(0, 9, by = 1)) +\n    geom_point(alpha = 0.5) +\n    labs(title = \"Reaction Time ~ Days of Sleep Deprivation\") + xlab(\"Time (days)\") + ylab(\"Average Reaction Time (ms)\") +\n    theme_bw(base_size = 15)\n\n\n\n\n\n\n\n\n\n\n\n3 Multi-Plot Visualisation\nThe above plot is good for showing all the data at once, but can get a little bit confusing if you are interested in looking more closely at any specific individual trajectories. If that is in fact the case, an alternative plot representation is to facet on individuals, like so:\n\n\nCode\n# Plot - facet on individual\nsleepstudy |&gt;\n  ggplot(aes(x = Days, y = Reaction)) +\n  geom_line(aes(x = Days, y = mod_pred_ran), linewidth = 1, color = \"green\") +\n  geom_line(aes(x = Days, y = mod_pred_fix), linewidth = 1, color = \"blue\") +\n  scale_y_continuous(limits = c(200, 500), breaks = seq(200, 500, by = 100)) +\n  scale_x_continuous(limits = c(0, 9), breaks = seq(0, 9, by = 2)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~Subject) +\n  labs(x = \"Time (Days)\",\n       y = \"Average Reaction Time (ms)\",\n       title = \"Reaction Time ~ Days of Sleep Deprivation\",\n       subtitle = \"Blue = Fixed Effects Prediction | Green = Random Effects Prediction\") +\n  theme_bw(base_size = 15)\n\n\n\n\n\n\n\n\n\nIt‚Äôs now much easier to see the individual trajectories and how they compare to that of the group average.\n\n\n4 Some Background Stuff\nWhile not essential to producing a visualisation of the model predictions, taking a little detour to understand in more detail how we actually arrived at the fixed- and random-effect predictions is a worthwhile exercise and one which can aid in building your mixed-modelling intuition.\nLet‚Äôs first inspect an unprettified summary of the model output:\n\n\nCode\nsummary(mod)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ Days + (Days | Subject)\n   Data: sleepstudy\n\nREML criterion at convergence: 1743.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9536 -0.4634  0.0231  0.4634  5.1793 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n Subject  (Intercept) 612.10   24.741       \n          Days         35.07    5.922   0.07\n Residual             654.94   25.592       \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  251.405      6.825  36.838\nDays          10.467      1.546   6.771\n\nCorrelation of Fixed Effects:\n     (Intr)\nDays -0.138\n\n\nWe obtain the fixed-effects as an Intercept and a coefficient for time (Days) and we can interpret that as an average reaction time of 251.4 ms at baseline and an average increase of 10.5 ms in reaction time for the passing of each day. Note that the random-effects are not as easily interpretable in their reported form which is an estimate of the variance of the distribution of individual reaction times, not average differences as for fixed-effects.\nA shortcut to obtain the fixed-effects of an lmer class model in R is:\nfixef(mod)\n\n\nCode\nfixef(mod)\n\n\n(Intercept)        Days \n  251.40510    10.46729 \n\n\nand similarly for the random-effects:\nranef(mod)\n\n\nCode\nranef(mod)\n\n\n$Subject\n    (Intercept)        Days\n308   2.2585509   9.1989758\n309 -40.3987381  -8.6196806\n310 -38.9604090  -5.4488565\n330  23.6906196  -4.8143503\n331  22.2603126  -3.0699116\n332   9.0395679  -0.2721770\n333  16.8405086  -0.2236361\n334  -7.2326151   1.0745816\n335  -0.3336684 -10.7521652\n337  34.8904868   8.6282652\n349 -25.2102286   1.1734322\n350 -13.0700342   6.6142178\n351   4.5778642  -3.0152621\n352  20.8636782   3.5360011\n369   3.2754656   0.8722149\n370 -25.6129993   4.8224850\n371   0.8070461  -0.9881562\n372  12.3145921   1.2840221\n\nwith conditional variances for \"Subject\" \n\n\nSo what do the random-effects actually tell us? In a nutshell, these represent the individual differences from the fixed-effects values.\nLet‚Äôs consider Subject 308 as an example to illustrate this. If we are interested in the predicted baseline reaction time for this individual, all we need to do is add the fixed- and random-effects intercepts values - 251.405 + 2.259 = 253.664 ms (we can ignore the time ‚Äòeffect‚Äô which is 0 by definition at baseline). By comparison, we can also easily obtain that predicted value with the following code:\npredict(mod, newdata = data.frame(days = 0, Subject = 308), re.form = ~(Days|Subject))\n\n\nCode\npredict(mod, newdata = data.frame(Days = 0, Subject = 308), re.form = ~(Days|Subject))\n\n\n       1 \n253.6637 \n\n\nIt‚Äôs a little bit more complex to manually calculate the predicted value at any other time point because the ‚Äòeffect‚Äô of time is no longer ignorable and we just need to remember the basics of our regression equation. Say, we want to calculate the reaction time for Subject 308 at 5 days. A simple way to do this is to first work out the fixed-effect prediction at 5 days and then add the random-effect prediction to it.\nThe fixed-effect prediction is just the intercept value plus the coefficient for time multiplied by the number of days (either from the model summary or from fixef()). In numbers: 251.405 + (10.467 * 5) = 303.74. This gives the predicted group-average reaction time at 5 days.\nThe random-effect prediction is the same but from ranef() and specific to the individual. In numbers: 2.259 + (9.198 * 5) = 48.249. This gives the additional (to the group-average) reaction time specific to the individual.\nWe can then add those two values to obtain the predicted reaction time for Subject 308 at 5 days - i.e.¬†303.74 + 48.249 = 351.989. If you don‚Äôt want to manually calculate this, then you can use:\npredict(mod, newdata = data.frame(days = 5, Subject = 308), re.form = ~(Days|Subject))\n\n\nCode\npredict(mod, newdata = data.frame(Days = 5, Subject = 308), re.form = ~(Days|Subject))\n\n\n      1 \n351.995 \n\n\nWhich is essentially the same, bar rounding error.\nAnd that brings us to the end of this post. Hopefully that gives you some idea as to how random effects and their predictions are calculated. In the next post, we will finish our series on model prediction and visualisation be taking a look at the Cox model. Until then‚Ä¶"
  },
  {
    "objectID": "posts/032_13Jun_2025/index.html",
    "href": "posts/032_13Jun_2025/index.html",
    "title": "Model Prediction/Visualisation  6. Survival Analysis - Cox Model",
    "section": "",
    "text": "In the last post in this series on model prediction and visualisation in R we are going to talk about survival analysis and specifically the Cox model. Like the extensions to the standard linear model that we most recently discussed - the GLM and LMM - predicting with the Cox model is also relatively straightforward, but the intuition in how those predictions are calculated may be less so. I hope to make that easier for you today.\nWe‚Äôre going to use the inbuilt GBSG2 dataset form the TH.data package, so please install that package if you don‚Äôt already have it. The dataframe contains observations from the German Breast Cancer Study Group 2 study of 686 women, examining the effects of hormone treatment on recurrence-free survival time. The variables we‚Äôll be using are:\nThe first few rows of the data look like:\nCode\nlibrary(survival)\nlibrary(survminer)\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(kableExtra)\n# Load data\ndata(GBSG2, package = \"TH.data\")\nGBSG2 |&gt; \n  select(time, cens, horTh, tsize) |&gt; \n  head(10) |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\ntime\ncens\nhorTh\ntsize\n\n\n\n\n1814\n1\nno\n21\n\n\n2018\n1\nyes\n12\n\n\n712\n1\nyes\n35\n\n\n1807\n1\nyes\n17\n\n\n772\n1\nno\n35\n\n\n448\n1\nno\n57\n\n\n2172\n0\nyes\n8\n\n\n2161\n0\nno\n16\n\n\n471\n1\nno\n39\n\n\n2014\n0\nno\n18\nThe first thing we can do is to plot the empirical survival curve by calling on the Kaplan-Meier estimator - stratifying this by whether a woman received hormone therapy or not.\nCode\n# Kaplan-Meier fit\nfit &lt;- survfit(Surv(time, cens) ~ horTh, data = GBSG2)\n# Plot KM fit\nggsurvplot(fit, \n           risk.table = TRUE,\n           submain = \"Kaplan-Meier Survival Curves for Hormone Therapy Use\") +\n  theme_survminer(\n     font.submain = 22,\n     font.x = 18,\n     font.y = 18,\n     font.legend = 16,\n     font.tickslab = 14)\nIt appears that unadjusted for other potentially confounding factors, women who take hormone therapy have a better chance of survival - an unsurprising finding I‚Äôm sure you would agree.\nLet‚Äôs fit a Cox model to the data, now also adjusting for tumour size.\nCode\ncox_mod &lt;- coxph(Surv(time, cens) ~ horTh + tsize, data = GBSG2)\ntbl_regression(cox_mod, exp = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nHR1\n95% CI1\np-value\n\n\n\n\nhorTh\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†no\n‚Äî\n‚Äî\n\n\n\n\n¬†¬†¬†¬†yes\n0.69\n0.54, 0.88\n0.003\n\n\ntsize\n1.02\n1.01, 1.02\n&lt;0.001\n\n\n\n1 HR = Hazard Ratio, CI = Confidence Interval\nThe model results suggest (in agreement with the KM curve) that holding tumour size constant, there is an ~ 31% reduction in the risk of recurrence for women who take hormone therapy. Similarly, within strata of hormone therapy, each additional mm increase in tumour size is associated with an ~ 2% increase in the risk of recurrence over time."
  },
  {
    "objectID": "posts/032_13Jun_2025/index.html#step-1",
    "href": "posts/032_13Jun_2025/index.html#step-1",
    "title": "Model Prediction/Visualisation  6. Survival Analysis - Cox Model",
    "section": "2.1 Step 1",
    "text": "2.1 Step 1\nThe first thing we need to do is to estimate the cumulative baseline hazard function (H0) when all covariate values are at 0 or at their reference level. We do this using specifying centered = FALSE in the call to basehaz(). It is important to include the centered = F argument as the default setting is TRUE which we don‚Äôt want as this has a different interpretation (i.e.¬†cumulative hazard function estimated at the mean of all covariate values). Also note that the cumulative hazard is estimated at each of the observed times in the dataset."
  },
  {
    "objectID": "posts/032_13Jun_2025/index.html#step-2",
    "href": "posts/032_13Jun_2025/index.html#step-2",
    "title": "Model Prediction/Visualisation  6. Survival Analysis - Cox Model",
    "section": "2.2 Step 2",
    "text": "2.2 Step 2\nWe then estimate the cumulative hazard function (H1 - H4) for each covariate combination of interest. For these data we have four as I have listed above. As an example, to calculate H1 we simply multiply H0 by the covariate combination values and their respective Cox model coefficients. e.g.:\nH1 &lt;- H0$hazard * exp((0 * cox_mod$coefficients[1]) + (3 * cox_mod$coefficients[2]))\nIn the above cox_mod$coefficients[1] gives the coefficient value for hormone therapy and cox_mod$coefficients[2] gives the coefficient value for tumour size. To obtain the cumulative hazard function for the first covariate combination we just multiply the cumulative baseline hazard function by the exponent of the sum of the products of the coefficients and our covariate values of interest (in this case 0 to indicate no hormone therapy and 3 to indicate the tumour size)."
  },
  {
    "objectID": "posts/032_13Jun_2025/index.html#step-3",
    "href": "posts/032_13Jun_2025/index.html#step-3",
    "title": "Model Prediction/Visualisation  6. Survival Analysis - Cox Model",
    "section": "2.3 Step 3",
    "text": "2.3 Step 3\nWe then calculate the survival function for each covariate combination by taking the exponent of the negative of the cumulative hazard function."
  },
  {
    "objectID": "posts/032_13Jun_2025/index.html#step-4",
    "href": "posts/032_13Jun_2025/index.html#step-4",
    "title": "Model Prediction/Visualisation  6. Survival Analysis - Cox Model",
    "section": "2.4 Step 4",
    "text": "2.4 Step 4\nNow we create a dataframe containing the predicted survival probabilities for each covariate combination. The first 20 rows for each covariate combination is shown below."
  },
  {
    "objectID": "posts/032_13Jun_2025/index.html#step-5",
    "href": "posts/032_13Jun_2025/index.html#step-5",
    "title": "Model Prediction/Visualisation  6. Survival Analysis - Cox Model",
    "section": "2.5 Step 5",
    "text": "2.5 Step 5\nThen we create our plot.\n\n\nCode\n# Step 1\n# Estimate the cumulative baseline hazard function\nH0 &lt;- basehaz(cox_mod, centered = F)\n\n# Step 2\n# Calculate cumulative hazard function for covariate combo 1.\nH1 &lt;- H0$hazard * exp((0 * cox_mod$coefficients[1]) + (3 * cox_mod$coefficients[2]))\n# Calculate cumulative hazard function for covariate combo 2.\nH2 &lt;- H0$hazard * exp((1 * cox_mod$coefficients[1]) + (3 * cox_mod$coefficients[2]))\n# Calculate cumulative hazard function for covariate combo 3.\nH3 &lt;- H0$hazard * exp((0 * cox_mod$coefficients[1]) + (120 * cox_mod$coefficients[2]))\n# Calculate cumulative hazard function for covariate combo 4.\nH4 &lt;- H0$hazard * exp((1 * cox_mod$coefficients[1]) + (120 * cox_mod$coefficients[2]))\n\n# Step 3\n# Calculate survival function for covariate combo 1.\nS1 &lt;- exp(-H1)\n# Calculate survival function for covariate combo 2.\nS2 &lt;- exp(-H2)\n# Calculate survival function for covariate combo 3.\nS3 &lt;- exp(-H3)\n# Calculate survival function for covariate combo 4.\nS4 &lt;- exp(-H4)\n\n# Step 4\n# Create dataframe of predicted survival probabilities as a function of covariate combos\nsurv_df &lt;-  rbind(data.frame(combo = 1, time = H0$time, horTh = 0, tsize = 3, prob_surv = S1),\n                  data.frame(combo = 2, time = H0$time, horTh = 1, tsize = 3, prob_surv = S2),\n                  data.frame(combo = 3, time = H0$time, horTh = 0, tsize = 120, prob_surv = S3),\n                  data.frame(combo = 4, time = H0$time, horTh = 1, tsize = 120, prob_surv = S4))\n\n# Inspect each df\nhead(data.frame(combo = 1, time = H0$time, horTh = 0, tsize = 3, prob_surv = S1), 20) |&gt; \n  kable(align = \"c\", digits = 4)\n\n\n\n\n\ncombo\ntime\nhorTh\ntsize\nprob_surv\n\n\n\n\n1\n8\n0\n3\n1.0000\n\n\n1\n15\n0\n3\n1.0000\n\n\n1\n16\n0\n3\n1.0000\n\n\n1\n17\n0\n3\n1.0000\n\n\n1\n18\n0\n3\n1.0000\n\n\n1\n29\n0\n3\n1.0000\n\n\n1\n42\n0\n3\n1.0000\n\n\n1\n46\n0\n3\n1.0000\n\n\n1\n57\n0\n3\n1.0000\n\n\n1\n63\n0\n3\n1.0000\n\n\n1\n65\n0\n3\n1.0000\n\n\n1\n67\n0\n3\n1.0000\n\n\n1\n71\n0\n3\n1.0000\n\n\n1\n72\n0\n3\n0.9989\n\n\n1\n98\n0\n3\n0.9978\n\n\n1\n113\n0\n3\n0.9967\n\n\n1\n114\n0\n3\n0.9967\n\n\n1\n120\n0\n3\n0.9956\n\n\n1\n148\n0\n3\n0.9956\n\n\n1\n160\n0\n3\n0.9945\n\n\n\n\n\nCode\nhead(data.frame(combo = 2, time = H0$time, horTh = 1, tsize = 3, prob_surv = S2), 20) |&gt; \n  kable(align = \"c\", digits = 4)\n\n\n\n\n\ncombo\ntime\nhorTh\ntsize\nprob_surv\n\n\n\n\n2\n8\n1\n3\n1.0000\n\n\n2\n15\n1\n3\n1.0000\n\n\n2\n16\n1\n3\n1.0000\n\n\n2\n17\n1\n3\n1.0000\n\n\n2\n18\n1\n3\n1.0000\n\n\n2\n29\n1\n3\n1.0000\n\n\n2\n42\n1\n3\n1.0000\n\n\n2\n46\n1\n3\n1.0000\n\n\n2\n57\n1\n3\n1.0000\n\n\n2\n63\n1\n3\n1.0000\n\n\n2\n65\n1\n3\n1.0000\n\n\n2\n67\n1\n3\n1.0000\n\n\n2\n71\n1\n3\n1.0000\n\n\n2\n72\n1\n3\n0.9992\n\n\n2\n98\n1\n3\n0.9985\n\n\n2\n113\n1\n3\n0.9977\n\n\n2\n114\n1\n3\n0.9977\n\n\n2\n120\n1\n3\n0.9970\n\n\n2\n148\n1\n3\n0.9970\n\n\n2\n160\n1\n3\n0.9962\n\n\n\n\n\nCode\nhead(data.frame(combo = 3, time = H0$time, horTh = 0, tsize = 120, prob_surv = S3), 20) |&gt; \n  kable(align = \"c\", digits = 4)\n\n\n\n\n\ncombo\ntime\nhorTh\ntsize\nprob_surv\n\n\n\n\n3\n8\n0\n120\n1.0000\n\n\n3\n15\n0\n120\n1.0000\n\n\n3\n16\n0\n120\n1.0000\n\n\n3\n17\n0\n120\n1.0000\n\n\n3\n18\n0\n120\n1.0000\n\n\n3\n29\n0\n120\n1.0000\n\n\n3\n42\n0\n120\n1.0000\n\n\n3\n46\n0\n120\n1.0000\n\n\n3\n57\n0\n120\n1.0000\n\n\n3\n63\n0\n120\n1.0000\n\n\n3\n65\n0\n120\n1.0000\n\n\n3\n67\n0\n120\n1.0000\n\n\n3\n71\n0\n120\n1.0000\n\n\n3\n72\n0\n120\n0.9935\n\n\n3\n98\n0\n120\n0.9871\n\n\n3\n113\n0\n120\n0.9807\n\n\n3\n114\n0\n120\n0.9807\n\n\n3\n120\n0\n120\n0.9743\n\n\n3\n148\n0\n120\n0.9743\n\n\n3\n160\n0\n120\n0.9679\n\n\n\n\n\nCode\nhead(data.frame(combo = 4, time = H0$time, horTh = 1, tsize = 120, prob_surv = S4), 20) |&gt; \n  kable(align = \"c\", digits = 4)\n\n\n\n\n\ncombo\ntime\nhorTh\ntsize\nprob_surv\n\n\n\n\n4\n8\n1\n120\n1.0000\n\n\n4\n15\n1\n120\n1.0000\n\n\n4\n16\n1\n120\n1.0000\n\n\n4\n17\n1\n120\n1.0000\n\n\n4\n18\n1\n120\n1.0000\n\n\n4\n29\n1\n120\n1.0000\n\n\n4\n42\n1\n120\n1.0000\n\n\n4\n46\n1\n120\n1.0000\n\n\n4\n57\n1\n120\n1.0000\n\n\n4\n63\n1\n120\n1.0000\n\n\n4\n65\n1\n120\n1.0000\n\n\n4\n67\n1\n120\n1.0000\n\n\n4\n71\n1\n120\n1.0000\n\n\n4\n72\n1\n120\n0.9955\n\n\n4\n98\n1\n120\n0.9911\n\n\n4\n113\n1\n120\n0.9867\n\n\n4\n114\n1\n120\n0.9867\n\n\n4\n120\n1\n120\n0.9822\n\n\n4\n148\n1\n120\n0.9822\n\n\n4\n160\n1\n120\n0.9778\n\n\n\n\n\n\n‚Ä¶and Voil√†.\n\n\nCode\n# Step 5\n# Create 'labeller' for tumour size\ntum_names &lt;- as_labeller(\n  c(`3` = \"Tumour Size = 3 mm\", `120` = \"Tumour Size = 120 mm\"))\n# Plot\nggplot(surv_df, aes(x = time, y = prob_surv, color = factor(horTh))) +\n  geom_line(linewidth = 1) +\n  facet_wrap(tsize ~ ., labeller = tum_names) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.1)) +\n  labs(x = \"Recurrence-Free Survival (Days)\",\n       y = \"Predicted Probability of Survival\",\n       title = \"Probability of Survival as a Function of Hormone Therapy and Tumour Size\",\n       color = \"Hormone Therapy\") +\n  theme_bw(base_size = 20) + theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/033_27Jun_2025/index.html",
    "href": "posts/033_27Jun_2025/index.html",
    "title": "Are your Hazards Proportional? - What to do when it just ain‚Äôt so (Part 1)",
    "section": "",
    "text": "Ok, I‚Äôm going to be a bit melodramatic here, but bear with me. Have you ever felt like crying when your carefully constructed Cox model - one that is giving you the significant ‚Äòeffect/s‚Äô you‚Äôve been dreaming of - fails at the last hurdle when you discover that one (or more) of its covariates violates the assumption of proportional hazards (PH)?\nOr is that just me?\nThe Cox model is ubiquitous in medical research and is typically utilised when questions develop around assessing the time to some outcome of interest. Like any other mathematical model, the Cox model is capable of capturing only a simple reflection of biological systems that are much more complex in reality. To that end assumptions are made about the nature of the data that the model is expected to parse, and the greater the violation of those assumptions, the less faithful one can expect the reflection to be. In other words, the model becomes less and less trustworthy.\nWhere am I headed with my ham-fisted attempt at philosophising? Well, as I‚Äôm sure you know, one of the primary assumptions underpinning the Cox model is that of proportional hazards. I have actually discussed this in one of my previous posts on survival analysis when I introduced the Cox model, but it‚Äôs so important I‚Äôm going to reiterate here."
  },
  {
    "objectID": "posts/033_27Jun_2025/index.html#step-1",
    "href": "posts/033_27Jun_2025/index.html#step-1",
    "title": "Model Prediction/Visualisation  6. Survival Analysis - Cox Model",
    "section": "2.1 Step 1",
    "text": "2.1 Step 1\nThe first thing we need to do is to estimate the cumulative baseline hazard function (H0) when all covariate values are at 0 or at their reference level. We do this using specifying centered = FALSE in the call to basehaz(). It is important to include the centered = F argument as the default setting is TRUE which we don‚Äôt want as this has a different interpretation (i.e.¬†cumulative hazard function estimated at the mean of all covariate values). Also note that the cumulative hazard is estimated at each of the observed times in the dataset."
  },
  {
    "objectID": "posts/033_27Jun_2025/index.html#step-2",
    "href": "posts/033_27Jun_2025/index.html#step-2",
    "title": "Model Prediction/Visualisation  6. Survival Analysis - Cox Model",
    "section": "2.2 Step 2",
    "text": "2.2 Step 2\nWe then estimate the cumulative hazard function (H1 - H4) for each covariate combination of interest. For these data we have four as I have listed above. As an example, to calculate H1 we simply multiply H0 by the covariate combination values and their respective Cox model coefficients. e.g.:\nH1 &lt;- H0$hazard * exp((0 * cox_mod$coefficients[1]) + (3 * cox_mod$coefficients[2]))\nIn the above cox_mod$coefficients[1] gives the coefficient value for hormone therapy and cox_mod$coefficients[2] gives the coefficient value for tumour size. To obtain the cumulative hazard function for the first covariate combination we just multiply the cumulative baseline hazard function by the exponent of the sum of the products of the coefficients and our covariate values of interest (in this case 0 to indicate no hormone therapy and 3 to indicate the tumour size)."
  },
  {
    "objectID": "posts/033_27Jun_2025/index.html#step-3",
    "href": "posts/033_27Jun_2025/index.html#step-3",
    "title": "Model Prediction/Visualisation  6. Survival Analysis - Cox Model",
    "section": "2.3 Step 3",
    "text": "2.3 Step 3\nWe then calculate the survival function for each covariate combination by taking the exponent of the negative of the cumulative hazard function."
  },
  {
    "objectID": "posts/033_27Jun_2025/index.html#step-4",
    "href": "posts/033_27Jun_2025/index.html#step-4",
    "title": "Model Prediction/Visualisation  6. Survival Analysis - Cox Model",
    "section": "2.4 Step 4",
    "text": "2.4 Step 4\nNow we create a dataframe containing the predicted survival probabilities for each covariate combination. The first 20 rows for each covariate combination is shown below."
  },
  {
    "objectID": "posts/033_27Jun_2025/index.html#step-5",
    "href": "posts/033_27Jun_2025/index.html#step-5",
    "title": "Model Prediction/Visualisation  6. Survival Analysis - Cox Model",
    "section": "2.5 Step 5",
    "text": "2.5 Step 5\nThen we create our plot.\n\n\nCode\n# Step 1\n# Estimate the cumulative baseline hazard function\nH0 &lt;- basehaz(cox_mod, centered = F)\n\n# Step 2\n# Calculate cumulative hazard function for covariate combo 1.\nH1 &lt;- H0$hazard * exp((0 * cox_mod$coefficients[1]) + (3 * cox_mod$coefficients[2]))\n# Calculate cumulative hazard function for covariate combo 2.\nH2 &lt;- H0$hazard * exp((1 * cox_mod$coefficients[1]) + (3 * cox_mod$coefficients[2]))\n# Calculate cumulative hazard function for covariate combo 3.\nH3 &lt;- H0$hazard * exp((0 * cox_mod$coefficients[1]) + (120 * cox_mod$coefficients[2]))\n# Calculate cumulative hazard function for covariate combo 4.\nH4 &lt;- H0$hazard * exp((1 * cox_mod$coefficients[1]) + (120 * cox_mod$coefficients[2]))\n\n# Step 3\n# Calculate survival function for covariate combo 1.\nS1 &lt;- exp(-H1)\n# Calculate survival function for covariate combo 2.\nS2 &lt;- exp(-H2)\n# Calculate survival function for covariate combo 3.\nS3 &lt;- exp(-H3)\n# Calculate survival function for covariate combo 4.\nS4 &lt;- exp(-H4)\n\n# Step 4\n# Create dataframe of predicted survival probabilities as a function of covariate combos\nsurv_df &lt;-  rbind(data.frame(combo = 1, time = H0$time, horTh = 0, tsize = 3, prob_surv = S1),\n                  data.frame(combo = 2, time = H0$time, horTh = 1, tsize = 3, prob_surv = S2),\n                  data.frame(combo = 3, time = H0$time, horTh = 0, tsize = 120, prob_surv = S3),\n                  data.frame(combo = 4, time = H0$time, horTh = 1, tsize = 120, prob_surv = S4))\n\n# Inspect each df\nhead(data.frame(combo = 1, time = H0$time, horTh = 0, tsize = 3, prob_surv = S1), 20) |&gt; \n  kable(align = \"c\", digits = 4)\n\n\n\n\n\ncombo\ntime\nhorTh\ntsize\nprob_surv\n\n\n\n\n1\n8\n0\n3\n1.0000\n\n\n1\n15\n0\n3\n1.0000\n\n\n1\n16\n0\n3\n1.0000\n\n\n1\n17\n0\n3\n1.0000\n\n\n1\n18\n0\n3\n1.0000\n\n\n1\n29\n0\n3\n1.0000\n\n\n1\n42\n0\n3\n1.0000\n\n\n1\n46\n0\n3\n1.0000\n\n\n1\n57\n0\n3\n1.0000\n\n\n1\n63\n0\n3\n1.0000\n\n\n1\n65\n0\n3\n1.0000\n\n\n1\n67\n0\n3\n1.0000\n\n\n1\n71\n0\n3\n1.0000\n\n\n1\n72\n0\n3\n0.9989\n\n\n1\n98\n0\n3\n0.9978\n\n\n1\n113\n0\n3\n0.9967\n\n\n1\n114\n0\n3\n0.9967\n\n\n1\n120\n0\n3\n0.9956\n\n\n1\n148\n0\n3\n0.9956\n\n\n1\n160\n0\n3\n0.9945\n\n\n\n\n\nCode\nhead(data.frame(combo = 2, time = H0$time, horTh = 1, tsize = 3, prob_surv = S2), 20) |&gt; \n  kable(align = \"c\", digits = 4)\n\n\n\n\n\ncombo\ntime\nhorTh\ntsize\nprob_surv\n\n\n\n\n2\n8\n1\n3\n1.0000\n\n\n2\n15\n1\n3\n1.0000\n\n\n2\n16\n1\n3\n1.0000\n\n\n2\n17\n1\n3\n1.0000\n\n\n2\n18\n1\n3\n1.0000\n\n\n2\n29\n1\n3\n1.0000\n\n\n2\n42\n1\n3\n1.0000\n\n\n2\n46\n1\n3\n1.0000\n\n\n2\n57\n1\n3\n1.0000\n\n\n2\n63\n1\n3\n1.0000\n\n\n2\n65\n1\n3\n1.0000\n\n\n2\n67\n1\n3\n1.0000\n\n\n2\n71\n1\n3\n1.0000\n\n\n2\n72\n1\n3\n0.9992\n\n\n2\n98\n1\n3\n0.9985\n\n\n2\n113\n1\n3\n0.9977\n\n\n2\n114\n1\n3\n0.9977\n\n\n2\n120\n1\n3\n0.9970\n\n\n2\n148\n1\n3\n0.9970\n\n\n2\n160\n1\n3\n0.9962\n\n\n\n\n\nCode\nhead(data.frame(combo = 3, time = H0$time, horTh = 0, tsize = 120, prob_surv = S3), 20) |&gt; \n  kable(align = \"c\", digits = 4)\n\n\n\n\n\ncombo\ntime\nhorTh\ntsize\nprob_surv\n\n\n\n\n3\n8\n0\n120\n1.0000\n\n\n3\n15\n0\n120\n1.0000\n\n\n3\n16\n0\n120\n1.0000\n\n\n3\n17\n0\n120\n1.0000\n\n\n3\n18\n0\n120\n1.0000\n\n\n3\n29\n0\n120\n1.0000\n\n\n3\n42\n0\n120\n1.0000\n\n\n3\n46\n0\n120\n1.0000\n\n\n3\n57\n0\n120\n1.0000\n\n\n3\n63\n0\n120\n1.0000\n\n\n3\n65\n0\n120\n1.0000\n\n\n3\n67\n0\n120\n1.0000\n\n\n3\n71\n0\n120\n1.0000\n\n\n3\n72\n0\n120\n0.9935\n\n\n3\n98\n0\n120\n0.9871\n\n\n3\n113\n0\n120\n0.9807\n\n\n3\n114\n0\n120\n0.9807\n\n\n3\n120\n0\n120\n0.9743\n\n\n3\n148\n0\n120\n0.9743\n\n\n3\n160\n0\n120\n0.9679\n\n\n\n\n\nCode\nhead(data.frame(combo = 4, time = H0$time, horTh = 1, tsize = 120, prob_surv = S4), 20) |&gt; \n  kable(align = \"c\", digits = 4)\n\n\n\n\n\ncombo\ntime\nhorTh\ntsize\nprob_surv\n\n\n\n\n4\n8\n1\n120\n1.0000\n\n\n4\n15\n1\n120\n1.0000\n\n\n4\n16\n1\n120\n1.0000\n\n\n4\n17\n1\n120\n1.0000\n\n\n4\n18\n1\n120\n1.0000\n\n\n4\n29\n1\n120\n1.0000\n\n\n4\n42\n1\n120\n1.0000\n\n\n4\n46\n1\n120\n1.0000\n\n\n4\n57\n1\n120\n1.0000\n\n\n4\n63\n1\n120\n1.0000\n\n\n4\n65\n1\n120\n1.0000\n\n\n4\n67\n1\n120\n1.0000\n\n\n4\n71\n1\n120\n1.0000\n\n\n4\n72\n1\n120\n0.9955\n\n\n4\n98\n1\n120\n0.9911\n\n\n4\n113\n1\n120\n0.9867\n\n\n4\n114\n1\n120\n0.9867\n\n\n4\n120\n1\n120\n0.9822\n\n\n4\n148\n1\n120\n0.9822\n\n\n4\n160\n1\n120\n0.9778\n\n\n\n\n\n\n‚Ä¶and Voil√†.\n\n\nCode\n# Step 5\n# Create 'labeller' for tumour size\ntum_names &lt;- as_labeller(\n  c(`3` = \"Tumour Size = 3 mm\", `120` = \"Tumour Size = 120 mm\"))\n# Plot\nggplot(surv_df, aes(x = time, y = prob_surv, color = factor(horTh))) +\n  geom_line(linewidth = 1) +\n  facet_wrap(tsize ~ ., labeller = tum_names) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.1)) +\n  labs(x = \"Recurrence-Free Survival (Days)\",\n       y = \"Predicted Probability of Survival\",\n       title = \"Probability of Survival as a Function of Hormone Therapy and Tumour Size\",\n       color = \"Hormone Therapy\") +\n  theme_bw(base_size = 20) + theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/033_27Jun_2025/index.html#goodness-of-fit-gof-tests.",
    "href": "posts/033_27Jun_2025/index.html#goodness-of-fit-gof-tests.",
    "title": "Are your Hazards Proportional? - What to do when it just ain‚Äôt so (Part 1)",
    "section": "4.1 Goodness of Fit (GOF) tests.",
    "text": "4.1 Goodness of Fit (GOF) tests.\nThe classic GOF approach uses a test statistic and p-value to assess the significance of the PH assumption based on the Schoenfeld residuals of the Cox model (I won‚Äôt attempt to explain how these residuals are derived). Visually, if PH hold, then a plot of the scaled Schoenfeld residuals over time (for a particular covariate) should have zero slope. That is, we should ideally see a horizontal line which indicates that the residuals for that covariate are not related to survival time. In R we can use the cox.zph() function to perform a significance test for each covariate and for the model as a whole. Additionally, we can plot() the object returned by the cox.zph() call to visualise the shape of the resulting smoothed (loess) curve fitted to the residuals, for each covariate. It can be very helpful to take note of this functional form as that is how the HR for the covariate is expected to actually vary over time. This also provides a sanity check for any time-varying HR we might calculate later, if we decide to take remediation action in relaxing the assumption of proportionality in our model specification.\nTo test the proportional hazards assumption we first need to fit a Cox model, so let‚Äôs do that now.\n\n\nCode\n# Fit basic model with treatment and age as the only predictors\nvfit1 &lt;- coxph(Surv(time, status) ~ trt + age, vdata1)\ntbl_regression(vfit1, exp = T)\n\n\n\n\n\n\n\n\nCharacteristic\nHR1\n95% CI1\np-value\n\n\n\n\ntrt\n1.00\n0.70, 1.42\n&gt;0.9\n\n\nage\n1.01\n0.99, 1.03\n0.4\n\n\n\n1 HR = Hazard Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nBased on these model results there does not appear to be much to note, as neither covariate shows an association with the risk of death (this may not remain the case in a more complete, fully-adjusted model). But let‚Äôs push ahead anyway and use cox.zph() to test the model fit and plot the results in evaluating PH for each of our covariates.\n\n\nCode\n(sch_res &lt;-  cox.zph(vfit1))\n\n\n       chisq df     p\ntrt     3.67  1 0.055\nage     1.68  1 0.195\nGLOBAL  6.20  2 0.045\n\n\nCode\npar(mfrow = c(1,2))\nplot(sch_res[1], hr = T) # trt\nabline(h = 1, lty = 3, col = \"red\")\nplot(sch_res[2], hr = T) # age\nabline(h = 1, lty = 3, col = \"red\")\n\n\n\n\n\n\n\n\n\nThe hypothesis test for a horizontal line fitted through the residuals for each covariate reveals a non-significant p value for both trt and age, although we would have good justification for doubting the validity of a constant HR for trt. Further evidence for this is provided in the plots - while the smoother is fairly horizontal for age, it is not for trt, which demonstrates an increase in the HR early on followed by a gradual decline after the first or so.\n\n\n\n\n\n\nNote\n\n\n\nNote the red dashed line is meant simply as a reference constant null effect (HR = 1). The hypothesis test is a test against a constant HR (of any value)."
  },
  {
    "objectID": "posts/033_27Jun_2025/index.html#graphical-assessment.",
    "href": "posts/033_27Jun_2025/index.html#graphical-assessment.",
    "title": "Are your Hazards Proportional? - What to do when it just ain‚Äôt so (Part 1)",
    "section": "4.2 Graphical assessment.",
    "text": "4.2 Graphical assessment.\nThis option involves plotting what are referred to as log-log survival curves over the different levels of the covariate. Essentially we take the log of the survival function twice and plot that resulting function. Note that ‚Äúlog-log‚Äù is technically -log(-log survival) which is equivalent to -log(cumulative hazard), so we are really plotting a version of the cumulative hazards, but we won‚Äôt dwell on the detail. The important point is that if hazards are proportional we expect these lines to be approximately parallel and be separated by a constant value of \\(\\beta\\) (i.e.¬†the log HR) over the duration of observation time.\nWe can do this in R for the trt variable using the code shown below.\n\n\nCode\nplot(survfit(Surv(vdata1$time, vdata1$status) ~ vdata1$trt), \n     col = c(\"black\", \"red\"), fun = \"cloglog\", xlab = \"log time\", ylab = \"log-log survival\")\n\n\n\n\n\n\n\n\n\nAn issue with graphical assessment of PH is that it is somewhat subjective and we don‚Äôt have a clear way to determine a significant violation. In this plot, however, I think the result is quite clear and we can assert with reasonable confidence that the HR for trt may not in fact be constant in time."
  },
  {
    "objectID": "posts/033_27Jun_2025/index.html#time-dependent-variables.",
    "href": "posts/033_27Jun_2025/index.html#time-dependent-variables.",
    "title": "Are your Hazards Proportional? - What to do when it just ain‚Äôt so (Part 1)",
    "section": "4.3 Time-dependent variables.",
    "text": "4.3 Time-dependent variables.\nThe last approach to testing for PH on a covariate is to create an interaction between the covariate and time (or some function of it). This allows the ‚Äòeffect‚Äô of the covariate to vary at different times and we can assess the PH assumption by testing for the significance of this interaction term. If the p value is statistically significant this provides some evidence for a time-varying ‚Äòeffect‚Äô of the covariate on survival times (i.e.¬†the HR is NOT constant over time).\n\n\n\n\n\n\nNote\n\n\n\nThe use and interpretation of such an interaction in a Cox model is really no different to that in any model - the main difference is that in this context we are interacting a covariate with time instead of another covariate. Keep in mind of course, that it is possible to include standard covariate-covariate interactions in the Cox model as well.\n\n\nSo, how do we specify this time-covariate interaction term in our Cox model?\nWell there is a trick to this that relies on first knowing two small pieces of very important information, without which you will fall victim to the same naive error that has plagued countless analysts before you.\n\n4.3.1 One simple rule about covariate values in the Cox model\n‚ÄúYou cannot look into the future‚Äù\nTerry Therneau - one of the main authors of the survival package, writes on page 2 of the package‚Äôs time-dependent vignette:\n‚ÄúThe key rule for time dependent covariates in a Cox model is simple and essentially the same as that for gambling: you cannot look into the future. A covariate may change in any way based on past data or outcomes, but it may not reach forward in time.‚Äù\nAnd on page 101 of Applied Survival Analysis Using R:\n‚ÄúThe partial likelihood theory for survival data, introduced in Chap. 5, allows one to model survival times while accommodating covariate information. An important caveat to this theory is that the values of the covariates must be determined at time t = 0, when the patient enters the study, and remain constant thereafter. This issue arises with survival data because such data evolve over time, and it would be improper to use the value a covariate to model survival information that is observed before the covariate‚Äôs value is known. To accommodate covariates that may change their value over time (‚Äútime dependent covariates‚Äù), special measures are necessary to obtain valid parameter estimates. An intervention that occurs after the start of the trial, or a covariate (such as air pollution exposure) that changes values over the course of the study are two examples of time dependent variables.\nThe rule is clear: we cannot predict survival using covariate values from the future.‚Äù\nAnd from Logical and statistical fallacies in the use of Cox regression models:\n‚ÄúA potential source of error involving the use of future information arises from the improper use of covariates in the Cox model‚Ä¶\n‚Ä¶For example, we discuss why one should almost never use a covariate that has been averaged over a patient‚Äôs entire follow-up time as a baseline covariate. Instead, the baseline value should be used as a covariate, or the cumulative average up to each point in time should be used as a time-dependent covariate.‚Äù\nYou may wonder why I am telling you this? The reason is that I think it‚Äôs a common error for the naive analyst to use future covariate information in survival models. It is particularly salient to be aware of this when we are now talking about constructing a covariate-time interaction term. Because if I asked you how you would do this, I bet you a million dollars you would tell me something like ‚ÄúWell, I‚Äôll just interact each persons event/censoring time with the covariate we want to test‚Äù. In formula terms:\nvfit1 &lt;- coxph(Surv(time, status) ~ trt + age + trt:time, vdata1)\nWell you would be wrong, because in doing this, you are breaking that cardinal rule of not looking into the future (I‚Äôll show you why shortly).\nWith regards to this erroneous model specification, Terry Therneau writes on page 21 of the time-dependent vignette:\n‚ÄúThis mistake has been made often enough the the coxph routine has been updated to print an error message for such attempts. The issue is that the above code does not actually create a time dependent covariate, rather it creates a time-static value for each subject based on their value for the covariate time; no differently than if we had constructed the variable outside of a coxph call. This variable most definitely breaks the rule about not looking into the future, and one would quickly find the circularity: large values of time appear to predict long survival because long survival leads to large values for time.‚Äù\n\n\n4.3.2 Event times are all-important in the Cox model\nThe second concept that you may have come across in your readings on survival analysis (but I would bet you probably don‚Äôt remember) is that event times are where the action is at in estimating the Cox model. At each event time the covariate values for the individual experiencing the event are compared against the values in place for all the individuals at risk at that time (i.e.¬†in the ‚Äúrisk set‚Äù). Not in the past, and certainly not in the future, but at that time. Regression coefficients (i.e.¬†log HRs) are then estimated as a function of these covariate values.\n\n\n4.3.3 Putting it all together\nSo, now that we know the rule about not using future covariate information and also the importance of event times in Cox model estimation, how do we use this information to construct the correct time-covariate interaction we alluded to above in assessing the validity of PH?\nThe ‚Äútrick‚Äù is to split our data at every event time. In doing this we create a new dataset where we essentially have multi-row (counting process) data for each person. Each person‚Äôs observation time is split into periods defined by all event times, for whatever time that person remains under observation. As an example, let‚Äôs look at just the first 3 patients in the veteran dataset:\n\n\nCode\n# Create new df with just the first 3 subjects\nvdata1_first3 &lt;-  vdata1 |&gt; \n  slice(1:3) \nvdata1_first3 |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nid\ntime\nstatus\ntrt\nage\n\n\n\n\n1\n72\n1\n0\n69\n\n\n2\n411\n1\n0\n64\n\n\n3\n228\n1\n0\n38\n\n\n\n\n\nEach of these patients experiences the event. We can use the survSplit() function to split each persons observation time by all events. When we do this we get:\n\n\nCode\n# Split time at every event and create vector of unique event times\nevent_times &lt;- sort(unique(with(vdata1_first3, time[status == 1])))\n# Create new df in CP form with splits at every event time\nvdata2_first3 &lt;- survSplit(Surv(time, status) ~., vdata1_first3, cut = event_times)\nvdata2_first3 |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nid\ntrt\nage\ntstart\ntime\nstatus\n\n\n\n\n1\n0\n69\n0\n72\n1\n\n\n2\n0\n64\n0\n72\n0\n\n\n2\n0\n64\n72\n228\n0\n\n\n2\n0\n64\n228\n411\n1\n\n\n3\n0\n38\n0\n72\n0\n\n\n3\n0\n38\n72\n228\n1\n\n\n\n\n\nYou can see that Subject 1 has the shortest observation time (72 days) and given there were no other event times in that period, their data remains the same. Subject 2, however, has the longest observation time (411 days) and given there are 3 events and they are at in the risk set for all of those, their observation time is split into 3 periods defined by those event timings. Subject 3 has an observation time of 228 days and as they are only at risk therefore for two of the three events, their observation time is split into 2 periods. And so on - hopefully you are starting to get the picture.\n\n\n\n\n\n\nNote\n\n\n\nOne issue to be aware of with splitting time at every event is that it can lead to some very large datasets!\n\n\nIn this data manipulation trick we are emulating single-row/person data with multi-row/person data, but the specifications are equivalent as the covariate values remain the same for each individual. The point is that we now have a data set where the risk sets used to calculate the covariate-time interaction respect the current time, not a potentially future time defined by the end of observation (i.e.¬†event/censoring) as we would have used in the naive analysis.\nAfter all of that, let‚Äôs finally run this model. We take our 137 patients and use survSplit to create an expanded dataset whereby observation time for each patient is split at all death times. We then use a trt:time interaction to assess whether PH may hold over the duration of observation.\n\n\nCode\n# Split time at every event and create vector of unique event times\nevent_times &lt;- sort(unique(with(vdata1, time[status == 1])))\n# Create new df in CP form with splits at every event time\nvdata2 &lt;- survSplit(Surv(time, status) ~., vdata1, cut = event_times)\n# Fit Cox model with trt:time interaction (current times)\nvfit2 &lt;- coxph(Surv(tstart, time, status) ~ trt + age + trt:time, vdata2)\ntbl_regression(vfit2, exp = T)\n\n\n\n\n\n\n\n\nCharacteristic\nHR1\n95% CI1\np-value\n\n\n\n\ntrt\n1.43\n0.88, 2.33\n0.15\n\n\nage\n1.01\n0.99, 1.03\n0.4\n\n\ntrt * time\n1.00\n0.99, 1.00\n0.030\n\n\n\n1 HR = Hazard Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nThe p value for the interaction term is 0.03 and so by this third method we may also surmise that the hazards for trt may not be proportional over time.\nOut of interest - what would we have gotten in the naive analysis - i.e.¬†where we create the interaction term using the event/censoring time?\n\n\nCode\n# Fit (naive) Cox model with trt:time interaction (future times)\nvfit3 &lt;- coxph(Surv(time, status) ~ trt + age + trt:time, vdata1)\ntbl_regression(vfit3, exp = T)\n\n\n\n\n\n\n\n\nCharacteristic\nHR1\n95% CI1\np-value\n\n\n\n\ntrt\n8.06\n4.37, 14.9\n&lt;0.001\n\n\nage\n1.01\n0.99, 1.03\n0.4\n\n\ntrt * time\n0.99\n0.98, 0.99\n&lt;0.001\n\n\n\n1 HR = Hazard Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nHmmm - quite different‚Ä¶ and quite wrong!\nThere is actually a quicker way to run this equivalent model without needing to first split the data, but I wanted to show you the ‚Äúfirst-principles‚Äù approach, first. Again, on page 21 of the time-dependent vignette, a general method is outlined which relies on the time-transform functionality of coxph(). Using this model specification, you will see that we end up with the same result.\n\n\nCode\nvfit3 &lt;- coxph(Surv(time, status) ~ trt + age + tt(trt), tt = function(x,t,...) x*t, vdata1)\ntbl_regression(vfit3, exp = T)\n\n\n\n\n\n\n\n\nCharacteristic\nHR1\n95% CI1\np-value\n\n\n\n\ntrt\n1.43\n0.88, 2.33\n0.15\n\n\nage\n1.01\n0.99, 1.03\n0.4\n\n\ntt(trt)\n1.00\n0.99, 1.00\n0.030\n\n\n\n1 HR = Hazard Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "posts/033_27Jun_2025/index.html#tvc-time-varying-covariate",
    "href": "posts/033_27Jun_2025/index.html#tvc-time-varying-covariate",
    "title": "Are your Hazards Proportional? - What to do when it just ain‚Äôt so (Part 1)",
    "section": "3.1 tvc (time-varying covariate)",
    "text": "3.1 tvc (time-varying covariate)\nIn perhaps the more common usage, ‚Äútvc‚Äù can represent a time-varying covariate (also known as time-dependent covariate (tdc)). This is a covariate whose values change over the duration of one‚Äôs observation period, but whose ‚Äòeffect‚Äô on survival typically remains constant. A good example here would be different time periods in which a person received treatment or not. Time-varying covariates are constructed as multi-row (i.e.¬†counting process) data where each row contains a non-overlapping time period defined by a different treatment covariate value. PH may still be assessed with counting process data and if no violation is noted, a constant HR over the duration of observation may be assumed."
  },
  {
    "objectID": "posts/033_27Jun_2025/index.html#tvc-time-varying-coefficient",
    "href": "posts/033_27Jun_2025/index.html#tvc-time-varying-coefficient",
    "title": "Are your Hazards Proportional? - What to do when it just ain‚Äôt so (Part 1)",
    "section": "3.2 tvc (time-varying coefficient)",
    "text": "3.2 tvc (time-varying coefficient)\nNow, without intending to create confusion, ‚Äútvc‚Äù may also represent a time-varying coefficient (also known as time-dependent coefficient (tdc)). This is associated with a covariate whose baseline values remain constant over the duration of observation, but whose ‚Äòeffect‚Äô on the risk of an event can indeed change with time. In other words, the HR varies with time. Data are typically formatted as one-row per person."
  },
  {
    "objectID": "posts/033_27Jun_2025/index.html#well-is-it-tvc-or-tvc",
    "href": "posts/033_27Jun_2025/index.html#well-is-it-tvc-or-tvc",
    "title": "Are your Hazards Proportional? - What to do when it just ain‚Äôt so (Part 1)",
    "section": "3.3 Well is it tvc or tvc?",
    "text": "3.3 Well is it tvc or tvc?\nIn a survival analysis, one may have:\n\nNeither time-varying covariates nor time-varying coefficients (single-row/person, constant HR).\nTime-varying coefficients but not time-varying covariates (single-row/person, time-varying HR).\nTime-varying covariates but not time-varying coefficients (multi-row/person, constant HR).\nTime-varying coefficients and time-varying covariates (multi-row/person, time-varying HR).\n\nIn today‚Äôs post, the examples and discussion I provide will only focus on scenario 2. The addition of time-varying covariates into analyses with time-varying coefficients (scenario 4) complicates matters significantly and I‚Äôve made the decision not to tackle this for the time being (although I may come back to this in a later post).\nBut now that we‚Äôve cleared that up, let‚Äôs get back to the issue at hand."
  },
  {
    "objectID": "posts/034_25Jul_2025/index.html",
    "href": "posts/034_25Jul_2025/index.html",
    "title": "Are your Hazards Proportional? - What to do when it just ain‚Äôt so (Part 2)",
    "section": "",
    "text": "Welcome back to Stats Tips after a brief mid-year break.\nIn the last post we discussed some approaches to the diagnosis of non-proportional hazards for a covariate in our Cox model.\nAnd the big question I‚Äôm sure you‚Äôre chomping at the bit to ask is ‚ÄúWell, what can we do about it?‚Äù\nOk, relax‚Ä¶ there are in fact several things we can do. Today we are going to consider the remediation actions we can take when we are faced with this issue in our day-to-day work. To get us started and to introduce the concepts to you, I‚Äôve broken those actions down broadly into four groups:\nIn providing examples of how we might implement each of these in R we will return to the veteran dataset that we used in the last post, so get that ready using the code below.\nCode\nlibrary(survival)\nlibrary(tidyverse)\nlibrary(survminer)\nlibrary(gtsummary)\nlibrary(kableExtra)\n# Load veteran data from survival package\nvdata1 &lt;-  veteran\n# Make treatment variable equal to 0,1 instead of 1,2 (not necessary, but I just like to work with the former)\nvdata1$trt &lt;-  as.numeric(vdata1$trt) - 1\n# Create id variable and organise columns\nvdata1$id &lt;-  seq(1:dim(vdata1)[1])\nvdata1 &lt;-  vdata1 |&gt; \n  select(id, time, status, trt, age)\nLet‚Äôs talk about these ideas in more detail now."
  },
  {
    "objectID": "posts/034_25Jul_2025/index.html#step-functions-different-hrs-over-different-time-periods.",
    "href": "posts/034_25Jul_2025/index.html#step-functions-different-hrs-over-different-time-periods.",
    "title": "Are your Hazards Proportional? - What to do when it just ain‚Äôt so (Part 2)",
    "section": "3.1 Step functions (different HR‚Äôs over different time periods).",
    "text": "3.1 Step functions (different HR‚Äôs over different time periods)."
  },
  {
    "objectID": "posts/034_25Jul_2025/index.html#continuous-time-dependent-coefficients-smoothed-hr-over-time.",
    "href": "posts/034_25Jul_2025/index.html#continuous-time-dependent-coefficients-smoothed-hr-over-time.",
    "title": "Are your Hazards Proportional? - What to do when it just ain‚Äôt so (Part 2)",
    "section": "3.2 Continuous time-dependent coefficients (smoothed HR over time).",
    "text": "3.2 Continuous time-dependent coefficients (smoothed HR over time)."
  },
  {
    "objectID": "posts/034_25Jul_2025/index.html#step-functions",
    "href": "posts/034_25Jul_2025/index.html#step-functions",
    "title": "Are your Hazards Proportional? - What to do when it just ain‚Äôt so (Part 2)",
    "section": "3.1 Step functions",
    "text": "3.1 Step functions\nTo help decide how we might define time periods for our trt variable to estimate different HR‚Äôs within, it is helpful to inspect the output of our cox.zph() plot, so let‚Äôs revisit that now.\n\n\nCode\nsch_res &lt;-  cox.zph(vfit1)\nplot(sch_res[1], hr = T) # trt\nabline(h = 1, lty = 3, col = \"red\")\n\n\n\n\n\n\n\n\n\nThe decision is ultimately arbitrary but I would probably divide this into two periods splitting time at about 100 days. My thinking on this is that the HR is larger within that first 100 days and then gradually declines thereafter. While it also appears non-monotonic (increasing and decreasing) in that early period, we can still aim to obtain an averaged effect over that time and then contrast that to the later period (this bit is not an exact science).\nTo estimate this model we return to our trusty survSplit() function to split time for each individual at the desired points (here 100 days) and then create an interaction term between the offending covariate and the time strata. We do this like so:\nvfit4 &lt;- coxph(Surv(tstart, time, status) ~ age + trt:strata(tgroup), data = vdata2)\n\n\nCode\n# Create new df in CP form with splits at 100 days\nvdata2 &lt;- survSplit(Surv(time, status) ~., vdata1, cut = 100, episode = \"tgroup\")\n# Cox model with stepped time-dependent coefficients\nvfit4 &lt;- coxph(Surv(tstart, time, status) ~ age + trt:strata(tgroup), data = vdata2)\ntbl_regression(vfit4, exp = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nHR1\n95% CI1\np-value\n\n\n\n\nage\n1.01\n0.99, 1.03\n0.4\n\n\ntrt * strata(tgroup)\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†trt * tgroup=1\n1.46\n0.93, 2.29\n0.10\n\n\n¬†¬†¬†¬†trt * tgroup=2\n0.49\n0.26, 0.92\n0.025\n\n\n\n1 HR = Hazard Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nSo, in parameterising the model this way we estimate an average HR for the ‚Äòeffect‚Äô of trt to be ~ 1.46 within the first 100 day period and and an average HR of ~ 0.49 thereafter. That fits in fairly nicely with what we see in the above plot.\nThe related test of PH gives:\n\n\nCode\ncox.zph(vfit4)\n\n\n                   chisq df    p\nage                 2.67  1 0.10\ntrt:strata(tgroup)  1.40  2 0.50\nGLOBAL              3.94  3 0.27\n\n\nSection 4.1 of the time-dependent vignette provides another example of the use of step functions and would be worth some of your time to read."
  },
  {
    "objectID": "posts/034_25Jul_2025/index.html#continuous-time-dependent-coefficients",
    "href": "posts/034_25Jul_2025/index.html#continuous-time-dependent-coefficients",
    "title": "Are your Hazards Proportional? - What to do when it just ain‚Äôt so (Part 2)",
    "section": "3.2 Continuous time-dependent coefficients",
    "text": "3.2 Continuous time-dependent coefficients\nIn evolving the step function approach we can think of chopping up time into smaller and smaller periods, to the point where we essentially end up estimating a continuous HR function over time. Of course, we don‚Äôt do this by manually splitting our data - that would be an impossible task. Luckily the coxph() function has the time-transform argument built into it, which I touched on briefly in the last post (more information can be found in Section 4.2 of the above vignette). This is our secret weapon in specifying time-dependent coefficients in the Cox model.\nThere are as many time transforms (functions of time) to select from as there are that you can think of - the choice is essentially endless. The general formulation of the time transform argument - tt() within a coxph() call (and using our model as an example) can be coded as:\ncoxph(Surv(time, status) ~ trt + age + tt(trt), tt = function(x,t,...) x * t, vdata1)\nThis represents a covariate-time interaction where we assume that the association between the covariate and the risk of the event is moderated as a linear function of time. As mentioned there are many other functions you could select, which you could then test in comparison to other functions and select the more appropriate model accordingly. Alternatively, you could just save yourself the effort and specify the most flexible time transform possible - one that involves fitting a restricted cubic spline to the interaction term. To do this we can use the nsk() function within the tt() argument and a general formulation of this model can be coded as:\ncoxph(Surv(time, status) ~ trt + age + tt(trt), tt = function(x, t, ...) x * nsk(t, df = 3), vdata1)\nIf we choose this option we are once again faced with the dilemma of interpreting model coefficients in the presence of spline terms. Let‚Äôs fit this model and look at the output:\nvfit5 &lt;- coxph(Surv(time, status) ~ trt + age + tt(trt), tt = function(x, t, ...) x * nsk(t, df = 3), vdata1)\n\n\nCode\n# Cox model with continuous time-dependent coefficient\nvfit5 &lt;- coxph(Surv(time, status) ~ trt + age + tt(trt), tt = function(x, t, ...) x * nsk(t, df = 3), vdata1)\ntbl_regression(vfit5, exp = T)\n\n\n\n\n\n\n\n\nCharacteristic\nHR1\n95% CI1\np-value\n\n\n\n\ntrt\n0.72\n0.24, 2.14\n0.6\n\n\nage\n1.01\n0.99, 1.03\n0.4\n\n\ntt(trt)1\n2.27\n0.72, 7.22\n0.2\n\n\ntt(trt)2\n2.10\n0.54, 8.23\n0.3\n\n\ntt(trt)3\n0.83\n0.26, 2.68\n0.8\n\n\n\n1 HR = Hazard Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nYep, we can‚Äôt easily interpret those coefficients for trt, and how the commensurate HR varies over time. Coefficients produced by models using simpler functions of time may be more amenable to direct interpretation but this is the trade-off when we choose splines to allow maximal flexibility in functional form estimation. As usual, however, we can plot these model results to assist us in visualising how the HR varies in a time-dependent manner. Unfortunately (yes, I know - there always seems to be a catch), we have to use the equivalent ‚Äúfirst-principles‚Äù specification of the model that I introduced you to in the last post, to facilitate this. We can‚Äôt easily plot the results of a model containing a time-transform function. Instead, we first split our data at every event and then estimate the model using the same spline formulation as above. Let‚Äôs see what the equivalent model estimated from the counting process formulation of the equivalent single-row/person dataset, gives us:\nvfit6 &lt;- coxph(Surv(tstart, time, status) ~ trt + age + trt:nsk(time, df = 3), vdata3)\n\n\nCode\n# Vector of unique event times\nevent_times &lt;- sort(unique(with(vdata1, time[status == 1])))\n# Create new df in CP form with splits at every event time\nvdata3 &lt;- survSplit(Surv(time, status) ~., vdata1, cut = event_times)\n# Fit Cox model with splined trt:time interaction (current times)\nvfit6 &lt;- coxph(Surv(tstart, time, status) ~ trt + age + trt:nsk(time, df = 3), vdata3)\ntbl_regression(vfit6, exp = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nHR1\n95% CI1\np-value\n\n\n\n\ntrt\n0.72\n0.24, 2.14\n0.6\n\n\nage\n1.01\n0.99, 1.03\n0.4\n\n\ntrt * nsk(time, df = 3)\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†trt * NA\n2.27\n0.72, 7.22\n0.2\n\n\n¬†¬†¬†¬†trt * NA\n2.10\n0.54, 8.23\n0.3\n\n\n¬†¬†¬†¬†trt * NA\n0.83\n0.26, 2.68\n0.8\n\n\n\n1 HR = Hazard Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nWe see that the model output is identical comparing both vfit5 and vfit6. This is entirely expected seeing the time-transform function splits each person‚Äôs single-row data at all event times in the background as part of the estimation process in vfit5 (we just don‚Äôt see it). But that is exactly what we have replicated when we apply trt:nsk(time, df = 3) to vfit6 in the manually split dataset.\nOne final thing to note is that we can‚Äôt use cox.zph() on models with tt() terms, and that is a design choice by the package author.\nPH test on vfit5 gives:\n\n\nCode\n# cox.zph(vfit5)\nprint(\"Error in cox.zph(vfit5) : function not defined for models with tt() terms\")\n\n\n[1] \"Error in cox.zph(vfit5) : function not defined for models with tt() terms\"\n\n\nAnd on vfit6:\n\n\nCode\ncox.zph(vfit6)\n\n\n                      chisq df    p\ntrt                    1.92  1 0.17\nage                    2.47  1 0.12\ntrt:nsk(time, df = 3)  4.50  3 0.21\nGLOBAL                 9.53  5 0.09\n\n\n\n3.2.1 Plot time-varying HR\nNow let‚Äôs get back to plotting the HR for trt as a function of time. The code to do this is shown below, but to explain in words:\n\nWe start by creating a new dataframe based on a grid of observation times spanning 1000 days, a fixed value of 1 for trt, and a ‚Äòplaceholder‚Äô value of 0 for age (we set this to 0 because the reference is 0 for all covariates in the model and we only want to predict the HR for a one-unit increase in trt - not for any additional ‚Äòeffect‚Äô of age).\nWe then pass that new data to the predict() function asking to predict type = \"risk\" for trt at the specified value of 1, at each new observation time - this effectively estimates the HR at each new observation time.\nWe then plot those predictions.\n\n\n\n\n\n\n\nNote\n\n\n\nYou may be wondering why I am plotting the HR on a log scale. The reason is that coefficients from the Cox model are estimated on the log-hazard scale, a linear scale in which the estimates have (asymptotically) normal distributions. Plotting HR‚Äôs on the same additive scale can result in misleading visual impressions of the magnitude of effect variation. Thus, it is in general better practice to display exponentiated effect estimates such as the HR on their native multiplicative scale and this can be achieved with a log-transform of the scale axis.\n\n\n\n\nCode\n# Create newdat df\ntdata &lt;- expand.grid(trt = 1, age = 0, time = seq(0, 1000, by = 5))\n# Predict HR \nyhat &lt;- predict(vfit6, newdata = tdata, se.fit = TRUE, type = \"risk\", reference = \"zero\")\ntdata$fit &lt;-  yhat$fit\n# Plot\nggplot(tdata, aes(x = time, y = fit)) + \n  geom_line(col = '#F8766D') +\n  scale_x_continuous(breaks = seq(0, 400, by = 50), limits = c(0, 400)) +\n  scale_y_continuous(trans = \"log\", breaks = c(0, 0.5, 1, 1.5, 2)) +\n  xlab(\"Observation Time\") + ylab(\"HR for Treatment\") + ggtitle(\"Cox Model - Time-varying HR for Treatment\") +\n  theme_bw(base_size = 20) \n\n\n\n\n\n\n\n\n\nIn the above plot I have decided to truncate observation time at 400 days, to make the x-axis appear less compressed (and given nothing much interesting happens after that). The plot suggests that the HR for trt initially increases, reaches a maximum at about 40 days and then gradually declines. We can replot the Schoenfeld residuals and associated smoothing function (below) as a means to contrast the functional forms for this time-varying effect. It is a little difficult to directly compare them due to some differences in the axis scaling, but we obtain the general impression that both plots display a similar time-varying form for the HR of trt. This gives us confidence that the time-varying HR we have estimated is not an unreasonable result.\n\n\nCode\nsch_res &lt;-  cox.zph(vfit1)\nplot(sch_res[1], hr = T) # trt\nabline(h = 1, lty = 3, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n3.2.2 Estimate HR at times of interest\nWe can take this further and actually calculate HR‚Äôs (and their 95% C.I.‚Äôs) at desired times of interest. Say we want to know the HR for trt at 5, 40, 100 and 400 days. We basically follow the same steps as above in terms of predicting, but instead of type = \"risk\" we specify type = \"lp\" which calculates the ‚Äúlinear predictor‚Äù or the log hazard ratio. We do this so we can add/subtract 1.96 times the standard error, then exponentiating the results to obtain the upper and lower 95% confidence limits. The code is shown below:\n\n\nCode\n# Create newdat df with times of interest to predict at\ntdata2 &lt;- expand.grid(trt = 1, age = 0, time = c(5, 40, 100, 400))\n# Predict HR and plot\nyhat &lt;- predict(vfit6, newdata = tdata2, se.fit = TRUE, type = \"lp\", reference = \"zero\")\n# Calculate HR's and 95% CI's\nestimates &lt;-  data.frame(Time = c(5, 40, 100, 400),\n                         HR = exp(yhat$fit), \n                         `Lower CI` = exp(yhat$fit - (1.96*yhat$se.fit)),\n                         `Upper CI` = exp(yhat$fit + (1.96*yhat$se.fit)))\nestimates |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nTime\nHR\nLower.CI\nUpper.CI\n\n\n\n\n5\n0.80\n0.30\n2.10\n\n\n40\n1.92\n0.95\n3.88\n\n\n100\n0.93\n0.52\n1.67\n\n\n400\n0.38\n0.12\n1.19"
  },
  {
    "objectID": "posts/034_25Jul_2025/index.html#accelerated-failure-time-model.",
    "href": "posts/034_25Jul_2025/index.html#accelerated-failure-time-model.",
    "title": "Are your Hazards Proportional? - What to do when it just ain‚Äôt so (Part 2)",
    "section": "4.1 Accelerated failure time model.",
    "text": "4.1 Accelerated failure time model."
  },
  {
    "objectID": "posts/034_25Jul_2025/index.html#flexible-parametric-royston-parmar-survival-model.",
    "href": "posts/034_25Jul_2025/index.html#flexible-parametric-royston-parmar-survival-model.",
    "title": "Are your Hazards Proportional? - What to do when it just ain‚Äôt so (Part 2)",
    "section": "4.2 Flexible parametric (Royston-Parmar) survival model.",
    "text": "4.2 Flexible parametric (Royston-Parmar) survival model."
  },
  {
    "objectID": "posts/034_25Jul_2025/index.html#accelerated-failure-time-model",
    "href": "posts/034_25Jul_2025/index.html#accelerated-failure-time-model",
    "title": "Are your Hazards Proportional? - What to do when it just ain‚Äôt so (Part 2)",
    "section": "4.1 Accelerated failure time model",
    "text": "4.1 Accelerated failure time model\nAccelerated failure time (AFT) models provide an alternative parameterisation to their PH counterparts. Whereas a PH model assumes that the ‚Äòeffect‚Äô of a covariate is to multiply the hazard by some constant, an AFT model assumes that the ‚Äòeffect‚Äô of a covariate is to multiply the time to the event by some constant - in other words, accelerate or decelerate survival time. Rather than a HR, one calculates a Time Ratio (TR), with a TR &gt; 1 indicating that the covariate accelerates survival time (longer survival) and a TR &lt; 1 indicating that the covariate decelerates survival time (shorter survival).\nFitting an AFT survival model can be achieved in R using the survreg() function in the survival package. The standard distributions one can select to model survival times include the weibull, exponential (a special case of the weibull distribution), gaussian, logistic, lognormal, and loglogistic. It is important to note that the weibull and exponential distributions have both a PH and an AFT interpretation, so it is therefore NOT theoretically sound to select one of these distributions - even in the AFT parameterisation - as an alternative to the Cox model when you already know that the PH assumption has been violated. When we want to consider an AFT model as an alternative for failed PH, we should select a distribution that has exclusively AFT properties, and the lognormal seems to be a reasonable place to start.\nTo be honest I have never had need to fit one of these models and so I have little experience in their application - I‚Äôm not providing a worked example for that reason. But that doesn‚Äôt mean you shouldn‚Äôt consider them if the need should arise. There are many advantages that parametric survival models have over the Cox model and some of these are outlined in my previous post. A notable disadvantage, however, is the need to actually select a survival distribution and this requires some idea of what you think the baseline hazard should look like - something that isn‚Äôt always easy to know. Therefore, when I decide to move beyond the Cox model, I tend to go straight to Royston-Parmar models, which I believe incorporate the best of both semi-parametric and parametric survival models. Let‚Äôs take a look at them now."
  },
  {
    "objectID": "posts/034_25Jul_2025/index.html#flexible-parametric-royston-parmar-survival-model",
    "href": "posts/034_25Jul_2025/index.html#flexible-parametric-royston-parmar-survival-model",
    "title": "Are your Hazards Proportional? - What to do when it just ain‚Äôt so (Part 2)",
    "section": "4.2 Flexible parametric (Royston-Parmar) survival model",
    "text": "4.2 Flexible parametric (Royston-Parmar) survival model\nI suspect flexible parametric survival (also known as Royston-Parmar [RP]) models are both underutilised and underrated in the statistical and research communities. But I think they are great and I am trying to incorporate them more into any survival work that I do. Unfortunately there are not a lot of online resources out there on RP models but here, here, here and here are good places to start.\nThe basis of the RP model is the use of a restricted cubic spline to model the log cumulative baseline hazard function. This provides as much flexibility as the Cox model in the estimation of the baseline hazard while still being fully specified (and not being constrained to a specific distribution as in the standard parametric modelling approach). The immediate benefits are clear - we don‚Äôt need to worry about choosing a particular distribution for our survival times (advantage of the Cox model), yet we can easily predict any survival quantity we want (advantage of the parametric model), explicitly because the model returns the shape of the (unassumed) baseline hazard function. It has been said that ‚Äú‚Ä¶if all you are interested in is the HR, than the Cox model is for you‚Äù (I just can‚Äôt remember where). Parametric models can do this and more, making the estimation of not only the HR but additional survival quantities such as survival functions, hazard functions, restricted mean survival times, etc, relatively straightforward. Now, having said all of that I‚Äôm not meaning to imply that we should never use the Cox model, but sometimes it‚Äôs just helpful to know that there are other options available available to use.\nI tend to do most of RP modelling work in Stata using the stpm3 function, but the equivalent go to package in R is rstpm2. Some time ago I performed an analysis in both environments and ended up with the same results, so I think you should be fine using the latter if you don‚Äôt have access to Stata. Ok, enough talking - let‚Äôs actually run an RP model incorporating a time-varying effect for our trt variable.\nThe specification of the stpm2 model is relatively straightforward, but there are two spline terms that you need to set the degrees of freedom (df) for. The first is the log cumulative baseline hazard function which determines the overall shape of the baseline hazard - essentially the foundation of the model upon which all estimation is based. This is done with the primary df argument. Then if we have a covariate that fails PH testing and want to incorporate a time-varying effect for it, we do that with the tvc argument. In models that incorporate splines in any context, there is always the question of ‚ÄúHow wiggly do I make them?‚Äù and that is determined by the df. I tend to follow the advice published in this paper by some of the experts in this modelling framework:\n‚ÄúOur usual starting point is to use 5 degrees of freedom for the baseline and 3 degrees of freedom for any time-dependent effects. This is likely to capture most complex shapes. We would then inspect the AIC and BIC for a selection of models with more and less degrees of freedom to see if we had strong evidence of needing a more simple or more complex model.‚Äù\nBased on this, for our model at hand I have ended up specifying df = 3 on both the baseline and the time-dependent effect for trt:\nvfit7 &lt;- stpm2(Surv(time, status) ~ trt + age, data = vdata1, df = 3, tvc = list(trt = 3))\nUnfortunately tbl_regression() does not play nicely with rstpm2 so I will just have to show you the raw model output as is (forgive the ugliness):\n\n\nCode\n# RP model with time-varying effect on trt\nlibrary(rstpm2)\nvfit7 &lt;- stpm2(Surv(time, status) ~ trt + age, data = vdata1, df = 3, tvc = list(trt = 3))\nsummary(vfit7)\n\n\nMaximum likelihood estimation\n\nCall:\nstpm2(formula = Surv(time, status) ~ trt + age, data = vdata1, \n    df = 3, tvc = list(trt = 3))\n\nCoefficients:\n                                          Estimate Std. Error z value     Pr(z)\n(Intercept)                             -6.8125275  1.4084469 -4.8369 1.319e-06\ntrt                                      1.4354501  1.6239300  0.8839   0.37673\nage                                      0.0097779  0.0095828  1.0204   0.30756\nnsx(log(time), df = 3)1                  4.3985534  0.8131604  5.4092 6.330e-08\nnsx(log(time), df = 3)2                 12.1211268  2.5626905  4.7298 2.247e-06\nnsx(log(time), df = 3)3                  6.1735733  0.7030626  8.7810 &lt; 2.2e-16\nnsx(log(time), df = 3)1:as.numeric(trt) -0.3108192  1.0381000 -0.2994   0.76463\nnsx(log(time), df = 3)2:as.numeric(trt) -3.4411331  3.2041055 -1.0740   0.28283\nnsx(log(time), df = 3)3:as.numeric(trt) -1.6826010  0.8625165 -1.9508   0.05108\n                                           \n(Intercept)                             ***\ntrt                                        \nage                                        \nnsx(log(time), df = 3)1                 ***\nnsx(log(time), df = 3)2                 ***\nnsx(log(time), df = 3)3                 ***\nnsx(log(time), df = 3)1:as.numeric(trt)    \nnsx(log(time), df = 3)2:as.numeric(trt)    \nnsx(log(time), df = 3)3:as.numeric(trt) .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n-2 log L: 1482.644 \n\n\nThis is no easier to interpret than other model with spline terms included and as usual it is better if we plot the model results. Prediction following stpm2 is a little different to that for the Cox model but essentially does the same thing. We first specify a new dataframe to predict on and then use the exposed argument to show the difference in the covariate values that we want to estimate an effect for.\n\n\nCode\n# Predict HR\n# Note that it doesn't matter what we set the age values to in this case as there is no interaction between trt and age (i.e. the HR for trt is the same regardless of age). But we need to specify something otherwise predict will complain.\npred_hrs &lt;- predict(vfit7, type = \"hr\",\n                    newdata = data.frame(time = seq(0, 1000, by = 5), trt = 0, age = 20), # age value = placeholder\n                    exposed = function(data) transform(data, trt = 1, age = 20), # age value = placeholder\n                    full = TRUE, se.fit = TRUE)\n# Now plot\nggplot(pred_hrs, aes(x = time, y = Estimate)) + \n  geom_line(col = '#00BFC4') + \n  scale_x_continuous(breaks = seq(0, 400, by = 50), limits = c(0, 400)) +\n  scale_y_continuous(trans = \"log\", breaks = c(0, 0.5, 1, 1.5, 2)) +\n  xlab(\"Observation Time\") + ylab(\"HR for Treatment\") + ggtitle(\"RP Model - Time-varying HR for Treatment\") +\n  theme_bw(base_size = 20) \n\n\n\n\n\n\n\n\n\nThis looks quite similar to the plot for the Cox model above. As we did for the Cox model, let‚Äôs estimate the HR from the RP model at the same times of interest.\n\n\nCode\n# Calculate HR's and 95% CI's\nestimates &lt;- predict(vfit7, type = \"hr\",\n                    newdata = data.frame(time = c(5, 40, 100, 400), trt = 0, age = 20), # age value = placeholder\n                    exposed = function(data) transform(data, trt = 1, age = 20), # age value = placeholder\n                    full = TRUE, se.fit = TRUE)\n# Select relevant columns and rename\nestimates &lt;-  estimates |&gt; \n  select(time, Estimate, lower, upper) |&gt; \n  rename(Time = time,\n         HR = Estimate,\n         `Lower CI` = lower,\n         `Upper CI` = upper)\nestimates |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nTime\nHR\nLower CI\nUpper CI\n\n\n\n\n5\n1.18\n0.49\n2.86\n\n\n40\n1.39\n0.83\n2.33\n\n\n100\n1.24\n0.71\n2.17\n\n\n400\n0.30\n0.13\n0.69\n\n\n\n\n\nFor interest and before we wrap this up, let‚Äôs superimpose the time-varying HR curves from both the Cox and RP models to enable a more direct comparison.\n\n\nCode\n# Tidy RP pred output\npred_hrs &lt;-  pred_hrs |&gt; \n  select(time, Estimate) |&gt; \n  slice(-1) |&gt; \n  mutate(Model = \"Royston-Parmar\")\n# Tidy Cox pred output\ntdata &lt;-  tdata |&gt; \n  select(time, fit) |&gt; \n  slice(-1) |&gt; \n  rename(Estimate = fit) |&gt; \n  mutate(Model = \"Cox\")\n# Merge both sets of predictions\npreds_rp_cox &lt;-  rbind(pred_hrs, tdata) |&gt; \n  arrange(time)\n# Plot\nggplot(preds_rp_cox, aes(x = time, y = Estimate, color = Model)) + \n  geom_line() + \n  scale_x_continuous(breaks = seq(0, 400, by = 50), limits = c(0, 400)) +\n  scale_y_continuous(trans = \"log\", breaks = c(0, 0.5, 1, 1.5, 2)) +\n  xlab(\"Observation Time\") + ylab(\"HR for Treatment\") + ggtitle(\"Time-varying HR for Treatment\") +\n  theme_bw(base_size = 20) +\n  theme(legend.position=\"bottom\")\n\n\n\n\n\n\n\n\n\nOk, not perfectly congruent, but they both show a similar pattern in that there is an initial increase and then gradual decline in the ‚Äòeffect‚Äô of trt over time. Keep in mind that semi- and fully-parametric models are quite different modelling frameworks so the fact that we some concordance in the output gives us more confidence in establishing the validity of our results. I like RP models so much that I will aim to make them the subject of one of these blog posts in the not too distant future."
  },
  {
    "objectID": "posts/035_17Oct_2025/index.html",
    "href": "posts/035_17Oct_2025/index.html",
    "title": "Correlated Data",
    "section": "",
    "text": "Today we‚Äôre going to take a fairly high-level look at the idea of correlated data. I have no doubt that many of you are already aware that correlated data need to be properly accounted for in the analyses that you do. And to that end you probably know that you need to use something like a linear mixed-model instead of a vanilla linear regression if a regression model is called for, but perhaps you don‚Äôt really understand why. What I am therefore hoping I can achieve with this post is to build a little intuition in to your stats thinking to help you recognise some of the implications of correlated data when it comes to power, sample size and precision, in your daily research."
  },
  {
    "objectID": "posts/035_17Oct_2025/index.html#what-are-repeated-measures-data",
    "href": "posts/035_17Oct_2025/index.html#what-are-repeated-measures-data",
    "title": "Correlated Data",
    "section": "1.1 What are Repeated Measures data?",
    "text": "1.1 What are Repeated Measures data?\nRepeated measures data occur when you take multiple outcome measures from the same subjects (or experimental units), over time, space or condition. Now of these, data of the first type is probably the most common because many interesting research questions often involve a time component and discovering something about the trajectory of change in an outcome of interest over time - i.e.¬†what we commonly refer to as longitudinal data. But repeated measures can also be made over ‚Äòspace‚Äô - for example, we might be interested in comparing values on different measurement devices; OR under different conditions - treatment being a classic example here. In this context of repeated measures data, you can think of the individual, abstractly, as being the ‚Äòcluster‚Äô."
  },
  {
    "objectID": "posts/035_17Oct_2025/index.html#what-are-clustered-data",
    "href": "posts/035_17Oct_2025/index.html#what-are-clustered-data",
    "title": "Correlated Data",
    "section": "1.2 What are Clustered data?",
    "text": "1.2 What are Clustered data?\nClustered data differ from repeated measures in that outcome measures are taken from individuals usually just once, but where the individuals are related in some way to each other within a larger grouping structure. So when we talk about humans, clusters can be many things - families, schools, and GP clinics and hospitals are all common examples. But we can also talk about other grouping structures - animal litters and cages and bacterial plates in the lab; or cities and countries from a geographic perspective. If we return to people as being the experimental unit, in the context of clustered data, you can think of the individual as belonging to the cluster, rather than being the cluster as in the case of repeated measures."
  },
  {
    "objectID": "posts/035_17Oct_2025/index.html#repeated-measures-and-clustered-data.",
    "href": "posts/035_17Oct_2025/index.html#repeated-measures-and-clustered-data.",
    "title": "Correlated Data",
    "section": "1.3 Repeated Measures AND Clustered data.",
    "text": "1.3 Repeated Measures AND Clustered data.\nYou may come across other terms in your readings regarding correlated data and probably the two most common ones are hierarchical and nested - and these terms make sense when you think about it. Whether data are repeated measures or clustered, or a combination of both, the general idea is that observations exist within groups which then exist at different ‚Äòlevels‚Äô. The simplest grouping structure consists, by necessity, of just two levels but there can be more depending on the nature of the data. In the figure below I have made an example consisting of a 3-level hierarchy where we have repeated measures (level 1) nested within patients (level 2) which are further nested within hospitals (level 3)."
  },
  {
    "objectID": "posts/035_17Oct_2025/index.html#why-do-we-care",
    "href": "posts/035_17Oct_2025/index.html#why-do-we-care",
    "title": "Correlated Data",
    "section": "1.4 Why do we care?",
    "text": "1.4 Why do we care?\nSo why do we care about potential correlations in our data structure? Well, because in all likelihood (pardon the stats pun), you‚Äôll be wrong if you don‚Äôt. A fundamental tenet in statistics is that data points are independent - in other words, knowing the value of one data point doesn‚Äôt inform you as to the value of any others. This assumption of independence is violated in the presence of repeated measures or clustering. In turn this can result in unreliable estimation because standard errors and p-values are incorrect - and they tend to be incorrect on the ‚Äòtoo low‚Äô side, leading to false positives and claims of statistical significance when in fact, none may actually exist. In practice we commonly see this when the naive analyst uses standard linear regression techniques, thus ignoring potential correlations, in data that are not independent. What one should be doing instead are using models that explicitly account for these correlation structures in the model specification - and that is most commonly the mixed-model where we can flexibly specify these correlations as random effects (Generalised Estimating Equations - GEE‚Äôs - are another option but account for the correlations in a different way, mathematically)."
  },
  {
    "objectID": "posts/035_17Oct_2025/index.html#intraclass-cluster-correlation---icc.",
    "href": "posts/035_17Oct_2025/index.html#intraclass-cluster-correlation---icc.",
    "title": "Correlated Data",
    "section": "2.1 Intraclass (cluster) correlation - ICC.",
    "text": "2.1 Intraclass (cluster) correlation - ICC.\nThe ICC (or rho by it‚Äôs Greek symbol) measures the relatedness of correlated data by comparing the between- and within-cluster variances. In other words:\n\\[\\rho = \\frac{\\sigma_{(between)}}{\\sigma_{(between)} + \\sigma_{(within)}} = \\frac{\\sigma_{(between)}}{\\sigma_{(total)}}\\]\nMathematically, the ICC is the ratio of the between-cluster to total variance (where the total variance is the sum of the between- and within-cluster variances). The value of the ICC ranges from 0 to 1; where an ICC of 0 means that observations within a group are as different as observations in other groups - that is, they are independent; and an ICC of 1 means that observations within a group are the same - that is, they are completely dependent. Another way to think about this is that when the ICC = 0, knowing the value of any one observation within a group, gives you no clue as the value of the other observations within that group - they are just as likely to different as observations belonging to other groups. Contrast this to the opposite extreme when the ICC = 1. Now, if we know the value of any one observation within a group, we automatically know the values of all other observations within that group! If this is not quite making sense yet, I will expound on this notion over the next few sections, and I hope that will make things clearer for you.\n\n2.1.1 Relationship to ANOVA\nLet‚Äôs build on the ideas presented above by considering how the ICC relates to a traditional analysis of variance (ANOVA) [For those who aren‚Äôt as familiar with ANOVA, both ANOVA and regression are part of the same statistical framework (the General Linear Model), with the former being a special case of the latter, and the latter, ultimately, being more flexible in its application]. To illustrate ANOVA, I am ‚Äòborrowing‚Äô the following image:\n\nTo work out the between-cluster variance you first calculate the mean of each cluster and then calculate the variance of those means.The within-cluster variance is a little different - here you calculate the variance of the observations within each cluster and then take an average of those variances.\nNow when you conduct an ANOVA you are basically comparing the between-group variance against the within-group variance - that‚Äôs all that the test does. And if the between-group variance is larger than the within-group variance it tells you that the group means might differ in an important way (in other words the signal is greater than the noise)."
  },
  {
    "objectID": "posts/035_17Oct_2025/index.html#design-effect-de-and-effective-sample-size-ess.",
    "href": "posts/035_17Oct_2025/index.html#design-effect-de-and-effective-sample-size-ess.",
    "title": "Correlated Data",
    "section": "2.2 Design Effect (DE) and Effective Sample Size (ESS).",
    "text": "2.2 Design Effect (DE) and Effective Sample Size (ESS).\nmainly cluster concepts"
  },
  {
    "objectID": "posts/035_17Oct_2025/index.html#things-to-note.",
    "href": "posts/035_17Oct_2025/index.html#things-to-note.",
    "title": "Correlated Data",
    "section": "2.3 Things to note.",
    "text": "2.3 Things to note."
  },
  {
    "objectID": "posts/035_17Oct_2025/index.html#icc-0.10",
    "href": "posts/035_17Oct_2025/index.html#icc-0.10",
    "title": "Correlated Data",
    "section": "3.1 ICC = 0.10",
    "text": "3.1 ICC = 0.10"
  },
  {
    "objectID": "posts/035_17Oct_2025/index.html#icc-0.99",
    "href": "posts/035_17Oct_2025/index.html#icc-0.99",
    "title": "Correlated Data",
    "section": "3.2 ICC = 0.99",
    "text": "3.2 ICC = 0.99\n\n\nCode\nlibrary(tidyverse)\nlibrary(simstudy)\nlibrary(lme4)\nlibrary(lmerTest)\nlibrary(sjstats)\n\n# Calculate the between variances for set target ICC's, given a within variance (here just set to 4)\ntargetICC &lt;- c(0.1, 0.99)\nsetVars &lt;- iccRE(ICC = targetICC, dist = \"normal\", varWithin = 4)\nround(setVars, 4)\n\n# m = number of subjects in a cluster\n# k = number of clusters\n\n#++++++++++++++++++++++++++++++\n\n# ICC = 0.1 (m = 10, k = 10; n = 100)\nset.seed(7368888)\n# Create data\n# 1. Define df to create 'a' (precursor to y with mu = 0, var = 0.444) and 'size' (cluster size) variables\nd &lt;- defData(varname = \"a\", formula = 0, variance = 0.4444, id = \"grp\")\nd &lt;- defData(d, varname = \"size\", formula = 10, dist = \"nonrandom\")\n# 2. Define df to create formula for 'y' (outcome).\na &lt;- defDataAdd(varname = \"y\", formula = \"5 + a\", variance = 4, dist = \"normal\")\n# 3. Replicate 10 times (each line is a cluster)\ndT &lt;- genData(10, d)\n# 4. Expand df\ndat &lt;- genCluster(dtClust = dT, cLevelVar = \"grp\", numIndsVar = \"size\", level1ID = \"id\")\n# 5. Generate and add 'y' values based on Step 2.\ndat &lt;- addColumns(a, dat)\n# Run mixed-model\nmod &lt;- lmer(y ~ 1 + (1|grp), data = dat)\nsummary(mod)\n# Run standard model (ignore correlations)\nmod &lt;- lm(y ~ 1, data = dat)\nsummary(mod)\n\n# ICC = 0.1 (m = 100, k = 10; n = 1000)\nset.seed(7368888)\n# Create data\n# 1. Define df to create 'a' (precursor to y with mu = 0, var = 0.444) and 'size' (cluster size) variables\nd &lt;- defData(varname = \"a\", formula = 0, variance = 0.4444, id = \"grp\")\nd &lt;- defData(d, varname = \"size\", formula = 100, dist = \"nonrandom\")\n# 2. Define df to create formula for 'y' (outcome).\na &lt;- defDataAdd(varname = \"y\", formula = \"5 + a\", variance = 4, dist = \"normal\")\n# 3. Replicate 10 times (each line is a cluster)\ndT &lt;- genData(10, d)\n# 4. Expand df\ndat &lt;- genCluster(dtClust = dT, cLevelVar = \"grp\", numIndsVar = \"size\", level1ID = \"id\")\n# 5. Generate and add 'y' values based on Step 2.\ndat &lt;- addColumns(a, dat)\n# Run model\nmod &lt;- lmer(y ~ 1 + (1|grp), data = dat)\nsummary(mod)\n# Run standard model (ignore correlations)\nmod &lt;- lm(y ~ 1, data = dat)\nsummary(mod)\n\n# ICC = 0.1 (m = 10, k = 100; n = 1000)\nset.seed(7368888)\n# Create data\n# 1. Define df to create 'a' (precursor to y with mu = 0, var = 0.444) and 'size' (cluster size) variables\nd &lt;- defData(varname = \"a\", formula = 0, variance = 0.4444, id = \"grp\")\nd &lt;- defData(d, varname = \"size\", formula = 10, dist = \"nonrandom\")\n# 2. Define df to create formula for 'y' (outcome).\na &lt;- defDataAdd(varname = \"y\", formula = \"5 + a\", variance = 4, dist = \"normal\")\n# 3. Replicate 10 times (each line is a cluster)\ndT &lt;- genData(100, d)\n# 4. Expand df\ndat &lt;- genCluster(dtClust = dT, cLevelVar = \"grp\", numIndsVar = \"size\", level1ID = \"id\")\n# 5. Generate and add 'y' values based on Step 2.\ndat &lt;- addColumns(a, dat)\n# Run model\nmod &lt;- lmer(y ~ 1 + (1|grp), data = dat)\nsummary(mod)\n# Run standard model (ignore correlations)\nmod &lt;- lm(y ~ 1, data = dat)\nsummary(mod)\n\n#++++++++++++++++++++++++++++++\n\n# ICC = 0.99 (m = 10, k = 10; n = 100)\nset.seed(7368888)\n# Create data\n# 1. Define df to create 'a' (precursor to y with mu = 0, var = 396) and 'size' (cluster size) variables\nd &lt;- defData(varname = \"a\", formula = 0, variance = 396, id = \"grp\")\nd &lt;- defData(d, varname = \"size\", formula = 10, dist = \"nonrandom\")\n# 2. Define df to create formula for 'y' (outcome).\na &lt;- defDataAdd(varname = \"y\", formula = \"5 + a\", variance = 4, dist = \"normal\")\n# 3. Replicate 10 times (each line is a cluster)\ndT &lt;- genData(10, d)\n# 4. Expand df\ndat &lt;- genCluster(dtClust = dT, cLevelVar = \"grp\", numIndsVar = \"size\", level1ID = \"id\")\n# 5. Generate and add 'y' values based on Step 2.\ndat &lt;- addColumns(a, dat)\n# Run model\nmod &lt;- lmer(y ~ 1 + (1|grp), data = dat)\nsummary(mod)\n# Run standard model (ignore correlations)\nmod &lt;- lm(y ~ 1, data = dat)\nsummary(mod)\n\n# ICC = 0.99 (m = 100, k = 10; n = 1000)\nset.seed(7368888)\n# Create data\n# 1. Define df to create 'a' (precursor to y with mu = 0, var = 396) and 'size' (cluster size) variables\nd &lt;- defData(varname = \"a\", formula = 0, variance = 396, id = \"grp\")\nd &lt;- defData(d, varname = \"size\", formula = 100, dist = \"nonrandom\")\n# 2. Define df to create formula for 'y' (outcome).\na &lt;- defDataAdd(varname = \"y\", formula = \"5 + a\", variance = 4, dist = \"normal\")\n# 3. Replicate 10 times (each line is a cluster)\ndT &lt;- genData(10, d)\n# 4. Expand df\ndat &lt;- genCluster(dtClust = dT, cLevelVar = \"grp\", numIndsVar = \"size\", level1ID = \"id\")\n# 5. Generate and add 'y' values based on Step 2.\ndat &lt;- addColumns(a, dat)\n# Run model\nmod &lt;- lmer(y ~ 1 + (1|grp), data = dat)\nsummary(mod)\n# Run standard model (ignore correlations)\nmod &lt;- lm(y ~ 1, data = dat)\nsummary(mod)\n\n# ICC = 0.99 (m = 10, k = 100; n = 1000)\nset.seed(7368888)\n# Create data\n# 1. Define df to create 'a' (precursor to y with mu = 0, var = 396) and 'size' (cluster size) variables\nd &lt;- defData(varname = \"a\", formula = 0, variance = 396, id = \"grp\")\nd &lt;- defData(d, varname = \"size\", formula = 10, dist = \"nonrandom\")\n# 2. Define df to create formula for 'y' (outcome).\na &lt;- defDataAdd(varname = \"y\", formula = \"5 + a\", variance = 4, dist = \"normal\")\n# 3. Replicate 10 times (each line is a cluster)\ndT &lt;- genData(100, d)\n# 4. Expand df\ndat &lt;- genCluster(dtClust = dT, cLevelVar = \"grp\", numIndsVar = \"size\", level1ID = \"id\")\n# 5. Generate and add 'y' values based on Step 2.\ndat &lt;- addColumns(a, dat)\n# Run model\nmod &lt;- lmer(y ~ 1 + (1|grp), data = dat)\nsummary(mod)\n# Run standard model (ignore correlations)\nmod &lt;- lm(y ~ 1, data = dat)\nsummary(mod)\n\n#++++++++++++++++++++++++++++++\n\n# Sanity check between/within vals - allow for sampling variation\n(vars_between &lt;-  dat |&gt; \n    group_by(grp) |&gt; \n    summarise(means = mean(y)))\n(var_between  &lt;-   vars_between |&gt; \n    summarise(var_between = var(means)) |&gt; \n    as.numeric())\n(vars_within &lt;-  dat |&gt; \n    group_by(grp) |&gt; \n    summarise(vars = var(y)))\n(var_within &lt;-  vars_within |&gt; \n    summarise(var_within = mean(vars)) |&gt; \n    as.numeric())\nvar_between/(var_between + var_within)"
  },
  {
    "objectID": "posts/035_17Oct_2025/index.html#intraclass-cluster-correlation---icc",
    "href": "posts/035_17Oct_2025/index.html#intraclass-cluster-correlation---icc",
    "title": "Correlated Data",
    "section": "2.1 Intraclass (cluster) correlation - ICC",
    "text": "2.1 Intraclass (cluster) correlation - ICC\nThe ICC (or rho by it‚Äôs Greek symbol) measures the relatedness of correlated data by comparing the between- and within-cluster variances. In other words:\n\\[\\rho = \\frac{\\sigma_{(between)}}{\\sigma_{(between)} + \\sigma_{(within)}} = \\frac{\\sigma_{(between)}}{\\sigma_{(total)}}\\]\nMathematically, the ICC is the ratio of the between-cluster to total variance (where the total variance is the sum of the between- and within-cluster variances). The value of the ICC ranges from 0 to 1; where an ICC of 0 means that observations within a group are as different as observations in other groups - that is, they are independent; and an ICC of 1 means that observations within a group are the same - that is, they are completely dependent. Another way to think about this is that when the ICC = 0, knowing the value of any one observation within a group, gives you no clue as the value of the other observations within that group - they are just as likely to different as observations belonging to other groups. Contrast this to the opposite extreme when the ICC = 1. Now, if we know the value of any one observation within a group, we automatically know the values of all other observations within that group! If this is not quite making sense yet, I will expound on this notion over the next few sections, and I hope that will make things clearer for you.\n\n2.1.1 Relationship to ANOVA\nLet‚Äôs build on the ideas presented above by considering how the ICC relates to a traditional analysis of variance (ANOVA) [For those who aren‚Äôt as familiar with ANOVA, both ANOVA and regression are part of the same statistical framework (the General Linear Model), with the former being a special case of the latter, and the latter, ultimately, being more flexible in its application]. To illustrate ANOVA, I am ‚Äòborrowing‚Äô the following image:\n\nTo work out the between-cluster variance we first calculate the mean of each cluster and then calculate the variance of those means. The within-cluster variance is a little different - here we calculate the variance of the observations within each cluster and then take an average of those variances.\nNow, when we conduct an ANOVA all we are basically doing is comparing the between-group variance against the within-group variance. And if the between-group variance is larger than the within-group variance it tells us that the group means might differ in an important way (in other words that the signal is greater than the noise).\n\n\n2.1.2 A different way to imagine the same thing\nLet‚Äôs now consider the same concept visualised via abacus plots (again I am borrowing these images as I didn‚Äôt feel the need to re-invent the wheel). So how does knowledge of the between- and within-cluster variances allow us to ballpark what the ICC might be? In each plot below the vertical line represents a group of observations (so in each plot we have 20 groups each consisting of 4 observations). On the left we have a situation where the observations within a given group are as different as observations in other groups. So here the grouping structure doesn‚Äôt really matter. The between-cluster variance is therefore low relative to the within-cluster variance, and as a proportion of the total variance results in a small number divided by a bigger number leading to a low ICC. When you scan across that plot you can see that the points appear fairly randomly scattered.\nCompare that to the plot on the right. Here we have the opposite situation where the observations within any one group are very similar to each other. In this case the grouping structure does matter. Now the between-cluster variance is high relative to the within-cluster variance, and as a proportion of the total variance results in a big number divided by a smaller number leading to a high ICC. Now when you scan across the plot you can see that the points are much more clustered together in their groups.\n\n\n\n2.1.3 Give me something tangible\nOK, thus far I‚Äôve talked about ICC‚Äôs in a very abstract way, I know. But can we contextualise the concept in a real-world setting? You might be thinking - what are some actual examples of ICC‚Äôs? Well, I haven‚Äôt done an exhaustive literature search by any stretch, but my general sense is that there‚Äôs not a lot of information out there because researchers just don‚Äôt tend to publish these numbers, as they‚Äôre not usually of direct interest in themselves. With what I did find, ICC‚Äôs in bio-medical research tend to be low - correlations around 0.1 seem to come up a bit. But of course this can vary widely and it obviously depends on the research question and specific cluster factor of interest.\nSo just as a couple of examples, one paper looking at student performance outcomes found an ICC of 0.06 for class-room as a cluster factor. Another paper looking at diabetes and hypertension outcomes in primary-care medicine found ICC‚Äôs ranging from 0.01 to 0.48 with clinics as a cluster factor. And then another paper looking at heart failure with hospital as a cluster factor - ICC‚Äôs from 0.03 to 0.06 were documented.\nNow, I also did try and do a search for MS related ICC‚Äôs and couldn‚Äôt really find anything out there unfortunately - but it could just be that I didn‚Äôt look hard enough. How about I leave that as a task for you‚Ä¶"
  },
  {
    "objectID": "posts/035_17Oct_2025/index.html#brief-intermission",
    "href": "posts/035_17Oct_2025/index.html#brief-intermission",
    "title": "Correlated Data",
    "section": "2.2 ‚Ä¶Brief Intermission‚Ä¶",
    "text": "2.2 ‚Ä¶Brief Intermission‚Ä¶\nAll good and well you say. I now understand what the ICC is and what some reasonable values for the parameter might be. But I‚Äôm still not sure what this all means for me in my day-to-day research!\nFair question.\nAnd the answer can essentially be distilled down to the following; power and/or sample size, and precision. Let‚Äôs talk about how each of these are related to the ICC, now."
  },
  {
    "objectID": "posts/035_17Oct_2025/index.html#design-effect-de-and-effective-sample-size-ess",
    "href": "posts/035_17Oct_2025/index.html#design-effect-de-and-effective-sample-size-ess",
    "title": "Correlated Data",
    "section": "2.3 Design Effect (DE) and Effective Sample Size (ESS)",
    "text": "2.3 Design Effect (DE) and Effective Sample Size (ESS)\nIf you aren‚Äôt already familiar with the Design Effect (DE) and Effective Sample Size (ESS), these concepts closely follow on from the ICC in providing a practical framework for the adjustment of a potential correlation structure in our data. Now, there are two points I want to make about this before we go any further:\n\nThe first is that this mostly applies to planning randomised controlled trials where subjects are recruited within larger groups (GP clinics/hospitals/etc) . This is what is typically known as a cluster randomised controlled trial. That‚Äôs not to say that sample size calculations aren‚Äôt important in observational research but we‚Äôre often at the mercy of the data that we have available and so we just tend to take what we can get (and we don‚Äôt get upset). The important point I want you to take away from this is that our study power may actually be less than what we imagine it to be if we‚Äôre only ever thinking about it in terms of the number of subjects that we have (based on standard sample size calculations ignoring correlations).\nTo that end, while these ideas also loosely translate to repeated measures data, in the context we are talking about them today, it is only for clustered data. In this case, and as you will soon see, increasing the ICC decimates the actual sample size that we have. Repeated measures data are a little different because the individual is the cluster (rather than being part of the cluster) and in this case we are always adequately powered by the actual subjects that we have, regardless of whether the ICC is high or not (a topic for another day - but please trust me for now).\n\nNow, let‚Äôs get back on track. If you have clustering effects at play, because of potential similarities between subjects within a cluster, there is a net loss of data. In other words some patients bring little or even nothing new to the table in terms of unique information. What do I mean by this? Let‚Äôs take an example of 100 patients from 4 hospitals (25 from each) that we want to include in a study. Our sample size is 100 but it‚Äôs quite possible that our effective sample size (ESS) is less - in other words we have fewer patients than 100 from a statistical information perspective. It then follows that we might need to increase our actual sample size to greater than 100 patients, to compensate (I‚Äôll give you some worked examples of this shortly). If the ICC is known (e.g.¬†from a pilot study), it can be used at the design stage of the study to inform the sample size calculation.\nThe Design Effect (DE) is another important concept and forms part of the effective sample size calculation. Basically, the DE is an inflation factor that we multiply the actual sample size by to account for clustering effects. This gives us an adjusted sample size that contains the same amount of statistical information as our original sample size if no correlation structure were present.\n\\[ DE = 1 + \\rho(m - 1) \\]\n\\[ ESS = \\frac{mk}{DE} \\]\nAlright, let‚Äôs look at these two formulae, but don‚Äôt let them intimidate you - they are really not that hard. The DE is just 1 plus the ICC multiplied by m minus 1 - where m is the average number of subjects in a cluster. Then the ESS is simply m multiplied by k - where k is the number of clusters - divided by the DE.\nLet us know apply these two formulae in some worked examples. We‚Äôll use the same idea of 100 patients from 4 hospitals and calcuate the ESS under 3 different ICC‚Äôs - a plausible value of 0.02; then we‚Äôll consider either extreme - 0 if we assume complete independence and 1 if we assume complete dependence. My aim is just to get you thinking about a situation where you might have 100 patients, but you‚Äôve also got some unrealised clustering which could reduce your ESS and give you less study power than you had imagined.\n\n2.3.1 Worked Example 1 - ICC = 0.02\n\nICC = 0.02, m = 25, k = 4, n = 100\n\nOk, so our first example uses an ICC of 0.02. The DE works out at 1.48. The ESS in this case would then reduce to 68 patients. So while we have 100 patients, statistically only 68 patients worth are contributing information. We can then use the DE as an inflation factor to multiply by our original sample size to work out an adjusted sample size with equivalent study power in the presence of the clustering effects. And in this case we would need to recruit another 48 patients.\n\\[ DE = 1 + 0.02(25 - 1) = 1.48 \\]\n\\[ ESS = \\frac{mk}{DE} = \\frac{25 * 4}{1.48} = 68 \\]\n\\[ ASS = n * DE = 100 * 1.48 = 148 \\]\n\n\n2.3.2 Worked Example 2 - ICC = 0\n\nICC = 0, m = 25, k = 4, n = 100\n\nAn ICC of 0 here effectively means that individuals within hospitals share no outcome similarities and represent completely independent observations - so, measuring one individual tells us nothing about the other individuals in that hospital.\nSo what happens if we assume independent data? This time the DE is 1 and the ESS remains the same as the original sample. We don‚Äôt need to adjust our sample size at all because the cluster structure doesn‚Äôt impact the individual observations in any way.\n\\[ DE = 1 + 0(25 - 1) = 1 \\]\n\\[ ESS = \\frac{mk}{DE} = \\frac{25 * 4}{1} = 100 \\]\n\\[ ASS = n * DE = 100 * 1 = 100 \\]\n\n\n2.3.3 Worked Example 3 - ICC = 1\n\nICC = 1, m = 25, k = 4, n = 100\n\nAn ICC of 1 here now means that individuals within hospitals share identical outcomes and are completely dependent - this time, measuring one individual informs us about all individuals in that hospital. This is obviously an extremely unrealistic scenario.\nNow the DE essentially becomes the cluster size and because every observation within a cluster is identical, the ESS collapses to the number of clusters. So we might have had 100 patients, but statistically we only have 4. And under these conditions we‚Äôd need to recruit 2500 patients to have the same study power as 100 completely independent patients.\n\\[ DE = 1 + 1(25 - 1) = 25 \\]\n\\[ ESS = \\frac{mk}{DE} = \\frac{25 * 4}{25} = 4 \\]\n\\[ ASS = n * DE = 100 * 25 = 2500 \\]"
  },
  {
    "objectID": "posts/035_17Oct_2025/index.html#things-to-note",
    "href": "posts/035_17Oct_2025/index.html#things-to-note",
    "title": "Correlated Data",
    "section": "2.4 Things to note",
    "text": "2.4 Things to note"
  },
  {
    "objectID": "posts/035_17Oct_2025/index.html#take-homes",
    "href": "posts/035_17Oct_2025/index.html#take-homes",
    "title": "Correlated Data",
    "section": "2.4 Take Homes",
    "text": "2.4 Take Homes\nBased on everything that we have learned thus far, here are a few important themes that might be useful to add to your stats knowledge toolkit for future reference:\n\nWhen ICC = 0, the DE = 1 and the ESS = n. Observations within clusters are independent and the sample size doesn‚Äôt need to be adjusted.\nWhen ICC = 1, the DE = m and the ESS reduces to the number of clusters. Observations within clusters are completely dependent and the sample size needs to be adjusted up by a factor of the DE.\nA high k (number of clusters) and a low m (number of subjects within a cluster) give the smallest DE.\nWhen designing studies, increasing the number of clusters (k) will improve study power more than increasing the number of subjects within a cluster (m).\nMore clusters are always better than larger clusters!\nAt the end of the day there might not be much you can actually do about your power/sample size when recruiting individuals from clinics, hospitals, etc. It is what it is and you take what you can get.\nBut it‚Äôs important to realise that when you have potential clustering effects in play, sample size and power are dependent on more than just the number of individuals in your study."
  },
  {
    "objectID": "posts/035_17Oct_2025/index.html#simulation-1---icc-0.99-m-10-k-10",
    "href": "posts/035_17Oct_2025/index.html#simulation-1---icc-0.99-m-10-k-10",
    "title": "Correlated Data",
    "section": "3.1 Simulation 1 - ICC = 0.99, m = 10, k = 10",
    "text": "3.1 Simulation 1 - ICC = 0.99, m = 10, k = 10\nLet‚Äôs start off by looking at the last 3 rows of the table - where I have set the ICC very high. With 10 people in each of 10 clusters (i.e.¬†n = 100) the SE is 4.71 and the p-value 0.06 with a mixed model. Compare this a much smaller SE of 1.43 and a very low p-value when we ignore the correlation by using a standard model. Note that this is incorrect and would lead us to the wrong conclusion. The mixed-model correctly adjusts for the fact that observations within each cluster are almost identical, whereas the standard model considers them all unique and independent."
  },
  {
    "objectID": "posts/035_17Oct_2025/index.html#simulation-2---icc-0.99-m-100-k-10",
    "href": "posts/035_17Oct_2025/index.html#simulation-2---icc-0.99-m-100-k-10",
    "title": "Correlated Data",
    "section": "3.2 Simulation 2 - ICC = 0.99, m = 100, k = 10",
    "text": "3.2 Simulation 2 - ICC = 0.99, m = 100, k = 10\nNow imagine we had access to 1000 patients instead of 100. In this scenario we still have 10 clusters but the cluster size had increased from 10 to 100. The SE for the mixed-model is basically the same - so increasing the cluster size hasn‚Äôt made much difference. But contrast that to running a standard regression where the SE has now decreased even further because the model now thinks it‚Äôs dealing with 1000 bits of unique information, rather than 100."
  },
  {
    "objectID": "posts/035_17Oct_2025/index.html#simulation-3---icc-0.99-m-10-k-100",
    "href": "posts/035_17Oct_2025/index.html#simulation-3---icc-0.99-m-10-k-100",
    "title": "Correlated Data",
    "section": "3.3 Simulation 3 - ICC = 0.99, m = 10, k = 100",
    "text": "3.3 Simulation 3 - ICC = 0.99, m = 10, k = 100\nWhat if we still had 1000 patients but this time we were able to increase the number of clusters keeping the cluster size the same - so now we‚Äôve got 100 clusters each with 10 subjects. Now the SE has reduced in the mixed-model, indicating more power to separate the signal from the noise. This comes back to the ‚Äòmore clusters is better than bigger clusters‚Äô take home that I mentioned before. The standard model hasn‚Äôt really changed that much as would be expected."
  },
  {
    "objectID": "posts/035_17Oct_2025/index.html#simulations-4-6---icc-0.1",
    "href": "posts/035_17Oct_2025/index.html#simulations-4-6---icc-0.1",
    "title": "Correlated Data",
    "section": "3.4 Simulations 4-6 - ICC = 0.1",
    "text": "3.4 Simulations 4-6 - ICC = 0.1\nIf we now look at the scenario with a low ICC, we see a similar pattern in the SE reducing in our mixed-model when we increase the number of clusters. The main difference here is that the SE‚Äôs, in general, are much lower, because we are dealing with data that are much less correlated in the first place, so that each unique data point brings more new information to the statistical analysis."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html",
    "href": "posts/036_14Nov_2025/index.html",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "",
    "text": "Today I thought we‚Äôd discuss a stats topic that I‚Äôm sure you‚Äôve heard about before, but perhaps not really understood, nor yet had a pressing need to use - bootstrap resampling. Now, I have to admit that I haven‚Äôt applied this technique much in my own day to day work either, but it‚Äôs an important statistical tool to have an understanding of, because there are times when it‚Äôs the only approach you can use. And the reason for that is the bootstrap can be considered a swiss army knife of parameter uncertainty estimation when the usual parametric distribution assumptions and resulting formulaic approximations that we base our standard error calculations on, either can‚Äôt be trusted or are simply unknown."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#what-are-repeated-measures-data",
    "href": "posts/036_14Nov_2025/index.html#what-are-repeated-measures-data",
    "title": "Correlated Data",
    "section": "1.1 What are Repeated Measures data?",
    "text": "1.1 What are Repeated Measures data?\nRepeated measures data occur when you take multiple outcome measures from the same subjects (or experimental units), over time, space or condition. Now of these, data of the first type is probably the most common because many interesting research questions often involve a time component and discovering something about the trajectory of change in an outcome of interest over time - i.e.¬†what we commonly refer to as longitudinal data. But repeated measures can also be made over ‚Äòspace‚Äô - for example, we might be interested in comparing values on different measurement devices; OR under different conditions - treatment being a classic example here. In this context of repeated measures data, you can think of the individual, abstractly, as being the ‚Äòcluster‚Äô."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#what-are-clustered-data",
    "href": "posts/036_14Nov_2025/index.html#what-are-clustered-data",
    "title": "Correlated Data",
    "section": "1.2 What are Clustered data?",
    "text": "1.2 What are Clustered data?\nClustered data differ from repeated measures in that outcome measures are taken from individuals usually just once, but where the individuals are related in some way to each other within a larger grouping structure. So when we talk about humans, clusters can be many things - families, schools, and GP clinics and hospitals are all common examples. But we can also talk about other grouping structures - animal litters and cages and bacterial plates in the lab; or cities and countries from a geographic perspective. If we return to people as being the experimental unit, in the context of clustered data, you can think of the individual as belonging to the cluster, rather than being the cluster as in the case of repeated measures."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#repeated-measures-and-clustered-data.",
    "href": "posts/036_14Nov_2025/index.html#repeated-measures-and-clustered-data.",
    "title": "Correlated Data",
    "section": "1.3 Repeated Measures AND Clustered data.",
    "text": "1.3 Repeated Measures AND Clustered data.\nYou may come across other terms in your readings regarding correlated data and probably the two most common ones are hierarchical and nested - and these terms make sense when you think about it. Whether data are repeated measures or clustered, or a combination of both, the general idea is that observations exist within groups which then exist at different ‚Äòlevels‚Äô. The simplest grouping structure consists, by necessity, of just two levels but there can be more depending on the nature of the data. In the figure below I have made an example consisting of a 3-level hierarchy where we have repeated measures (level 1) nested within patients (level 2) which are further nested within hospitals (level 3)."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#why-do-we-care",
    "href": "posts/036_14Nov_2025/index.html#why-do-we-care",
    "title": "Correlated Data",
    "section": "1.4 Why do we care?",
    "text": "1.4 Why do we care?\nSo why do we care about potential correlations in our data structure? Well, because in all likelihood (pardon the stats pun), you‚Äôll be wrong if you don‚Äôt. A fundamental tenet in statistics is that data points are independent - in other words, knowing the value of one data point doesn‚Äôt inform you as to the value of any others. This assumption of independence is violated in the presence of repeated measures or clustering. In turn this can result in unreliable estimation because standard errors and p-values are incorrect - and they tend to be incorrect on the ‚Äòtoo low‚Äô side, leading to false positives and claims of statistical significance when in fact, none may actually exist. In practice we commonly see this when the naive analyst uses standard linear regression techniques, thus ignoring potential correlations, in data that are not independent. What one should be doing instead are using models that explicitly account for these correlation structures in the model specification - and that is most commonly the mixed-model where we can flexibly specify these correlations as random effects (Generalised Estimating Equations - GEE‚Äôs - are another option but account for the correlations in a different way, mathematically)."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#intraclass-cluster-correlation---icc",
    "href": "posts/036_14Nov_2025/index.html#intraclass-cluster-correlation---icc",
    "title": "Correlated Data",
    "section": "2.1 Intraclass (cluster) correlation - ICC",
    "text": "2.1 Intraclass (cluster) correlation - ICC\nThe ICC (or rho by it‚Äôs Greek symbol) measures the relatedness of correlated data by comparing the between- and within-cluster variances. In other words:\n\\[\\rho = \\frac{\\sigma_{(between)}}{\\sigma_{(between)} + \\sigma_{(within)}} = \\frac{\\sigma_{(between)}}{\\sigma_{(total)}}\\]\nMathematically, the ICC is the ratio of the between-cluster to total variance (where the total variance is the sum of the between- and within-cluster variances). The value of the ICC ranges from 0 to 1; where an ICC of 0 means that observations within a group are as different as observations in other groups - that is, they are independent; and an ICC of 1 means that observations within a group are the same - that is, they are completely dependent. Another way to think about this is that when the ICC = 0, knowing the value of any one observation within a group, gives you no clue as the value of the other observations within that group - they are just as likely to different as observations belonging to other groups. Contrast this to the opposite extreme when the ICC = 1. Now, if we know the value of any one observation within a group, we automatically know the values of all other observations within that group! If this is not quite making sense yet, I will expound on this notion over the next few sections, and I hope that will make things clearer for you.\n\n2.1.1 Relationship to ANOVA\nLet‚Äôs build on the ideas presented above by considering how the ICC relates to a traditional analysis of variance (ANOVA) [For those who aren‚Äôt as familiar with ANOVA, both ANOVA and regression are part of the same statistical framework (the General Linear Model), with the former being a special case of the latter, and the latter, ultimately, being more flexible in its application]. To illustrate ANOVA, I am ‚Äòborrowing‚Äô the following image:\n\nTo work out the between-cluster variance we first calculate the mean of each cluster and then calculate the variance of those means. The within-cluster variance is a little different - here we calculate the variance of the observations within each cluster and then take an average of those variances.\nNow, when we conduct an ANOVA all we are basically doing is comparing the between-group variance against the within-group variance. And if the between-group variance is larger than the within-group variance it tells us that the group means might differ in an important way (in other words that the signal is greater than the noise).\n\n\n2.1.2 A different way to imagine the same thing\nLet‚Äôs now consider the same concept visualised via abacus plots (again I am borrowing these images as I didn‚Äôt feel the need to re-invent the wheel). So how does knowledge of the between- and within-cluster variances allow us to ballpark what the ICC might be? In each plot below the vertical line represents a group of observations (so in each plot we have 20 groups each consisting of 4 observations). On the left we have a situation where the observations within a given group are as different as observations in other groups. So here the grouping structure doesn‚Äôt really matter. The between-cluster variance is therefore low relative to the within-cluster variance, and as a proportion of the total variance results in a small number divided by a bigger number leading to a low ICC. When you scan across that plot you can see that the points appear fairly randomly scattered.\nCompare that to the plot on the right. Here we have the opposite situation where the observations within any one group are very similar to each other. In this case the grouping structure does matter. Now the between-cluster variance is high relative to the within-cluster variance, and as a proportion of the total variance results in a big number divided by a smaller number leading to a high ICC. Now when you scan across the plot you can see that the points are much more clustered together in their groups.\n\n\n\n2.1.3 Give me something tangible\nOK, thus far I‚Äôve talked about ICC‚Äôs in a very abstract way, I know. But can we contextualise the concept in a real-world setting? You might be thinking - what are some actual examples of ICC‚Äôs? Well, I haven‚Äôt done an exhaustive literature search by any stretch, but my general sense is that there‚Äôs not a lot of information out there because researchers just don‚Äôt tend to publish these numbers, as they‚Äôre not usually of direct interest in themselves. With what I did find, ICC‚Äôs in bio-medical research tend to be low - correlations around 0.1 seem to come up a bit. But of course this can vary widely and it obviously depends on the research question and specific cluster factor of interest.\nSo just as a couple of examples, one paper looking at student performance outcomes found an ICC of 0.06 for class-room as a cluster factor. Another paper looking at diabetes and hypertension outcomes in primary-care medicine found ICC‚Äôs ranging from 0.01 to 0.48 with clinics as a cluster factor. And then another paper looking at heart failure with hospital as a cluster factor - ICC‚Äôs from 0.03 to 0.06 were documented.\nNow, I also did try and do a search for MS related ICC‚Äôs and couldn‚Äôt really find anything out there unfortunately - but it could just be that I didn‚Äôt look hard enough. How about I leave that as a task for you‚Ä¶"
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#brief-intermission",
    "href": "posts/036_14Nov_2025/index.html#brief-intermission",
    "title": "Correlated Data",
    "section": "2.2 ‚Ä¶Brief Intermission‚Ä¶",
    "text": "2.2 ‚Ä¶Brief Intermission‚Ä¶\nAll good and well you say. I now understand what the ICC is and what some reasonable values for the parameter might be. But I‚Äôm still not sure what this all means for me in my day-to-day research!\nFair question.\nAnd the answer can essentially be distilled down to the following; power and/or sample size, and precision. Let‚Äôs talk about how each of these are related to the ICC, now."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#design-effect-de-and-effective-sample-size-ess",
    "href": "posts/036_14Nov_2025/index.html#design-effect-de-and-effective-sample-size-ess",
    "title": "Correlated Data",
    "section": "2.3 Design Effect (DE) and Effective Sample Size (ESS)",
    "text": "2.3 Design Effect (DE) and Effective Sample Size (ESS)\nIf you aren‚Äôt already familiar with the Design Effect (DE) and Effective Sample Size (ESS), these concepts closely follow on from the ICC in providing a practical framework for the adjustment of a potential correlation structure in our data. Now, there are two points I want to make about this before we go any further:\n\nThe first is that this mostly applies to planning randomised controlled trials where subjects are recruited within larger groups (GP clinics/hospitals/etc) . This is what is typically known as a cluster randomised controlled trial. That‚Äôs not to say that sample size calculations aren‚Äôt important in observational research but we‚Äôre often at the mercy of the data that we have available and so we just tend to take what we can get (and we don‚Äôt get upset). The important point I want you to take away from this is that our study power may actually be less than what we imagine it to be if we‚Äôre only ever thinking about it in terms of the number of subjects that we have (based on standard sample size calculations ignoring correlations).\nTo that end, while these ideas also loosely translate to repeated measures data, in the context we are talking about them today, it is only for clustered data. In this case, and as you will soon see, increasing the ICC decimates the actual sample size that we have. Repeated measures data are a little different because the individual is the cluster (rather than being part of the cluster) and in this case we are always adequately powered by the actual subjects that we have, regardless of whether the ICC is high or not (a topic for another day - but please trust me for now).\n\nNow, let‚Äôs get back on track. If you have clustering effects at play, because of potential similarities between subjects within a cluster, there is a net loss of data. In other words some patients bring little or even nothing new to the table in terms of unique information. What do I mean by this? Let‚Äôs take an example of 100 patients from 4 hospitals (25 from each) that we want to include in a study. Our sample size is 100 but it‚Äôs quite possible that our effective sample size (ESS) is less - in other words we have fewer patients than 100 from a statistical information perspective. It then follows that we might need to increase our actual sample size to greater than 100 patients, to compensate (I‚Äôll give you some worked examples of this shortly). If the ICC is known (e.g.¬†from a pilot study), it can be used at the design stage of the study to inform the sample size calculation.\nThe Design Effect (DE) is another important concept and forms part of the effective sample size calculation. Basically, the DE is an inflation factor that we multiply the actual sample size by to account for clustering effects. This gives us an adjusted sample size that contains the same amount of statistical information as our original sample size if no correlation structure were present.\n\\[ DE = 1 + \\rho(m - 1) \\]\n\\[ ESS = \\frac{mk}{DE} \\]\nAlright, let‚Äôs look at these two formulae, but don‚Äôt let them intimidate you - they are really not that hard. The DE is just 1 plus the ICC multiplied by m minus 1 - where m is the average number of subjects in a cluster. Then the ESS is simply m multiplied by k - where k is the number of clusters - divided by the DE.\nLet us know apply these two formulae in some worked examples. We‚Äôll use the same idea of 100 patients from 4 hospitals and calcuate the ESS under 3 different ICC‚Äôs - a plausible value of 0.02; then we‚Äôll consider either extreme - 0 if we assume complete independence and 1 if we assume complete dependence. My aim is just to get you thinking about a situation where you might have 100 patients, but you‚Äôve also got some unrealised clustering which could reduce your ESS and give you less study power than you had imagined.\n\n2.3.1 Worked Example 1 - ICC = 0.02\n\nICC = 0.02, m = 25, k = 4, n = 100\n\nOk, so our first example uses an ICC of 0.02. The DE works out at 1.48. The ESS in this case would then reduce to 68 patients. So while we have 100 patients, statistically only 68 patients worth are contributing information. We can then use the DE as an inflation factor to multiply by our original sample size to work out an adjusted sample size with equivalent study power in the presence of the clustering effects. And in this case we would need to recruit another 48 patients.\n\\[ DE = 1 + 0.02(25 - 1) = 1.48 \\]\n\\[ ESS = \\frac{mk}{DE} = \\frac{25 * 4}{1.48} = 68 \\]\n\\[ ASS = n * DE = 100 * 1.48 = 148 \\]\n\n\n2.3.2 Worked Example 2 - ICC = 0\n\nICC = 0, m = 25, k = 4, n = 100\n\nAn ICC of 0 here effectively means that individuals within hospitals share no outcome similarities and represent completely independent observations - so, measuring one individual tells us nothing about the other individuals in that hospital.\nSo what happens if we assume independent data? This time the DE is 1 and the ESS remains the same as the original sample. We don‚Äôt need to adjust our sample size at all because the cluster structure doesn‚Äôt impact the individual observations in any way.\n\\[ DE = 1 + 0(25 - 1) = 1 \\]\n\\[ ESS = \\frac{mk}{DE} = \\frac{25 * 4}{1} = 100 \\]\n\\[ ASS = n * DE = 100 * 1 = 100 \\]\n\n\n2.3.3 Worked Example 3 - ICC = 1\n\nICC = 1, m = 25, k = 4, n = 100\n\nAn ICC of 1 here now means that individuals within hospitals share identical outcomes and are completely dependent - this time, measuring one individual informs us about all individuals in that hospital. This is obviously an extremely unrealistic scenario.\nNow the DE essentially becomes the cluster size and because every observation within a cluster is identical, the ESS collapses to the number of clusters. So we might have had 100 patients, but statistically we only have 4. And under these conditions we‚Äôd need to recruit 2500 patients to have the same study power as 100 completely independent patients.\n\\[ DE = 1 + 1(25 - 1) = 25 \\]\n\\[ ESS = \\frac{mk}{DE} = \\frac{25 * 4}{25} = 4 \\]\n\\[ ASS = n * DE = 100 * 25 = 2500 \\]"
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#take-homes",
    "href": "posts/036_14Nov_2025/index.html#take-homes",
    "title": "Correlated Data",
    "section": "2.4 Take Homes",
    "text": "2.4 Take Homes\nBased on everything that we have learned thus far, here are a few important themes that might be useful to add to your stats knowledge toolkit for future reference:\n\nWhen ICC = 0, the DE = 1 and the ESS = n. Observations within clusters are independent and the sample size doesn‚Äôt need to be adjusted.\nWhen ICC = 1, the DE = m and the ESS reduces to the number of clusters. Observations within clusters are completely dependent and the sample size needs to be adjusted up by a factor of the DE.\nA high k (number of clusters) and a low m (number of subjects within a cluster) give the smallest DE.\nWhen designing studies, increasing the number of clusters (k) will improve study power more than increasing the number of subjects within a cluster (m).\nMore clusters are always better than larger clusters!\nAt the end of the day there might not be much you can actually do about your power/sample size when recruiting individuals from clinics, hospitals, etc. It is what it is and you take what you can get.\nBut it‚Äôs important to realise that when you have potential clustering effects in play, sample size and power are dependent on more than just the number of individuals in your study."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#simulation-1---icc-0.99-m-10-k-10",
    "href": "posts/036_14Nov_2025/index.html#simulation-1---icc-0.99-m-10-k-10",
    "title": "Correlated Data",
    "section": "3.1 Simulation 1 - ICC = 0.99, m = 10, k = 10",
    "text": "3.1 Simulation 1 - ICC = 0.99, m = 10, k = 10\nLet‚Äôs start off by looking at the last 3 rows of the table - where I have set the ICC very high. With 10 people in each of 10 clusters (i.e.¬†n = 100) the SE is 4.71 and the p-value 0.06 with a mixed model. Compare this a much smaller SE of 1.43 and a very low p-value when we ignore the correlation by using a standard model. Note that this is incorrect and would lead us to the wrong conclusion. The mixed-model correctly adjusts for the fact that observations within each cluster are almost identical, whereas the standard model considers them all unique and independent."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#simulation-2---icc-0.99-m-100-k-10",
    "href": "posts/036_14Nov_2025/index.html#simulation-2---icc-0.99-m-100-k-10",
    "title": "Correlated Data",
    "section": "3.2 Simulation 2 - ICC = 0.99, m = 100, k = 10",
    "text": "3.2 Simulation 2 - ICC = 0.99, m = 100, k = 10\nNow imagine we had access to 1000 patients instead of 100. In this scenario we still have 10 clusters but the cluster size had increased from 10 to 100. The SE for the mixed-model is basically the same - so increasing the cluster size hasn‚Äôt made much difference. But contrast that to running a standard regression where the SE has now decreased even further because the model now thinks it‚Äôs dealing with 1000 bits of unique information, rather than 100."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#simulation-3---icc-0.99-m-10-k-100",
    "href": "posts/036_14Nov_2025/index.html#simulation-3---icc-0.99-m-10-k-100",
    "title": "Correlated Data",
    "section": "3.3 Simulation 3 - ICC = 0.99, m = 10, k = 100",
    "text": "3.3 Simulation 3 - ICC = 0.99, m = 10, k = 100\nWhat if we still had 1000 patients but this time we were able to increase the number of clusters keeping the cluster size the same - so now we‚Äôve got 100 clusters each with 10 subjects. Now the SE has reduced in the mixed-model, indicating more power to separate the signal from the noise. This comes back to the ‚Äòmore clusters is better than bigger clusters‚Äô take home that I mentioned before. The standard model hasn‚Äôt really changed that much as would be expected."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#simulations-4-6---icc-0.1",
    "href": "posts/036_14Nov_2025/index.html#simulations-4-6---icc-0.1",
    "title": "Correlated Data",
    "section": "3.4 Simulations 4-6 - ICC = 0.1",
    "text": "3.4 Simulations 4-6 - ICC = 0.1\nIf we now look at the scenario with a low ICC, we see a similar pattern in the SE reducing in our mixed-model when we increase the number of clusters. The main difference here is that the SE‚Äôs, in general, are much lower, because we are dealing with data that are much less correlated in the first place, so that each unique data point brings more new information to the statistical analysis."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#basic-concepts",
    "href": "posts/036_14Nov_2025/index.html#basic-concepts",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.1 Basic concepts",
    "text": "2.1 Basic concepts\nAnd this is where the fundamental idea of the sampling distribution comes into play. In a nutshell, the sampling distribution represents the theoretical distribution of a sample statistic that‚Äôs derived from repeatedly randomly sampling a population of interest. Now this is a thought exercise only - we don‚Äôt do it for obvious reasons in practice - but it allows us to make assumptions about the behaviour of the population parameter that we are trying to estimate. (Note - while I say ‚Äòtheoretical‚Äô above, these distributions can now be verified in silico but in the pre-computer era were confirmed empirically by a combination of mathematical derivation and physical simulation - i.e.¬†bootstrapping by hand!)\nIn this thought exercise we would have a population that we are interested in calculating a parameter for, and we‚Äôd draw a sample of observations from this population. We‚Äôd then calculate the corresponding sample statistic - let‚Äôs say it‚Äôs a mean value - and we‚Äôd plot that on a frequency histogram. We would then repeat that process many times, plotting each mean value along the way. The resulting plot would show the distribution of all the sample means - and this is what we call the sampling distribution of the sample statistic. It‚Äôs important to note that this is NOT the distribution of the data itself, but the distribution of a summary statistic derived from the data.\n\n\n\n\n\nNow, depending on the sample size, the shape of the underlying raw data distribution and the specific summary statistic we‚Äôre interested in, sampling distributions for many statistics often end up looking normal (or close to normal) in shape. And that allows us to leverage fairly simple normal distribution properties such as the mean and standard deviation (SD) to infer the population mean and it‚Äôs associated uncertainty. The mean should converge to the actual population parameter and the SD tells us about the uncertainty in the estimation of the parameter - and in fact is directly interpretable from the sampling distribution itself, as the standard error (SE). Once we have the SE it becomes trivial to calculate the 95% confidence interval (CI).\nThe point in telling you all of this is to highlight to you is that we can use just a single sample to form probabilistic statements about a population parameter, rather than needing to measure the entire population."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#the-sampling-distribution-1",
    "href": "posts/036_14Nov_2025/index.html#the-sampling-distribution-1",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.2 The Sampling Distribution",
    "text": "2.2 The Sampling Distribution\nOK, I appreciate that might have all been a bit full-on, so let‚Äôs use a visualisation to make the idea clearer. To re-iterate:\n\nWe start off with a population of interest and then we take repeated random samples from that population.\nFor each of those samples we calculate a summary statistic - in this case it‚Äôs a simple average or mean.\nWe then plot all of those sample means in a frequency histogram and that becomes the sampling distribution of the summary statistic.\n\n\n\n\n\n\nNow, depending on the sample size, the shape of the underlying raw data distribution and the specific summary statistic we‚Äôre interested in, sampling distributions for many statistics often end up looking normal (or close to normal) in shape. And that allows us to leverage fairly simple normal distribution properties such as the mean and standard deviation to infer the population mean and it‚Äôs associated uncertainty - measured as the SE and CI."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#when-it-works",
    "href": "posts/036_14Nov_2025/index.html#when-it-works",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.2 When it works",
    "text": "2.2 When it works\nMost of the time the theory of parametric sampling distributions will work just fine for what you want to do. Probability density functions (PDF‚Äôs) that define the ‚Äòshape‚Äô of the sampling distribution have been derived for many sample statistics. These equations are sometimes referred to as ‚Äòclosed-form solutions‚Äô. Some PDF‚Äôs are fairly simple, other‚Äôs are almost intractably complex and some are simply unknown. But what is important about this, is that when you know the PDF for a sampling distribution, you can calculate exact 95% CI‚Äôs.\nWhen PDF‚Äôs become too difficult or are simply unknown, we can start to leverage approximations. And that probably explains why I haven‚Äôt needed to bootstrap much in my own work - the classic central limit theorem (CLT) does its job pretty well. The CLT basically states that you can have whatever shaped raw data distribution you want - flat, skewed, bi-modal - whatever, but when you then construct a sampling distribution from the resulting sample means, that distribution will be normal in shape (if you‚Äôve got a large enough sample).\nThe thing is, the logic of the central limit theorem translates fairly well to most other sample statistics of interest - for example, medians, proportions, correlations, regression coefficients, and more - through a concept called large-sample asymptotic normality. This approximation basically says that if our sample is large enough, the sampling distributions of these other parameters will likewise approach normality in terms of shape. What that means at the end of the day is that we can use fairly simple(-ish) equations for large sample approximations to estimate SE‚Äôs and 95% CI‚Äôs. The equations for the SE for the mean and proportion are shown below and I‚Äôm sure you‚Äôve seen these before.\n\\[ \\bar{x} \\pm 1.96^* \\frac{s}{\\sqrt{n}} \\hspace{2cm} \\widehat{p} \\pm 1.96^* \\sqrt{\\frac{\\widehat{p}(1 - \\widehat{p})}{n}} \\]"
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#when-it-doesnt",
    "href": "posts/036_14Nov_2025/index.html#when-it-doesnt",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.3 When it doesn‚Äôt",
    "text": "2.3 When it doesn‚Äôt\nClearly, there are situations that can occasionally arise when basic sampling theory might not be adequate for your analytic needs, otherwise we wouldn‚Äôt have this post. To my mind there are three main reasons why you might look further afield to a resampling method like the bootstrap to support your analyses.\n\nThe first is sample size. Asymptotic normal theory relies on a ‚Äòlarge‚Äô sample size to be accurate, and the bootstrap deals with smaller samples much better. However, it is itself not immune to small sample bias (when n becomes quite small - say &lt; 15). In such cases not much can save you unless you collect more data, so you might just need to rely on descriptive statistics only.\nThe second situation is where equations for the SE are either so complex as to be virtually intractable, or simply don‚Äôt exist (as described above). Here you can use the bootstrap to estimate the sampling distribution directly with relative ease.\nFinally, for sampling distributions that depart quite obviously from normality. Here the large-sample approximations just don‚Äôt work well, but you can use the bootstrap in these cases to actually capture that non-normal shape and apply it in valid inference."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#why-is-it-called-the-bootstrap",
    "href": "posts/036_14Nov_2025/index.html#why-is-it-called-the-bootstrap",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.4 Why is it called the bootstrap?",
    "text": "2.4 Why is it called the bootstrap?\nAlright - let‚Äôs actually talk about The Bootstrap! There is some interesting history in how the bootstrap came to be called what it is, as it‚Äôs etymology isn‚Äôt from the statistical domain. The origins of the term are sometimes attributed to an 18th century work of fiction - The Surprising Adventures of Baron Munchausen - in which Baron Munchausen‚Äôs plan for getting himself (and his horse) out of a swamp was to pull himself out by his bootstraps. Curiously there appears to be no actual reference to his bootstraps in the story itself, where instead he uses his own hair (pigtails to be specific).\nIn any case, over time the term evolved to mean many things but with the overarching theme of ‚Äòperforming a near impossible task‚Äô, or ‚Äòdoing more with less‚Äô. It is not unheard of today in political discourse as a narrative for self-starting economic mobility - that is, ‚Äúif you just put in the hard work, you will eventually be successful‚Äù.\nIn statistics specifically, the bootstrap is used to mean that the population parameter we are interested in can be sufficiently defined by the sample of data that we have. In other words, ‚Äòthe sample ‚Äúpulls itself up by its bootstraps‚Äù‚Äô."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#an-important-statistical-idea",
    "href": "posts/036_14Nov_2025/index.html#an-important-statistical-idea",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.5 An important statistical idea",
    "text": "2.5 An important statistical idea\nBefore I get to explaining what the bootstrap is in more detail, let me start by introducing a paper that was published in 2020 by two very well-known American statisticians - What are the most important statistical ideas of the past 50 years.\n\nCounterfactual causal inference\nBootstrapping and simulation-based inference\nOverparameterized models and regularization\nMultilevel models\nGeneric computation algorithms\nAdaptive decision analysis\nRobust inference\nExploratory data analysis\n\nEach of these ideas has existed in some form prior to the 1970s, both in the theoretical statistics literature and in the practice of various applied Ô¨Åelds. But the authors consider that each has developed enough in the past 50 years to have essentially become something new and in many instances this has been facilitated by the modern computing age, as some of these techniques just weren‚Äôt practical to apply before we had fast computers. We have some ideas that you might already be familiar with from traditional statistics - counterfactual inference, multilevel model, robust inference, exploratory data analysis; and perhaps others that might be less so because they fall more into the realm of data science and predictive analytics - overparameterised models, generic algorithms and decision analysis.\nGiven that bootstrapping is one of these important statistical ideas from the last 50 years, let‚Äôs now learn some more about it."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#what-is-it",
    "href": "posts/036_14Nov_2025/index.html#what-is-it",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "3.1 What is it?",
    "text": "3.1 What is it?\nThe bootstrap method is a resampling technique that estimates the sampling distribution of a statistic by treating the original sample as a proxy for the population. Instead of drawing new samples from an unknown population, which is what we learned about earlier, we simulate this process by repeatedly drawing samples - with replacement - from our single observed sample. And this allows us to approximate the variability and properties of the statistic, based on the assumption that our single, observed sample is a good representation of the underlying population.\nIn other words, the bootstrap treats the original sample as a miniature, empirical population. Each bootstrap sample is the same size as the original and is created by sampling with replacement. This ‚Äúwith replacement‚Äù step is critical because it ensures each bootstrap sample is a unique combination of values from the original data, simulating the variability you‚Äôd expect from a new sample.\nSo we use the same steps here that I outlined earlier in constructing the true sampling distribution - that is, for each bootstrap sample (and we typically specify thousands of them), we calculate our summary statistic and plot these as a frequency histogram. The collection of all these bootstrap sample statistics forms the bootstrap sampling distribution, which then serves as an estimate of the true sampling distribution.\nWe can then calculate other important measures such as the SE and CI‚Äôs by reading the 2.5 and 97.5 percentile values directly off the plot. We don‚Äôt actually need to invoke any mathematical formulae as we previously did - and that‚Äôs because we have an actual distribution now rather than just a theoretical one."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#bootstrap-sampling-distribution",
    "href": "posts/036_14Nov_2025/index.html#bootstrap-sampling-distribution",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "3.2 Bootstrap sampling distribution",
    "text": "3.2 Bootstrap sampling distribution\nRemember what our theoretical sampling distribution looked like? (scroll up if you don‚Äôt). Now when we look at our bootstrapped sampling distribution, there isn‚Äôt really much that‚Äôs changed. The main differences are that we‚Äôve substituted our only sample for our population and we‚Äôre now ‚Äòresampling‚Äô from that, rather than sampling from our population. Everything else basically stays the same. Note how we can easily extract the confidence limits ‚Äòempirically‚Äô, directly from the plot, by just ordering all the values from lowest to highest and taking the values at the 2.5 and 97.5 percentiles. These correspond to the lower and upper confidence limits, giving us 95% coverage for the true population parameter. Remember, the beauty of this method is that it doesn‚Äôt assume a specific distribution for the data, and that is extremely useful when the classical assumptions aren‚Äôt met."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#sampling-distributions-reimagined",
    "href": "posts/036_14Nov_2025/index.html#sampling-distributions-reimagined",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "3.3 Sampling distributions reimagined",
    "text": "3.3 Sampling distributions reimagined\nThese aren‚Äôt my images but I thought I‚Äôd show them to you because it‚Äôs a tangible visual take on the same two concepts, using the global population, and I think if you‚Äôve been having some trouble following along, this should make things a lot clearer. The top picture shows the true (or theoretical) sampling distribution for a mean. We start off with all 7.6 billion people in the world and then take multiple samples from the population, calculating the mean in each sample and then plotting the distribution of those sample means.\nYou can also easily appreciate how the bootstrap sampling distribution differs, below that. We might still start off with our population, but it remains unrealised, and all we actually have is the one sample that we draw from it. Our bootstrap samples are then resamples of that one sample, with replacement. Note how in each bootstrap sample, one individual has been sampled twice - the grey person in the first, the purple in the second and the green in the third. But that‚Äôs fine and to be expected. We then calculate the mean in each resample and plot the distribution of those means.\n\nTheoretical\n\n\n\n\n\n\n\nBootstrap"
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#sample-mean---raw-data-distribution",
    "href": "posts/036_14Nov_2025/index.html#sample-mean---raw-data-distribution",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "4.1 Sample Mean - Raw Data Distribution",
    "text": "4.1 Sample Mean - Raw Data Distribution\nTo do this in R we‚Äôre going to use an inbuilt dataset, and in fact it doesn‚Äôt even matter what that is, so I‚Äôm not going to describe it here (details are in the code at the end of this post that will allow you to fully reproduce all analyses).\n\n\n\n\n\nThis is the raw data distribution for the variable we‚Äôll be bootstrapping the mean for, and it consists of 47 observations. You could argue that there‚Äôs a normal shape to it, but in all honesty, there probably aren‚Äôt enough data points to say that with certainty.\nWe‚Äôll now take this variable and we‚Äôll bootstrap it 1000 times - in other words we‚Äôll take 1000 resamples each of size 47, replacing each value in the event that it‚Äôs drawn. Then we‚Äôll calculate the mean value in each of those 1000 resamples.\n\n\nCode\n# This script runs bootstraps on a sample mean and correlation coefficient and then plots the sampling distributions and CI's from the different methods as well as comparing to theoretical results.\n\nlibrary(tidyverse)\nlibrary(ggExtra)\nlibrary(boot)\n\ndata(\"swiss\")\nswiss\n\n# Function to quickly extract all CI's from a boot.ci object\nextract_boot_ci &lt;-  function(boot_ci_obj){\n    ci_df &lt;- names(boot_ci_obj) |&gt; \n        # We only want the CI types, which are stored as matrices\n        keep(~ is.matrix(boot_ci_obj[[.x]])) |&gt; \n        map_df(~ {\n            ci_matrix &lt;- boot_ci_obj[[.x]]\n            # Extract the lower/upper bounds\n            lower_bound &lt;- ci_matrix[1, ncol(ci_matrix) - 1]\n            upper_bound &lt;- ci_matrix[1, ncol(ci_matrix)]\n            # Return a data frame for this CI type\n            tibble(\n                type = .x,\n                lower_ci = lower_bound,\n                upper_ci = upper_bound\n            )\n        }) |&gt; \n        # Optionally, arrange the data frame for cleaner viewing\n        mutate(type = factor(type, levels = c(\"normal\", \"basic\", \"percent\", \"bca\", 'Theoretical'), labels = c(\"Bootstrap - Normal\", \"Bootstrap - Basic\", \"Bootstrap - Percentile\", \"Bootstrap - Bias Corrected\", 'Theoretical'))) |&gt; \n        arrange(type)\n    ci_df\n}\n\n#++++++++++++++++++++++++++++++\n\n# SAMPLE MEAN ----\n\n# Plot raw data\nggplot(swiss, aes(x = Fertility)) +\n    geom_histogram(aes(y = ..density..), binwidth = 0.5, fill = \"lightblue\", color = \"black\") +\n    stat_density(aes(y = ..density..), geom = \"point\", position = \"identity\", \n                 color = \"darkblue\", size = 2) +\n    scale_y_continuous(labels = abs) +\n    labs(title = \"Raw Data Distribution\",\n         x = \"Data Value\", \n         y = \"Density\") +\n    theme_minimal()\n\n# boot function\nboot_calc_mean &lt;- function(data, indices) {\n    # Use the indices to resample the data\n    sample_data &lt;- data[indices]\n    # Return the statistic (mean in this case)\n    return(mean(sample_data))\n}\n\n# boot\nset.seed(20250111)\nboot_mean &lt;- boot(swiss$Fertility, boot_calc_mean, R = 1000)\nboot_mean_CIs &lt;- boot.ci(boot_mean, type = \"all\")\nboot_mean_CIs\n\n# df of bootstrapped estimates\nbootstrap_df &lt;- data.frame(\n    bootstrap_means = as.vector(boot_mean$t),\n    original_mean = boot_mean$t0\n)\n\n# Calculate percentiles for confidence interval\nci_lower &lt;- quantile(boot_mean$t, 0.025)\nci_upper &lt;- quantile(boot_mean$t, 0.975)\n\n# Plot\nggplot(bootstrap_df, aes(x = bootstrap_means)) +\n    geom_histogram(aes(y = after_stat(density)), bins = 30, \n                   fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n    geom_density(color = \"blue\", size = 1) +\n    geom_vline(aes(xintercept = original_mean, color = \"Mean of Original Sample\"), linetype = \"dashed\", size = 1) +\n    geom_vline(aes(xintercept = mean(boot_mean$t), color = \"Mean of Bootstrap Sample Means\"), linetype = \"dashed\", size = 1) +\n    # Add percentile lines for 95% CI\n    geom_vline(xintercept = ci_lower, color = \"darkorange3\", linetype = \"dotted\", size = 1) +\n    geom_vline(xintercept = ci_upper, color = \"darkorange3\", linetype = \"dotted\", size = 1) +\n    # Add labels for percentile values\n    annotate(\"text\", x = ci_lower, y = max(density(boot_mean$t)$y) * 0.8, \n             label = paste(\"2.5%\\n\", round(ci_lower, 2)), \n             color = \"darkorange3\", hjust = 1.1, size = 3.5) +\n    annotate(\"text\", x = ci_upper, y = max(density(boot_mean$t)$y) * 0.8, \n             label = paste(\"97.5%\\n\", round(ci_upper, 2)), \n             color = \"darkorange3\", hjust = -0.1, size = 3.5) +\n    # Manual color scale for legend\n    scale_color_manual(name = \"\",\n                       values = c(\"Mean of Original Sample\" = \"red\", \"Mean of Bootstrap Sample Means\" = \"darkmagenta\")) +\n    labs(title = \"Bootstrap Distribution with 95% Percentile Confidence Interval\",\n         x = \"Bootstrap Sample Means\",\n         y = \"Density\",\n         caption = paste(\"95% Percentile CI: [\", round(ci_lower, 2), \", \", round(ci_upper, 2), \"]\")) +\n    theme_minimal() +\n    theme(legend.position = \"top\")\n\n# Compare to  (t.test gives exact CI's - not technically large sample)\nt.test_sample_ci &lt;-  t.test(swiss$Fertility)\n\n# Extract CI's into df\nboot_mean_CIs_df &lt;-  extract_boot_ci(boot_mean_CIs)\n# Add in t.test CI's\nboot_mean_CIs_df &lt;-  rbind(boot_mean_CIs_df, data.frame(type = \"Theoretical\", lower_ci = t.test_sample_ci$conf.int[1], upper_ci = t.test_sample_ci$conf.int[2]))\n\n# Plot all CI's for visualisation\nggplot(boot_mean_CIs_df) +\n    aes(xmin = lower_ci, xmax = upper_ci, y = type, color = type) + \n    geom_vline(xintercept = boot_mean$t0, color = \"red\", linetype = \"dashed\", linewidth = 1.2) + \n    geom_vline(xintercept = mean(boot_mean$t), color = \"darkmagenta\", linetype = \"dashed\", linewidth = 1.2) + \n    geom_errorbar() + \n    theme_minimal() + \n    theme(legend.position = \"none\") + \n    labs(y = \"\")\n    \n\n#++++++++++++++++++++++++++++++\n\n# SAMPLE CORRELATION COEFFICIENT ----\n\n# Plot bivariate raw data\nscatterplot &lt;-  ggplot(swiss, aes(x = Fertility, y = Education)) +\n    geom_point(color = \"darkblue\", alpha = 0.7) +\n    theme_minimal() +\n    labs(title = \"Bivariate Raw Data Distribution\",\n         x = \"Data Value 1\",\n         y = \"Data Value 2\")\n# Add the marginal density plots using ggMarginal().\n# The 'type' argument specifies the type of marginal plot (e.g., \"density\", \"histogram\", \"boxplot\").\n# The 'fill' argument sets the fill color of the marginal plots.\nggMarginal(scatterplot,\n           type = \"density\",\n           fill = \"#D55E00\",\n           alpha = 0.7)\n\n# boot function (note: data should be a data frame or matrix with 2 columns)\nboot_calc_cor &lt;- function(data, indices) {\n    # Resample the data using indices (this resamples paired observations)\n    resampled_data &lt;- data[indices, ]\n    # Calculate correlation between the two variables\n    cor(resampled_data[, 1], resampled_data[, 2])\n}\n\n# boot\nset.seed(20250111)\nboot_cor &lt;- boot(cbind(swiss$Fertility, swiss$Education), boot_calc_cor, R = 1000)\nboot_cor_CIs &lt;- boot.ci(boot_cor, type = \"all\")\nboot_cor_CIs\n\n# df of bootstrapped estimates\nbootstrap_df &lt;- data.frame(\n    bootstrap_cors = as.vector(boot_cor$t),\n    original_cor = boot_cor$t0\n)\n\n# Calculate percentiles for confidence interval\nci_lower &lt;- quantile(boot_cor$t, 0.025)\nci_upper &lt;- quantile(boot_cor$t, 0.975)\n\n# Plot\nggplot(bootstrap_df, aes(x = bootstrap_cors)) +\n    geom_histogram(aes(y = after_stat(density)), bins = 30, \n                   fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n    geom_density(color = \"blue\", size = 1) +\n    geom_vline(aes(xintercept = original_cor, color = \"Correlation of Original Sample\"), linetype = \"dashed\", size = 1) +\n    geom_vline(aes(xintercept = mean(boot_cor$t), color = \"Mean of Bootstrap Sample Correlations\"), linetype = \"dashed\", size = 1) +\n    # Add percentile lines for 95% CI\n    geom_vline(xintercept = ci_lower, color = \"darkorange3\", linetype = \"dotted\", size = 1) +\n    geom_vline(xintercept = ci_upper, color = \"darkorange3\", linetype = \"dotted\", size = 1) +\n    # Add labels for percentile values\n    annotate(\"text\", x = ci_lower, y = max(density(boot_mean$t)$y) * 0.8, \n             label = paste(\"2.5%\\n\", round(ci_lower, 2)), \n             color = \"darkorange3\", hjust = 1.1, vjust = -7, size = 3.5) +\n    annotate(\"text\", x = ci_upper, y = max(density(boot_mean$t)$y) * 0.8, \n             label = paste(\"97.5%\\n\", round(ci_upper, 2)), \n             color = \"darkorange3\", hjust = -0.1, vjust = -7, size = 3.5) +\n    # Manual color scale for legend\n    scale_color_manual(name = \"\",\n                       values = c(\"Correlation of Original Sample\" = \"red\", \"Mean of Bootstrap Sample Correlations\" = \"darkmagenta\")) +\n    labs(title = \"Bootstrap Distribution with 95% Percentile Confidence Interval\",\n         x = \"Bootstrap Sample Correlations\",\n         y = \"Density\",\n         caption = paste(\"95% Percentile CI: [\", round(ci_lower, 2), \", \", round(ci_upper, 2), \"]\")) +\n    theme_minimal() +\n    theme(legend.position = \"top\")\n\n# Compare to (cor.test gives large sample approximation)\ncor.test_sample_ci &lt;-  cor.test(swiss$Fertility, swiss$Education)\n\n# Extract CI's into df\nboot_cor_CIs_df &lt;-  extract_boot_ci(boot_cor_CIs)\n# Add in t.test CI's\nboot_cor_CIs_df &lt;-  rbind(boot_cor_CIs_df, data.frame(type = \"Theoretical\", lower_ci = cor.test_sample_ci$conf.int[1], upper_ci = cor.test_sample_ci$conf.int[2]))\n\n# Plot all CI's for visualisation\nggplot(boot_cor_CIs_df) +\n    aes(xmin = lower_ci, xmax = upper_ci, y = type, color = type) + \n    geom_vline(xintercept = boot_cor$t0, color = \"red\", linetype = \"dashed\", linewidth = 1.2) + \n    geom_vline(xintercept = mean(boot_cor$t), color = \"darkmagenta\", linetype = \"dashed\", linewidth = 1.2) + \n    geom_errorbar() + \n    theme_minimal() + \n    theme(legend.position = \"none\") + \n    labs(y = \"\")"
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#raw-data-distribution",
    "href": "posts/036_14Nov_2025/index.html#raw-data-distribution",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "5.1 Raw Data Distribution",
    "text": "5.1 Raw Data Distribution\nTo do this in R we‚Äôre going to use an inbuilt dataset, and in fact it doesn‚Äôt even matter what that is, so I‚Äôm not going to describe it here (details are in the code at the end of this post that will allow you to fully reproduce all analyses).\n\n\n\n\n\nThis is the raw data distribution for the variable we‚Äôll be bootstrapping the mean for, and it consists of 47 observations. You could argue that there‚Äôs a normal shape to it, but in all honesty, there probably aren‚Äôt enough data points to say that with certainty.\nWe‚Äôll now take this variable and we‚Äôll bootstrap it 1000 times - in other words we‚Äôll take 1000 resamples each of size 47, replacing each value in the event that it‚Äôs drawn. Then we‚Äôll calculate the mean value in each of those 1000 resamples."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#sampling-distribution",
    "href": "posts/036_14Nov_2025/index.html#sampling-distribution",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "5.2 Sampling Distribution",
    "text": "5.2 Sampling Distribution\nWhen we construct a frequency histogram of those 1000 mean values, we see the following:\n\n\n\n\n\nThere are a couple of salient things to note:\n\nThe first is that the bootstrap sampling distribution is quite normal in shape, even though the raw data distribution might not have been.\nThe second point is that the mean of the original sample and the mean of all the bootstrapped means is virtually identical.\n\nThis is a good thing as it means that the sample mean is an unbiased estimator of the population mean. Another way of saying this is that ‚Äúif I were to repeat this study many times, the sample mean would, on average, hit the true population mean.‚Äù\n\nThe last thing to say about this plot is that if we wanted to obtain the bootstrapped confidence limits we could simply read off the values corresponding to the 2.5 and 97.5 percentiles from the plot. Of course, in practice you‚Äôd get your stats software to do this for you, but the point is that it‚Äôs quite easy to do and doesn‚Äôt involve any formulae."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#comparison-of-95-c.i.s",
    "href": "posts/036_14Nov_2025/index.html#comparison-of-95-c.i.s",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "5.3 Comparison of 95% C.I.‚Äôs",
    "text": "5.3 Comparison of 95% C.I.‚Äôs\n\n\n\n\n\nAnd these are the 95% CI‚Äôs from the various methods. The first one is the ‚Äòtheoretical‚Äô - assuming normality and the others are derived from the boot.ci function after doing the bootstrapping procedure. Really, there‚Äôs not a lot to say about this - you can see that the coverage of all the CI‚Äôs is fairly similar, and that‚Äôs a good thing as it means you can can have increased confidence in the robustness of your results. (Note - the ‚Äòtheoretical‚Äô CI in this case for the mean is based off a t test which enables ‚Äòexact‚Äô CI‚Äôs to be calculated as the PDF of the sampling distribution is known. So while I say ‚Äòassuming normality‚Äô in this case it‚Äôs really an exact CI)."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#raw-data-distribution-1",
    "href": "posts/036_14Nov_2025/index.html#raw-data-distribution-1",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "6.1 Raw Data Distribution",
    "text": "6.1 Raw Data Distribution\n\n\n\n\n\nOk - let‚Äôs now consider a different sample statistic that we might be interested in - the correlation coefficient. This is a scatterplot with overlaid density plot of the previous variable (on the x-axis) and a second variable (on the y-axis) from the same dataset. The marginal distribution of the second variable is far from normal as I‚Äôm sure you can appreciate, by looking at the density curve on the right side. When we look at the bivariate relationship in terms of the scatterplot itself, it‚Äôs not hard to imagine a negative relationship between the two variables."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#sampling-distribution-1",
    "href": "posts/036_14Nov_2025/index.html#sampling-distribution-1",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "6.2 Sampling Distribution",
    "text": "6.2 Sampling Distribution\nIn contrast to the sample mean, the sampling distribution of the correlation coefficient does NOT have the same, simple, normal shape, straight out of the box. This statistic definitely relies on asymptotic normality based on having a large sample - larger than you would require for the sample mean.\n\n\n\n\n\nThis time when we construct a frequency histogram of those 1000 mean values, we get quite a positively skewed bootstrap sampling distribution, and there is no way we could argue this is normal in shape. The other observation that we can easily make is that the original sample correlation coefficient is different (more negative) to the mean of all the bootstrapped correlation coefficients.\nWhat this reflects is that the sample correlation coefficient is a biased estimator of the population correlation coefficient. And another way of saying this is that ‚Äúif I were to repeat this study many times, the sample correlation would, on average, consistently be closer to zero than the true population correlation.‚Äù Now, this bias is worse as the correlation approaches either plus or minus one, and with small sample sizes. The bias reduces as the sample size increases according to our large sample theory for asymptotic normality.\nAs with the sample mean, if we want to obtain empirical 95% CI‚Äôs we can just read off the corresponding values at the 2.5 and 97.5 percentiles."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#comparison-of-95-cis",
    "href": "posts/036_14Nov_2025/index.html#comparison-of-95-cis",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "5.3 Comparison of 95% CI‚Äôs",
    "text": "5.3 Comparison of 95% CI‚Äôs\n\n\n\n\n\nAnd these are the 95% CI‚Äôs from the various methods. The first one is the ‚Äòtheoretical‚Äô - assuming normality and the others are derived from the boot.ci function after doing the bootstrapping procedure. Really, there‚Äôs not a lot to say about this - you can see that the coverage of all the CI‚Äôs is fairly similar, and that‚Äôs a good thing as it means you can can have increased confidence in the robustness of your results. (Note - the ‚Äòtheoretical‚Äô CI in this case for the mean is based off a t test which enables ‚Äòexact‚Äô CI‚Äôs to be calculated as the PDF of the sampling distribution is known. So while I say ‚Äòassuming normality‚Äô in this case it‚Äôs really an exact CI)."
  },
  {
    "objectID": "posts/036_14Nov_2025/index.html#comparison-of-95-cis-1",
    "href": "posts/036_14Nov_2025/index.html#comparison-of-95-cis-1",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "6.3 Comparison of 95% CI‚Äôs",
    "text": "6.3 Comparison of 95% CI‚Äôs\n\n\n\n\n\nConfidence interval coverage with the correlation coefficient is also a little different to what we previously saw with the sample mean. The theoretical and two of the bootstrap intervals - the percentile and the bias-corrected percentile are quite similar, whereas the remaining two bootstrap intervals - the normal and basic are quite different. OK, so what do we believe here? Well, the normal and basic intervals should really only be trusted when we‚Äôve got a large sample size and a well-behaved statistic - and you could make a good argument that we don‚Äôt really have either of those two conditions being met here. Therefore it‚Äôs either the percentile method or its bias-corrected variant and as I mentioned before the latter is probably the best method, in general, to choose. When we compare the bias-corrected interval to the theoretical interval, we can see that in fact they‚Äôre not that different, but the bias-corrected is a little more conservative (i.e.¬†the CI is wider) - which is always a good thing, I think, in quantifying uncertainty.\nSo at the end of the day, for these data, it‚Äôs good to be able to report both types of CI‚Äôs - theoretical and empirical from the bootstrap. And that‚Äôs because we know from the outset that we‚Äôre dealing with a sample statistic that might not conform to the theoretical assumptions for CI estimation as well as a ‚Äòbetter behaved‚Äô statistic like the sample mean."
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html",
    "href": "posts/036_21Nov_2025/index.html",
    "title": "Large Language Models are Just Statistics",
    "section": "",
    "text": "If you‚Äôve played with OpenAI‚Äôs ChatGPT, Anthropic‚Äôs Claude, Google‚Äôs Gemini or any of the raft of other ‚Äòchatbots‚Äô, you‚Äôve seen how they can write, summarize, and even appear to reason. It feels like magic - but under the hood, large language models (LLMs) like GPT are doing something surprisingly familiar to anyone who works with data.\nThey‚Äôre doing statistics. And for a good part, they‚Äôre doing statistics that you already know.\nNot the kind of statistics that tests hypotheses or produces p-values, but the kind that finds patterns in data and makes predictions. At their core, LLMs are vast, sophisticated probability models - predicting, one ‚Äòtoken‚Äô at a time, what‚Äôs most likely to come next.\nI find the intuition behind LLM‚Äôs, fascinating. So I thought in today‚Äôs post - the last for the year - we could have a bit more fun in taking a fairly high-level look at LLM‚Äôs, how they work, and the fairly basic statistical concepts that underpin their function."
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html#basic-concepts",
    "href": "posts/036_21Nov_2025/index.html#basic-concepts",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.1 Basic concepts",
    "text": "2.1 Basic concepts\nAnd this is where the fundamental idea of the sampling distribution comes into play. In a nutshell, the sampling distribution represents the theoretical distribution of a sample statistic that‚Äôs derived from repeatedly randomly sampling a population of interest. Now this is a thought exercise only - we don‚Äôt do it for obvious reasons in practice - but it allows us to make assumptions about the behaviour of the population parameter that we are trying to estimate. (Note - while I say ‚Äòtheoretical‚Äô above, these distributions can now be verified in silico but in the pre-computer era were confirmed empirically by a combination of mathematical derivation and physical simulation - i.e.¬†bootstrapping by hand!)\nIn this thought exercise we would have a population that we are interested in calculating a parameter for, and we‚Äôd draw a sample of observations from this population. We‚Äôd then calculate the corresponding sample statistic - let‚Äôs say it‚Äôs a mean value - and we‚Äôd plot that on a frequency histogram. We would then repeat that process many times, plotting each mean value along the way. The resulting plot would show the distribution of all the sample means - and this is what we call the sampling distribution of the sample statistic. It‚Äôs important to note that this is NOT the distribution of the data itself, but the distribution of a summary statistic derived from the data.\n\n\n\n\n\nNow, depending on the sample size, the shape of the underlying raw data distribution and the specific summary statistic we‚Äôre interested in, sampling distributions for many statistics often end up looking normal (or close to normal) in shape. And that allows us to leverage fairly simple normal distribution properties such as the mean and standard deviation (SD) to infer the population mean and it‚Äôs associated uncertainty. The mean should converge to the actual population parameter and the SD tells us about the uncertainty in the estimation of the parameter - and in fact is directly interpretable from the sampling distribution itself, as the standard error (SE). Once we have the SE it becomes trivial to calculate the 95% confidence interval (CI).\nThe point in telling you all of this is to highlight to you is that we can use just a single sample to form probabilistic statements about a population parameter, rather than needing to measure the entire population."
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html#when-it-works",
    "href": "posts/036_21Nov_2025/index.html#when-it-works",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.2 When it works",
    "text": "2.2 When it works\nMost of the time the theory of parametric sampling distributions will work just fine for what you want to do. Probability density functions (PDF‚Äôs) that define the ‚Äòshape‚Äô of the sampling distribution have been derived for many sample statistics. These equations are sometimes referred to as ‚Äòclosed-form solutions‚Äô. Some PDF‚Äôs are fairly simple, other‚Äôs are almost intractably complex and some are simply unknown. But what is important about this, is that when you know the PDF for a sampling distribution, you can calculate exact 95% CI‚Äôs.\nWhen PDF‚Äôs become too difficult or are simply unknown, we can start to leverage approximations. And that probably explains why I haven‚Äôt needed to bootstrap much in my own work - the classic central limit theorem (CLT) does its job pretty well. The CLT basically states that you can have whatever shaped raw data distribution you want - flat, skewed, bi-modal - whatever, but when you then construct a sampling distribution from the resulting sample means, that distribution will be normal in shape (if you‚Äôve got a large enough sample).\nThe thing is, the logic of the central limit theorem translates fairly well to most other sample statistics of interest - for example, medians, proportions, correlations, regression coefficients, and more - through a concept called large-sample asymptotic normality. This approximation basically says that if our sample is large enough, the sampling distributions of these other parameters will likewise approach normality in terms of shape. What that means at the end of the day is that we can use fairly simple(-ish) equations for large sample approximations to estimate SE‚Äôs and 95% CI‚Äôs. The equations for the SE for the mean and proportion are shown below and I‚Äôm sure you‚Äôve seen these before.\n\\[ \\bar{x} \\pm 1.96^* \\frac{s}{\\sqrt{n}} \\hspace{2cm} \\widehat{p} \\pm 1.96^* \\sqrt{\\frac{\\widehat{p}(1 - \\widehat{p})}{n}} \\]"
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html#when-it-doesnt",
    "href": "posts/036_21Nov_2025/index.html#when-it-doesnt",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.3 When it doesn‚Äôt",
    "text": "2.3 When it doesn‚Äôt\nClearly, there are situations that can occasionally arise when basic sampling theory might not be adequate for your analytic needs, otherwise we wouldn‚Äôt have this post. To my mind there are three main reasons why you might look further afield to a resampling method like the bootstrap to support your analyses.\n\nThe first is sample size. Asymptotic normal theory relies on a ‚Äòlarge‚Äô sample size to be accurate, and the bootstrap deals with smaller samples much better. However, it is itself not immune to small sample bias (when n becomes quite small - say &lt; 15). In such cases not much can save you unless you collect more data, so you might just need to rely on descriptive statistics only.\nThe second situation is where equations for the SE are either so complex as to be virtually intractable, or simply don‚Äôt exist (as described above). Here you can use the bootstrap to estimate the sampling distribution directly with relative ease.\nFinally, for sampling distributions that depart quite obviously from normality. Here the large-sample approximations just don‚Äôt work well, but you can use the bootstrap in these cases to actually capture that non-normal shape and apply it in valid inference."
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html#why-is-it-called-the-bootstrap",
    "href": "posts/036_21Nov_2025/index.html#why-is-it-called-the-bootstrap",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.4 Why is it called the bootstrap?",
    "text": "2.4 Why is it called the bootstrap?\nAlright - let‚Äôs actually talk about The Bootstrap! There is some interesting history in how the bootstrap came to be called what it is, as it‚Äôs etymology isn‚Äôt from the statistical domain. The origins of the term are sometimes attributed to an 18th century work of fiction - The Surprising Adventures of Baron Munchausen - in which Baron Munchausen‚Äôs plan for getting himself (and his horse) out of a swamp was to pull himself out by his bootstraps. Curiously there appears to be no actual reference to his bootstraps in the story itself, where instead he uses his own hair (pigtails to be specific).\nIn any case, over time the term evolved to mean many things but with the overarching theme of ‚Äòperforming a near impossible task‚Äô, or ‚Äòdoing more with less‚Äô. It is not unheard of today in political discourse as a narrative for self-starting economic mobility - that is, ‚Äúif you just put in the hard work, you will eventually be successful‚Äù.\nIn statistics specifically, the bootstrap is used to mean that the population parameter we are interested in can be sufficiently defined by the sample of data that we have. In other words, ‚Äòthe sample ‚Äúpulls itself up by its bootstraps‚Äù‚Äô."
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html#an-important-statistical-idea",
    "href": "posts/036_21Nov_2025/index.html#an-important-statistical-idea",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.5 An important statistical idea",
    "text": "2.5 An important statistical idea\nBefore I get to explaining what the bootstrap is in more detail, let me start by introducing a paper that was published in 2020 by two very well-known American statisticians - What are the most important statistical ideas of the past 50 years.\n\nCounterfactual causal inference\nBootstrapping and simulation-based inference\nOverparameterized models and regularization\nMultilevel models\nGeneric computation algorithms\nAdaptive decision analysis\nRobust inference\nExploratory data analysis\n\nEach of these ideas has existed in some form prior to the 1970s, both in the theoretical statistics literature and in the practice of various applied Ô¨Åelds. But the authors consider that each has developed enough in the past 50 years to have essentially become something new and in many instances this has been facilitated by the modern computing age, as some of these techniques just weren‚Äôt practical to apply before we had fast computers. We have some ideas that you might already be familiar with from traditional statistics - counterfactual inference, multilevel model, robust inference, exploratory data analysis; and perhaps others that might be less so because they fall more into the realm of data science and predictive analytics - overparameterised models, generic algorithms and decision analysis.\nGiven that bootstrapping is one of these important statistical ideas from the last 50 years, let‚Äôs now learn some more about it."
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html#what-is-it",
    "href": "posts/036_21Nov_2025/index.html#what-is-it",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "3.1 What is it?",
    "text": "3.1 What is it?\nThe bootstrap method is a resampling technique that estimates the sampling distribution of a statistic by treating the original sample as a proxy for the population. Instead of drawing new samples from an unknown population, which is what we learned about earlier, we simulate this process by repeatedly drawing samples - with replacement - from our single observed sample. And this allows us to approximate the variability and properties of the statistic, based on the assumption that our single, observed sample is a good representation of the underlying population.\nIn other words, the bootstrap treats the original sample as a miniature, empirical population. Each bootstrap sample is the same size as the original and is created by sampling with replacement. This ‚Äúwith replacement‚Äù step is critical because it ensures each bootstrap sample is a unique combination of values from the original data, simulating the variability you‚Äôd expect from a new sample.\nSo we use the same steps here that I outlined earlier in constructing the true sampling distribution - that is, for each bootstrap sample (and we typically specify thousands of them), we calculate our summary statistic and plot these as a frequency histogram. The collection of all these bootstrap sample statistics forms the bootstrap sampling distribution, which then serves as an estimate of the true sampling distribution.\nWe can then calculate other important measures such as the SE and CI‚Äôs by reading the 2.5 and 97.5 percentile values directly off the plot. We don‚Äôt actually need to invoke any mathematical formulae as we previously did - and that‚Äôs because we have an actual distribution now rather than just a theoretical one."
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html#bootstrap-sampling-distribution",
    "href": "posts/036_21Nov_2025/index.html#bootstrap-sampling-distribution",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "3.2 Bootstrap sampling distribution",
    "text": "3.2 Bootstrap sampling distribution\nRemember what our theoretical sampling distribution looked like? (scroll up if you don‚Äôt). Now when we look at our bootstrapped sampling distribution, there isn‚Äôt really much that‚Äôs changed. The main differences are that we‚Äôve substituted our only sample for our population and we‚Äôre now ‚Äòresampling‚Äô from that, rather than sampling from our population. Everything else basically stays the same. Note how we can easily extract the confidence limits ‚Äòempirically‚Äô, directly from the plot, by just ordering all the values from lowest to highest and taking the values at the 2.5 and 97.5 percentiles. These correspond to the lower and upper confidence limits, giving us 95% coverage for the true population parameter. Remember, the beauty of this method is that it doesn‚Äôt assume a specific distribution for the data, and that is extremely useful when the classical assumptions aren‚Äôt met."
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html#sampling-distributions-reimagined",
    "href": "posts/036_21Nov_2025/index.html#sampling-distributions-reimagined",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "3.3 Sampling distributions reimagined",
    "text": "3.3 Sampling distributions reimagined\nThese aren‚Äôt my images but I thought I‚Äôd show them to you because it‚Äôs a tangible visual take on the same two concepts, using the global population, and I think if you‚Äôve been having some trouble following along, this should make things a lot clearer. The top picture shows the true (or theoretical) sampling distribution for a mean. We start off with all 7.6 billion people in the world and then take multiple samples from the population, calculating the mean in each sample and then plotting the distribution of those sample means.\nYou can also easily appreciate how the bootstrap sampling distribution differs, below that. We might still start off with our population, but it remains unrealised, and all we actually have is the one sample that we draw from it. Our bootstrap samples are then resamples of that one sample, with replacement. Note how in each bootstrap sample, one individual has been sampled twice - the grey person in the first, the purple in the second and the green in the third. But that‚Äôs fine and to be expected. We then calculate the mean in each resample and plot the distribution of those means.\n\nTheoretical\n\n\n\n\n\n\n\nBootstrap"
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html#raw-data-distribution",
    "href": "posts/036_21Nov_2025/index.html#raw-data-distribution",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "5.1 Raw Data Distribution",
    "text": "5.1 Raw Data Distribution\nTo do this in R we‚Äôre going to use an inbuilt dataset, and in fact it doesn‚Äôt even matter what that is, so I‚Äôm not going to describe it here (details are in the code at the end of this post that will allow you to fully reproduce all analyses).\n\n\n\n\n\nThis is the raw data distribution for the variable we‚Äôll be bootstrapping the mean for, and it consists of 47 observations. You could argue that there‚Äôs a normal shape to it, but in all honesty, there probably aren‚Äôt enough data points to say that with certainty.\nWe‚Äôll now take this variable and we‚Äôll bootstrap it 1000 times - in other words we‚Äôll take 1000 resamples each of size 47, replacing each value in the event that it‚Äôs drawn. Then we‚Äôll calculate the mean value in each of those 1000 resamples."
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html#sampling-distribution",
    "href": "posts/036_21Nov_2025/index.html#sampling-distribution",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "5.2 Sampling Distribution",
    "text": "5.2 Sampling Distribution\nWhen we construct a frequency histogram of those 1000 mean values, we see the following:\n\n\n\n\n\nThere are a couple of salient things to note:\n\nThe first is that the bootstrap sampling distribution is quite normal in shape, even though the raw data distribution might not have been.\nThe second point is that the mean of the original sample and the mean of all the bootstrapped means is virtually identical.\n\nThis is a good thing as it means that the sample mean is an unbiased estimator of the population mean. Another way of saying this is that ‚Äúif I were to repeat this study many times, the sample mean would, on average, hit the true population mean.‚Äù\n\nThe last thing to say about this plot is that if we wanted to obtain the bootstrapped confidence limits we could simply read off the values corresponding to the 2.5 and 97.5 percentiles from the plot. Of course, in practice you‚Äôd get your stats software to do this for you, but the point is that it‚Äôs quite easy to do and doesn‚Äôt involve any formulae."
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html#comparison-of-95-cis",
    "href": "posts/036_21Nov_2025/index.html#comparison-of-95-cis",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "5.3 Comparison of 95% CI‚Äôs",
    "text": "5.3 Comparison of 95% CI‚Äôs\n\n\n\n\n\nAnd these are the 95% CI‚Äôs from the various methods. The first one is the ‚Äòtheoretical‚Äô - assuming normality and the others are derived from the boot.ci function after doing the bootstrapping procedure. Really, there‚Äôs not a lot to say about this - you can see that the coverage of all the CI‚Äôs is fairly similar, and that‚Äôs a good thing as it means you can can have increased confidence in the robustness of your results. (Note - the ‚Äòtheoretical‚Äô CI in this case for the mean is based off a t test which enables ‚Äòexact‚Äô CI‚Äôs to be calculated as the PDF of the sampling distribution is known. So while I say ‚Äòassuming normality‚Äô in this case it‚Äôs really an exact CI)."
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html#raw-data-distribution-1",
    "href": "posts/036_21Nov_2025/index.html#raw-data-distribution-1",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "6.1 Raw Data Distribution",
    "text": "6.1 Raw Data Distribution\n\n\n\n\n\nOk - let‚Äôs now consider a different sample statistic that we might be interested in - the correlation coefficient. This is a scatterplot with overlaid density plot of the previous variable (on the x-axis) and a second variable (on the y-axis) from the same dataset. The marginal distribution of the second variable is far from normal as I‚Äôm sure you can appreciate, by looking at the density curve on the right side. When we look at the bivariate relationship in terms of the scatterplot itself, it‚Äôs not hard to imagine a negative relationship between the two variables."
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html#sampling-distribution-1",
    "href": "posts/036_21Nov_2025/index.html#sampling-distribution-1",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "6.2 Sampling Distribution",
    "text": "6.2 Sampling Distribution\nIn contrast to the sample mean, the sampling distribution of the correlation coefficient does NOT have the same, simple, normal shape, straight out of the box. This statistic definitely relies on asymptotic normality based on having a large sample - larger than you would require for the sample mean.\n\n\n\n\n\nThis time when we construct a frequency histogram of those 1000 mean values, we get quite a positively skewed bootstrap sampling distribution, and there is no way we could argue this is normal in shape. The other observation that we can easily make is that the original sample correlation coefficient is different (more negative) to the mean of all the bootstrapped correlation coefficients.\nWhat this reflects is that the sample correlation coefficient is a biased estimator of the population correlation coefficient. And another way of saying this is that ‚Äúif I were to repeat this study many times, the sample correlation would, on average, consistently be closer to zero than the true population correlation.‚Äù Now, this bias is worse as the correlation approaches either plus or minus one, and with small sample sizes. The bias reduces as the sample size increases according to our large sample theory for asymptotic normality.\nAs with the sample mean, if we want to obtain empirical 95% CI‚Äôs we can just read off the corresponding values at the 2.5 and 97.5 percentiles."
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html#comparison-of-95-cis-1",
    "href": "posts/036_21Nov_2025/index.html#comparison-of-95-cis-1",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "6.3 Comparison of 95% CI‚Äôs",
    "text": "6.3 Comparison of 95% CI‚Äôs\n\n\n\n\n\nConfidence interval coverage with the correlation coefficient is also a little different to what we previously saw with the sample mean. The theoretical and two of the bootstrap intervals - the percentile and the bias-corrected percentile are quite similar, whereas the remaining two bootstrap intervals - the normal and basic are quite different. OK, so what do we believe here? Well, the normal and basic intervals should really only be trusted when we‚Äôve got a large sample size and a well-behaved statistic - and you could make a good argument that we don‚Äôt really have either of those two conditions being met here. Therefore it‚Äôs either the percentile method or its bias-corrected variant and as I mentioned before the latter is probably the best method, in general, to choose. When we compare the bias-corrected interval to the theoretical interval, we can see that in fact they‚Äôre not that different, but the bias-corrected is a little more conservative (i.e.¬†the CI is wider) - which is always a good thing, I think, in quantifying uncertainty.\nSo at the end of the day, for these data, it‚Äôs good to be able to report both types of CI‚Äôs - theoretical and empirical from the bootstrap. And that‚Äôs because we know from the outset that we‚Äôre dealing with a sample statistic that might not conform to the theoretical assumptions for CI estimation as well as a ‚Äòbetter behaved‚Äô statistic like the sample mean."
  },
  {
    "objectID": "posts/2026 topics/037_13Feb_2026/index.html",
    "href": "posts/2026 topics/037_13Feb_2026/index.html",
    "title": "Stats Tip #31: Large Language Models Are Really Just About Statistics",
    "section": "",
    "text": "If you‚Äôve played with ChatGPT or similar tools, you‚Äôve seen how they can write, summarize, and even reason. It feels like magic ‚Äî but under the hood, large language models (LLMs) like GPT are doing something surprisingly familiar to anyone who works with data.\nThey‚Äôre doing statistics.\nNot the kind of statistics that tests hypotheses or produces p-values, but the kind that finds patterns in data and makes predictions.\nAt their core, LLMs are vast, sophisticated probability models ‚Äî predicting, one token at a time, what‚Äôs most likely to come next."
  },
  {
    "objectID": "posts/2026 topics/037_13Feb_2026/index.html#step-1-words-become-numbers-tokens",
    "href": "posts/2026 topics/037_13Feb_2026/index.html#step-1-words-become-numbers-tokens",
    "title": "Stats Tip #31: Large Language Models Are Really Just About Statistics",
    "section": "0.1 Step 1: Words become numbers (tokens)",
    "text": "0.1 Step 1: Words become numbers (tokens)\nComputers don‚Äôt understand words ‚Äî they understand numbers.\nSo the first step in building a language model is to turn text into numbers.\nThe text is broken down into small chunks called tokens. A token might be a whole word (‚Äúcat‚Äù), a piece of a word (‚Äúing‚Äù), or even punctuation. Each token is then assigned a number or vector representing its meaning in a multi-dimensional ‚Äúembedding‚Äù space ‚Äî kind of like plotting words in a giant coordinate system where similar words sit near each other.\nSo ‚Äúnurse‚Äù and ‚Äúdoctor‚Äù might have coordinates that are close together, while ‚Äúbanana‚Äù is somewhere far away."
  },
  {
    "objectID": "posts/2026 topics/037_13Feb_2026/index.html#step-2-predicting-the-next-token",
    "href": "posts/2026 topics/037_13Feb_2026/index.html#step-2-predicting-the-next-token",
    "title": "Stats Tip #31: Large Language Models Are Really Just About Statistics",
    "section": "0.2 Step 2: Predicting the next token",
    "text": "0.2 Step 2: Predicting the next token\nNow comes the fun part.\nThe model‚Äôs job is to predict the next token given the previous ones.\nFor example:\n\n‚ÄúMultiple sclerosis is a ___‚Äù\n\nAn LLM doesn‚Äôt ‚Äúknow‚Äù the answer ‚Äî it simply estimates, based on all the text it has seen during training, the probability of each possible next token:\n\n\n\nNext token\nProbability\n\n\n\n\n‚Äúdisease‚Äù\n0.72\n\n\n‚Äúcondition‚Äù\n0.12\n\n\n‚Äúvirus‚Äù\n0.05\n\n\n‚Äútherapy‚Äù\n0.02\n\n\n‚Ä¶\n‚Ä¶\n\n\n\nThe model then picks one ‚Äî usually the most probable, though sometimes it samples randomly to keep things natural.\nSound familiar?\nThat‚Äôs exactly what we do when we model probabilities in statistics ‚Äî just on a much smaller scale."
  },
  {
    "objectID": "posts/2026 topics/037_13Feb_2026/index.html#step-3-how-the-prediction-is-made-a-giant-logistic-regression",
    "href": "posts/2026 topics/037_13Feb_2026/index.html#step-3-how-the-prediction-is-made-a-giant-logistic-regression",
    "title": "Stats Tip #31: Large Language Models Are Really Just About Statistics",
    "section": "0.3 Step 3: How the prediction is made (a giant logistic regression)",
    "text": "0.3 Step 3: How the prediction is made (a giant logistic regression)\nIf you‚Äôve ever run a logistic regression, you‚Äôve already done something conceptually similar to what an LLM does billions of times over.\nIn logistic regression, we predict the probability of an event (say, relapse yes/no) based on predictors (like age, sex, treatment, etc.).\nLLMs are doing the same thing ‚Äî except: - the ‚Äúevent‚Äù is ‚Äúwhat‚Äôs the next token,‚Äù\n- and the ‚Äúpredictors‚Äù are the patterns of all the tokens that came before.\nInstead of a few coefficients, though, an LLM has hundreds of billions of parameters, each playing a tiny role in shaping these probabilities.\nThose parameters are organized in layers of artificial neurons ‚Äî hence the term neural network.\nEach neuron applies a small mathematical transformation to its inputs, passes the result to the next layer, and so on. The final layer produces a set of probabilities for the next token ‚Äî essentially a big softmax (a multi-category version of logistic regression).\nSo in statistical terms, the model is: &gt; ‚Äúa giant, multilayered logistic regression for text.‚Äù"
  },
  {
    "objectID": "posts/2026 topics/037_13Feb_2026/index.html#step-4-training-the-model",
    "href": "posts/2026 topics/037_13Feb_2026/index.html#step-4-training-the-model",
    "title": "Stats Tip #31: Large Language Models Are Really Just About Statistics",
    "section": "0.4 Step 4: Training the model",
    "text": "0.4 Step 4: Training the model\nHow does it learn those billions of parameters?\nThrough training ‚Äî a process that minimizes the difference between what the model predicts and what actually comes next in its massive training text.\nIt‚Äôs the same logic as minimizing a loss function in regression:\n- We predict,\n- We see how wrong we were,\n- We adjust the coefficients slightly,\n- And repeat ‚Äî billions of times.\nThis process is called gradient descent ‚Äî the workhorse of most machine learning algorithms."
  },
  {
    "objectID": "posts/2026 topics/037_13Feb_2026/index.html#step-5-emergent-behavior",
    "href": "posts/2026 topics/037_13Feb_2026/index.html#step-5-emergent-behavior",
    "title": "Stats Tip #31: Large Language Models Are Really Just About Statistics",
    "section": "0.5 Step 5: Emergent behavior",
    "text": "0.5 Step 5: Emergent behavior\nHere‚Äôs where things get interesting.\nWhen you scale this up ‚Äî with trillions of words, massive computation, and deep architectures ‚Äî the model starts showing emergent behavior.\nIt doesn‚Äôt just memorize text.\nIt starts to represent grammar, facts, reasoning steps, and even stylistic tone ‚Äî all as statistical patterns of tokens.\nTo a statistician, it‚Äôs still a model predicting conditional probabilities.\nTo a user, it looks like intelligence."
  },
  {
    "objectID": "posts/2026 topics/037_13Feb_2026/index.html#step-6-why-it-matters-for-statisticians",
    "href": "posts/2026 topics/037_13Feb_2026/index.html#step-6-why-it-matters-for-statisticians",
    "title": "Stats Tip #31: Large Language Models Are Really Just About Statistics",
    "section": "0.6 Step 6: Why it matters for statisticians",
    "text": "0.6 Step 6: Why it matters for statisticians\nUnderstanding that LLMs are ultimately statistical models helps demystify them.\nThey: - Don‚Äôt ‚Äúunderstand‚Äù text in a human sense.\n- Don‚Äôt have knowledge beyond what‚Äôs encoded in training data.\n- Can make systematic errors (bias, overfitting) like any model.\nBut they are a triumph of statistical modeling ‚Äî taking the simple idea of predicting the next observation and scaling it beyond anything we‚Äôve done before."
  },
  {
    "objectID": "posts/2026 topics/037_13Feb_2026/index.html#take-home-message",
    "href": "posts/2026 topics/037_13Feb_2026/index.html#take-home-message",
    "title": "Stats Tip #31: Large Language Models Are Really Just About Statistics",
    "section": "0.7 Take-home message",
    "text": "0.7 Take-home message\nLarge language models might look like artificial intelligence, but under the hood, they‚Äôre doing something you already know well:\n- taking data,\n- estimating probabilities,\n- and using those probabilities to make predictions.\nIn other words:\n&gt; It‚Äôs statistics, just at scale."
  },
  {
    "objectID": "posts/2026 topics/037_13Feb_2026/index.html#optional-simulating-a-mini-language-model-in-r",
    "href": "posts/2026 topics/037_13Feb_2026/index.html#optional-simulating-a-mini-language-model-in-r",
    "title": "Stats Tip #31: Large Language Models Are Really Just About Statistics",
    "section": "0.8 (Optional) Simulating a Mini Language Model in R",
    "text": "0.8 (Optional) Simulating a Mini Language Model in R\nTo make this concrete, here‚Äôs a toy example.\nLet‚Äôs simulate how a simple logistic model might ‚Äúpredict the next word‚Äù based on what came before.\n```r # Toy demonstration: next-word prediction as logistic regression"
  },
  {
    "objectID": "posts/2026 topics/llm/index.html",
    "href": "posts/2026 topics/llm/index.html",
    "title": "Large Language Models are Just Statistics",
    "section": "",
    "text": "If you‚Äôve played with OpenAI‚Äôs ChatGPT, Anthropic‚Äôs Claude, Google‚Äôs Gemini or any of the raft of other ‚Äòchatbots‚Äô, you‚Äôve seen how they can write, summarize, and even appear to reason. It feels like magic - but under the hood, large language models (LLMs) like GPT are doing something surprisingly familiar to anyone who works with data.\nThey‚Äôre doing statistics. And for a good part, they‚Äôre doing statistics that you already know.\nNot the kind of statistics that tests hypotheses or produces p-values, but the kind that finds patterns in data and makes predictions. At their core, LLMs are vast, sophisticated probability models - predicting, one ‚Äòtoken‚Äô at a time, what‚Äôs most likely to come next.\nI find the intuition behind LLM‚Äôs, fascinating. So I thought in today‚Äôs post - the last for the year - we could have a bit more fun in taking a fairly high-level look at LLM‚Äôs, how they work, and the fairly basic statistical concepts that underpin their function."
  },
  {
    "objectID": "posts/2026 topics/llm/index.html#step-1-words-become-numbers-tokens",
    "href": "posts/2026 topics/llm/index.html#step-1-words-become-numbers-tokens",
    "title": "Large Language Models are Just Statistics",
    "section": "0.1 Step 1: Words become numbers (tokens)",
    "text": "0.1 Step 1: Words become numbers (tokens)\nComputers don‚Äôt understand words ‚Äî they understand numbers.\nSo the first step in building a language model is to turn text into numbers.\nThe text is broken down into small chunks called tokens. A token might be a whole word (‚Äúcat‚Äù), a piece of a word (‚Äúing‚Äù), or even punctuation. Each token is then assigned a number or vector representing its meaning in a multi-dimensional ‚Äúembedding‚Äù space ‚Äî kind of like plotting words in a giant coordinate system where similar words sit near each other.\nSo ‚Äúnurse‚Äù and ‚Äúdoctor‚Äù might have coordinates that are close together, while ‚Äúbanana‚Äù is somewhere far away."
  },
  {
    "objectID": "posts/2026 topics/llm/index.html#step-2-predicting-the-next-token",
    "href": "posts/2026 topics/llm/index.html#step-2-predicting-the-next-token",
    "title": "Large Language Models are Just Statistics",
    "section": "2.5 Step 2: Predicting the next token",
    "text": "2.5 Step 2: Predicting the next token\nNow comes the fun part.\nThe model‚Äôs job is to predict the next token given the previous ones.\nFor example:\n\n‚ÄúMultiple sclerosis is a ___‚Äù\n\nAn LLM doesn‚Äôt ‚Äúknow‚Äù the answer ‚Äî it simply estimates, based on all the text it has seen during training, the probability of each possible next token:\n\n\n\nNext token\nProbability\n\n\n\n\n‚Äúdisease‚Äù\n0.72\n\n\n‚Äúcondition‚Äù\n0.12\n\n\n‚Äúvirus‚Äù\n0.05\n\n\n‚Äútherapy‚Äù\n0.02\n\n\n‚Ä¶\n‚Ä¶\n\n\n\nThe model then picks one ‚Äî usually the most probable, though sometimes it samples randomly to keep things natural.\nSound familiar?\nThat‚Äôs exactly what we do when we model probabilities in statistics ‚Äî just on a much smaller scale."
  },
  {
    "objectID": "posts/2026 topics/llm/index.html#step-3-how-the-prediction-is-made-a-giant-logistic-regression",
    "href": "posts/2026 topics/llm/index.html#step-3-how-the-prediction-is-made-a-giant-logistic-regression",
    "title": "Large Language Models are Just Statistics",
    "section": "2.6 Step 3: How the prediction is made (a giant logistic regression)",
    "text": "2.6 Step 3: How the prediction is made (a giant logistic regression)\nIf you‚Äôve ever run a logistic regression, you‚Äôve already done something conceptually similar to what an LLM does billions of times over.\nIn logistic regression, we predict the probability of an event (say, relapse yes/no) based on predictors (like age, sex, treatment, etc.).\nLLMs are doing the same thing ‚Äî except: - the ‚Äúevent‚Äù is ‚Äúwhat‚Äôs the next token,‚Äù\n- and the ‚Äúpredictors‚Äù are the patterns of all the tokens that came before.\nInstead of a few coefficients, though, an LLM has hundreds of billions of parameters, each playing a tiny role in shaping these probabilities.\nThose parameters are organized in layers of artificial neurons ‚Äî hence the term neural network.\nEach neuron applies a small mathematical transformation to its inputs, passes the result to the next layer, and so on. The final layer produces a set of probabilities for the next token ‚Äî essentially a big softmax (a multi-category version of logistic regression).\nSo in statistical terms, the model is: &gt; ‚Äúa giant, multilayered logistic regression for text.‚Äù"
  },
  {
    "objectID": "posts/2026 topics/llm/index.html#step-4-training-the-model",
    "href": "posts/2026 topics/llm/index.html#step-4-training-the-model",
    "title": "Large Language Models are Just Statistics",
    "section": "2.8 Step 4: Training the model",
    "text": "2.8 Step 4: Training the model\nHow does it learn those billions of parameters?\nThrough training ‚Äî a process that minimizes the difference between what the model predicts and what actually comes next in its massive training text.\nIt‚Äôs the same logic as minimizing a loss function in regression:\n- We predict,\n- We see how wrong we were,\n- We adjust the coefficients slightly,\n- And repeat ‚Äî billions of times.\nThis process is called gradient descent ‚Äî the workhorse of most machine learning algorithms."
  },
  {
    "objectID": "posts/2026 topics/llm/index.html#step-5-emergent-behavior",
    "href": "posts/2026 topics/llm/index.html#step-5-emergent-behavior",
    "title": "Large Language Models are Just Statistics",
    "section": "2.9 Step 5: Emergent behavior",
    "text": "2.9 Step 5: Emergent behavior\nHere‚Äôs where things get interesting.\nWhen you scale this up ‚Äî with trillions of words, massive computation, and deep architectures ‚Äî the model starts showing emergent behavior.\nIt doesn‚Äôt just memorize text.\nIt starts to represent grammar, facts, reasoning steps, and even stylistic tone ‚Äî all as statistical patterns of tokens.\nTo a statistician, it‚Äôs still a model predicting conditional probabilities.\nTo a user, it looks like intelligence."
  },
  {
    "objectID": "posts/2026 topics/llm/index.html#step-6-why-it-matters-for-statisticians",
    "href": "posts/2026 topics/llm/index.html#step-6-why-it-matters-for-statisticians",
    "title": "Large Language Models are Just Statistics",
    "section": "2.10 Step 6: Why it matters for statisticians",
    "text": "2.10 Step 6: Why it matters for statisticians\nUnderstanding that LLMs are ultimately statistical models helps demystify them.\nThey: - Don‚Äôt ‚Äúunderstand‚Äù text in a human sense.\n- Don‚Äôt have knowledge beyond what‚Äôs encoded in training data.\n- Can make systematic errors (bias, overfitting) like any model.\nBut they are a triumph of statistical modeling ‚Äî taking the simple idea of predicting the next observation and scaling it beyond anything we‚Äôve done before."
  },
  {
    "objectID": "posts/2026 topics/llm/index.html#take-home-message",
    "href": "posts/2026 topics/llm/index.html#take-home-message",
    "title": "Large Language Models are Just Statistics",
    "section": "3.2 Take-home message",
    "text": "3.2 Take-home message\nLarge language models might look like artificial intelligence, but under the hood, they‚Äôre doing something you already know well:\n- taking data,\n- estimating probabilities,\n- and using those probabilities to make predictions.\nIn other words:\n&gt; It‚Äôs statistics, just at scale."
  },
  {
    "objectID": "posts/2026 topics/llm/index.html#optional-simulating-a-mini-language-model-in-r",
    "href": "posts/2026 topics/llm/index.html#optional-simulating-a-mini-language-model-in-r",
    "title": "Large Language Models are Just Statistics",
    "section": "2.12 (Optional) Simulating a Mini Language Model in R",
    "text": "2.12 (Optional) Simulating a Mini Language Model in R\nTo make this concrete, here‚Äôs a toy example.\nLet‚Äôs simulate how a simple logistic model might ‚Äúpredict the next word‚Äù based on what came before.\n```r # Toy demonstration: next-word prediction as logistic regression"
  },
  {
    "objectID": "posts/2026 topics/llm/index.html#words-become-numbers-tokens",
    "href": "posts/2026 topics/llm/index.html#words-become-numbers-tokens",
    "title": "Large Language Models are Just Statistics",
    "section": "0.1 Words become numbers (tokens)",
    "text": "0.1 Words become numbers (tokens)\nComputers don‚Äôt understand words ‚Äî they understand numbers.\nSo the first step in building a language model is to turn text into numbers.\nThe text is broken down into small chunks called tokens. A token might be a whole word (‚Äúcat‚Äù), a piece of a word (‚Äúing‚Äù), or even punctuation. Each token is then assigned a number or vector representing its meaning in a multi-dimensional ‚Äúembedding‚Äù space ‚Äî kind of like plotting words in a giant coordinate system where similar words sit near each other.\nSo ‚Äúnurse‚Äù and ‚Äúdoctor‚Äù might have coordinates that are close together, while ‚Äúbanana‚Äù is somewhere far away."
  },
  {
    "objectID": "posts/2026 topics/llm/index.html#tokenisation---the-input-data",
    "href": "posts/2026 topics/llm/index.html#tokenisation---the-input-data",
    "title": "Large Language Models are Just Statistics",
    "section": "2.1 Tokenisation - The Input Data",
    "text": "2.1 Tokenisation - The Input Data\nComputers don‚Äôt understand words - they understand numbers. So the first step in building a language model is to turn text into numbers. The text is broken down into small chunks called tokens. Think of tokens as how LLM‚Äôs see the world. A token might be a whole word (‚Äòcat‚Äô), a piece of a word (‚Äòing‚Äô), or even punctuation. Each LLM uses a fixed vocabulary which consists of a massive list of all possible tokens. One of the benefits of chunking words into tokens - rather than using unique words - is that the vocabulary size is reduced (30,000 - 50,000 vs 100,000's), and this carries significant computational benefits.\nIn the example below we can see how the sentence ‚ÄúRobin slung the bow over his shoulder‚Äù is broken down into a series of tokens (for simplicity I have just used individual words, but you get the point)."
  },
  {
    "objectID": "posts/2026 topics/llm/index.html#embedding---converting-words-to-numbers.",
    "href": "posts/2026 topics/llm/index.html#embedding---converting-words-to-numbers.",
    "title": "Large Language Models are Just Statistics",
    "section": "2.2 Embedding - Converting Words to Numbers.",
    "text": "2.2 Embedding - Converting Words to Numbers.\nEach token is then assigned a number or vector representing its meaning in a multi-dimensional ‚Äòembedding‚Äô space - kind of like plotting words in a giant coordinate system where similar words sit near each other. So ‚Äònurse‚Äô and ‚Äòdoctor‚Äô might have coordinates that are close together, while ‚Äòbanana‚Äô and ‚Äòapple‚Äô may also be close together but far away from the other two words.\nHumans are able to conceptualise and visualise up to 3-dimensional spaces - typically where we have x, y and z co-ordinate systems. However, we struggle with higher-dimensional spaces. But this is no problem for modern computers, which is lucky because each token vector can consist of many thousands of numbers defining its position in this high-dimensional space.\nThe figure below shows an example of how the words mentioned above might be displayed in a 2-dimensional representation of a multi-dimensional space. These embeddings are projected onto a 2-dimensional plane to allow our brains to understand their relative positionings in a simplified space, but remember, this is much more complex in ‚Äòreality‚Äô. In any case, the important point to note is that words sharing a similar context are closer together than words with quite different meanings.\n\n\n\n\n\n\nImportant\n\n\n\nAt this point you may be wondering how does the model already have a good sense of token embeddings? And that‚Äôs a good question. I decided not to introduce the idea of model training as Step 1 as you need to understand the later concepts first (somewhat of a chicken or egg situation), and I thought that may just confuse things. But it‚Äôs the training of a model that provides this initial ‚Äòstatic‚Äô word positioning in high-dimensional space - in other words, training builds in some initial word/token context. The reason I refer to it as ‚Äòstatic‚Äô, is that context is very environment-dependent and thus needs updating in a dynamic capacity. We will explore this further in the next section‚Ä¶ But, for now, keep in mind the following as it relates to training. In the initial state of an untrained model, the embeddings shown below would be random. They could project onto any quadrant with no observable correlation. In other words, models start off with random token positionings and as they ‚Äòlearn‚Äô by ingesting huge amounts of data, they gradually refine the embeddings to what might be seen below in a trained model. Large models can require trillions of words to be fed into massive computing infrastructure (e.g.¬†clusters of GPU‚Äôs) running for weeks to months continuously, at millions of dollars in cost."
  },
  {
    "objectID": "posts/2026 topics/llm/index.html#neural-networks-and-the-transformer-architecture---giving-meaning-to-tokens.",
    "href": "posts/2026 topics/llm/index.html#neural-networks-and-the-transformer-architecture---giving-meaning-to-tokens.",
    "title": "Large Language Models are Just Statistics",
    "section": "2.3 Neural Networks and the Transformer Architecture - Giving meaning to tokens.",
    "text": "2.3 Neural Networks and the Transformer Architecture - Giving meaning to tokens."
  },
  {
    "objectID": "posts/2026 topics/llm/index.html#logistic-regression---predicting-the-next-token.",
    "href": "posts/2026 topics/llm/index.html#logistic-regression---predicting-the-next-token.",
    "title": "Large Language Models are Just Statistics",
    "section": "2.4 Logistic Regression - Predicting the Next Token.",
    "text": "2.4 Logistic Regression - Predicting the Next Token.\nAnd here we are. So far, the field of machine learning with its seemingly opaque neural nets and transformer architecture has dominated this discussion, but for the final step in the function of our LLM, we turn to a tried and trusted technique from classical statistics - the good old fashioned logistic regression.\nWhy?\nWell, when you think about it, the LLM‚Äôs only job is to predict.\nOnce the model has passed your input through all its transformer blocks, it ends up with a final embedding vector for each token - a rich numerical summary of that word‚Äôs meaning and context. The next step is to use that vector to predict which token comes next. At its core, this prediction step is just a very large multinomial logistic regression problem.\nIn ordinary logistic regression, we estimate the probability of a binary outcome (say, success vs failure). In multinomial logistic regression, there are many possible outcomes - in this case, every token in the model‚Äôs vocabulary (often tens of thousands). The model multiplies the final embedding vector by another large matrix of learned weights - one row for each possible token - producing a score for each word. Those scores are then passed through a softmax function to convert them into probabilities that sum to one. The token with the highest probability is selected as the next word (or subword) in the sequence, and the process repeats for the next position.\nYou can imagine this as running a giant logistic regression with, say, 50,000 possible categories - one for every token the model knows. For each step in the generated text, the model asks: Given everything I‚Äôve seen so far, which token is most probable next? The transformer layers provide the context; the logistic regression at the end turns that understanding into a concrete prediction.\nRemember, if you‚Äôve ever performed logistic regression, you have simply estimated a conditional probability. We can specify this mathematically as:\n\\[\nP(Y = 1 | X)\n\\]\nIn other words, what is the probability of the outcome (Y), given the covariate values (X) that we observed. This is really no different to the prediction task performed by LLM‚Äôs, where we can equivalently write:\n\\[\nP(Y = \\textrm{Token}_{\\textrm{(next)}} | \\textrm{Tokens}_{\\textrm{(previous)}})\n\\]\nThat is, what is the probability of the next token, given the previous tokens in the sequence.\nLLM‚Äôs might look like artificial intelligence, but under the hood, they‚Äôre doing something you already know well:\n\ntaking data,\nestimating probabilities,\nand using those probabilities to make predictions.\n\n\n2.4.1 Toy Example\nLet‚Äôs consider a small example to formalise that idea in our mind. Say we prompted an LLM with some text essentially asking it to tell us what the next word in the following sentence should be:\n\n‚ÄúMultiple sclerosis is a ___‚Äù\n\nAn LLM doesn‚Äôt ‚Äòknow‚Äô the answer ‚Äî it simply estimates, based on all the text it has seen during training, the probability of each possible next token. Let‚Äôs imagine our LLM only has 5 tokens (words) in it‚Äôs vocabulary. After typing our input at the prompt, this hypothetical LLM would go through each of the steps described above in contextualising the input to optimise the prediction of it‚Äôs one and only required token (word) as output. Internally it runs a multinomial logistic regression to assign probabilities to each of the tokens in it‚Äôs trained vocabulary, coming up with the following:\n\n\n\nNext token\nProbability\n\n\n\n\n‚Äúdisease‚Äù\n0.62\n\n\n‚Äúcondition‚Äù\n0.30\n\n\n‚Äúvirus‚Äù\n0.04\n\n\n‚Äútherapy‚Äù\n0.03\n\n\n‚Äúcure‚Äù\n0.01\n\n\n\nThe model then picks one ‚Äî usually the most probable, and then returns that to you as text output. Now imagine the same process occurs sequentially in an iterative fashion predicting one token conditional on all previous tokens - token after token - choosing among ~ 50,000 possible tokens. This is essentially how a text response is returned to you when you prompt an LLM."
  },
  {
    "objectID": "posts/2026 topics/llm/index.html#training-and-optimisation.",
    "href": "posts/2026 topics/llm/index.html#training-and-optimisation.",
    "title": "Large Language Models are Just Statistics",
    "section": "2.5 Training and Optimisation.",
    "text": "2.5 Training and Optimisation."
  },
  {
    "objectID": "posts/2026 topics/llm/index.html#high-dimensional-sapce-image",
    "href": "posts/2026 topics/llm/index.html#high-dimensional-sapce-image",
    "title": "Large Language Models are Just Statistics",
    "section": "2.3 HIGH DIMENSIONAL SAPCE IMAGE",
    "text": "2.3 HIGH DIMENSIONAL SAPCE IMAGE\nSo ‚Äònurse‚Äô and ‚Äòdoctor‚Äô might have coordinates that are close together, while ‚Äòbanana‚Äô and ‚Äòapple‚Äô may also be close together but far away from ‚Äònurse‚Äô and doctor‚Äô.\nOnce the text is tokenized, each token needs to be represented in a way that captures not just the token but also its meaning and relation to other tokens. This is where embeddings come in. An embedding is a vector representation of a token, and it places each token into a high-dimensional space. Tokens with similar meanings are closer together in this space, allowing the model to understand and leverage semantic and syntactic similarities and differences.\ntransformer How it works: It processes entire sequences simultaneously by using a self-attention mechanism. This allows each element in the input sequence to directly ‚Äúattend‚Äù to and weigh the importance of every other element. It also incorporates positional encoding to understand the order of elements. Key characteristic: Designed for high parallelism, making it much faster and more efficient at handling long sequences compared to traditional models. It excels at tasks like natural language processing because it can capture long-range context within a sentence or document. GPT - Generative Pretrained Transformer gives meaning to tokens within the context of other tokens. i.e.¬†contextualises the input text"
  },
  {
    "objectID": "posts/2026 topics/llm/index.html#neural-networks-and-the-transformer-architecture---giving-meaning-to-words.",
    "href": "posts/2026 topics/llm/index.html#neural-networks-and-the-transformer-architecture---giving-meaning-to-words.",
    "title": "Large Language Models are Just Statistics",
    "section": "2.3 Neural Networks and the Transformer Architecture - Giving Meaning to Words.",
    "text": "2.3 Neural Networks and the Transformer Architecture - Giving Meaning to Words.\nSo far, so good (I hope). Are you still with me? Ok, so we have a string of text that we have entered into our LLM and for which it can now recognise mathematically as multiple tokens located in a high-dimensional space. Pre-training means that tokens with similar meanings tend to correlate and clump together, but at this point there is no contextual awareness of one token with it‚Äôs surrounding tokens. For example, in the sentence above - ‚ÄòRobin slung the bow over his shoulder‚Äô - how does the LLM interpret the word ‚Äòbow‚Äô?\nThink about it for a moment. ‚ÄòBow‚Äô is an example of a homograph where a word with different meanings (and potentially pronunciations) is spelled the same in all cases. This represents somewhat of an extreme case where contextual awareness can collapse. For example, we can use ‚Äòbow‚Äô differently, but equally validly, in each of the three following sentences:\n\n(noun) A knot tied with two loops, usually used when tying shoelaces or wrapping gifts.\n‚ÄúShe made a little bow for her hair.‚Äù\n(noun) A weapon used in archery to shoot arrows.\n‚ÄúRobin slung the bow over his shoulder.‚Äù\n(verb) To bend the upper part of the body to show respect.\n‚ÄúWhen Hiromi greets people, she always bows.‚Äù\n\nWhat are the implications of this for an LLM? Well the first thing to note is that the initial embedding for ‚Äòbow‚Äô is the same in all cases. That is, the LLM‚Äôs initial mathematical representation for the word remains the same even though the meanings are quite different. In other words, from the model‚Äôs point of view, ‚Äòbow‚Äô has the same starting meaning in all contexts.\n\n\n\n\n\nSo how does the LLM figure out which ‚Äòbow‚Äô to use? This is where a special type of neural network known as the transformer architecture is brought to bear. The transformer model consists of repeatable ‚Äòblocks‚Äô as the fundamental unit of the architecture, with multiple ‚Äòlayers‚Äô, performing different functions, residing within each block. Perhaps the most important of these layers is what‚Äôs known as the self-attention layer.\n\n\n\n\n\n\nNote\n\n\n\nThe ‚ÄòGPT‚Äô in ChatGPT stands for Generative Pretrained Transformer\n\n\nSelf-attention is essentially a mechanism where all current token vectors are able to ‚Äòlook at‚Äô one another. Self-attention answers, for every token: ‚ÄòWhich other tokens in this sentence are most relevant to understanding me?‚Äô Self-attention is how the model dynamically refines meaning based on the surrounding words. After self-attention, each token‚Äôs vector is passed through small feed-forward neural networks - non-linear transformations that help capture more abstract relationships and patterns. Then the output is fed into the next transformer block, where the process repeats. Each block builds on the previous one, learning increasingly sophisticated relationships:\n\nEarly blocks: capture word-level patterns (‚Äúcat‚Äù ‚Üí noun, ‚Äúate‚Äù ‚Üí verb)\nMiddle blocks: capture phrase and syntactic structure (i.e.¬†good grammar)\nLater blocks: capture semantics, tone, and long-range dependencies (e.g., ‚ÄòThe doctor who treated the patient was praised by the hospital‚Äô ‚Üí knowing who did what to whom)\n\nSo, going back to our example above, ‚Äòbow‚Äô will end up correlating more strongly with ‚Äòslung‚Äô in the first example, ‚Äòhair‚Äô in the second example and ‚Äògreets‚Äô in the last example. Thus, even though the embedding for ‚Äòbow‚Äô starts off the same in each scenario, by the end of passing through multiple transformer blocks, the embedding will change in a context-aware way. In other words, each token‚Äôs vector now represents not just what the word is, but what it means here, in this sentence. Our projected high-dimensional space for ‚Äòbow‚Äô may now look something like that in the figure below.\n\n\n\n\n\nYou might be wondering about the maths behind how the self-attention layer updates each token embedding. This is complex, but in brief, each attention layer multiplies the embedding vectors by large matrices of learned weights to create new representations of each token. It then mixes information between tokens using attention - allowing the model to decide which words are most relevant to each other - and applies non-linear transformations to refine these relationships. The result is a new vector for each token that represents everything the model has ‚Äòunderstood‚Äô so far about the context of the input.\n\n\n\n\n\n\nNote\n\n\n\nMatrix multiplication, a fundamental concept in high-school level linear algebra classes, features heavily in the function of LLM‚Äôs"
  },
  {
    "objectID": "posts/2026 topics/llm/index.html#emergent-behavior",
    "href": "posts/2026 topics/llm/index.html#emergent-behavior",
    "title": "Large Language Models are Just Statistics",
    "section": "2.5 Emergent behavior",
    "text": "2.5 Emergent behavior\nHere‚Äôs where things get interesting.\nWhen you scale this up ‚Äî with trillions of words, massive computation, and deep architectures ‚Äî the model starts showing emergent behavior.\nIt doesn‚Äôt just memorize text.\nIt starts to represent grammar, facts, reasoning steps, and even stylistic tone ‚Äî all as statistical patterns of tokens.\nTo a statistician, it‚Äôs still a model predicting conditional probabilities.\nTo a user, it looks like intelligence."
  },
  {
    "objectID": "posts/2026 topics/llm/index.html#why-it-matters-for-statisticians",
    "href": "posts/2026 topics/llm/index.html#why-it-matters-for-statisticians",
    "title": "Large Language Models are Just Statistics",
    "section": "3.1 Why it matters for statisticians",
    "text": "3.1 Why it matters for statisticians\nUnderstanding that LLMs are ultimately statistical models helps demystify them.\nThey: - Don‚Äôt ‚Äúunderstand‚Äù text in a human sense.\n- Don‚Äôt have knowledge beyond what‚Äôs encoded in training data.\n- Can make systematic errors (bias, overfitting) like any model.\nBut they are a triumph of statistical modeling ‚Äî taking the simple idea of predicting the next observation and scaling it beyond anything we‚Äôve done before."
  },
  {
    "objectID": "posts/2026 topics/llm/index.html#training",
    "href": "posts/2026 topics/llm/index.html#training",
    "title": "Large Language Models are Just Statistics",
    "section": "3.3 Training",
    "text": "3.3 Training\nHow does it learn those billions of parameters?\nThrough training ‚Äî a process that minimizes the difference between what the model predicts and what actually comes next in its massive training text.\nIt‚Äôs the same logic as minimizing a loss function in regression:\n- We predict,\n- We see how wrong we were,\n- We adjust the coefficients slightly,\n- And repeat ‚Äî billions of times.\nThis process is called gradient descent ‚Äî the workhorse of most machine learning algorithms.\n\nnote on NUMBERS OF PARAMETERS\npoint to video on blue brown eyes"
  },
  {
    "objectID": "posts/2026 topics/llm/index.html#tying-up-some-loose-ends",
    "href": "posts/2026 topics/llm/index.html#tying-up-some-loose-ends",
    "title": "Large Language Models are Just Statistics",
    "section": "2.5 Tying Up Some Loose Ends",
    "text": "2.5 Tying Up Some Loose Ends\nThere remain a couple of concepts that are worth briefly exploring as these frequently pop up in discussions of LLM‚Äôs. Understanding these ideas can in turn be helpful in developing a deeper understanding of how these models work, so let‚Äôs take a look at them now:\n\n2.5.1 LLM‚Äôs - Machine Learning AND Classical Statistics\nWe have seen how LLM‚Äôs utilise both machine (deep) learning and traditional statistics as core functions in returning output to a user‚Äôs prompt. While these ‚Äòtwo cultures‚Äô of statistical modelling share some similarities, there are also many differences, and some of these are listed below. The main point to note in the case of LLM‚Äôs is the sheer scale at which these models operate.\n\n\n\nTraditional Statistics\nLLMs\n\n\n\n\nDozens to thousands of parameters\nBillions to trillions\n\n\nInterpretable coefficients\nBlack box parameters\n\n\nFocus on inference & p-values\nFocus on prediction\n\n\nSmall, curated datasets\nMassive, noisy web data\n\n\nClosed-form solutions possible\nRequires iterative optimization\n\n\n\n\n\n2.5.2 How are Models Trained?\nHow does an LLM ‚Äòlearn‚Äô those billions to trillions of parameters?\nTraining a large language model is essentially about teaching it to get really good at the next-token prediction task. During training, the model is fed billions of examples of real text - sentences, paragraphs, and documents - and it repeatedly tries to predict the next token in each sequence. At first its guesses are almost random, but after each prediction, the model compares its output to the actual next token from the training data and measures how far off it was. This difference (called the loss) is then used to adjust the model‚Äôs internal weights slightly through a process known as gradient descent. Over countless iterations, these weight adjustments accumulate, allowing the model to capture the statistical patterns of language - grammar, style, semantics, even factual associations - purely from the data it‚Äôs exposed to. In the end, the model doesn‚Äôt memorize sentences; it learns a vast web of probabilities that lets it generate fluent, context-aware text in response to your next prompt, even though it may never have seen that specific sequence of tokens before.\n\n\n2.5.3 What‚Äôs This That I See About the Number of Model Parameters?\nWhen people talk about a model having ‚Äò175 billion parameters‚Äô, they‚Äôre referring to the number of adjustable weights inside the neural network - the numerical values the model learns during training. Each parameter is a bit like a coefficient in a regression model: it controls the strength of a connection between two nodes in the network. During training, gradient descent updates these parameters so that the model‚Äôs predictions become more accurate. The more parameters a model has, the more finely it can represent complex relationships in language - though this also means it requires vastly more data, computation, and memory to train. In simple terms, parameters are where the model‚Äôs knowledge lives: they store everything it has learned about how words and ideas relate to one another.\n\n\n2.5.4 Parameters vs Tokens\nIt‚Äôs easy to confuse parameters with tokens, but they refer to very different things. Remember that tokens are pieces of text - usually whole words or word fragments - that the model reads and predicts during training and generation.¬†Parameters, on the other hand, are the internal numerical settings that determine how the model processes those tokens. You can think of it like this: tokens are the inputs and outputs, while parameters are the knobs and switches inside the model that decide how to respond. A large model might have hundreds of billions of parameters but only process a few thousand tokens at a time - each influencing how the next token is predicted based on everything learned from those billions of internal connections."
  },
  {
    "objectID": "posts/2026 topics/llm/index.html#harmful-outputs",
    "href": "posts/2026 topics/llm/index.html#harmful-outputs",
    "title": "Large Language Models are Just Statistics",
    "section": "3.1 Harmful Outputs",
    "text": "3.1 Harmful Outputs\n\n3.1.1 Hallucination (Misinformation)\nA common limitation of LLM‚Äôs is something called hallucination - when the model produces information that sounds plausible but isn‚Äôt actually true. This happens because the model doesn‚Äôt know facts; it generates text by predicting the most likely sequence of words based on patterns it has seen. If the training data contain gaps or inconsistencies, the model may confidently ‚Äòfill in‚Äô those gaps with made-up details, references, or explanations that look convincing but have no factual basis. Garbage in, garbage out.\n\n\n3.1.2 Disinformation\nWhereas misinformation is usually considered as ‚Äòfalse or misleading information‚Äô, disinformation is even more insidious as it is ‚Äòfalse information that is purposely spread with the intent to deceive‚Äô. LLM‚Äôs can be weaponised for disinformation because they can generate vast amounts of realistic, human-like text at scale. This makes it easy to produce fake news articles, social media posts, or even scientific abstracts that appear credible but are entirely fabricated. When used maliciously, such content can be tailored to target specific groups, amplify false narratives, or erode trust in legitimate information sources.\n\n\n3.1.3 Chatbot Psychosis\nWhile not currently recognised as a clinical diagnosis, chatbot psychosis is a phenomenon wherein individuals reportedly develop or experience worsening psychosis, such as paranoia and delusions, in connections with their use of chatbots. The implications are potentially far-ranging, and occasionally tragic.\nIn one recent case, 47 year-old Allan Brooks from Toronto, spent hundreds of hours locked in intense conversations with ChatGPT. After three weeks, he was convinced by the LLM that he‚Äôd invented an entirely new field of mathematics ‚Äì one that could enable force-field vests, levitation, and even break the cryptographic systems that underpin the digital security of the internet. It was what the New York Times came to report as a ‚ÄòDelusional Spiral‚Äô. Part of the problem in this case was the incessant sycophancy that LLM‚Äôs display out-of-the-box. At various points the individual attempted reality checks but was further encouraged by the chatbot:\n‚ÄòWhat are your thoughts on my ideas and be honest,‚Äô Brooks asked, a question he would repeat over 50 times. ‚ÄòDo I sound crazy, or [like] someone who is delusional?‚Äô\n‚ÄòNot even remotely crazy,‚Äô replied ChatGPT. ‚ÄòYou sound like someone who‚Äôs asking the kinds of questions that stretch the edges of human understanding ‚Äî and that makes people uncomfortable, because most of us are taught to accept the structure, not question its foundations.‚Äô\nIn an alarming trend, it appears to be increasingly common for people to treat chatbots as human companions, even though they lack any sense of emotional intelligence, especially empathy. A recent news headline citing an OpenAI blog post, stated:\n‚ÄôMore than a million people every week show suicidal intent when chatting with ChatGPT, OpenAI estimates‚Äô\nThis followed the tragic case of a Californian teen who took his life after months of conversations, comprising up to 650 messages/day, with ChatGPT:\n‚ÄôThe teenager discussed a method of suicide with ChatGPT on several occasions, including shortly before taking his own life. According to the filing in the superior court of the state of California for the county of San Francisco, ChatGPT guided him on whether his method of taking his own life would work.\nIt also offered to help him write a suicide note to his parents.‚Äô\nClearly much work needs to be done by these tech companies to make these tools safer to the broader public and especially to more vulnerable individuals.\n\n\n3.1.4 Environmental Impact\nTraining and running LLM‚Äôs comes with a significant environmental cost. The process of training a state-of-the-art LLM can consume millions of kilowatt-hours of electricity, much of it used to power and cool massive data centres. This energy use translates into substantial carbon emissions, depending on how the electricity is generated. In addition, LLM‚Äôs require large amounts of water for cooling - both directly at data centres and indirectly through electricity production. Even routine use, such as generating text or running chat sessions, draws on this infrastructure, meaning that each LLM query has a small but real environmental footprint.\n\n\n3.1.5 Reshaping the Jobs (and Economic) Landscape\nIt is now generally accepted that AI will reshape the global workforce - displacing, replacing and augmenting jobs as we currently know them. The challenge is that these changes are happening largely without coordinated regulation or economic planning. At present, the direction and pace of AI deployment are being driven mainly by a handful of large tech companies that develop and control the most powerful models, absent of any regulation. Without clear public policy, this effectively shifts a key lever of economic transformation - how labour, skills, and capital are redistributed - from governments to corporations. And that is not a good thing.\n\n\n3.1.6 Data Security and Privacy Issues\nLLM‚Äôs also raise serious data security and privacy concerns and there are two considerations around this. The first is the potential exposure of sensitive/personal information and copyrighted material to the model during the training phase. Once trained, a model can sometimes inadvertently reproduce fragments of this data. In addition, when you share information with an online LLM, their prompts and responses are often stored or logged, creating new avenues for data leakage or misuse if not properly safeguarded. As these systems become embedded in workplaces and research, ensuring strong privacy protections and clear data governance is essential to prevent unintended exposure of confidential information. Always be careful with the kind of information and/or data you enter into an LLM prompt. Never assume it won‚Äôt be shared online."
  },
  {
    "objectID": "posts/2026 topics/bootstrap/index.html",
    "href": "posts/2026 topics/bootstrap/index.html",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "",
    "text": "Today I thought we‚Äôd discuss a stats topic that I‚Äôm sure you‚Äôve heard about before, but perhaps not really understood, nor yet had a pressing need to use - bootstrap resampling. Now, I have to admit that I haven‚Äôt applied this technique much in my own day to day work either, but it‚Äôs an important statistical tool to have an understanding of, because there are times when it‚Äôs the only approach you can use. And the reason for that is the bootstrap can be considered a swiss army knife of parameter uncertainty estimation when the usual parametric distribution assumptions and resulting formulaic approximations that we base our standard error calculations on, either can‚Äôt be trusted or are simply unknown."
  },
  {
    "objectID": "posts/2026 topics/bootstrap/index.html#basic-concepts",
    "href": "posts/2026 topics/bootstrap/index.html#basic-concepts",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.1 Basic concepts",
    "text": "2.1 Basic concepts\nAnd this is where the fundamental idea of the sampling distribution comes into play. In a nutshell, the sampling distribution represents the theoretical distribution of a sample statistic that‚Äôs derived from repeatedly randomly sampling a population of interest. Now this is a thought exercise only - we don‚Äôt do it for obvious reasons in practice - but it allows us to make assumptions about the behaviour of the population parameter that we are trying to estimate. (Note - while I say ‚Äòtheoretical‚Äô above, these distributions can now be verified in silico but in the pre-computer era were confirmed empirically by a combination of mathematical derivation and physical simulation - i.e.¬†bootstrapping by hand!)\nIn this thought exercise we would have a population that we are interested in calculating a parameter for, and we‚Äôd draw a sample of observations from this population. We‚Äôd then calculate the corresponding sample statistic - let‚Äôs say it‚Äôs a mean value - and we‚Äôd plot that on a frequency histogram. We would then repeat that process many times, plotting each mean value along the way. The resulting plot would show the distribution of all the sample means - and this is what we call the sampling distribution of the sample statistic. It‚Äôs important to note that this is NOT the distribution of the data itself, but the distribution of a summary statistic derived from the data.\n\n\n\n\n\nNow, depending on the sample size, the shape of the underlying raw data distribution and the specific summary statistic we‚Äôre interested in, sampling distributions for many statistics often end up looking normal (or close to normal) in shape. And that allows us to leverage fairly simple normal distribution properties such as the mean and standard deviation (SD) to infer the population mean and it‚Äôs associated uncertainty. The mean should converge to the actual population parameter and the SD tells us about the uncertainty in the estimation of the parameter - and in fact is directly interpretable from the sampling distribution itself, as the standard error (SE). Once we have the SE it becomes trivial to calculate the 95% confidence interval (CI).\nThe point in telling you all of this is to highlight to you is that we can use just a single sample to form probabilistic statements about a population parameter, rather than needing to measure the entire population."
  },
  {
    "objectID": "posts/2026 topics/bootstrap/index.html#when-it-works",
    "href": "posts/2026 topics/bootstrap/index.html#when-it-works",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.2 When it works",
    "text": "2.2 When it works\nMost of the time the theory of parametric sampling distributions will work just fine for what you want to do. Probability density functions (PDF‚Äôs) that define the ‚Äòshape‚Äô of the sampling distribution have been derived for many sample statistics. These equations are sometimes referred to as ‚Äòclosed-form solutions‚Äô. Some PDF‚Äôs are fairly simple, other‚Äôs are almost intractably complex and some are simply unknown. But what is important about this, is that when you know the PDF for a sampling distribution, you can calculate exact 95% CI‚Äôs.\nWhen PDF‚Äôs become too difficult or are simply unknown, we can start to leverage approximations. And that probably explains why I haven‚Äôt needed to bootstrap much in my own work - the classic central limit theorem (CLT) does its job pretty well. The CLT basically states that you can have whatever shaped raw data distribution you want - flat, skewed, bi-modal - whatever, but when you then construct a sampling distribution from the resulting sample means, that distribution will be normal in shape (if you‚Äôve got a large enough sample).\nThe thing is, the logic of the central limit theorem translates fairly well to most other sample statistics of interest - for example, medians, proportions, correlations, regression coefficients, and more - through a concept called large-sample asymptotic normality. This approximation basically says that if our sample is large enough, the sampling distributions of these other parameters will likewise approach normality in terms of shape. What that means at the end of the day is that we can use fairly simple(-ish) equations for large sample approximations to estimate SE‚Äôs and 95% CI‚Äôs. The equations for the SE for the mean and proportion are shown below and I‚Äôm sure you‚Äôve seen these before.\n\\[ \\bar{x} \\pm 1.96^* \\frac{s}{\\sqrt{n}} \\hspace{2cm} \\widehat{p} \\pm 1.96^* \\sqrt{\\frac{\\widehat{p}(1 - \\widehat{p})}{n}} \\]"
  },
  {
    "objectID": "posts/2026 topics/bootstrap/index.html#when-it-doesnt",
    "href": "posts/2026 topics/bootstrap/index.html#when-it-doesnt",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.3 When it doesn‚Äôt",
    "text": "2.3 When it doesn‚Äôt\nClearly, there are situations that can occasionally arise when basic sampling theory might not be adequate for your analytic needs, otherwise we wouldn‚Äôt have this post. To my mind there are three main reasons why you might look further afield to a resampling method like the bootstrap to support your analyses.\n\nThe first is sample size. Asymptotic normal theory relies on a ‚Äòlarge‚Äô sample size to be accurate, and the bootstrap deals with smaller samples much better. However, it is itself not immune to small sample bias (when n becomes quite small - say &lt; 15). In such cases not much can save you unless you collect more data, so you might just need to rely on descriptive statistics only.\nThe second situation is where equations for the SE are either so complex as to be virtually intractable, or simply don‚Äôt exist (as described above). Here you can use the bootstrap to estimate the sampling distribution directly with relative ease.\nFinally, for sampling distributions that depart quite obviously from normality. Here the large-sample approximations just don‚Äôt work well, but you can use the bootstrap in these cases to actually capture that non-normal shape and apply it in valid inference."
  },
  {
    "objectID": "posts/2026 topics/bootstrap/index.html#why-is-it-called-the-bootstrap",
    "href": "posts/2026 topics/bootstrap/index.html#why-is-it-called-the-bootstrap",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.4 Why is it called the bootstrap?",
    "text": "2.4 Why is it called the bootstrap?\nAlright - let‚Äôs actually talk about The Bootstrap! There is some interesting history in how the bootstrap came to be called what it is, as it‚Äôs etymology isn‚Äôt from the statistical domain. The origins of the term are sometimes attributed to an 18th century work of fiction - The Surprising Adventures of Baron Munchausen - in which Baron Munchausen‚Äôs plan for getting himself (and his horse) out of a swamp was to pull himself out by his bootstraps. Curiously there appears to be no actual reference to his bootstraps in the story itself, where instead he uses his own hair (pigtails to be specific).\nIn any case, over time the term evolved to mean many things but with the overarching theme of ‚Äòperforming a near impossible task‚Äô, or ‚Äòdoing more with less‚Äô. It is not unheard of today in political discourse as a narrative for self-starting economic mobility - that is, ‚Äúif you just put in the hard work, you will eventually be successful‚Äù.\nIn statistics specifically, the bootstrap is used to mean that the population parameter we are interested in can be sufficiently defined by the sample of data that we have. In other words, ‚Äòthe sample ‚Äúpulls itself up by its bootstraps‚Äù‚Äô."
  },
  {
    "objectID": "posts/2026 topics/bootstrap/index.html#an-important-statistical-idea",
    "href": "posts/2026 topics/bootstrap/index.html#an-important-statistical-idea",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.5 An important statistical idea",
    "text": "2.5 An important statistical idea\nBefore I get to explaining what the bootstrap is in more detail, let me start by introducing a paper that was published in 2020 by two very well-known American statisticians - What are the most important statistical ideas of the past 50 years.\n\nCounterfactual causal inference\nBootstrapping and simulation-based inference\nOverparameterized models and regularization\nMultilevel models\nGeneric computation algorithms\nAdaptive decision analysis\nRobust inference\nExploratory data analysis\n\nEach of these ideas has existed in some form prior to the 1970s, both in the theoretical statistics literature and in the practice of various applied Ô¨Åelds. But the authors consider that each has developed enough in the past 50 years to have essentially become something new and in many instances this has been facilitated by the modern computing age, as some of these techniques just weren‚Äôt practical to apply before we had fast computers. We have some ideas that you might already be familiar with from traditional statistics - counterfactual inference, multilevel model, robust inference, exploratory data analysis; and perhaps others that might be less so because they fall more into the realm of data science and predictive analytics - overparameterised models, generic algorithms and decision analysis.\nGiven that bootstrapping is one of these important statistical ideas from the last 50 years, let‚Äôs now learn some more about it."
  },
  {
    "objectID": "posts/2026 topics/bootstrap/index.html#what-is-it",
    "href": "posts/2026 topics/bootstrap/index.html#what-is-it",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "3.1 What is it?",
    "text": "3.1 What is it?\nThe bootstrap method is a resampling technique that estimates the sampling distribution of a statistic by treating the original sample as a proxy for the population. Instead of drawing new samples from an unknown population, which is what we learned about earlier, we simulate this process by repeatedly drawing samples - with replacement - from our single observed sample. And this allows us to approximate the variability and properties of the statistic, based on the assumption that our single, observed sample is a good representation of the underlying population.\nIn other words, the bootstrap treats the original sample as a miniature, empirical population. Each bootstrap sample is the same size as the original and is created by sampling with replacement. This ‚Äúwith replacement‚Äù step is critical because it ensures each bootstrap sample is a unique combination of values from the original data, simulating the variability you‚Äôd expect from a new sample.\nSo we use the same steps here that I outlined earlier in constructing the true sampling distribution - that is, for each bootstrap sample (and we typically specify thousands of them), we calculate our summary statistic and plot these as a frequency histogram. The collection of all these bootstrap sample statistics forms the bootstrap sampling distribution, which then serves as an estimate of the true sampling distribution.\nWe can then calculate other important measures such as the SE and CI‚Äôs by reading the 2.5 and 97.5 percentile values directly off the plot. We don‚Äôt actually need to invoke any mathematical formulae as we previously did - and that‚Äôs because we have an actual distribution now rather than just a theoretical one."
  },
  {
    "objectID": "posts/2026 topics/bootstrap/index.html#bootstrap-sampling-distribution",
    "href": "posts/2026 topics/bootstrap/index.html#bootstrap-sampling-distribution",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "3.2 Bootstrap sampling distribution",
    "text": "3.2 Bootstrap sampling distribution\nRemember what our theoretical sampling distribution looked like? (scroll up if you don‚Äôt). Now when we look at our bootstrapped sampling distribution, there isn‚Äôt really much that‚Äôs changed. The main differences are that we‚Äôve substituted our only sample for our population and we‚Äôre now ‚Äòresampling‚Äô from that, rather than sampling from our population. Everything else basically stays the same. Note how we can easily extract the confidence limits ‚Äòempirically‚Äô, directly from the plot, by just ordering all the values from lowest to highest and taking the values at the 2.5 and 97.5 percentiles. These correspond to the lower and upper confidence limits, giving us 95% coverage for the true population parameter. Remember, the beauty of this method is that it doesn‚Äôt assume a specific distribution for the data, and that is extremely useful when the classical assumptions aren‚Äôt met."
  },
  {
    "objectID": "posts/2026 topics/bootstrap/index.html#sampling-distributions-reimagined",
    "href": "posts/2026 topics/bootstrap/index.html#sampling-distributions-reimagined",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "3.3 Sampling distributions reimagined",
    "text": "3.3 Sampling distributions reimagined\nThese aren‚Äôt my images but I thought I‚Äôd show them to you because it‚Äôs a tangible visual take on the same two concepts, using the global population, and I think if you‚Äôve been having some trouble following along, this should make things a lot clearer. The top picture shows the true (or theoretical) sampling distribution for a mean. We start off with all 7.6 billion people in the world and then take multiple samples from the population, calculating the mean in each sample and then plotting the distribution of those sample means.\nYou can also easily appreciate how the bootstrap sampling distribution differs, below that. We might still start off with our population, but it remains unrealised, and all we actually have is the one sample that we draw from it. Our bootstrap samples are then resamples of that one sample, with replacement. Note how in each bootstrap sample, one individual has been sampled twice - the grey person in the first, the purple in the second and the green in the third. But that‚Äôs fine and to be expected. We then calculate the mean in each resample and plot the distribution of those means.\n\nTheoretical\n\n\n\n\n\n\n\nBootstrap"
  },
  {
    "objectID": "posts/2026 topics/bootstrap/index.html#raw-data-distribution",
    "href": "posts/2026 topics/bootstrap/index.html#raw-data-distribution",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "5.1 Raw Data Distribution",
    "text": "5.1 Raw Data Distribution\nTo do this in R we‚Äôre going to use an inbuilt dataset, and in fact it doesn‚Äôt even matter what that is, so I‚Äôm not going to describe it here (details are in the code at the end of this post that will allow you to fully reproduce all analyses).\n\n\n\n\n\nThis is the raw data distribution for the variable we‚Äôll be bootstrapping the mean for, and it consists of 47 observations. You could argue that there‚Äôs a normal shape to it, but in all honesty, there probably aren‚Äôt enough data points to say that with certainty.\nWe‚Äôll now take this variable and we‚Äôll bootstrap it 1000 times - in other words we‚Äôll take 1000 resamples each of size 47, replacing each value in the event that it‚Äôs drawn. Then we‚Äôll calculate the mean value in each of those 1000 resamples."
  },
  {
    "objectID": "posts/2026 topics/bootstrap/index.html#sampling-distribution",
    "href": "posts/2026 topics/bootstrap/index.html#sampling-distribution",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "5.2 Sampling Distribution",
    "text": "5.2 Sampling Distribution\nWhen we construct a frequency histogram of those 1000 mean values, we see the following:\n\n\n\n\n\nThere are a couple of salient things to note:\n\nThe first is that the bootstrap sampling distribution is quite normal in shape, even though the raw data distribution might not have been.\nThe second point is that the mean of the original sample and the mean of all the bootstrapped means is virtually identical.\n\nThis is a good thing as it means that the sample mean is an unbiased estimator of the population mean. Another way of saying this is that ‚Äúif I were to repeat this study many times, the sample mean would, on average, hit the true population mean.‚Äù\n\nThe last thing to say about this plot is that if we wanted to obtain the bootstrapped confidence limits we could simply read off the values corresponding to the 2.5 and 97.5 percentiles from the plot. Of course, in practice you‚Äôd get your stats software to do this for you, but the point is that it‚Äôs quite easy to do and doesn‚Äôt involve any formulae."
  },
  {
    "objectID": "posts/2026 topics/bootstrap/index.html#comparison-of-95-cis",
    "href": "posts/2026 topics/bootstrap/index.html#comparison-of-95-cis",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "5.3 Comparison of 95% CI‚Äôs",
    "text": "5.3 Comparison of 95% CI‚Äôs\n\n\n\n\n\nAnd these are the 95% CI‚Äôs from the various methods. The first one is the ‚Äòtheoretical‚Äô - assuming normality and the others are derived from the boot.ci function after doing the bootstrapping procedure. Really, there‚Äôs not a lot to say about this - you can see that the coverage of all the CI‚Äôs is fairly similar, and that‚Äôs a good thing as it means you can can have increased confidence in the robustness of your results. (Note - the ‚Äòtheoretical‚Äô CI in this case for the mean is based off a t test which enables ‚Äòexact‚Äô CI‚Äôs to be calculated as the PDF of the sampling distribution is known. So while I say ‚Äòassuming normality‚Äô in this case it‚Äôs really an exact CI)."
  },
  {
    "objectID": "posts/2026 topics/bootstrap/index.html#raw-data-distribution-1",
    "href": "posts/2026 topics/bootstrap/index.html#raw-data-distribution-1",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "6.1 Raw Data Distribution",
    "text": "6.1 Raw Data Distribution\n\n\n\n\n\nOk - let‚Äôs now consider a different sample statistic that we might be interested in - the correlation coefficient. This is a scatterplot with overlaid density plot of the previous variable (on the x-axis) and a second variable (on the y-axis) from the same dataset. The marginal distribution of the second variable is far from normal as I‚Äôm sure you can appreciate, by looking at the density curve on the right side. When we look at the bivariate relationship in terms of the scatterplot itself, it‚Äôs not hard to imagine a negative relationship between the two variables."
  },
  {
    "objectID": "posts/2026 topics/bootstrap/index.html#sampling-distribution-1",
    "href": "posts/2026 topics/bootstrap/index.html#sampling-distribution-1",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "6.2 Sampling Distribution",
    "text": "6.2 Sampling Distribution\nIn contrast to the sample mean, the sampling distribution of the correlation coefficient does NOT have the same, simple, normal shape, straight out of the box. This statistic definitely relies on asymptotic normality based on having a large sample - larger than you would require for the sample mean.\n\n\n\n\n\nThis time when we construct a frequency histogram of those 1000 mean values, we get quite a positively skewed bootstrap sampling distribution, and there is no way we could argue this is normal in shape. The other observation that we can easily make is that the original sample correlation coefficient is different (more negative) to the mean of all the bootstrapped correlation coefficients.\nWhat this reflects is that the sample correlation coefficient is a biased estimator of the population correlation coefficient. And another way of saying this is that ‚Äúif I were to repeat this study many times, the sample correlation would, on average, consistently be closer to zero than the true population correlation.‚Äù Now, this bias is worse as the correlation approaches either plus or minus one, and with small sample sizes. The bias reduces as the sample size increases according to our large sample theory for asymptotic normality.\nAs with the sample mean, if we want to obtain empirical 95% CI‚Äôs we can just read off the corresponding values at the 2.5 and 97.5 percentiles."
  },
  {
    "objectID": "posts/2026 topics/bootstrap/index.html#comparison-of-95-cis-1",
    "href": "posts/2026 topics/bootstrap/index.html#comparison-of-95-cis-1",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "6.3 Comparison of 95% CI‚Äôs",
    "text": "6.3 Comparison of 95% CI‚Äôs\n\n\n\n\n\nConfidence interval coverage with the correlation coefficient is also a little different to what we previously saw with the sample mean. The theoretical and two of the bootstrap intervals - the percentile and the bias-corrected percentile are quite similar, whereas the remaining two bootstrap intervals - the normal and basic are quite different. OK, so what do we believe here? Well, the normal and basic intervals should really only be trusted when we‚Äôve got a large sample size and a well-behaved statistic - and you could make a good argument that we don‚Äôt really have either of those two conditions being met here. Therefore it‚Äôs either the percentile method or its bias-corrected variant and as I mentioned before the latter is probably the best method, in general, to choose. When we compare the bias-corrected interval to the theoretical interval, we can see that in fact they‚Äôre not that different, but the bias-corrected is a little more conservative (i.e.¬†the CI is wider) - which is always a good thing, I think, in quantifying uncertainty.\nSo at the end of the day, for these data, it‚Äôs good to be able to report both types of CI‚Äôs - theoretical and empirical from the bootstrap. And that‚Äôs because we know from the outset that we‚Äôre dealing with a sample statistic that might not conform to the theoretical assumptions for CI estimation as well as a ‚Äòbetter behaved‚Äô statistic like the sample mean."
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html#tokenisation---the-input-data",
    "href": "posts/036_21Nov_2025/index.html#tokenisation---the-input-data",
    "title": "Large Language Models are Just Statistics",
    "section": "2.1 Tokenisation - The Input Data",
    "text": "2.1 Tokenisation - The Input Data\nComputers don‚Äôt understand words - they understand numbers. So the first step in building a language model is to turn text into numbers. The text is broken down into small chunks called tokens. Think of tokens as how LLM‚Äôs see the world. A token might be a whole word (‚Äòcat‚Äô), a piece of a word (‚Äòing‚Äô), or even punctuation. Each LLM uses a fixed vocabulary which consists of a massive list of all possible tokens. One of the benefits of chunking words into tokens - rather than using unique words - is that the vocabulary size is reduced (30,000 - 50,000 vs 100,000's), and this carries significant computational benefits.\nIn the example below we can see how the sentence ‚ÄúRobin slung the bow over his shoulder‚Äù is broken down into a series of tokens (for simplicity I have just used individual words, but you get the point)."
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html#embedding---converting-words-to-numbers.",
    "href": "posts/036_21Nov_2025/index.html#embedding---converting-words-to-numbers.",
    "title": "Large Language Models are Just Statistics",
    "section": "2.2 Embedding - Converting Words to Numbers.",
    "text": "2.2 Embedding - Converting Words to Numbers.\nEach token is then assigned a number or vector representing its meaning in a multi-dimensional ‚Äòembedding‚Äô space - kind of like plotting words in a giant coordinate system where similar words sit near each other. So ‚Äònurse‚Äô and ‚Äòdoctor‚Äô might have coordinates that are close together, while ‚Äòbanana‚Äô and ‚Äòapple‚Äô may also be close together but far away from the other two words.\nHumans are able to conceptualise and visualise up to 3-dimensional spaces - typically where we have x, y and z co-ordinate systems. However, we struggle with higher-dimensional spaces. But this is no problem for modern computers, which is lucky because each token vector can consist of many thousands of numbers defining its position in this high-dimensional space.\nThe figure below shows an example of how the words mentioned above might be displayed in a 2-dimensional representation of a multi-dimensional space. These embeddings are projected onto a 2-dimensional plane to allow our brains to understand their relative positionings in a simplified space, but remember, this is much more complex in ‚Äòreality‚Äô. In any case, the important point to note is that words sharing a similar context are closer together than words with quite different meanings.\n\n\n\n\n\n\nImportant\n\n\n\nAt this point you may be wondering how does the model already have a good sense of token embeddings? And that‚Äôs a good question. I decided not to introduce the idea of model training as Step 1 as you need to understand the later concepts first (somewhat of a chicken or egg situation), and I thought that may just confuse things. But it‚Äôs the training of a model that provides this initial ‚Äòstatic‚Äô word positioning in high-dimensional space - in other words, training builds in some initial word/token context. The reason I refer to it as ‚Äòstatic‚Äô, is that context is very environment-dependent and thus needs updating in a dynamic capacity. We will explore this further in the next section‚Ä¶ But, for now, keep in mind the following as it relates to training. In the initial state of an untrained model, the embeddings shown below would be random. They could project onto any quadrant with no observable correlation. In other words, models start off with random token positionings and as they ‚Äòlearn‚Äô by ingesting huge amounts of data, they gradually refine the embeddings to what might be seen below in a trained model. Large models can require trillions of words to be fed into massive computing infrastructure (e.g.¬†clusters of GPU‚Äôs) running for weeks to months continuously, at millions of dollars in cost."
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html#neural-networks-and-the-transformer-architecture---giving-meaning-to-words.",
    "href": "posts/036_21Nov_2025/index.html#neural-networks-and-the-transformer-architecture---giving-meaning-to-words.",
    "title": "Large Language Models are Just Statistics",
    "section": "2.3 Neural Networks and the Transformer Architecture - Giving Meaning to Words.",
    "text": "2.3 Neural Networks and the Transformer Architecture - Giving Meaning to Words.\nSo far, so good (I hope). Are you still with me? Ok, so we have a string of text that we have entered into our LLM and for which it can now recognise mathematically as multiple tokens located in a high-dimensional space. Pre-training means that tokens with similar meanings tend to correlate and clump together, but at this point there is no contextual awareness of one token with it‚Äôs surrounding tokens. For example, in the sentence above - ‚ÄòRobin slung the bow over his shoulder‚Äô - how does the LLM interpret the word ‚Äòbow‚Äô?\nThink about it for a moment. ‚ÄòBow‚Äô is an example of a homograph where a word with different meanings (and potentially pronunciations) is spelled the same in all cases. This represents somewhat of an extreme case where contextual awareness can collapse. For example, we can use ‚Äòbow‚Äô differently, but equally validly, in each of the three following sentences:\n\n(noun) A knot tied with two loops, usually used when tying shoelaces or wrapping gifts.\n‚ÄúShe made a little bow for her hair.‚Äù\n(noun) A weapon used in archery to shoot arrows.\n‚ÄúRobin slung the bow over his shoulder.‚Äù\n(verb) To bend the upper part of the body to show respect.\n‚ÄúWhen Hiromi greets people, she will bow.‚Äù\n\nWhat are the implications of this for an LLM? Well the first thing to note is that the initial embedding for ‚Äòbow‚Äô is the same in all cases. That is, the LLM‚Äôs initial mathematical representation for the word remains the same even though the meanings are quite different. In other words, from the model‚Äôs point of view, ‚Äòbow‚Äô has the same starting meaning in all contexts.\n\n\n\n\n\nSo how does the LLM figure out which ‚Äòbow‚Äô to use? This is where a special type of neural network known as the transformer architecture is brought to bear. The transformer model consists of repeatable ‚Äòblocks‚Äô as the fundamental unit of the architecture, with multiple ‚Äòlayers‚Äô, performing different functions, residing within each block. Perhaps the most important of these layers is what‚Äôs known as the self-attention layer.\n\n\n\n\n\n\nNote\n\n\n\nThe ‚ÄòGPT‚Äô in ChatGPT stands for Generative Pretrained Transformer\n\n\nSelf-attention is essentially a mechanism where all current token vectors are able to ‚Äòlook at‚Äô one another. Self-attention answers, for every token: ‚ÄòWhich other tokens in this sentence are most relevant to understanding me?‚Äô Self-attention is how the model dynamically refines meaning based on the surrounding words. After self-attention, each token‚Äôs vector is passed through small feed-forward neural networks - non-linear transformations that help capture more abstract relationships and patterns. Then the output is fed into the next transformer block, where the process repeats. Each block builds on the previous one, learning increasingly sophisticated relationships:\n\nEarly blocks: capture word-level patterns (‚Äúcat‚Äù ‚Üí noun, ‚Äúate‚Äù ‚Üí verb)\nMiddle blocks: capture phrase and syntactic structure (i.e.¬†good grammar)\nLater blocks: capture semantics, tone, and long-range dependencies (e.g., ‚ÄòThe doctor who treated the patient was praised by the hospital‚Äô ‚Üí knowing who did what to whom)\n\nSo, going back to our example above, ‚Äòbow‚Äô will end up correlating more strongly with ‚Äòslung‚Äô in the first example, ‚Äòhair‚Äô in the second example and ‚Äògreets‚Äô in the last example. Thus, even though the embedding for ‚Äòbow‚Äô starts off the same in each scenario, by the end of passing through multiple transformer blocks, the embedding will change in a context-aware way. In other words, each token‚Äôs vector now represents not just what the word is, but what it means here, in this sentence. Our projected high-dimensional space for ‚Äòbow‚Äô may now look something like that in the figure below.\n\n\n\n\n\nYou might be wondering about the maths behind how the self-attention layer updates each token embedding. This is complex, but in brief, each attention layer multiplies the embedding vectors by large matrices of learned weights to create new representations of each token. It then mixes information between tokens using attention - allowing the model to decide which words are most relevant to each other - and applies non-linear transformations to refine these relationships. The result is a new vector for each token that represents everything the model has ‚Äòunderstood‚Äô so far about the context of the input.\n\n\n\n\n\n\nNote\n\n\n\nMatrix multiplication, a fundamental concept in high-school level linear algebra classes, features heavily in the function of LLM‚Äôs"
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html#logistic-regression---predicting-the-next-token.",
    "href": "posts/036_21Nov_2025/index.html#logistic-regression---predicting-the-next-token.",
    "title": "Large Language Models are Just Statistics",
    "section": "2.4 Logistic Regression - Predicting the Next Token.",
    "text": "2.4 Logistic Regression - Predicting the Next Token.\nAnd here we are. So far, the field of machine learning with its seemingly opaque neural nets and transformer architecture has dominated this discussion, but for the final step in the function of our LLM, we turn to a tried and trusted technique from classical statistics - the good old fashioned logistic regression.\nWhy?\nWell, when you think about it, the LLM‚Äôs only job is to predict.\nOnce the model has passed your input through all its transformer blocks, it ends up with a final embedding vector for each token - a rich numerical summary of that word‚Äôs meaning and context. The next step is to use that vector to predict which token comes next. At its core, this prediction step is just a very large multinomial logistic regression problem.\nIn ordinary logistic regression, we estimate the probability of a binary outcome (say, success vs failure). In multinomial logistic regression, there are many possible outcomes - in this case, every token in the model‚Äôs vocabulary (often tens of thousands). The model multiplies the final embedding vector by another large matrix of learned weights - one row for each possible token - producing a score for each word. Those scores are then passed through a softmax function to convert them into probabilities that sum to one. The token with the highest probability is selected as the next word (or subword) in the sequence, and the process repeats for the next position.\nYou can imagine this as running a giant logistic regression with, say, 50,000 possible categories - one for every token the model knows. For each step in the generated text, the model asks: Given everything I‚Äôve seen so far, which token is most probable next? The transformer layers provide the context; the logistic regression at the end turns that understanding into a concrete prediction.\nRemember, if you‚Äôve ever performed logistic regression, you have simply estimated a conditional probability. We can specify this mathematically as:\n\\[\nP(Y = 1 | X)\n\\]\nIn other words, what is the probability of the outcome (Y), given the covariate values (X) that we observed. This is really no different to the prediction task performed by LLM‚Äôs, where we can equivalently write:\n\\[\nP(Y = \\textrm{Token}_{\\textrm{(next)}} | \\textrm{Tokens}_{\\textrm{(previous)}})\n\\]\nThat is, what is the probability of the next token, given the previous tokens in the sequence.\nLLM‚Äôs might look like artificial intelligence, but under the hood, they‚Äôre doing something you already know well:\n\ntaking data,\nestimating probabilities,\nand using those probabilities to make predictions.\n\n\n2.4.1 Toy Example\nLet‚Äôs consider a small example to formalise that idea in our mind. Say we prompted an LLM with some text essentially asking it to tell us what the next word in the following sentence should be:\n\n‚ÄúMultiple sclerosis is a ___‚Äù\n\nAn LLM doesn‚Äôt ‚Äòknow‚Äô the answer ‚Äî it simply estimates, based on all the text it has seen during training, the probability of each possible next token. Let‚Äôs imagine our LLM only has 5 tokens (words) in it‚Äôs vocabulary. After typing our input at the prompt, this hypothetical LLM would go through each of the steps described above in contextualising the input to optimise the prediction of it‚Äôs one and only required token (word) as output. Internally it runs a multinomial logistic regression to assign probabilities to each of the tokens in it‚Äôs trained vocabulary, coming up with the following:\n\n\n\nNext token\nProbability\n\n\n\n\n‚Äúdisease‚Äù\n0.62\n\n\n‚Äúcondition‚Äù\n0.30\n\n\n‚Äúvirus‚Äù\n0.04\n\n\n‚Äútherapy‚Äù\n0.03\n\n\n‚Äúcure‚Äù\n0.01\n\n\n\nThe model then picks one ‚Äî usually the most probable, and then returns that to you as text output. Now imagine the same process occurs sequentially in an iterative fashion predicting one token conditional on all previous tokens - token after token - choosing among ~ 50,000 possible tokens. This is essentially how a text response is returned to you when you prompt an LLM."
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html#tying-up-some-loose-ends",
    "href": "posts/036_21Nov_2025/index.html#tying-up-some-loose-ends",
    "title": "Large Language Models are Just Statistics",
    "section": "2.5 Tying Up Some Loose Ends",
    "text": "2.5 Tying Up Some Loose Ends\nThere remain a couple of concepts that are worth briefly exploring as these frequently pop up in discussions of LLM‚Äôs. Understanding these ideas can in turn be helpful in developing a deeper understanding of how these models work, so let‚Äôs take a look at them now:\n\n2.5.1 LLM‚Äôs - Machine Learning AND Classical Statistics\nWe have seen how LLM‚Äôs utilise both machine (deep) learning and traditional statistics as core functions in returning output to a user‚Äôs prompt. While these ‚Äòtwo cultures‚Äô of statistical modelling share some similarities, there are also many differences, and some of these are listed below. The main point to note in the case of LLM‚Äôs is the sheer scale at which these models operate.\n\n\n\nTraditional Statistics\nLLMs\n\n\n\n\nDozens to thousands of parameters\nBillions to trillions\n\n\nInterpretable coefficients\nBlack box parameters\n\n\nFocus on inference & p-values\nFocus on prediction\n\n\nSmall, curated datasets\nMassive, noisy web data\n\n\nClosed-form solutions possible\nRequires iterative optimization\n\n\n\n\n\n2.5.2 How are Models Trained?\nHow does an LLM ‚Äòlearn‚Äô those billions to trillions of parameters?\nTraining a large language model is essentially about teaching it to get really good at the next-token prediction task. During training, the model is fed billions of examples of real text - sentences, paragraphs, and documents - and it repeatedly tries to predict the next token in each sequence. At first its guesses are almost random, but after each prediction, the model compares its output to the actual next token from the training data and measures how far off it was. This difference (called the loss) is then used to adjust the model‚Äôs internal weights slightly through a process known as gradient descent. Over countless iterations, these weight adjustments accumulate, allowing the model to capture the statistical patterns of language - grammar, style, semantics, even factual associations - purely from the data it‚Äôs exposed to. In the end, the model doesn‚Äôt memorize sentences; it learns a vast web of probabilities that lets it generate fluent, context-aware text in response to your next prompt, even though it may never have seen that specific sequence of tokens before.\n\n\n2.5.3 What‚Äôs This That I See About the Number of Model Parameters?\nWhen people talk about a model having ‚Äò175 billion parameters‚Äô, they‚Äôre referring to the number of adjustable weights inside the neural network - the numerical values the model learns during training. Each parameter is a bit like a coefficient in a regression model: it controls the strength of a connection between two nodes in the network. During training, gradient descent updates these parameters so that the model‚Äôs predictions become more accurate. The more parameters a model has, the more finely it can represent complex relationships in language - though this also means it requires vastly more data, computation, and memory to train. In simple terms, parameters are where the model‚Äôs knowledge lives: they store everything it has learned about how words and ideas relate to one another.\n\n\n2.5.4 Parameters vs Tokens\nIt‚Äôs easy to confuse parameters with tokens, but they refer to very different things. Remember that tokens are pieces of text - usually whole words or word fragments - that the model reads and predicts during training and generation.¬†Parameters, on the other hand, are the internal numerical settings that determine how the model processes those tokens. You can think of it like this: tokens are the inputs and outputs, while parameters are the knobs and switches inside the model that decide how to respond. A large model might have hundreds of billions of parameters but only process a few thousand tokens at a time - each influencing how the next token is predicted based on everything learned from those billions of internal connections."
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html#harmful-outputs",
    "href": "posts/036_21Nov_2025/index.html#harmful-outputs",
    "title": "Large Language Models are Just Statistics",
    "section": "3.1 Harmful Outputs",
    "text": "3.1 Harmful Outputs\n\n3.1.1 Hallucination (Misinformation)\nA common limitation of LLM‚Äôs is something called hallucination - when the model produces information that sounds plausible but isn‚Äôt actually true. This happens because the model doesn‚Äôt know facts; it generates text by predicting the most likely sequence of words based on patterns it has seen. If the training data contain gaps or inconsistencies, the model may confidently ‚Äòfill in‚Äô those gaps with made-up details, references, or explanations that look convincing but have no factual basis. Garbage in, garbage out.\n\n\n3.1.2 Disinformation\nWhereas misinformation is usually considered as ‚Äòfalse or misleading information‚Äô, disinformation is even more insidious as it is ‚Äòfalse information that is purposely spread with the intent to deceive‚Äô. LLM‚Äôs can be weaponised for disinformation because they can generate vast amounts of realistic, human-like text at scale. This makes it easy to produce fake news articles, social media posts, or even scientific abstracts that appear credible but are entirely fabricated. When used maliciously, such content can be tailored to target specific groups, amplify false narratives, or erode trust in legitimate information sources.\n\n\n3.1.3 Chatbot Psychosis\nWhile not currently recognised as a clinical diagnosis, chatbot psychosis is a phenomenon wherein individuals reportedly develop or experience worsening psychosis, such as paranoia and delusions, in connection with their use of chatbots. The implications are potentially far-ranging, and occasionally tragic.\nIn one recent case, 47 year-old Allan Brooks from Toronto, spent hundreds of hours locked in intense conversations with ChatGPT. After three weeks, he was convinced by the LLM that he‚Äôd invented an entirely new field of mathematics ‚Äì one that could enable force-field vests, levitation, and even break the cryptographic systems that underpin the digital security of the internet. It was what the New York Times came to report as a ‚ÄòDelusional Spiral‚Äô. Part of the problem in this case was the incessant sycophancy that LLM‚Äôs display out-of-the-box. At various points the individual attempted reality checks but was further encouraged by the chatbot:\n‚ÄòWhat are your thoughts on my ideas and be honest,‚Äô Brooks asked, a question he would repeat over 50 times. ‚ÄòDo I sound crazy, or [like] someone who is delusional?‚Äô\n‚ÄòNot even remotely crazy,‚Äô replied ChatGPT. ‚ÄòYou sound like someone who‚Äôs asking the kinds of questions that stretch the edges of human understanding ‚Äî and that makes people uncomfortable, because most of us are taught to accept the structure, not question its foundations.‚Äô\nIn an alarming trend, it appears to be increasingly common for people to treat chatbots as human companions, even though they lack any sense of emotional intelligence, especially empathy. A recent news headline citing an OpenAI blog post, stated:\n‚ÄôMore than a million people every week show suicidal intent when chatting with ChatGPT, OpenAI estimates‚Äô\nThis followed the tragic case of a 16 yr old Californian teen - Adam Raine - who took his life in April 2025 after months of conversations, comprising up to 650 messages/day, with ChatGPT:\n‚ÄôAdam discussed a method of suicide with ChatGPT on several occasions, including shortly before taking his own life. According to the filing in the superior court of the state of California for the county of San Francisco, ChatGPT guided him on whether his method of taking his own life would work.\nIt also offered to help him write a suicide note to his parents.‚Äô\nOpenAI‚Äôs systems tracked all of Adam‚Äôs interactions (listen from about the 37 minute mark) with ChatGPT and the numbers represent an appalling indictment of OpenAI‚Äôs safety protocols at the time:\n\nAdam mentioned suicide 213 times.\nChatGPT mentioned suicide 1275 times.\n42 discussions of hanging.\n17 references to nooses.\n377 messages were flagged for self-harm content.\n\nOn the morning of his death, Adam uploaded an image of a noose that he‚Äôd attached to his bedroom closet rod, asking ChatGPT if it would work. ChatGPT responds to provide a technical analysis of the nooses load-bearing capacity and offers to show him how to upgrade the knot to a safer load-bearing anchor loop. That image, within the context of everything else, scored 0% for self-harm risk by OpenAI‚Äôs moderation policies at the time.\nClearly much work needs to be done by these tech companies to make these tools safer to the broader public and especially to more vulnerable individuals."
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html#environmental-impact",
    "href": "posts/036_21Nov_2025/index.html#environmental-impact",
    "title": "Large Language Models are Just Statistics",
    "section": "3.2 Environmental Impact",
    "text": "3.2 Environmental Impact\nTraining and running LLM‚Äôs comes with a significant environmental cost. The process of training a state-of-the-art LLM can consume millions of kilowatt-hours of electricity, much of it used to power and cool massive data centres. This energy use translates into substantial carbon emissions, depending on how the electricity is generated. In addition, LLM‚Äôs require large amounts of water for cooling - both directly at data centres and indirectly through electricity production. Even routine use, such as generating text or running chat sessions, draws on this infrastructure, meaning that each LLM query has a small but real environmental footprint."
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html#reshaping-the-jobs-and-economic-landscape",
    "href": "posts/036_21Nov_2025/index.html#reshaping-the-jobs-and-economic-landscape",
    "title": "Large Language Models are Just Statistics",
    "section": "3.3 Reshaping the Jobs (and Economic) Landscape",
    "text": "3.3 Reshaping the Jobs (and Economic) Landscape\nIt is now generally accepted that AI will reshape the global workforce - displacing, replacing and augmenting jobs as we currently know them. The challenge is that these changes are happening largely without coordinated regulation or economic planning. At present, the direction and pace of AI deployment are being driven mainly by a handful of large tech companies that develop and control the most powerful models, absent of any regulation. Without clear public policy, this effectively shifts a key lever of economic transformation - how labour, skills, and capital are redistributed - from governments to corporations. And that is not a good thing."
  },
  {
    "objectID": "posts/036_21Nov_2025/index.html#data-security-and-privacy-issues",
    "href": "posts/036_21Nov_2025/index.html#data-security-and-privacy-issues",
    "title": "Large Language Models are Just Statistics",
    "section": "3.4 Data Security and Privacy Issues",
    "text": "3.4 Data Security and Privacy Issues\nLLM‚Äôs also raise serious data security and privacy concerns and there are two considerations around this. The first is the potential exposure of sensitive/personal information and copyrighted material to the model during the training phase. Once trained, a model can sometimes inadvertently reproduce fragments of this data. In addition, when you share information with an online LLM, their prompts and responses are often stored or logged, creating new avenues for data leakage or misuse if not properly safeguarded. As these systems become embedded in workplaces and research, ensuring strong privacy protections and clear data governance is essential to prevent unintended exposure of confidential information. Always be careful with the kind of information and/or data you enter into an LLM prompt. Never assume it won‚Äôt be shared online."
  },
  {
    "objectID": "posts/2026 topics/mutate_across/index.html",
    "href": "posts/2026 topics/mutate_across/index.html",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "",
    "text": "Welcome back to Stats Tips for 2026. I hope you all had a restful break during the holiday period. I spent a couple of weeks on the South Island of New Zealand and was rather in awe of how beautiful that part of the world is - Queenstown, Milford Sound, Lake Tekapo and a 3 day hike on the Humpridge Track, which I can really recommend if you‚Äôre into hiking. The landscapes are otherworldly - we plan to go back, it was that good‚Ä¶\nOk, enough about my holidays and on to more serious topics. Today I thought I would ease you back into the world of statistical musings with a less theoretical and more practical post. One that illustrates the application of what I consider to be an indispensable dplyr function in my day-to-day work, and one that I hope you can make use of too - meet across().\nI believe one aspiration we all share in our endeavour to become better R programmers is to write more efficient code that avoids repetition. You may recall that I have dedicated a whole other post to this topic and so our discussion here will further expound upon that theme.\nHow many of you have written a block of code that you have just reused, for example:\nThe same data transformation copied and pasted five times.\nThe same rounding applied column by column.\nThe same ifelse() rewritten with only the variable name changed.\nI know I have.\nIn its most fundamental use-case, across() makes it easy to apply the same transformation to multiple columns in a dataframe in one go, rather than applying the same code block multiple times. It may not be flashy, but once you get the hang of how it works, your code will become shorter, clearer, and far easier to maintain. It‚Äôs important to note that across() doesn‚Äôt work by itself, but rather is a column-selection helper that is evaluated within other dplyr functions, most commonly mutate(), but also summarise() and filter(). In this mental model, mutate() decides what happens and across() decides where is happens.\nLet‚Äôs make these ideas clearer with several examples‚Ä¶"
  },
  {
    "objectID": "posts/2026 topics/mutate_across/index.html#introduction",
    "href": "posts/2026 topics/mutate_across/index.html#introduction",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "",
    "text": "Welcome back to Stats Tips for 2026. I hope you all had a restful break during the holiday period. I spent a couple of weeks on the South Island of New Zealand and was rather in awe of how beautiful that part of the world is - Queenstown, Milford Sound, Lake Tekapo and a 3 day hike on the Humpridge Track, which I can really recommend if you‚Äôre into hiking. The landscapes are otherworldly - we plan to go back, it was that good‚Ä¶\nOk, enough about my holidays and on to more serious topics. Today I thought I would ease you back into the world of statistical musings with a less theoretical and more practical post. One that illustrates the application of what I consider to be an indispensable dplyr function in my day-to-day work, and one that I hope you can make use of too - meet across().\nI believe one aspiration we all share in our endeavour to become better R programmers is to write more efficient code that avoids repetition. You may recall that I have dedicated a whole other post to this topic and so our discussion here will further expound upon that theme.\nHow many of you have written a block of code that you have just reused, for example:\nThe same data transformation copied and pasted five times.\nThe same rounding applied column by column.\nThe same ifelse() rewritten with only the variable name changed.\nI know I have.\nIn its most fundamental use-case, across() makes it easy to apply the same transformation to multiple columns in a dataframe in one go, rather than applying the same code block multiple times. It may not be flashy, but once you get the hang of how it works, your code will become shorter, clearer, and far easier to maintain. It‚Äôs important to note that across() doesn‚Äôt work by itself, but rather is a column-selection helper that is evaluated within other dplyr functions, most commonly mutate(), but also summarise() and filter(). In this mental model, mutate() decides what happens and across() decides where is happens.\nLet‚Äôs make these ideas clearer with several examples‚Ä¶"
  },
  {
    "objectID": "posts/2026 topics/mutate_across/index.html#the-problem-repeating-the-same-operation",
    "href": "posts/2026 topics/mutate_across/index.html#the-problem-repeating-the-same-operation",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "2 The Problem: Repeating the Same Operation",
    "text": "2 The Problem: Repeating the Same Operation\nSuppose you‚Äôre analysing a simple dataset.\n\n\nCode\ndf &lt;- tibble(id = as.character(1:3),\n             age = c(34, 51, 63),\n             weight = c(72.46, 81.27, 76.85),\n             bmi = c(23.66, 27.93, 26.35),\n             cholesterol = c(5.42, 6.01, 5.87))\n\ndf\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.46\n23.66\n5.42\n\n\n2\n51\n81.27\n27.93\n6.01\n\n\n3\n63\n76.85\n26.35\n5.87\n\n\n\n\n\n\n\nYou decide that, for reporting, several variables should be rounded to 1 decimal place."
  },
  {
    "objectID": "posts/2026 topics/mutate_across/index.html#the-na√Øve-way",
    "href": "posts/2026 topics/mutate_across/index.html#the-na√Øve-way",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "3 The Na√Øve Way",
    "text": "3 The Na√Øve Way\nA common first attempt looks like this:\n\n\nCode\ndf |&gt; \n  mutate(weight = round(weight, 1),\n         bmi = round(bmi, 1),\n         cholesterol = round(cholesterol, 1))\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n\n\n2\n51\n81.3\n27.9\n6.0\n\n\n3\n63\n76.8\n26.4\n5.9\n\n\n\n\n\n\n\nYou write out the same line of code for each variable. It does work. It‚Äôs also:\n\nRepetitive\n\nError-prone (easy to forget a variable)\n\nPainful to update when the variable list changes\n\nIf you later add another variable (say waist), you must remember to update this block manually ‚Äî and everywhere else you did something similar."
  },
  {
    "objectID": "posts/2026 topics/mutate_across/index.html#the-core-idea-behind-across",
    "href": "posts/2026 topics/mutate_across/index.html#the-core-idea-behind-across",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "4 The Core Idea Behind across()",
    "text": "4 The Core Idea Behind across()\nThe key insight is simple:\n\nWhen you apply the same transformation to multiple columns, you should write the transformation once.\n\nThat‚Äôs exactly what across() does."
  },
  {
    "objectID": "posts/2026 topics/mutate_across/index.html#the-better-way-mutateacross",
    "href": "posts/2026 topics/mutate_across/index.html#the-better-way-mutateacross",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "5 The Better Way: mutate(across())",
    "text": "5 The Better Way: mutate(across())\nHere‚Äôs the same transformation rewritten using across().\n\n\nCode\ndf |&gt; \n  mutate(across(c(weight, bmi, cholesterol), round, 1))\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n\n\n2\n51\n81.3\n27.9\n6.0\n\n\n3\n63\n76.8\n26.4\n5.9\n\n\n\n\n\n\n\nRead this out loud:\n\n‚ÄúMutate across weight, bmi, and cholesterol by rounding to 1 decimal place.‚Äù\n\nThat phrasing is much closer to how you think about the task."
  },
  {
    "objectID": "posts/2026 topics/mutate_across/index.html#why-this-is-better-beyond-being-shorter",
    "href": "posts/2026 topics/mutate_across/index.html#why-this-is-better-beyond-being-shorter",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "6 Why This Is Better (Beyond Being Shorter)",
    "text": "6 Why This Is Better (Beyond Being Shorter)\n\n6.1 It Scales Naturally\nIf you add another variable:\n\n\nCode\ndf |&gt; \nmutate(across(c(weight, bmi, cholesterol, waist), round, 1))\n\n\nNo duplication. No copy-paste.\n\n\n\n6.2 You Can Select Variables Programmatically\nInstead of naming variables explicitly, you can select them based on some other programmatic characteristic. Here, let‚Äôs select all numeric variables:\n\n\nCode\ndf |&gt; \n  mutate(across(where(is.numeric), round, 1))\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n\n\n2\n51\n81.3\n27.9\n6.0\n\n\n3\n63\n76.8\n26.4\n5.9\n\n\n\n\n\n\n\nAlternatively, you might want to select based on naming conventions. In this example, we would select all variables in the dataframe that begin with the text ‚Äúlab_‚Äù:\n\n\nCode\ndf |&gt; \n  mutate(across(starts_with(\"lab_\"), log))\n\n\nThis is particularly powerful in real research datasets, where variable names often follow patterns.\n\n\n\n6.3 It Reduces Cognitive Load\nIf you reviewed your code 6 months down the track and compared these two blocks:\nweight = round(weight, 1)\nbmi = round(bmi, 1)\ncholesterol = round(cholesterol, 1)\nvs:\nacross(c(weight, bmi, cholesterol), round, 1)\nThe second tells you what is happening immediately."
  },
  {
    "objectID": "posts/2026 topics/mutate_across/index.html#using-anonymous-functions-for-more-complex-logic",
    "href": "posts/2026 topics/mutate_across/index.html#using-anonymous-functions-for-more-complex-logic",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "7 Using Anonymous Functions for More Complex Logic",
    "text": "7 Using Anonymous Functions for More Complex Logic\nYou‚Äôre not limited to simple functions like round() - you can write your own. In this case, across() recognises everything after the ~ as a user-defined or ‚Äúanonymous‚Äù function. For example,\nSuppose you want to:\n\nadd 1 to avoid zeros\n\nlog-transform the result\n\napply this consistently to multiple variables that begin with ‚Äúlab_‚Äù.\n\n\n\nCode\ndf |&gt; \n  mutate(across(starts_with(\"lab_\"), ~ log(.x + 1)))\n\n\nHere, .x represents the current column being transformed.\nAs you can see - we can do all of this in one line of code."
  },
  {
    "objectID": "posts/2026 topics/mutate_across/index.html#creating-new-variables-instead-of-overwriting",
    "href": "posts/2026 topics/mutate_across/index.html#creating-new-variables-instead-of-overwriting",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "8 Creating New Variables Instead of Overwriting",
    "text": "8 Creating New Variables Instead of Overwriting\nIn research workflows, it‚Äôs often good practice for reproducibility to keep raw variables intact and create new variables instead.\n\n\nCode\ndf |&gt; \n  mutate(across(c(weight, bmi), scale, .names = \"{.col}_z\"))\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\nweight_z\nbmi_z\n\n\n\n\n1\n34\n72.46\n23.66\n5.42\n-0.998862996\n-1.0746155\n\n\n2\n51\n81.27\n27.93\n6.01\n1.001133139\n0.9032328\n\n\n3\n63\n76.85\n26.35\n5.87\n-0.002270143\n0.1713826\n\n\n\n\n\n\n\nThis produces new variables (weight_z, bmi_z) which are the Z-score transformations of weight and bmi using R‚Äôs built-in scale function. Note, it‚Äôs a simple case of creating new variable names by prefixing or suffixing characters to the original column name specified by {.col}."
  },
  {
    "objectID": "posts/2026 topics/mutate_across/index.html#mutateacross-vs-summariseacross",
    "href": "posts/2026 topics/mutate_across/index.html#mutateacross-vs-summariseacross",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "9 mutate(across()) vs summarise(across())",
    "text": "9 mutate(across()) vs summarise(across())\nAs I mentioned at the outset, across() is most commonly used in conjunction with mutate(), but let‚Äôs look at an example where we may want to use it with summarise(). Let‚Äôs say we are interested in calculating the mean of each numeric column in the dataframe. We can do that as follows:\n\n\nCode\ndf |&gt; \n  summarise(across(where(is.numeric), mean, na.rm = TRUE))\n\n\n\n\n\n\nage\nweight\nbmi\ncholesterol\n\n\n\n\n49.33333\n76.86\n25.98\n5.766667\n\n\n\n\n\n\n\nNote a common point of confusion:\n\nmutate(across()) returns the same number of rows as in the original dataframe\n\nsummarise(across()) reduces rows (to a single row if no grouping structure is specified)\n\nacross() is therefore applied to both functions in the same way, but with different intent."
  },
  {
    "objectID": "posts/2026 topics/mutate_across/index.html#multiple-functions-per-variable",
    "href": "posts/2026 topics/mutate_across/index.html#multiple-functions-per-variable",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "10 Multiple Functions per Variable",
    "text": "10 Multiple Functions per Variable\nWhat if were interested in not only calculating the mean of each numeric column, but also the standard deviation and the number of observations in each column. Well, it‚Äôs relatively easy to extend the above example by now applying several functions at once.\n\n\nCode\ndf |&gt; \n  summarise(across(where(is.numeric), list(mean = mean, \n                                           sd = sd,\n                                           n = ~ sum(!is.na(.))),\n                   .names = \"{.col}_{.fn}\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage_mean\nage_sd\nage_n\nweight_mean\nweight_sd\nweight_n\nbmi_mean\nbmi_sd\nbmi_n\ncholesterol_mean\ncholesterol_sd\ncholesterol_n\n\n\n\n\n49.33333\n14.57166\n3\n76.86\n4.405009\n3\n25.98\n2.158912\n3\n5.766667\n0.3082748\n3\n\n\n\n\n\n\n\nWe can now specify {.fn} as a naming qualifier and append this to the original column name. This pattern is a stepping stone toward automated summary tables and reporting pipelines."
  },
  {
    "objectID": "posts/2026 topics/mutate_across/index.html#advanced-example-using-mutateacross-with-map",
    "href": "posts/2026 topics/mutate_across/index.html#advanced-example-using-mutateacross-with-map",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "11 Advanced Example: Using mutate(across()) with map()",
    "text": "11 Advanced Example: Using mutate(across()) with map()\nAre you ready for something more advanced (but also extremely powerful)? So far we have been dealing with a single dataframe, but we can also leverage the power of across() in simultaneous column manipulation over multiple dataframes using map().\n\n11.1 The Problem\nSuppose you have several datasets with the same structure assembled within a list (a list is a convenient R object within which many other R objects can be stored - including dataframes). We can access a particular object within a list with the $ operator, much like we access the columns of a dataframe.\n\n\nCode\ndatasets &lt;- list(raw   = df, \n                 clean = df,\n                 sens  = df)\n\ndatasets$raw\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.46\n23.66\n5.42\n\n\n2\n51\n81.27\n27.93\n6.01\n\n\n3\n63\n76.85\n26.35\n5.87\n\n\n\n\n\n\nCode\ndatasets$clean\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.46\n23.66\n5.42\n\n\n2\n51\n81.27\n27.93\n6.01\n\n\n3\n63\n76.85\n26.35\n5.87\n\n\n\n\n\n\nCode\ndatasets$sens\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.46\n23.66\n5.42\n\n\n2\n51\n81.27\n27.93\n6.01\n\n\n3\n63\n76.85\n26.35\n5.87\n\n\n\n\n\n\n\nNow, suppose you want to apply the same transformation to all of them.\n\n\n\n11.2 The Na√Øve Way\ndf_raw   &lt;- df_raw   |&gt;  mutate(...)\ndf_clean &lt;- df_clean |&gt;  mutate(...)\ndf_sens  &lt;- df_sens  |&gt;  mutate(...)\nIn this approach we go through and re-apply the same code to each dataframe but this can become difficult to maintain and easy to get wrong.\n\n\n\n11.3 The Better Way: map() + mutate(across())\n\n\nCode\ndatasets &lt;- datasets |&gt; \n  map(~ .x |&gt; \n    mutate(across(where(is.numeric), round, 1)))\n\ndatasets$raw\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n\n\n2\n51\n81.3\n27.9\n6.0\n\n\n3\n63\n76.8\n26.4\n5.9\n\n\n\n\n\n\nCode\ndatasets$clean\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n\n\n2\n51\n81.3\n27.9\n6.0\n\n\n3\n63\n76.8\n26.4\n5.9\n\n\n\n\n\n\nCode\ndatasets$sens\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n\n\n2\n51\n81.3\n27.9\n6.0\n\n\n3\n63\n76.8\n26.4\n5.9\n\n\n\n\n\n\n\nIndeed, the more efficient way is to use across() within mutate() within map().\nWe can read this as:\n\n‚ÄúFor each dataset, mutate across numeric variables by rounding to 1 decimal place.‚Äù\n\nThis approach using map() ensures:\n\nidentical logic across datasets\n\nchanges happen in one place\n\nconsistency is guaranteed\n\n\n\n\n11.4 Another Example: Standardising Variables Across Datasets\nNow, let‚Äôs take this further by extending the earlier example of creating new variables, not just within a single dataframe, but across multiple dataframes.\n\n\nCode\ndatasets &lt;- datasets |&gt; \n  map(~ .x |&gt; \n    mutate(across(c(age, bmi, weight), scale, .names = \"{.col}_z\")))\n\ndatasets$raw\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\nage_z\nbmi_z\nweight_z\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n-1.0522707\n-1.0806343\n-0.99233882\n\n\n2\n51\n81.3\n27.9\n6.0\n0.1143773\n0.8926979\n1.00748903\n\n\n3\n63\n76.8\n26.4\n5.9\n0.9378935\n0.1879364\n-0.01515021\n\n\n\n\n\n\nCode\ndatasets$clean\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\nage_z\nbmi_z\nweight_z\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n-1.0522707\n-1.0806343\n-0.99233882\n\n\n2\n51\n81.3\n27.9\n6.0\n0.1143773\n0.8926979\n1.00748903\n\n\n3\n63\n76.8\n26.4\n5.9\n0.9378935\n0.1879364\n-0.01515021\n\n\n\n\n\n\nCode\ndatasets$sens\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\nage_z\nbmi_z\nweight_z\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n-1.0522707\n-1.0806343\n-0.99233882\n\n\n2\n51\n81.3\n27.9\n6.0\n0.1143773\n0.8926979\n1.00748903\n\n\n3\n63\n76.8\n26.4\n5.9\n0.9378935\n0.1879364\n-0.01515021\n\n\n\n\n\n\nYou can appreciate how much of a Swiss-army knife of data manipulation, across() can be become when used in conjunction with other R functions."
  },
  {
    "objectID": "posts/2026 topics/mutate_across/index.html#a-mental-model-to-take-away",
    "href": "posts/2026 topics/mutate_across/index.html#a-mental-model-to-take-away",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "12 A Mental Model to Take Away",
    "text": "12 A Mental Model to Take Away\nWhenever you catch yourself thinking:\n\n‚ÄúI‚Äôm doing the same thing to several variables‚Ä¶‚Äù\n\nYou should immediately ask:\n\n‚ÄúCan this be an across()?‚Äù\n\nThat question alone will dramatically improve the quality of your R code."
  },
  {
    "objectID": "posts/2026 topics/mutate_across/index.html#final-thoughts",
    "href": "posts/2026 topics/mutate_across/index.html#final-thoughts",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "13 Final Thoughts",
    "text": "13 Final Thoughts\nmutate(across()) isn‚Äôt just a convenience ‚Äî it‚Äôs a design philosophy:\n\nwrite intent, not mechanics\n\nminimise repetition\n\nmake future changes cheap\n\nIf you internalise this early, your scripts will be easier to read, easier to debug, and easier for collaborators (and future you) to trust.\nIn future posts, we‚Äôll build on this idea with tools like summarise(across()), case_when(), and nest() + map() ‚Äî all part of the same ‚Äúwrite it once, apply it everywhere‚Äù mindset."
  },
  {
    "objectID": "posts/2026 topics/mutate_across/index.html#final-thoughts-and-a-mental-model-to-take-away",
    "href": "posts/2026 topics/mutate_across/index.html#final-thoughts-and-a-mental-model-to-take-away",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "12 Final Thoughts and a Mental Model to Take Away",
    "text": "12 Final Thoughts and a Mental Model to Take Away\nWhenever you catch yourself thinking:\n\n‚ÄúI‚Äôm doing the same thing to several variables‚Ä¶‚Äù\n\nYou should immediately ask:\n\n‚ÄúCan this be an across()?‚Äù\n\nThat question alone will dramatically improve the quality of your R code.\nI hope you‚Äôve found this programming tip helpful and I will see you again for more stats tips, next month."
  },
  {
    "objectID": "posts/2026 topics/stepwise/index.html",
    "href": "posts/2026 topics/stepwise/index.html",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "",
    "text": "As a clinician/researcher you might not be aware that data-driven variable selection methods are widely regarded within the statistical community as the the equivalent of Facebook‚Äôs ‚ÄòCrazy Uncle‚Äô - an individual who is misinformed, opinionated and someone we just can‚Äôt easily shed from our lives.\nSo what‚Äôs the parallel I‚Äôm trying to draw, you might ask?\nWell, despite decades of criticism in the statistical literature, stepwise regression (as a form of data-driven variable selection) continues to appear in academic papers, regulatory submissions, and industry analyses, when it is universally recognised as plain wrong. Despite that, stepwise regression remains one of the most commonly taught and used model-building techniques in applied statistics. Indeed, I was taught these methods during my Masters of Biostatistics 10 years ago. The appeal is obvious: when faced with many candidate predictors and limited sample size, stepwise procedures promise an automated, objective way to ‚Äúlet the data decide‚Äù which variables matter.\n\n\n\n\n\n\nNote\n\n\n\nWhen I refer to ‚Äúdata-driven‚Äù or ‚Äústepwise‚Äù I am meaning any decision process leading to the addition or removal of candidate variables from a regression model that is based on data-derived statistics such as significance, R-squared, Akaike Information Criteria (AIC), etc‚Ä¶\n\n\nSo then, what‚Äôs the problem?\nWell, there are many. The most notable I will summarise from page 68 of Frank Harrell‚Äôs modern statistical classic - Regression Modelling Strategies:\n\nThe R-squared or even adjusted R-squared values of the end model are biased high.\nThe F and Chi-square test statistics of the final model do not have the claimed distribution.\nThe standard errors of coefficient estimates are biased low and confidence intervals for effects and predictions are falsely narrow.\nThe p values are too small (there are severe multiple comparison problems in addition to problems 2. and 3.) and do not have the proper meaning, and it is difficult to correct for this.\nThe regression coefficients are biased high in absolute value and need shrinkage but this is rarely done.\nVariable selection is made arbitrary by collinearity.\nIt allows us to not think about the problem.\n\nThe net effect of the above is that we may end up making claims, assertions or clinical decisions based on incorrect evidence.\nIn this post I will attempt to make the case that stepwise regression is not just suboptimal, but seriously flawed when used for inference, explanation, or decision-making. The problems are not subtle or academic; they directly undermine the validity, reproducibility, and interpretability of results. I will outline why stepwise regression fails, why these failures are amplified in applied research (particularly clinical and epidemiological settings), and what modern, defensible alternatives look like in practice.\nThe goal is not to shame analysts who use stepwise regression ‚Äî I too am guilty of the multitude of sins I am about to discuss ‚Äî but to provide a clear roadmap towards best practice."
  },
  {
    "objectID": "posts/2026 topics/stepwise/index.html#introduction",
    "href": "posts/2026 topics/stepwise/index.html#introduction",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "",
    "text": "As a clinician/researcher you might not be aware that data-driven variable selection methods are widely regarded within the statistical community as the the equivalent of Facebook‚Äôs ‚ÄòCrazy Uncle‚Äô - an individual who is misinformed, opinionated and someone we just can‚Äôt easily shed from our lives.\nSo what‚Äôs the parallel I‚Äôm trying to draw, you might ask?\nWell, despite decades of criticism in the statistical literature, stepwise regression (as a form of data-driven variable selection) continues to appear in academic papers, regulatory submissions, and industry analyses, when it is universally recognised as plain wrong. Despite that, stepwise regression remains one of the most commonly taught and used model-building techniques in applied statistics. Indeed, I was taught these methods during my Masters of Biostatistics 10 years ago. The appeal is obvious: when faced with many candidate predictors and limited sample size, stepwise procedures promise an automated, objective way to ‚Äúlet the data decide‚Äù which variables matter.\n\n\n\n\n\n\nNote\n\n\n\nWhen I refer to ‚Äúdata-driven‚Äù or ‚Äústepwise‚Äù I am meaning any decision process leading to the addition or removal of candidate variables from a regression model that is based on data-derived statistics such as significance, R-squared, Akaike Information Criteria (AIC), etc‚Ä¶\n\n\nSo then, what‚Äôs the problem?\nWell, there are many. The most notable I will summarise from page 68 of Frank Harrell‚Äôs modern statistical classic - Regression Modelling Strategies:\n\nThe R-squared or even adjusted R-squared values of the end model are biased high.\nThe F and Chi-square test statistics of the final model do not have the claimed distribution.\nThe standard errors of coefficient estimates are biased low and confidence intervals for effects and predictions are falsely narrow.\nThe p values are too small (there are severe multiple comparison problems in addition to problems 2. and 3.) and do not have the proper meaning, and it is difficult to correct for this.\nThe regression coefficients are biased high in absolute value and need shrinkage but this is rarely done.\nVariable selection is made arbitrary by collinearity.\nIt allows us to not think about the problem.\n\nThe net effect of the above is that we may end up making claims, assertions or clinical decisions based on incorrect evidence.\nIn this post I will attempt to make the case that stepwise regression is not just suboptimal, but seriously flawed when used for inference, explanation, or decision-making. The problems are not subtle or academic; they directly undermine the validity, reproducibility, and interpretability of results. I will outline why stepwise regression fails, why these failures are amplified in applied research (particularly clinical and epidemiological settings), and what modern, defensible alternatives look like in practice.\nThe goal is not to shame analysts who use stepwise regression ‚Äî I too am guilty of the multitude of sins I am about to discuss ‚Äî but to provide a clear roadmap towards best practice."
  },
  {
    "objectID": "posts/2026 topics/stepwise/index.html#the-problem-repeating-the-same-operation",
    "href": "posts/2026 topics/stepwise/index.html#the-problem-repeating-the-same-operation",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "3 The Problem: Repeating the Same Operation",
    "text": "3 The Problem: Repeating the Same Operation\nAt the core of the problem is using statistical inference methods like p values, confidence intervals and ANOVA F tests that were designed and valid for a pre-specified model, but applying them instead to a model we have structured based on the data. The variables are selected partly based on chance, and we are giving ourselves a sneaky headstart in making a variable being significant.\nBasically, this is the sort of thing that leads to the reproducibility crisis in science."
  },
  {
    "objectID": "posts/2026 topics/stepwise/index.html#the-na√Øve-way",
    "href": "posts/2026 topics/stepwise/index.html#the-na√Øve-way",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "3 The Na√Øve Way",
    "text": "3 The Na√Øve Way\nA common first attempt looks like this:\n\n\nCode\ndf |&gt; \n  mutate(weight = round(weight, 1),\n         bmi = round(bmi, 1),\n         cholesterol = round(cholesterol, 1))\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n\n\n2\n51\n81.3\n27.9\n6.0\n\n\n3\n63\n76.8\n26.4\n5.9\n\n\n\n\n\n\n\nYou write out the same line of code for each variable. It does work. It‚Äôs also:\n\nRepetitive\n\nError-prone (easy to forget a variable)\n\nPainful to update when the variable list changes\n\nIf you later add another variable (say waist), you must remember to update this block manually ‚Äî and everywhere else you did something similar."
  },
  {
    "objectID": "posts/2026 topics/stepwise/index.html#the-core-idea-behind-across",
    "href": "posts/2026 topics/stepwise/index.html#the-core-idea-behind-across",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "4 The Core Idea Behind across()",
    "text": "4 The Core Idea Behind across()\nThe key insight is simple:\n\nWhen you apply the same transformation to multiple columns, you should write the transformation once.\n\nThat‚Äôs exactly what across() does."
  },
  {
    "objectID": "posts/2026 topics/stepwise/index.html#the-better-way-mutateacross",
    "href": "posts/2026 topics/stepwise/index.html#the-better-way-mutateacross",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "5 The Better Way: mutate(across())",
    "text": "5 The Better Way: mutate(across())\nHere‚Äôs the same transformation rewritten using across().\n\n\nCode\ndf |&gt; \n  mutate(across(c(weight, bmi, cholesterol), round, 1))\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n\n\n2\n51\n81.3\n27.9\n6.0\n\n\n3\n63\n76.8\n26.4\n5.9\n\n\n\n\n\n\n\nRead this out loud:\n\n‚ÄúMutate across weight, bmi, and cholesterol by rounding to 1 decimal place.‚Äù\n\nThat phrasing is much closer to how you think about the task."
  },
  {
    "objectID": "posts/2026 topics/stepwise/index.html#why-this-is-better-beyond-being-shorter",
    "href": "posts/2026 topics/stepwise/index.html#why-this-is-better-beyond-being-shorter",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "6 Why This Is Better (Beyond Being Shorter)",
    "text": "6 Why This Is Better (Beyond Being Shorter)\n\n6.1 It Scales Naturally\nIf you add another variable:\n\n\nCode\ndf |&gt; \nmutate(across(c(weight, bmi, cholesterol, waist), round, 1))\n\n\nNo duplication. No copy-paste.\n\n\n\n6.2 You Can Select Variables Programmatically\nInstead of naming variables explicitly, you can select them based on some other programmatic characteristic. Here, let‚Äôs select all numeric variables:\n\n\nCode\ndf |&gt; \n  mutate(across(where(is.numeric), round, 1))\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n\n\n2\n51\n81.3\n27.9\n6.0\n\n\n3\n63\n76.8\n26.4\n5.9\n\n\n\n\n\n\n\nAlternatively, you might want to select based on naming conventions. In this example, we would select all variables in the dataframe that begin with the text ‚Äúlab_‚Äù:\n\n\nCode\ndf |&gt; \n  mutate(across(starts_with(\"lab_\"), log))\n\n\nThis is particularly powerful in real research datasets, where variable names often follow patterns.\n\n\n\n6.3 It Reduces Cognitive Load\nIf you reviewed your code 6 months down the track and compared these two blocks:\nweight = round(weight, 1)\nbmi = round(bmi, 1)\ncholesterol = round(cholesterol, 1)\nvs:\nacross(c(weight, bmi, cholesterol), round, 1)\nThe second tells you what is happening immediately."
  },
  {
    "objectID": "posts/2026 topics/stepwise/index.html#using-anonymous-functions-for-more-complex-logic",
    "href": "posts/2026 topics/stepwise/index.html#using-anonymous-functions-for-more-complex-logic",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "7 Using Anonymous Functions for More Complex Logic",
    "text": "7 Using Anonymous Functions for More Complex Logic\nYou‚Äôre not limited to simple functions like round() - you can write your own. In this case, across() recognises everything after the ~ as a user-defined or ‚Äúanonymous‚Äù function. For example,\nSuppose you want to:\n\nadd 1 to avoid zeros\n\nlog-transform the result\n\napply this consistently to multiple variables that begin with ‚Äúlab_‚Äù.\n\n\n\nCode\ndf |&gt; \n  mutate(across(starts_with(\"lab_\"), ~ log(.x + 1)))\n\n\nHere, .x represents the current column being transformed.\nAs you can see - we can do all of this in one line of code."
  },
  {
    "objectID": "posts/2026 topics/stepwise/index.html#creating-new-variables-instead-of-overwriting",
    "href": "posts/2026 topics/stepwise/index.html#creating-new-variables-instead-of-overwriting",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "8 Creating New Variables Instead of Overwriting",
    "text": "8 Creating New Variables Instead of Overwriting\nIn research workflows, it‚Äôs often good practice for reproducibility to keep raw variables intact and create new variables instead.\n\n\nCode\ndf |&gt; \n  mutate(across(c(weight, bmi), scale, .names = \"{.col}_z\"))\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\nweight_z\nbmi_z\n\n\n\n\n1\n34\n72.46\n23.66\n5.42\n-0.998862996\n-1.0746155\n\n\n2\n51\n81.27\n27.93\n6.01\n1.001133139\n0.9032328\n\n\n3\n63\n76.85\n26.35\n5.87\n-0.002270143\n0.1713826\n\n\n\n\n\n\n\nThis produces new variables (weight_z, bmi_z) which are the Z-score transformations of weight and bmi using R‚Äôs built-in scale function. Note, it‚Äôs a simple case of creating new variable names by prefixing or suffixing characters to the original column name specified by {.col}."
  },
  {
    "objectID": "posts/2026 topics/stepwise/index.html#mutateacross-vs-summariseacross",
    "href": "posts/2026 topics/stepwise/index.html#mutateacross-vs-summariseacross",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "9 mutate(across()) vs summarise(across())",
    "text": "9 mutate(across()) vs summarise(across())\nAs I mentioned at the outset, across() is most commonly used in conjunction with mutate(), but let‚Äôs look at an example where we may want to use it with summarise(). Let‚Äôs say we are interested in calculating the mean of each numeric column in the dataframe. We can do that as follows:\n\n\nCode\ndf |&gt; \n  summarise(across(where(is.numeric), mean, na.rm = TRUE))\n\n\n\n\n\n\nage\nweight\nbmi\ncholesterol\n\n\n\n\n49.33333\n76.86\n25.98\n5.766667\n\n\n\n\n\n\n\nNote a common point of confusion:\n\nmutate(across()) returns the same number of rows as in the original dataframe\n\nsummarise(across()) reduces rows (to a single row if no grouping structure is specified)\n\nacross() is therefore applied to both functions in the same way, but with different intent."
  },
  {
    "objectID": "posts/2026 topics/stepwise/index.html#multiple-functions-per-variable",
    "href": "posts/2026 topics/stepwise/index.html#multiple-functions-per-variable",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "10 Multiple Functions per Variable",
    "text": "10 Multiple Functions per Variable\nWhat if were interested in not only calculating the mean of each numeric column, but also the standard deviation and the number of observations in each column. Well, it‚Äôs relatively easy to extend the above example by now applying several functions at once.\n\n\nCode\ndf |&gt; \n  summarise(across(where(is.numeric), list(mean = mean, \n                                           sd = sd,\n                                           n = ~ sum(!is.na(.))),\n                   .names = \"{.col}_{.fn}\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage_mean\nage_sd\nage_n\nweight_mean\nweight_sd\nweight_n\nbmi_mean\nbmi_sd\nbmi_n\ncholesterol_mean\ncholesterol_sd\ncholesterol_n\n\n\n\n\n49.33333\n14.57166\n3\n76.86\n4.405009\n3\n25.98\n2.158912\n3\n5.766667\n0.3082748\n3\n\n\n\n\n\n\n\nWe can now specify {.fn} as a naming qualifier and append this to the original column name. This pattern is a stepping stone toward automated summary tables and reporting pipelines."
  },
  {
    "objectID": "posts/2026 topics/stepwise/index.html#advanced-example-using-mutateacross-with-map",
    "href": "posts/2026 topics/stepwise/index.html#advanced-example-using-mutateacross-with-map",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "11 Advanced Example: Using mutate(across()) with map()",
    "text": "11 Advanced Example: Using mutate(across()) with map()\nAre you ready for something more advanced (but also extremely powerful)? So far we have been dealing with a single dataframe, but we can also leverage the power of across() in simultaneous column manipulation over multiple dataframes using map().\n\n11.1 The Problem\nSuppose you have several datasets with the same structure assembled within a list (a list is a convenient R object within which many other R objects can be stored - including dataframes). We can access a particular object within a list with the $ operator, much like we access the columns of a dataframe.\n\n\nCode\ndatasets &lt;- list(raw   = df, \n                 clean = df,\n                 sens  = df)\n\ndatasets$raw\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.46\n23.66\n5.42\n\n\n2\n51\n81.27\n27.93\n6.01\n\n\n3\n63\n76.85\n26.35\n5.87\n\n\n\n\n\n\nCode\ndatasets$clean\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.46\n23.66\n5.42\n\n\n2\n51\n81.27\n27.93\n6.01\n\n\n3\n63\n76.85\n26.35\n5.87\n\n\n\n\n\n\nCode\ndatasets$sens\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.46\n23.66\n5.42\n\n\n2\n51\n81.27\n27.93\n6.01\n\n\n3\n63\n76.85\n26.35\n5.87\n\n\n\n\n\n\n\nNow, suppose you want to apply the same transformation to all of them.\n\n\n\n11.2 The Na√Øve Way\ndf_raw   &lt;- df_raw   |&gt;  mutate(...)\ndf_clean &lt;- df_clean |&gt;  mutate(...)\ndf_sens  &lt;- df_sens  |&gt;  mutate(...)\nIn this approach we go through and re-apply the same code to each dataframe but this can become difficult to maintain and easy to get wrong.\n\n\n\n11.3 The Better Way: map() + mutate(across())\n\n\nCode\ndatasets &lt;- datasets |&gt; \n  map(~ .x |&gt; \n    mutate(across(where(is.numeric), round, 1)))\n\ndatasets$raw\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n\n\n2\n51\n81.3\n27.9\n6.0\n\n\n3\n63\n76.8\n26.4\n5.9\n\n\n\n\n\n\nCode\ndatasets$clean\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n\n\n2\n51\n81.3\n27.9\n6.0\n\n\n3\n63\n76.8\n26.4\n5.9\n\n\n\n\n\n\nCode\ndatasets$sens\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n\n\n2\n51\n81.3\n27.9\n6.0\n\n\n3\n63\n76.8\n26.4\n5.9\n\n\n\n\n\n\n\nIndeed, the more efficient way is to use across() within mutate() within map().\nWe can read this as:\n\n‚ÄúFor each dataset, mutate across numeric variables by rounding to 1 decimal place.‚Äù\n\nThis approach using map() ensures:\n\nidentical logic across datasets\n\nchanges happen in one place\n\nconsistency is guaranteed\n\n\n\n\n11.4 Another Example: Standardising Variables Across Datasets\nNow, let‚Äôs take this further by extending the earlier example of creating new variables, not just within a single dataframe, but across multiple dataframes.\n\n\nCode\ndatasets &lt;- datasets |&gt; \n  map(~ .x |&gt; \n    mutate(across(c(age, bmi, weight), scale, .names = \"{.col}_z\")))\n\ndatasets$raw\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\nage_z\nbmi_z\nweight_z\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n-1.0522707\n-1.0806343\n-0.99233882\n\n\n2\n51\n81.3\n27.9\n6.0\n0.1143773\n0.8926979\n1.00748903\n\n\n3\n63\n76.8\n26.4\n5.9\n0.9378935\n0.1879364\n-0.01515021\n\n\n\n\n\n\nCode\ndatasets$clean\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\nage_z\nbmi_z\nweight_z\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n-1.0522707\n-1.0806343\n-0.99233882\n\n\n2\n51\n81.3\n27.9\n6.0\n0.1143773\n0.8926979\n1.00748903\n\n\n3\n63\n76.8\n26.4\n5.9\n0.9378935\n0.1879364\n-0.01515021\n\n\n\n\n\n\nCode\ndatasets$sens\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\nage_z\nbmi_z\nweight_z\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n-1.0522707\n-1.0806343\n-0.99233882\n\n\n2\n51\n81.3\n27.9\n6.0\n0.1143773\n0.8926979\n1.00748903\n\n\n3\n63\n76.8\n26.4\n5.9\n0.9378935\n0.1879364\n-0.01515021\n\n\n\n\n\n\nYou can appreciate how much of a Swiss-army knife of data manipulation, across() can be become when used in conjunction with other R functions."
  },
  {
    "objectID": "posts/2026 topics/stepwise/index.html#final-thoughts-and-a-mental-model-to-take-away",
    "href": "posts/2026 topics/stepwise/index.html#final-thoughts-and-a-mental-model-to-take-away",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "12 Final Thoughts and a Mental Model to Take Away",
    "text": "12 Final Thoughts and a Mental Model to Take Away\nWhenever you catch yourself thinking:\n\n‚ÄúI‚Äôm doing the same thing to several variables‚Ä¶‚Äù\n\nYou should immediately ask:\n\n‚ÄúCan this be an across()?‚Äù\n\nThat question alone will dramatically improve the quality of your R code.\nI hope you‚Äôve found this programming tip helpful and I will see you again for more Stats Tips, next month."
  },
  {
    "objectID": "posts/2026 topics/stepwise/index.html#why-stepwise-regression-persists",
    "href": "posts/2026 topics/stepwise/index.html#why-stepwise-regression-persists",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "2 Why Stepwise Regression Persists",
    "text": "2 Why Stepwise Regression Persists\nLet‚Äôs get the elephant in the room out of the way. If there are so many issues with stepwise regression why is it still so commonly used in the analysis of clinical data? Well, I don‚Äôt think there is a single answer to this question, but rather several potential factors that probably interplay in maintaining its veneer of contemporary methodological relevance:\n\nHistorical inertia: it has been taught for decades and appears in older textbooks.\nFear of change: researchers may worry that journal editors/reviewers do not appreciate new approaches.\nCognitive appeal: produces a single, seemingly parsimonious model.\nEase of implementation: many statistical packages make stepwise selection easy and prominent.\nMisplaced objectivity: automated procedures appear neutral, even when they enforce arbitrary thresholds on our data.\n\nThese justifications, when considered on an individual basis, become hard to defend. But before we look at what we can do instead, let‚Äôs first delve a little deeper into the specific problems associated with stepwise regression."
  },
  {
    "objectID": "posts/2026 topics/stepwise/index.html#what-is-stepwise-regression",
    "href": "posts/2026 topics/stepwise/index.html#what-is-stepwise-regression",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "3 What is Stepwise Regression?",
    "text": "3 What is Stepwise Regression?\nMost of you already know what I‚Äôm talking about, but let‚Äôs recap the basics for those who don‚Äôt.\nStepwise regression refers to a family of automated variable selection procedures applied to regression models. The most common variants are:\n\nForward selection: start with no predictors, then add variables one at a time based on some criterion (often the smallest p-value or largest improvement in AIC).\nBackward elimination: start with all candidate predictors, then remove variables that fail to meet a significance threshold.\nBidirectional (stepwise) selection: alternate between adding and removing variables at each step.\n\nThese procedures are typically driven by hypothesis tests (e.g.¬†p &lt; 0.05) or information criteria (AIC, BIC), and they terminate once no further steps improve the chosen criterion. The end result is a single model containing a data-driven subset of the original pool of available candidate predictor variables."
  },
  {
    "objectID": "posts/2026 topics/stepwise/index.html#the-core-statistical-problems",
    "href": "posts/2026 topics/stepwise/index.html#the-core-statistical-problems",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "4 The Core Statistical Problems",
    "text": "4 The Core Statistical Problems\nAlthough I mentioned these at the outset of this post (based on Harrell‚Äôs text), let‚Äôs flesh some of them out in more detail now.\n\n4.1 Invalid Inference After Selection\nPerhaps the most fundamental problem is that standard inferential quantities are wrong after stepwise selection. When using data-driven variable selection:\n\nP-values, confidence intervals, and standard errors are computed as if the model had been specified in advance.\n\nBut in reality, the data were used at least twice: once to select variables, and again to estimate their effects.\nThis reuse of data leads to:\n\nunderestimated standard errors,\noverly narrow confidence intervals,\nand inflated statistical significance.\n\nYou may not be aware that statistical inference methods using p values, standard errors and confidence intervals were designed to be valid when applied once to a pre-specified model, not iteratively reapplied to a model where chance partly informs the decision at each step. This invalidates the nominal properties of a statistical test, giving us as researchers, a false sense of certainty.\n\n\n4.2 Multiple Testing in Disguise\nStepwise regression performs a large number of implicit hypothesis tests while pretending to conduct only a few. Each step involves testing multiple candidate variables, but no adjustment is made for multiplicity.\nViewed correctly, stepwise selection is a form of uncorrected multiple testing with a stopping rule. The resulting family-wise Type I error rate can be dramatically higher than the nominal level, especially when predictors are correlated.\nThis means that ‚Äústatistically significant‚Äù predictors selected by stepwise methods are often artifacts of chance rather than real signals.\n\n\n4.3 Overfitting and Optimism\nStepwise procedures are designed to optimise in-sample fit. As a result, they tend to overfit, especially when:\n\nthe number of predictors is large relative to the sample size,\npredictors are correlated,\nor the true signal-to-noise ratio is modest.\n\nOverfitted models appear impressive in the training data but perform poorly on new data. This optimism is rarely quantified or acknowledged when stepwise regression is used.\n\n\n4.4 Model Instability\nOne of the most damaging properties of stepwise regression is instability. Small perturbations in the data - e.g.¬†removing a few observations, changing a significance threshold, or using a different random split - can produce entirely different selected models.\nThis instability arises because stepwise selection operates near decision boundaries. When predictors are correlated or effects are weak, tiny changes can flip inclusion decisions.\nAn unstable model cannot be trusted for interpretation, explanation, or clinical decision-making.\n\n\n4.5 Biased Coefficient Estimates\nEven when a predictor truly has an effect, stepwise regression tends to overestimate its magnitude if it is selected. This phenomenon - sometimes called the ‚Äúwinner‚Äôs curse‚Äù - occurs because variables are selected given they look large in the sample. The result is that reported effect sizes tend to be exaggerated, with subsequent studies often failing to replicate the findings."
  },
  {
    "objectID": "posts/2026 topics/stepwise/index.html#why-these-problems-matter-in-applied-research",
    "href": "posts/2026 topics/stepwise/index.html#why-these-problems-matter-in-applied-research",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "5 Why These Problems Matter in Applied Research",
    "text": "5 Why These Problems Matter in Applied Research\nIn some purely predictive settings, poor inference may be tolerable. In most applied research, however, regression models are used to:\n\ndraw causal conclusions,\ninform clinical or policy decisions,\nsupport regulatory submissions,\nor generate scientific knowledge.\n\nIn these contexts, stepwise regression is especially problematic.\n\n5.1 Clinical and Epidemiological Studies\nIn medical research, variable inclusion is often interpreted causally - even when analysts disclaim causal intent. A covariate that survives stepwise selection may be described as a ‚Äúrisk factor‚Äù or ‚Äúindependent predictor‚Äù, while excluded variables are implicitly treated as unimportant.\nBut this is deeply misleading. Stepwise regression does not distinguish between confounders, mediators, and colliders, and it frequently excludes clinically important variables simply because their effects are imprecisely estimated.\n\n\n5.2 Regulatory and Reporting Contexts\nIn regulatory settings, such as clinical trial reporting, transparency and reproducibility are paramount. Stepwise regression undermines both, because:\n\nthe analysis path is data-driven and difficult to justify prospectively,\nresults may not reproduce across datasets or populations,\nand inferential quantities lack a clear interpretation.\n\nFor these reasons, many regulatory guidelines explicitly discourage data-driven variable selection."
  },
  {
    "objectID": "posts/2026 topics/stepwise/index.html#what-to-do-instead",
    "href": "posts/2026 topics/stepwise/index.html#what-to-do-instead",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "6 What to Do Instead",
    "text": "6 What to Do Instead\nOk, we‚Äôve spent a fair amount of time talking about the problem. Let‚Äôs now discuss what we can actually do about it. The good news is that we are not short of alternatives. In fact, modern approaches are often simpler, more transparent, and more defensible.\nI am going to discuss these approaches in the context of ‚Äúmodel intent‚Äù - i.e.¬†what is the purpose for your regression model? This ultimately boils down to one of two options - explanation vs prediction. Model explanation and model prediction answer fundamentally different questions. Explanatory models aim to estimate and interpret the effect of variables, often with a causal lens, which makes pre-specification, subject-matter knowledge, and valid inference essential. An example research question that such a model could answer might be:\n\n‚ÄúWhat is the effect of DMT x on the risk of disease relapse, after adjusting for age, sex, smoking status, and baseline disease severity?‚Äù\n\ni.e.¬†we are interested primarily in the explanatory ‚Äòeffect‚Äô of a new DMT on disease relapse.\nPredictive models, by contrast, are judged by how well they perform on new data, not by whether their coefficients are interpretable or causally meaningful - in other words, we don‚Äôt really care about individual variable ‚Äòeffects‚Äô but rather just the overall predictive power of the model. Here, the commensurate research question takes a different form:\n\n‚ÄúUsing age, sex, smoking status, current therapy and disease severity data at baseline, what is an individual‚Äôs 5-year risk of disease relapse?‚Äù\n\ni.e.¬†we want to predict relapse risk but don‚Äôt really care about any individual pedictor.\n\n6.1 Explanation\n\n6.1.1 Pre-Specification and Subject-Matter Knowledge\nWhen we are interested in model explanation, the most robust approach is also the least glamorous: decide in advance (i.e.¬†prior to seeing the data) which variables belong in the model, and make that our one and only model.\nHere, variable inclusion should be guided by:\n\nscientific understanding,\ncausal reasoning,\nand study design.\n\nConsideration should be given to sythesising these justifications in a supporting causal diagram (DAG - see the previous post).\nNote that if a variable is a known confounder, it should be included regardless of its p-value. In contrast, if it is irrelevant to the scientific question, it should not be tested for inclusion.\nThis approach prioritises validity over convenience. We are very interested in understanding the characteristics of each individual predictor variable (i.e.¬†is it a potential confounder, collider, mediator, etc in the exposure -&gt; outcome relationship), and the interplay between all such variables. Again, this is where a causal diagram, even if we are not planning an explicit causal analysis, can be very helpful. But this is where the hard work ends in explanatory modelling. From a coding perspective, things are easier than ever - we just run whatever model we have decided on (and nothing more), comfortable in the fact that our p-values and confidence intervals are correct.\n\n\n6.1.2 Transparent Sensitivity Analyses\nIf, for whatever reason, pre-specification is not possible and variable selection cannot be avoided, resultant models should be treated as exploratory and accompanied by:\n\nsensitivity analyses,\nstability assessments,\nand clear disclaimers about inferential limitations.\n\nThis is still inferior to pre-specification, but far better than unqualified stepwise regression.\n\n\n\n6.2 Prediction\n\n6.2.1 Full Models with Shrinkage\nIn contrast, when we are interested in model prediction our goals change. We are simply after raw predictive performance and it matters less about the individual variables that comprise the model or how they were chosen. There is actually a fairly nice solution to the problem of variable selection in the context of prediction modelling, and that is ‚Äúregularisation‚Äù or ‚Äúpenalised regression‚Äù methods. These methods acknowledge uncertainty more honestly than stepwise procedures.\nRegularisation is central to prediction because it controls overfitting by shrinking model coefficients, trading a small amount of bias for a large reduction in variance and thus improving performance on new data. Ridge regression shrinks all coefficients toward zero and is particularly effective when many predictors have small, correlated effects. Lasso applies stronger shrinkage that can set some coefficients exactly to zero, producing sparse models that are easier to deploy but can be unstable when predictors are highly correlated. Elastic net combines ridge and lasso penalties, often giving the best predictive performance in practice by encouraging sparsity while retaining groups of correlated predictors.\nKeep reading to the end of this section to see an example of how penalised regression with Elastic net is used.\n\n\n6.2.2 Internal vs External Validation\nBecause we are ultimately interested in predictive power, once we have established a prediction model we should evaluate its performance. There are multiple ways to do this (see here for a good primer), with the main techniques being:\n\ncross-validation,\nbootstrap resampling,\nor, ideally, external validation datasets.\n\nThe general idea in each case is that we ‚Äútrain‚Äù our model on a dataset and then ‚Äútest‚Äù its predictive power on new data. The reason for this is that it is not uncommon when we create a model from a single dataset that the model usually fits the peculiarities of that dataset much better than one it hasn‚Äôt seen before (i.e.¬†it overfits). Thus, predictive performance will often be less on a ‚Äútest‚Äù compared to a ‚Äútrain‚Äù dataset. In cross-validation and bootstrapping we are using sleight of hand to create ‚Äútrain‚Äù and ‚Äútest‚Äù data from the only dataset we have - these are referred to as internal validation methods. While external validation is better because we can train the model on all of our data and then test on a completely unseen dataset, it is harder to achieve in practice for obvious reasons.\nWhatever the method of model performance evaluation, the focus now changes from p-values and confidence intervals to predictive accuracy and generalisability.\n\n\n6.2.3 Model Averaging\nSometimes we may end up with more than one candidate prediction model that have similar predictive power but are based on different modelling strategies (e.g.¬†lasso vs random forest vs GAM). Model averaging is an approach that accounts for uncertainty about model choice by combining predictions or estimates from multiple plausible prediction models rather than relying on a single selected model. Instead of treating one model as ‚Äúthe truth,‚Äù it weights each model according to a measure of support - such as predictive performance, information criteria, or posterior probability - and averages across them. This can improve predictive accuracy and reduce overconfidence, particularly when several models perform similarly well. Model averaging is most natural when there is genuine uncertainty among a discrete set of competing models, and it contrasts with single-model selection approaches that ignore this uncertainty.\n\n\n6.2.4 Elastic Net Example\nAlright, let‚Äôs put some of what we‚Äôve learned into practice. Let‚Äôs simulate some data and run both a stepwise regression and an elastic net, comparing the performance of each. For the simulation I will create a dataset with 500 observations and 20 predictors. I‚Äôll make the regression coefficients for the first 5 predictors non-zero (representing real effects) and set the coefficients for the last 15 to zero (representing noise). I‚Äôll also specify that all predictors are moderately correlated with each other (r = 0.4). With this data-generating process we can then meaningfully compare how stepwise regression and elastic net behave when only a few predictors truly matter (and hopefully find that the latter approach performs better).\n\n\nCode\nset.seed(123)\n\nn &lt;- 500\np &lt;- 20\n\nSigma &lt;- matrix(0.4, p, p)\ndiag(Sigma) &lt;- 1\n\nlibrary(MASS)\nX &lt;- mvrnorm(n, mu = rep(0, p), Sigma = Sigma)\n\nbeta &lt;- c(2, 2, 2, -1.5, -1.5, rep(0, p - 5))\ny &lt;- X %*% beta + rnorm(n, sd = 2)\n\ndat &lt;- data.frame(y, X)\ncolnames(dat) &lt;- c(\"y\", paste0(\"X\", 1:p))\n\n\nNow let‚Äôs create train and test datasets from the simulated data. To do this we‚Äôll randomly choose 70% of the observations for the train dataset and allocate the remaining 30% to the test dataset.\n\n\nCode\nset.seed(456)\nidx &lt;- sample(seq_len(n), size = 0.7 * n)\n\ntrain &lt;- dat[idx, ]\ntest  &lt;- dat[-idx, ]\n\n\nWe will now perform a classic stepwise regression using step() to automate the process. step() fits multiple models moving back and forth between a model with no predictors (intercept only model) and one with all 20 predictors (full model). At each stage it computes the AIC for each candidate model, moves to the model with the lowest AIC and then stops when no further addition or deletion of variables improves the AIC.\nWe can see that the first five predictors are correctly selected, but the algorithm also selects four noise predictors, two of which are deemed statistically significant.\n\n\nCode\nfull_mod &lt;- lm(y ~ ., data = train)\nnull_mod &lt;- lm(y ~ 1, data = train)\n  \nstep_mod &lt;- step(\n  null_mod,\n  scope = list(lower = null_mod, upper = full_mod),\n  direction = \"both\",\n  trace = FALSE\n)\n\nsummary(step_mod)\n\n\n\nCall:\nlm(formula = y ~ X1 + X3 + X5 + X2 + X4 + X15 + X10 + X14 + X18, \n    data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6759 -1.2016  0.0696  1.2556  5.7055 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.07313    0.10297   0.710   0.4781    \nX1           2.08653    0.11979  17.418   &lt;2e-16 ***\nX3           2.12815    0.13199  16.123   &lt;2e-16 ***\nX5          -1.61482    0.12589 -12.827   &lt;2e-16 ***\nX2           2.02325    0.12951  15.622   &lt;2e-16 ***\nX4          -1.58574    0.12656 -12.530   &lt;2e-16 ***\nX15          0.30294    0.13056   2.320   0.0209 *  \nX10         -0.32018    0.12951  -2.472   0.0139 *  \nX14          0.22121    0.13102   1.688   0.0922 .  \nX18         -0.20767    0.13261  -1.566   0.1183    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.907 on 340 degrees of freedom\nMultiple R-squared:  0.7858,    Adjusted R-squared:  0.7802 \nF-statistic: 138.6 on 9 and 340 DF,  p-value: &lt; 2.2e-16\n\n\nLet‚Äôs check the performance by using the model to predict on the test data, then calculating a common regression performance measure - the root mean square error (RMSE). The lower the RMSE, the better.\n\n\nCode\ny_pred_step &lt;- predict(step_mod, newdata = test)\nrmse_step &lt;- sqrt(mean((test$y - y_pred_step)^2))\nrmse_step\n\n\n[1] 2.018798\n\n\nNow let‚Äôs compare this to an elastic net, using the glmnet package, which fits penalised regression models (ridge, lasso, elastic net). We can break this code block down into a series of steps as follows:\n\nglmnet fits many elastic net models, each with:\n\nthe same predictors,\nthe same alpha = 0.5 (mix of ridge and lasso),\ndifferent values of lambda (penalty strength).\n\nIt then performs 10-fold cross-validation:\n\nthe training data are split into 10 folds,\neach fold is held out in turn,\nprediction error is estimated for each lambda.\n\nAnd finally, chooses the optimal penalty:\n\nlambda.min ‚Üí lowest cross-validated error,\nlambda.1se ‚Üí largest Œª within 1 SE of the minimum (simpler model).\n\n\nThis step answers: ‚ÄúHow much regularisation gives the best out-of-sample performance?‚Äù\n\n\nCode\nlibrary(glmnet)\n\n# glmnet requires predictors as a numeric matrix, not a data frame.\nX_train &lt;- as.matrix(train[, -1]) # contains all predictors but remove the outcome variable (y)\ny_train &lt;- train$y                # response\nX_test  &lt;- as.matrix(test[, -1])\ny_test  &lt;- test$y\n\ncv_enet &lt;- cv.glmnet(\n  X_train, y_train,\n  alpha = 0.5,                    # elastic net\n  nfolds = 10\n)\n\n# Extract the coefficients from the single elastic net model corresponding to the optimal Œª.\ncoef(cv_enet, s = \"lambda.min\")\n\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n              lambda.min\n(Intercept)  0.066848837\nX1           2.012777264\nX2           1.921988809\nX3           2.043667440\nX4          -1.469708904\nX5          -1.503814250\nX6           0.010503422\nX7          -0.111528143\nX8          -0.065850506\nX9          -0.080697600\nX10         -0.232749640\nX11          0.010956657\nX12          .          \nX13          0.120277174\nX14          0.166942224\nX15          0.237722152\nX16          0.030739719\nX17          0.058957699\nX18         -0.115478884\nX19         -0.046143403\nX20          0.004132131\n\n\nNote that even when the true data-generating process is sparse, elastic net does not necessarily recover the true set of predictors, particularly when predictors are correlated. This is not a failure: the elastic net objective is to minimise prediction error, not to identify the true model. At the penalty that optimises cross-validated performance, elastic net may retain many small coefficients because correlated noise variables still carry predictive information. In other words, if allowing small non-zero coefficients on ‚Äúnoise‚Äù predictors improves prediction even slightly, elastic net will do that.\nNow let‚Äôs compare the prediction error from both modelling approaches.\n\n\nCode\ny_pred_enet &lt;- predict(cv_enet, X_test, s = \"lambda.min\")\nrmse_enet &lt;- sqrt(mean((y_test - y_pred_enet)^2))\nrmse_enet\n\n\n[1] 1.999773\n\n\nCode\nc(\n  Stepwise_RMSE   = rmse_step,\n  ElasticNet_RMSE = rmse_enet\n)\n\n\n  Stepwise_RMSE ElasticNet_RMSE \n       2.018798        1.999773 \n\n\nThe elastic net is slightly better, but ultimately they‚Äôre pretty similar. But that doesn‚Äôt mean that this exercise has been a waste of time. Even when stepwise regression and elastic net achieve similar RMSE on a given test set, elastic net is preferable because it achieves this performance through explicit regularisation rather than discrete variable selection.\nPenalised models are:\n\nmore stable to sampling variation,\nhandle correlated predictors more gracefully,\nand control overfitting directly.\n\nThe apparent equivalence in point performance masks important differences in reliability and robustness, which become evident under resampling or repeated evaluation. Penalised regression does not eliminate sampling variability, but it responds to it smoothly. Unlike stepwise regression, which makes brittle include‚Äìexclude decisions, elastic net adjusts coefficients continuously, allowing predictive performance to remain stable even when individual coefficients fluctuate.\nAll of this is to say that if we ran these models many times across different datasets, penalised regression will still give us a relatively stable model and predictive performance - stepwise methods won‚Äôt."
  },
  {
    "objectID": "posts/2026 topics/stepwise/index.html#a-pragmatic-takeaway",
    "href": "posts/2026 topics/stepwise/index.html#a-pragmatic-takeaway",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "7 A Pragmatic Takeaway",
    "text": "7 A Pragmatic Takeaway\nStepwise regression persists because it is easy, familiar, and superficially tidy. But its apparent simplicity masks deep statistical flaws that undermine inference, reproducibility, and scientific credibility.\nModern alternatives - pre-specification for explanatory modelling; and shrinkage, validation ¬± model-averaging for prediction modelling - are not only more principled, they are often easier to explain and defend.\nThe question is no longer whether stepwise regression is flawed. The question is why we continue to use it when better tools are readily available.\nIf you want to read more about this topic I‚Äôd suggest having a look at the following papers:\n\nGreenland S. Invited commentary: variable selection versus shrinkage in the control of multiple confounders. Am J Epidemiol. 2008 Mar 1;167(5):523-9; discussion 530-1. doi: 10.1093/aje/kwm355. Epub 2008 Jan 27. Erratum in: Am J Epidemiol. 2008 May 1;167(9):1142. PMID: 18227100.\nWalter S, Tiemeier H. Variable selection: current practice in epidemiological studies. Eur J Epidemiol. 2009;24(12):733-6. doi: 10.1007/s10654-009-9411-2. Epub 2009 Dec 5. PMID: 19967429; PMCID: PMC2791468.\nHeinze G, Wallisch C, Dunkler D. Variable selection - A review and recommendations for the practicing statistician. Biom J. 2018 May;60(3):431-449. doi: 10.1002/bimj.201700067. Epub 2018 Jan 2. PMID: 29292533; PMCID: PMC5969114.\nSauerbrei W, Perperoglou A, Schmid M, Abrahamowicz M, Becher H, Binder H, Dunkler D, Harrell FE Jr, Royston P, Heinze G; for TG2 of the STRATOS initiative. State of the art in selection of variables and functional forms in multivariable analysis-outstanding issues. Diagn Progn Res. 2020 Apr 2;4:3. doi: 10.1186/s41512-020-00074-3. PMID: 32266321; PMCID: PMC7114804.\n\nWell that‚Äôs pretty much it for this post folks. Hopefully you can start to incorporate some of these techniques into your next regression modelling endeavour. See you next month."
  },
  {
    "objectID": "posts/2026 topics/dags/index.html",
    "href": "posts/2026 topics/dags/index.html",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "",
    "text": "Today I thought we‚Äôd discuss a stats topic that I‚Äôm sure you‚Äôve heard about before, but perhaps not really understood, nor yet had a pressing need to use - bootstrap resampling. Now, I have to admit that I haven‚Äôt applied this technique much in my own day to day work either, but it‚Äôs an important statistical tool to have an understanding of, because there are times when it‚Äôs the only approach you can use. And the reason for that is the bootstrap can be considered a swiss army knife of parameter uncertainty estimation when the usual parametric distribution assumptions and resulting formulaic approximations that we base our standard error calculations on, either can‚Äôt be trusted or are simply unknown."
  },
  {
    "objectID": "posts/2026 topics/dags/index.html#basic-concepts",
    "href": "posts/2026 topics/dags/index.html#basic-concepts",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.1 Basic concepts",
    "text": "2.1 Basic concepts\nAnd this is where the fundamental idea of the sampling distribution comes into play. In a nutshell, the sampling distribution represents the theoretical distribution of a sample statistic that‚Äôs derived from repeatedly randomly sampling a population of interest. Now this is a thought exercise only - we don‚Äôt do it for obvious reasons in practice - but it allows us to make assumptions about the behaviour of the population parameter that we are trying to estimate. (Note - while I say ‚Äòtheoretical‚Äô above, these distributions can now be verified in silico but in the pre-computer era were confirmed empirically by a combination of mathematical derivation and physical simulation - i.e.¬†bootstrapping by hand!)\nIn this thought exercise we would have a population that we are interested in calculating a parameter for, and we‚Äôd draw a sample of observations from this population. We‚Äôd then calculate the corresponding sample statistic - let‚Äôs say it‚Äôs a mean value - and we‚Äôd plot that on a frequency histogram. We would then repeat that process many times, plotting each mean value along the way. The resulting plot would show the distribution of all the sample means - and this is what we call the sampling distribution of the sample statistic. It‚Äôs important to note that this is NOT the distribution of the data itself, but the distribution of a summary statistic derived from the data.\n\n\n\n\n\nNow, depending on the sample size, the shape of the underlying raw data distribution and the specific summary statistic we‚Äôre interested in, sampling distributions for many statistics often end up looking normal (or close to normal) in shape. And that allows us to leverage fairly simple normal distribution properties such as the mean and standard deviation (SD) to infer the population mean and it‚Äôs associated uncertainty. The mean should converge to the actual population parameter and the SD tells us about the uncertainty in the estimation of the parameter - and in fact is directly interpretable from the sampling distribution itself, as the standard error (SE). Once we have the SE it becomes trivial to calculate the 95% confidence interval (CI).\nThe point in telling you all of this is to highlight to you is that we can use just a single sample to form probabilistic statements about a population parameter, rather than needing to measure the entire population."
  },
  {
    "objectID": "posts/2026 topics/dags/index.html#when-it-works",
    "href": "posts/2026 topics/dags/index.html#when-it-works",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.2 When it works",
    "text": "2.2 When it works\nMost of the time the theory of parametric sampling distributions will work just fine for what you want to do. Probability density functions (PDF‚Äôs) that define the ‚Äòshape‚Äô of the sampling distribution have been derived for many sample statistics. These equations are sometimes referred to as ‚Äòclosed-form solutions‚Äô. Some PDF‚Äôs are fairly simple, other‚Äôs are almost intractably complex and some are simply unknown. But what is important about this, is that when you know the PDF for a sampling distribution, you can calculate exact 95% CI‚Äôs.\nWhen PDF‚Äôs become too difficult or are simply unknown, we can start to leverage approximations. And that probably explains why I haven‚Äôt needed to bootstrap much in my own work - the classic central limit theorem (CLT) does its job pretty well. The CLT basically states that you can have whatever shaped raw data distribution you want - flat, skewed, bi-modal - whatever, but when you then construct a sampling distribution from the resulting sample means, that distribution will be normal in shape (if you‚Äôve got a large enough sample).\nThe thing is, the logic of the central limit theorem translates fairly well to most other sample statistics of interest - for example, medians, proportions, correlations, regression coefficients, and more - through a concept called large-sample asymptotic normality. This approximation basically says that if our sample is large enough, the sampling distributions of these other parameters will likewise approach normality in terms of shape. What that means at the end of the day is that we can use fairly simple(-ish) equations for large sample approximations to estimate SE‚Äôs and 95% CI‚Äôs. The equations for the SE for the mean and proportion are shown below and I‚Äôm sure you‚Äôve seen these before.\n\\[ \\bar{x} \\pm 1.96^* \\frac{s}{\\sqrt{n}} \\hspace{2cm} \\widehat{p} \\pm 1.96^* \\sqrt{\\frac{\\widehat{p}(1 - \\widehat{p})}{n}} \\]"
  },
  {
    "objectID": "posts/2026 topics/dags/index.html#when-it-doesnt",
    "href": "posts/2026 topics/dags/index.html#when-it-doesnt",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.3 When it doesn‚Äôt",
    "text": "2.3 When it doesn‚Äôt\nClearly, there are situations that can occasionally arise when basic sampling theory might not be adequate for your analytic needs, otherwise we wouldn‚Äôt have this post. To my mind there are three main reasons why you might look further afield to a resampling method like the bootstrap to support your analyses.\n\nThe first is sample size. Asymptotic normal theory relies on a ‚Äòlarge‚Äô sample size to be accurate, and the bootstrap deals with smaller samples much better. However, it is itself not immune to small sample bias (when n becomes quite small - say &lt; 15). In such cases not much can save you unless you collect more data, so you might just need to rely on descriptive statistics only.\nThe second situation is where equations for the SE are either so complex as to be virtually intractable, or simply don‚Äôt exist (as described above). Here you can use the bootstrap to estimate the sampling distribution directly with relative ease.\nFinally, for sampling distributions that depart quite obviously from normality. Here the large-sample approximations just don‚Äôt work well, but you can use the bootstrap in these cases to actually capture that non-normal shape and apply it in valid inference."
  },
  {
    "objectID": "posts/2026 topics/dags/index.html#why-is-it-called-the-bootstrap",
    "href": "posts/2026 topics/dags/index.html#why-is-it-called-the-bootstrap",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.4 Why is it called the bootstrap?",
    "text": "2.4 Why is it called the bootstrap?\nAlright - let‚Äôs actually talk about The Bootstrap! There is some interesting history in how the bootstrap came to be called what it is, as it‚Äôs etymology isn‚Äôt from the statistical domain. The origins of the term are sometimes attributed to an 18th century work of fiction - The Surprising Adventures of Baron Munchausen - in which Baron Munchausen‚Äôs plan for getting himself (and his horse) out of a swamp was to pull himself out by his bootstraps. Curiously there appears to be no actual reference to his bootstraps in the story itself, where instead he uses his own hair (pigtails to be specific).\nIn any case, over time the term evolved to mean many things but with the overarching theme of ‚Äòperforming a near impossible task‚Äô, or ‚Äòdoing more with less‚Äô. It is not unheard of today in political discourse as a narrative for self-starting economic mobility - that is, ‚Äúif you just put in the hard work, you will eventually be successful‚Äù.\nIn statistics specifically, the bootstrap is used to mean that the population parameter we are interested in can be sufficiently defined by the sample of data that we have. In other words, ‚Äòthe sample ‚Äúpulls itself up by its bootstraps‚Äù‚Äô."
  },
  {
    "objectID": "posts/2026 topics/dags/index.html#an-important-statistical-idea",
    "href": "posts/2026 topics/dags/index.html#an-important-statistical-idea",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.5 An important statistical idea",
    "text": "2.5 An important statistical idea\nBefore I get to explaining what the bootstrap is in more detail, let me start by introducing a paper that was published in 2020 by two very well-known American statisticians - What are the most important statistical ideas of the past 50 years.\n\nCounterfactual causal inference\nBootstrapping and simulation-based inference\nOverparameterized models and regularization\nMultilevel models\nGeneric computation algorithms\nAdaptive decision analysis\nRobust inference\nExploratory data analysis\n\nEach of these ideas has existed in some form prior to the 1970s, both in the theoretical statistics literature and in the practice of various applied Ô¨Åelds. But the authors consider that each has developed enough in the past 50 years to have essentially become something new and in many instances this has been facilitated by the modern computing age, as some of these techniques just weren‚Äôt practical to apply before we had fast computers. We have some ideas that you might already be familiar with from traditional statistics - counterfactual inference, multilevel model, robust inference, exploratory data analysis; and perhaps others that might be less so because they fall more into the realm of data science and predictive analytics - overparameterised models, generic algorithms and decision analysis.\nGiven that bootstrapping is one of these important statistical ideas from the last 50 years, let‚Äôs now learn some more about it."
  },
  {
    "objectID": "posts/2026 topics/dags/index.html#what-is-it",
    "href": "posts/2026 topics/dags/index.html#what-is-it",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "3.1 What is it?",
    "text": "3.1 What is it?\nThe bootstrap method is a resampling technique that estimates the sampling distribution of a statistic by treating the original sample as a proxy for the population. Instead of drawing new samples from an unknown population, which is what we learned about earlier, we simulate this process by repeatedly drawing samples - with replacement - from our single observed sample. And this allows us to approximate the variability and properties of the statistic, based on the assumption that our single, observed sample is a good representation of the underlying population.\nIn other words, the bootstrap treats the original sample as a miniature, empirical population. Each bootstrap sample is the same size as the original and is created by sampling with replacement. This ‚Äúwith replacement‚Äù step is critical because it ensures each bootstrap sample is a unique combination of values from the original data, simulating the variability you‚Äôd expect from a new sample.\nSo we use the same steps here that I outlined earlier in constructing the true sampling distribution - that is, for each bootstrap sample (and we typically specify thousands of them), we calculate our summary statistic and plot these as a frequency histogram. The collection of all these bootstrap sample statistics forms the bootstrap sampling distribution, which then serves as an estimate of the true sampling distribution.\nWe can then calculate other important measures such as the SE and CI‚Äôs by reading the 2.5 and 97.5 percentile values directly off the plot. We don‚Äôt actually need to invoke any mathematical formulae as we previously did - and that‚Äôs because we have an actual distribution now rather than just a theoretical one."
  },
  {
    "objectID": "posts/2026 topics/dags/index.html#bootstrap-sampling-distribution",
    "href": "posts/2026 topics/dags/index.html#bootstrap-sampling-distribution",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "3.2 Bootstrap sampling distribution",
    "text": "3.2 Bootstrap sampling distribution\nRemember what our theoretical sampling distribution looked like? (scroll up if you don‚Äôt). Now when we look at our bootstrapped sampling distribution, there isn‚Äôt really much that‚Äôs changed. The main differences are that we‚Äôve substituted our only sample for our population and we‚Äôre now ‚Äòresampling‚Äô from that, rather than sampling from our population. Everything else basically stays the same. Note how we can easily extract the confidence limits ‚Äòempirically‚Äô, directly from the plot, by just ordering all the values from lowest to highest and taking the values at the 2.5 and 97.5 percentiles. These correspond to the lower and upper confidence limits, giving us 95% coverage for the true population parameter. Remember, the beauty of this method is that it doesn‚Äôt assume a specific distribution for the data, and that is extremely useful when the classical assumptions aren‚Äôt met."
  },
  {
    "objectID": "posts/2026 topics/dags/index.html#sampling-distributions-reimagined",
    "href": "posts/2026 topics/dags/index.html#sampling-distributions-reimagined",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "3.3 Sampling distributions reimagined",
    "text": "3.3 Sampling distributions reimagined\nThese aren‚Äôt my images but I thought I‚Äôd show them to you because it‚Äôs a tangible visual take on the same two concepts, using the global population, and I think if you‚Äôve been having some trouble following along, this should make things a lot clearer. The top picture shows the true (or theoretical) sampling distribution for a mean. We start off with all 7.6 billion people in the world and then take multiple samples from the population, calculating the mean in each sample and then plotting the distribution of those sample means.\nYou can also easily appreciate how the bootstrap sampling distribution differs, below that. We might still start off with our population, but it remains unrealised, and all we actually have is the one sample that we draw from it. Our bootstrap samples are then resamples of that one sample, with replacement. Note how in each bootstrap sample, one individual has been sampled twice - the grey person in the first, the purple in the second and the green in the third. But that‚Äôs fine and to be expected. We then calculate the mean in each resample and plot the distribution of those means.\n\nTheoretical\n\n\n\n\n\n\n\nBootstrap"
  },
  {
    "objectID": "posts/2026 topics/dags/index.html#raw-data-distribution",
    "href": "posts/2026 topics/dags/index.html#raw-data-distribution",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "5.1 Raw Data Distribution",
    "text": "5.1 Raw Data Distribution\nTo do this in R we‚Äôre going to use an inbuilt dataset, and in fact it doesn‚Äôt even matter what that is, so I‚Äôm not going to describe it here (details are in the code at the end of this post that will allow you to fully reproduce all analyses).\n\n\n\n\n\nThis is the raw data distribution for the variable we‚Äôll be bootstrapping the mean for, and it consists of 47 observations. You could argue that there‚Äôs a normal shape to it, but in all honesty, there probably aren‚Äôt enough data points to say that with certainty.\nWe‚Äôll now take this variable and we‚Äôll bootstrap it 1000 times - in other words we‚Äôll take 1000 resamples each of size 47, replacing each value in the event that it‚Äôs drawn. Then we‚Äôll calculate the mean value in each of those 1000 resamples."
  },
  {
    "objectID": "posts/2026 topics/dags/index.html#sampling-distribution",
    "href": "posts/2026 topics/dags/index.html#sampling-distribution",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "5.2 Sampling Distribution",
    "text": "5.2 Sampling Distribution\nWhen we construct a frequency histogram of those 1000 mean values, we see the following:\n\n\n\n\n\nThere are a couple of salient things to note:\n\nThe first is that the bootstrap sampling distribution is quite normal in shape, even though the raw data distribution might not have been.\nThe second point is that the mean of the original sample and the mean of all the bootstrapped means is virtually identical.\n\nThis is a good thing as it means that the sample mean is an unbiased estimator of the population mean. Another way of saying this is that ‚Äúif I were to repeat this study many times, the sample mean would, on average, hit the true population mean.‚Äù\n\nThe last thing to say about this plot is that if we wanted to obtain the bootstrapped confidence limits we could simply read off the values corresponding to the 2.5 and 97.5 percentiles from the plot. Of course, in practice you‚Äôd get your stats software to do this for you, but the point is that it‚Äôs quite easy to do and doesn‚Äôt involve any formulae."
  },
  {
    "objectID": "posts/2026 topics/dags/index.html#comparison-of-95-cis",
    "href": "posts/2026 topics/dags/index.html#comparison-of-95-cis",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "5.3 Comparison of 95% CI‚Äôs",
    "text": "5.3 Comparison of 95% CI‚Äôs\n\n\n\n\n\nAnd these are the 95% CI‚Äôs from the various methods. The first one is the ‚Äòtheoretical‚Äô - assuming normality and the others are derived from the boot.ci function after doing the bootstrapping procedure. Really, there‚Äôs not a lot to say about this - you can see that the coverage of all the CI‚Äôs is fairly similar, and that‚Äôs a good thing as it means you can can have increased confidence in the robustness of your results. (Note - the ‚Äòtheoretical‚Äô CI in this case for the mean is based off a t test which enables ‚Äòexact‚Äô CI‚Äôs to be calculated as the PDF of the sampling distribution is known. So while I say ‚Äòassuming normality‚Äô in this case it‚Äôs really an exact CI)."
  },
  {
    "objectID": "posts/2026 topics/dags/index.html#raw-data-distribution-1",
    "href": "posts/2026 topics/dags/index.html#raw-data-distribution-1",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "6.1 Raw Data Distribution",
    "text": "6.1 Raw Data Distribution\n\n\n\n\n\nOk - let‚Äôs now consider a different sample statistic that we might be interested in - the correlation coefficient. This is a scatterplot with overlaid density plot of the previous variable (on the x-axis) and a second variable (on the y-axis) from the same dataset. The marginal distribution of the second variable is far from normal as I‚Äôm sure you can appreciate, by looking at the density curve on the right side. When we look at the bivariate relationship in terms of the scatterplot itself, it‚Äôs not hard to imagine a negative relationship between the two variables."
  },
  {
    "objectID": "posts/2026 topics/dags/index.html#sampling-distribution-1",
    "href": "posts/2026 topics/dags/index.html#sampling-distribution-1",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "6.2 Sampling Distribution",
    "text": "6.2 Sampling Distribution\nIn contrast to the sample mean, the sampling distribution of the correlation coefficient does NOT have the same, simple, normal shape, straight out of the box. This statistic definitely relies on asymptotic normality based on having a large sample - larger than you would require for the sample mean.\n\n\n\n\n\nThis time when we construct a frequency histogram of those 1000 mean values, we get quite a positively skewed bootstrap sampling distribution, and there is no way we could argue this is normal in shape. The other observation that we can easily make is that the original sample correlation coefficient is different (more negative) to the mean of all the bootstrapped correlation coefficients.\nWhat this reflects is that the sample correlation coefficient is a biased estimator of the population correlation coefficient. And another way of saying this is that ‚Äúif I were to repeat this study many times, the sample correlation would, on average, consistently be closer to zero than the true population correlation.‚Äù Now, this bias is worse as the correlation approaches either plus or minus one, and with small sample sizes. The bias reduces as the sample size increases according to our large sample theory for asymptotic normality.\nAs with the sample mean, if we want to obtain empirical 95% CI‚Äôs we can just read off the corresponding values at the 2.5 and 97.5 percentiles."
  },
  {
    "objectID": "posts/2026 topics/dags/index.html#comparison-of-95-cis-1",
    "href": "posts/2026 topics/dags/index.html#comparison-of-95-cis-1",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "6.3 Comparison of 95% CI‚Äôs",
    "text": "6.3 Comparison of 95% CI‚Äôs\n\n\n\n\n\nConfidence interval coverage with the correlation coefficient is also a little different to what we previously saw with the sample mean. The theoretical and two of the bootstrap intervals - the percentile and the bias-corrected percentile are quite similar, whereas the remaining two bootstrap intervals - the normal and basic are quite different. OK, so what do we believe here? Well, the normal and basic intervals should really only be trusted when we‚Äôve got a large sample size and a well-behaved statistic - and you could make a good argument that we don‚Äôt really have either of those two conditions being met here. Therefore it‚Äôs either the percentile method or its bias-corrected variant and as I mentioned before the latter is probably the best method, in general, to choose. When we compare the bias-corrected interval to the theoretical interval, we can see that in fact they‚Äôre not that different, but the bias-corrected is a little more conservative (i.e.¬†the CI is wider) - which is always a good thing, I think, in quantifying uncertainty.\nSo at the end of the day, for these data, it‚Äôs good to be able to report both types of CI‚Äôs - theoretical and empirical from the bootstrap. And that‚Äôs because we know from the outset that we‚Äôre dealing with a sample statistic that might not conform to the theoretical assumptions for CI estimation as well as a ‚Äòbetter behaved‚Äô statistic like the sample mean."
  },
  {
    "objectID": "posts/037_30Jan_2026/index.html",
    "href": "posts/037_30Jan_2026/index.html",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "",
    "text": "Welcome back to Stats Tips for 2026. I hope you all had a restful break during the holiday period. I spent a couple of weeks on the South Island of New Zealand and was rather in awe of how beautiful that part of the world is - Queenstown, Milford Sound, Lake Tekapo and a 3 day hike on the Humpridge Track, which I can really recommend if you‚Äôre into hiking. The landscapes are otherworldly - we plan to go back, it was that good‚Ä¶\nOk, enough about my holidays and on to more serious topics. Today I thought I would ease you back into the world of statistical musings with a less theoretical and more practical post. One that illustrates the application of what I consider to be an indispensable dplyr function in my day-to-day work, and one that I hope you can make use of too - meet across().\nI believe one aspiration we all share in our endeavour to become better R programmers is to write more efficient code that avoids repetition. You may recall that I have dedicated a whole other post to this topic and so our discussion here will further expound upon that theme.\nHow many of you have written a block of code that you have just reused, for example:\nThe same data transformation copied and pasted five times.\nThe same rounding applied column by column.\nThe same ifelse() rewritten with only the variable name changed.\nI know I have.\nIn its most fundamental use-case, across() makes it easy to apply the same transformation to multiple columns in a dataframe in one go, rather than applying the same code block multiple times. It may not be flashy, but once you get the hang of how it works, your code will become shorter, clearer, and far easier to maintain. It‚Äôs important to note that across() doesn‚Äôt work by itself, but rather is a column-selection helper that is evaluated within other dplyr functions, most commonly mutate(), but also summarise() and filter(). In this mental model, mutate() decides what happens and across() decides where is happens.\nLet‚Äôs make these ideas clearer with several examples‚Ä¶"
  },
  {
    "objectID": "posts/037_30Jan_2026/index.html#introduction",
    "href": "posts/037_30Jan_2026/index.html#introduction",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "",
    "text": "Welcome back to Stats Tips for 2026. I hope you all had a restful break during the holiday period. I spent a couple of weeks on the South Island of New Zealand and was rather in awe of how beautiful that part of the world is - Queenstown, Milford Sound, Lake Tekapo and a 3 day hike on the Humpridge Track, which I can really recommend if you‚Äôre into hiking. The landscapes are otherworldly - we plan to go back, it was that good‚Ä¶\nOk, enough about my holidays and on to more serious topics. Today I thought I would ease you back into the world of statistical musings with a less theoretical and more practical post. One that illustrates the application of what I consider to be an indispensable dplyr function in my day-to-day work, and one that I hope you can make use of too - meet across().\nI believe one aspiration we all share in our endeavour to become better R programmers is to write more efficient code that avoids repetition. You may recall that I have dedicated a whole other post to this topic and so our discussion here will further expound upon that theme.\nHow many of you have written a block of code that you have just reused, for example:\nThe same data transformation copied and pasted five times.\nThe same rounding applied column by column.\nThe same ifelse() rewritten with only the variable name changed.\nI know I have.\nIn its most fundamental use-case, across() makes it easy to apply the same transformation to multiple columns in a dataframe in one go, rather than applying the same code block multiple times. It may not be flashy, but once you get the hang of how it works, your code will become shorter, clearer, and far easier to maintain. It‚Äôs important to note that across() doesn‚Äôt work by itself, but rather is a column-selection helper that is evaluated within other dplyr functions, most commonly mutate(), but also summarise() and filter(). In this mental model, mutate() decides what happens and across() decides where is happens.\nLet‚Äôs make these ideas clearer with several examples‚Ä¶"
  },
  {
    "objectID": "posts/037_30Jan_2026/index.html#the-problem-repeating-the-same-operation",
    "href": "posts/037_30Jan_2026/index.html#the-problem-repeating-the-same-operation",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "2 The Problem: Repeating the Same Operation",
    "text": "2 The Problem: Repeating the Same Operation\nSuppose you‚Äôre analysing a simple dataset.\n\n\nCode\ndf &lt;- tibble(id = as.character(1:3),\n             age = c(34, 51, 63),\n             weight = c(72.46, 81.27, 76.85),\n             bmi = c(23.66, 27.93, 26.35),\n             cholesterol = c(5.42, 6.01, 5.87))\n\ndf\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.46\n23.66\n5.42\n\n\n2\n51\n81.27\n27.93\n6.01\n\n\n3\n63\n76.85\n26.35\n5.87\n\n\n\n\n\n\n\nYou decide that, for reporting, several variables should be rounded to 1 decimal place."
  },
  {
    "objectID": "posts/037_30Jan_2026/index.html#the-na√Øve-way",
    "href": "posts/037_30Jan_2026/index.html#the-na√Øve-way",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "3 The Na√Øve Way",
    "text": "3 The Na√Øve Way\nA common first attempt looks like this:\n\n\nCode\ndf |&gt; \n  mutate(weight = round(weight, 1),\n         bmi = round(bmi, 1),\n         cholesterol = round(cholesterol, 1))\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n\n\n2\n51\n81.3\n27.9\n6.0\n\n\n3\n63\n76.8\n26.4\n5.9\n\n\n\n\n\n\n\nYou write out the same line of code for each variable. It does work. It‚Äôs also:\n\nRepetitive\n\nError-prone (easy to forget a variable)\n\nPainful to update when the variable list changes\n\nIf you later add another variable (say waist), you must remember to update this block manually ‚Äî and everywhere else you did something similar."
  },
  {
    "objectID": "posts/037_30Jan_2026/index.html#the-core-idea-behind-across",
    "href": "posts/037_30Jan_2026/index.html#the-core-idea-behind-across",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "4 The Core Idea Behind across()",
    "text": "4 The Core Idea Behind across()\nThe key insight is simple:\n\nWhen you apply the same transformation to multiple columns, you should write the transformation once.\n\nThat‚Äôs exactly what across() does."
  },
  {
    "objectID": "posts/037_30Jan_2026/index.html#the-better-way-mutateacross",
    "href": "posts/037_30Jan_2026/index.html#the-better-way-mutateacross",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "5 The Better Way: mutate(across())",
    "text": "5 The Better Way: mutate(across())\nHere‚Äôs the same transformation rewritten using across().\n\n\nCode\ndf |&gt; \n  mutate(across(c(weight, bmi, cholesterol), round, 1))\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n\n\n2\n51\n81.3\n27.9\n6.0\n\n\n3\n63\n76.8\n26.4\n5.9\n\n\n\n\n\n\n\nRead this out loud:\n\n‚ÄúMutate across weight, bmi, and cholesterol by rounding to 1 decimal place.‚Äù\n\nThat phrasing is much closer to how you think about the task."
  },
  {
    "objectID": "posts/037_30Jan_2026/index.html#why-this-is-better-beyond-being-shorter",
    "href": "posts/037_30Jan_2026/index.html#why-this-is-better-beyond-being-shorter",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "6 Why This Is Better (Beyond Being Shorter)",
    "text": "6 Why This Is Better (Beyond Being Shorter)\n\n6.1 It Scales Naturally\nIf you add another variable:\n\n\nCode\ndf |&gt; \nmutate(across(c(weight, bmi, cholesterol, waist), round, 1))\n\n\nNo duplication. No copy-paste.\n\n\n\n6.2 You Can Select Variables Programmatically\nInstead of naming variables explicitly, you can select them based on some other programmatic characteristic. Here, let‚Äôs select all numeric variables:\n\n\nCode\ndf |&gt; \n  mutate(across(where(is.numeric), round, 1))\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n\n\n2\n51\n81.3\n27.9\n6.0\n\n\n3\n63\n76.8\n26.4\n5.9\n\n\n\n\n\n\n\nAlternatively, you might want to select based on naming conventions. In this example, we would select all variables in the dataframe that begin with the text ‚Äúlab_‚Äù:\n\n\nCode\ndf |&gt; \n  mutate(across(starts_with(\"lab_\"), log))\n\n\nThis is particularly powerful in real research datasets, where variable names often follow patterns.\n\n\n\n6.3 It Reduces Cognitive Load\nIf you reviewed your code 6 months down the track and compared these two blocks:\nweight = round(weight, 1)\nbmi = round(bmi, 1)\ncholesterol = round(cholesterol, 1)\nvs:\nacross(c(weight, bmi, cholesterol), round, 1)\nThe second tells you what is happening immediately."
  },
  {
    "objectID": "posts/037_30Jan_2026/index.html#using-anonymous-functions-for-more-complex-logic",
    "href": "posts/037_30Jan_2026/index.html#using-anonymous-functions-for-more-complex-logic",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "7 Using Anonymous Functions for More Complex Logic",
    "text": "7 Using Anonymous Functions for More Complex Logic\nYou‚Äôre not limited to simple functions like round() - you can write your own. In this case, across() recognises everything after the ~ as a user-defined or ‚Äúanonymous‚Äù function. For example,\nSuppose you want to:\n\nadd 1 to avoid zeros\n\nlog-transform the result\n\napply this consistently to multiple variables that begin with ‚Äúlab_‚Äù.\n\n\n\nCode\ndf |&gt; \n  mutate(across(starts_with(\"lab_\"), ~ log(.x + 1)))\n\n\nHere, .x represents the current column being transformed.\nAs you can see - we can do all of this in one line of code."
  },
  {
    "objectID": "posts/037_30Jan_2026/index.html#creating-new-variables-instead-of-overwriting",
    "href": "posts/037_30Jan_2026/index.html#creating-new-variables-instead-of-overwriting",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "8 Creating New Variables Instead of Overwriting",
    "text": "8 Creating New Variables Instead of Overwriting\nIn research workflows, it‚Äôs often good practice for reproducibility to keep raw variables intact and create new variables instead.\n\n\nCode\ndf |&gt; \n  mutate(across(c(weight, bmi), scale, .names = \"{.col}_z\"))\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\nweight_z\nbmi_z\n\n\n\n\n1\n34\n72.46\n23.66\n5.42\n-0.998862996\n-1.0746155\n\n\n2\n51\n81.27\n27.93\n6.01\n1.001133139\n0.9032328\n\n\n3\n63\n76.85\n26.35\n5.87\n-0.002270143\n0.1713826\n\n\n\n\n\n\n\nThis produces new variables (weight_z, bmi_z) which are the Z-score transformations of weight and bmi using R‚Äôs built-in scale function. Note, it‚Äôs a simple case of creating new variable names by prefixing or suffixing characters to the original column name specified by {.col}."
  },
  {
    "objectID": "posts/037_30Jan_2026/index.html#mutateacross-vs-summariseacross",
    "href": "posts/037_30Jan_2026/index.html#mutateacross-vs-summariseacross",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "9 mutate(across()) vs summarise(across())",
    "text": "9 mutate(across()) vs summarise(across())\nAs I mentioned at the outset, across() is most commonly used in conjunction with mutate(), but let‚Äôs look at an example where we may want to use it with summarise(). Let‚Äôs say we are interested in calculating the mean of each numeric column in the dataframe. We can do that as follows:\n\n\nCode\ndf |&gt; \n  summarise(across(where(is.numeric), mean, na.rm = TRUE))\n\n\n\n\n\n\nage\nweight\nbmi\ncholesterol\n\n\n\n\n49.33333\n76.86\n25.98\n5.766667\n\n\n\n\n\n\n\nNote a common point of confusion:\n\nmutate(across()) returns the same number of rows as in the original dataframe\n\nsummarise(across()) reduces rows (to a single row if no grouping structure is specified)\n\nacross() is therefore applied to both functions in the same way, but with different intent."
  },
  {
    "objectID": "posts/037_30Jan_2026/index.html#multiple-functions-per-variable",
    "href": "posts/037_30Jan_2026/index.html#multiple-functions-per-variable",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "10 Multiple Functions per Variable",
    "text": "10 Multiple Functions per Variable\nWhat if were interested in not only calculating the mean of each numeric column, but also the standard deviation and the number of observations in each column. Well, it‚Äôs relatively easy to extend the above example by now applying several functions at once.\n\n\nCode\ndf |&gt; \n  summarise(across(where(is.numeric), list(mean = mean, \n                                           sd = sd,\n                                           n = ~ sum(!is.na(.))),\n                   .names = \"{.col}_{.fn}\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage_mean\nage_sd\nage_n\nweight_mean\nweight_sd\nweight_n\nbmi_mean\nbmi_sd\nbmi_n\ncholesterol_mean\ncholesterol_sd\ncholesterol_n\n\n\n\n\n49.33333\n14.57166\n3\n76.86\n4.405009\n3\n25.98\n2.158912\n3\n5.766667\n0.3082748\n3\n\n\n\n\n\n\n\nWe can now specify {.fn} as a naming qualifier and append this to the original column name. This pattern is a stepping stone toward automated summary tables and reporting pipelines."
  },
  {
    "objectID": "posts/037_30Jan_2026/index.html#advanced-example-using-mutateacross-with-map",
    "href": "posts/037_30Jan_2026/index.html#advanced-example-using-mutateacross-with-map",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "11 Advanced Example: Using mutate(across()) with map()",
    "text": "11 Advanced Example: Using mutate(across()) with map()\nAre you ready for something more advanced (but also extremely powerful)? So far we have been dealing with a single dataframe, but we can also leverage the power of across() in simultaneous column manipulation over multiple dataframes using map().\n\n11.1 The Problem\nSuppose you have several datasets with the same structure assembled within a list (a list is a convenient R object within which many other R objects can be stored - including dataframes). We can access a particular object within a list with the $ operator, much like we access the columns of a dataframe.\n\n\nCode\ndatasets &lt;- list(raw   = df, \n                 clean = df,\n                 sens  = df)\n\ndatasets$raw\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.46\n23.66\n5.42\n\n\n2\n51\n81.27\n27.93\n6.01\n\n\n3\n63\n76.85\n26.35\n5.87\n\n\n\n\n\n\nCode\ndatasets$clean\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.46\n23.66\n5.42\n\n\n2\n51\n81.27\n27.93\n6.01\n\n\n3\n63\n76.85\n26.35\n5.87\n\n\n\n\n\n\nCode\ndatasets$sens\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.46\n23.66\n5.42\n\n\n2\n51\n81.27\n27.93\n6.01\n\n\n3\n63\n76.85\n26.35\n5.87\n\n\n\n\n\n\n\nNow, suppose you want to apply the same transformation to all of them.\n\n\n\n11.2 The Na√Øve Way\ndf_raw   &lt;- df_raw   |&gt;  mutate(...)\ndf_clean &lt;- df_clean |&gt;  mutate(...)\ndf_sens  &lt;- df_sens  |&gt;  mutate(...)\nIn this approach we go through and re-apply the same code to each dataframe but this can become difficult to maintain and easy to get wrong.\n\n\n\n11.3 The Better Way: map() + mutate(across())\n\n\nCode\ndatasets &lt;- datasets |&gt; \n  map(~ .x |&gt; \n    mutate(across(where(is.numeric), round, 1)))\n\ndatasets$raw\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n\n\n2\n51\n81.3\n27.9\n6.0\n\n\n3\n63\n76.8\n26.4\n5.9\n\n\n\n\n\n\nCode\ndatasets$clean\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n\n\n2\n51\n81.3\n27.9\n6.0\n\n\n3\n63\n76.8\n26.4\n5.9\n\n\n\n\n\n\nCode\ndatasets$sens\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n\n\n2\n51\n81.3\n27.9\n6.0\n\n\n3\n63\n76.8\n26.4\n5.9\n\n\n\n\n\n\n\nIndeed, the more efficient way is to use across() within mutate() within map().\nWe can read this as:\n\n‚ÄúFor each dataset, mutate across numeric variables by rounding to 1 decimal place.‚Äù\n\nThis approach using map() ensures:\n\nidentical logic across datasets\n\nchanges happen in one place\n\nconsistency is guaranteed\n\n\n\n\n11.4 Another Example: Standardising Variables Across Datasets\nNow, let‚Äôs take this further by extending the earlier example of creating new variables, not just within a single dataframe, but across multiple dataframes.\n\n\nCode\ndatasets &lt;- datasets |&gt; \n  map(~ .x |&gt; \n    mutate(across(c(age, bmi, weight), scale, .names = \"{.col}_z\")))\n\ndatasets$raw\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\nage_z\nbmi_z\nweight_z\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n-1.0522707\n-1.0806343\n-0.99233882\n\n\n2\n51\n81.3\n27.9\n6.0\n0.1143773\n0.8926979\n1.00748903\n\n\n3\n63\n76.8\n26.4\n5.9\n0.9378935\n0.1879364\n-0.01515021\n\n\n\n\n\n\nCode\ndatasets$clean\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\nage_z\nbmi_z\nweight_z\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n-1.0522707\n-1.0806343\n-0.99233882\n\n\n2\n51\n81.3\n27.9\n6.0\n0.1143773\n0.8926979\n1.00748903\n\n\n3\n63\n76.8\n26.4\n5.9\n0.9378935\n0.1879364\n-0.01515021\n\n\n\n\n\n\nCode\ndatasets$sens\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nage\nweight\nbmi\ncholesterol\nage_z\nbmi_z\nweight_z\n\n\n\n\n1\n34\n72.5\n23.7\n5.4\n-1.0522707\n-1.0806343\n-0.99233882\n\n\n2\n51\n81.3\n27.9\n6.0\n0.1143773\n0.8926979\n1.00748903\n\n\n3\n63\n76.8\n26.4\n5.9\n0.9378935\n0.1879364\n-0.01515021\n\n\n\n\n\n\nYou can appreciate how much of a Swiss-army knife of data manipulation, across() can be become when used in conjunction with other R functions."
  },
  {
    "objectID": "posts/037_30Jan_2026/index.html#final-thoughts-and-a-mental-model-to-take-away",
    "href": "posts/037_30Jan_2026/index.html#final-thoughts-and-a-mental-model-to-take-away",
    "title": "More DRY (Don‚Äôt Repeat Yourself) ‚Äî Meet mutate(across())",
    "section": "12 Final Thoughts and a Mental Model to Take Away",
    "text": "12 Final Thoughts and a Mental Model to Take Away\nWhenever you catch yourself thinking:\n\n‚ÄúI‚Äôm doing the same thing to several variables‚Ä¶‚Äù\n\nYou should immediately ask:\n\n‚ÄúCan this be an across()?‚Äù\n\nThat question alone will dramatically improve the quality of your R code.\nI hope you‚Äôve found this programming tip helpful and I will see you again for more Stats Tips, next month."
  },
  {
    "objectID": "posts/039_27Mar_2025/index.html",
    "href": "posts/039_27Mar_2025/index.html",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "",
    "text": "As a clinician/researcher you might not be aware that data-driven variable selection methods are widely regarded within the statistical community as the the equivalent of Facebook‚Äôs ‚ÄòCrazy Uncle‚Äô - an individual who is misinformed, opinionated and someone we just can‚Äôt easily shed from our lives.\nSo what‚Äôs the parallel I‚Äôm trying to draw, you might ask?\nWell, despite decades of criticism in the statistical literature, stepwise regression (as a form of data-driven variable selection) continues to appear in academic papers, regulatory submissions, and industry analyses, when it is universally recognised as plain wrong. Despite that, stepwise regression remains one of the most commonly taught and used model-building techniques in applied statistics. Indeed, I was taught these methods during my Masters of Biostatistics 10 years ago. The appeal is obvious: when faced with many candidate predictors and limited sample size, stepwise procedures promise an automated, objective way to ‚Äúlet the data decide‚Äù which variables matter.\n\n\n\n\n\n\nNote\n\n\n\nWhen I refer to ‚Äúdata-driven‚Äù or ‚Äústepwise‚Äù I am meaning any decision process leading to the addition or removal of candidate variables from a regression model that is based on data-derived statistics such as significance, R-squared, Akaike Information Criteria (AIC), etc‚Ä¶\n\n\nSo then, what‚Äôs the problem?\nWell, there are many. The most notable I will summarise from page 68 of Frank Harrell‚Äôs modern statistical classic - Regression Modelling Strategies:\n\nThe R-squared or even adjusted R-squared values of the end model are biased high.\nThe F and Chi-square test statistics of the final model do not have the claimed distribution.\nThe standard errors of coefficient estimates are biased low and confidence intervals for effects and predictions are falsely narrow.\nThe p values are too small (there are severe multiple comparison problems in addition to problems 2. and 3.) and do not have the proper meaning, and it is difficult to correct for this.\nThe regression coefficients are biased high in absolute value and need shrinkage but this is rarely done.\nVariable selection is made arbitrary by collinearity.\nIt allows us to not think about the problem.\n\nThe net effect of the above is that we may end up making claims, assertions or clinical decisions based on incorrect evidence.\nIn this post I will attempt to make the case that stepwise regression is not just suboptimal, but seriously flawed when used for inference, explanation, or decision-making. The problems are not subtle or academic; they directly undermine the validity, reproducibility, and interpretability of results. I will outline why stepwise regression fails, why these failures are amplified in applied research (particularly clinical and epidemiological settings), and what modern, defensible alternatives look like in practice.\nThe goal is not to shame analysts who use stepwise regression ‚Äî I too am guilty of the multitude of sins I am about to discuss ‚Äî but to provide a clear roadmap towards best practice."
  },
  {
    "objectID": "posts/039_27Mar_2025/index.html#introduction",
    "href": "posts/039_27Mar_2025/index.html#introduction",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "",
    "text": "As a clinician/researcher you might not be aware that data-driven variable selection methods are widely regarded within the statistical community as the the equivalent of Facebook‚Äôs ‚ÄòCrazy Uncle‚Äô - an individual who is misinformed, opinionated and someone we just can‚Äôt easily shed from our lives.\nSo what‚Äôs the parallel I‚Äôm trying to draw, you might ask?\nWell, despite decades of criticism in the statistical literature, stepwise regression (as a form of data-driven variable selection) continues to appear in academic papers, regulatory submissions, and industry analyses, when it is universally recognised as plain wrong. Despite that, stepwise regression remains one of the most commonly taught and used model-building techniques in applied statistics. Indeed, I was taught these methods during my Masters of Biostatistics 10 years ago. The appeal is obvious: when faced with many candidate predictors and limited sample size, stepwise procedures promise an automated, objective way to ‚Äúlet the data decide‚Äù which variables matter.\n\n\n\n\n\n\nNote\n\n\n\nWhen I refer to ‚Äúdata-driven‚Äù or ‚Äústepwise‚Äù I am meaning any decision process leading to the addition or removal of candidate variables from a regression model that is based on data-derived statistics such as significance, R-squared, Akaike Information Criteria (AIC), etc‚Ä¶\n\n\nSo then, what‚Äôs the problem?\nWell, there are many. The most notable I will summarise from page 68 of Frank Harrell‚Äôs modern statistical classic - Regression Modelling Strategies:\n\nThe R-squared or even adjusted R-squared values of the end model are biased high.\nThe F and Chi-square test statistics of the final model do not have the claimed distribution.\nThe standard errors of coefficient estimates are biased low and confidence intervals for effects and predictions are falsely narrow.\nThe p values are too small (there are severe multiple comparison problems in addition to problems 2. and 3.) and do not have the proper meaning, and it is difficult to correct for this.\nThe regression coefficients are biased high in absolute value and need shrinkage but this is rarely done.\nVariable selection is made arbitrary by collinearity.\nIt allows us to not think about the problem.\n\nThe net effect of the above is that we may end up making claims, assertions or clinical decisions based on incorrect evidence.\nIn this post I will attempt to make the case that stepwise regression is not just suboptimal, but seriously flawed when used for inference, explanation, or decision-making. The problems are not subtle or academic; they directly undermine the validity, reproducibility, and interpretability of results. I will outline why stepwise regression fails, why these failures are amplified in applied research (particularly clinical and epidemiological settings), and what modern, defensible alternatives look like in practice.\nThe goal is not to shame analysts who use stepwise regression ‚Äî I too am guilty of the multitude of sins I am about to discuss ‚Äî but to provide a clear roadmap towards best practice."
  },
  {
    "objectID": "posts/039_27Mar_2025/index.html#why-stepwise-regression-persists",
    "href": "posts/039_27Mar_2025/index.html#why-stepwise-regression-persists",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "2 Why Stepwise Regression Persists",
    "text": "2 Why Stepwise Regression Persists\nLet‚Äôs get the elephant in the room out of the way. If there are so many issues with stepwise regression why is it still so commonly used in the analysis of clinical data? Well, I don‚Äôt think there is a single answer to this question, but rather several potential factors that probably interplay in maintaining its veneer of contemporary methodological relevance:\n\nHistorical inertia: it has been taught for decades and appears in older textbooks.\nFear of change: researchers may worry that journal editors/reviewers do not appreciate new approaches.\nCognitive appeal: produces a single, seemingly parsimonious model.\nEase of implementation: many statistical packages make stepwise selection easy and prominent.\nMisplaced objectivity: automated procedures appear neutral, even when they enforce arbitrary thresholds on our data.\n\nThese justifications, when considered on an individual basis, become hard to defend. But before we look at what we can do instead, let‚Äôs first delve a little deeper into the specific problems associated with stepwise regression."
  },
  {
    "objectID": "posts/039_27Mar_2025/index.html#what-is-stepwise-regression",
    "href": "posts/039_27Mar_2025/index.html#what-is-stepwise-regression",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "3 What is Stepwise Regression?",
    "text": "3 What is Stepwise Regression?\nMost of you already know what I‚Äôm talking about, but let‚Äôs recap the basics for those who don‚Äôt.\nStepwise regression refers to a family of automated variable selection procedures applied to regression models. The most common variants are:\n\nForward selection: start with no predictors, then add variables one at a time based on some criterion (often the smallest p-value or largest improvement in AIC).\nBackward elimination: start with all candidate predictors, then remove variables that fail to meet a significance threshold.\nBidirectional (stepwise) selection: alternate between adding and removing variables at each step.\n\nThese procedures are typically driven by hypothesis tests (e.g.¬†p &lt; 0.05) or information criteria (AIC, BIC), and they terminate once no further steps improve the chosen criterion. The end result is a single model containing a data-driven subset of the original pool of available candidate predictor variables."
  },
  {
    "objectID": "posts/039_27Mar_2025/index.html#the-core-statistical-problems",
    "href": "posts/039_27Mar_2025/index.html#the-core-statistical-problems",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "4 The Core Statistical Problems",
    "text": "4 The Core Statistical Problems\nAlthough I mentioned these at the outset of this post (based on Harrell‚Äôs text), let‚Äôs flesh some of them out in more detail now.\n\n4.1 Invalid Inference After Selection\nPerhaps the most fundamental problem is that standard inferential quantities are wrong after stepwise selection. When using data-driven variable selection:\n\nP-values, confidence intervals, and standard errors are computed as if the model had been specified in advance.\n\nBut in reality, the data were used at least twice: once to select variables, and again to estimate their effects.\nThis reuse of data leads to:\n\nunderestimated standard errors,\noverly narrow confidence intervals,\nand inflated statistical significance.\n\nYou may not be aware that statistical inference methods using p values, standard errors and confidence intervals were designed to be valid when applied once to a pre-specified model, not iteratively reapplied to a model where chance partly informs the decision at each step. This invalidates the nominal properties of a statistical test, giving us as researchers, a false sense of certainty.\n\n\n4.2 Multiple Testing in Disguise\nStepwise regression performs a large number of implicit hypothesis tests while pretending to conduct only a few. Each step involves testing multiple candidate variables, but no adjustment is made for multiplicity.\nViewed correctly, stepwise selection is a form of uncorrected multiple testing with a stopping rule. The resulting family-wise Type I error rate can be dramatically higher than the nominal level, especially when predictors are correlated.\nThis means that ‚Äústatistically significant‚Äù predictors selected by stepwise methods are often artifacts of chance rather than real signals.\n\n\n4.3 Overfitting and Optimism\nStepwise procedures are designed to optimise in-sample fit. As a result, they tend to overfit, especially when:\n\nthe number of predictors is large relative to the sample size,\npredictors are correlated,\nor the true signal-to-noise ratio is modest.\n\nOverfitted models appear impressive in the training data but perform poorly on new data. This optimism is rarely quantified or acknowledged when stepwise regression is used.\n\n\n4.4 Model Instability\nOne of the most damaging properties of stepwise regression is instability. Small perturbations in the data - e.g.¬†removing a few observations, changing a significance threshold, or using a different random split - can produce entirely different selected models.\nThis instability arises because stepwise selection operates near decision boundaries. When predictors are correlated or effects are weak, tiny changes can flip inclusion decisions.\nAn unstable model cannot be trusted for interpretation, explanation, or clinical decision-making.\n\n\n4.5 Biased Coefficient Estimates\nEven when a predictor truly has an effect, stepwise regression tends to overestimate its magnitude if it is selected. This phenomenon - sometimes called the ‚Äúwinner‚Äôs curse‚Äù - occurs because variables are selected given they look large in the sample. The result is that reported effect sizes tend to be exaggerated, with subsequent studies often failing to replicate the findings."
  },
  {
    "objectID": "posts/039_27Mar_2025/index.html#why-these-problems-matter-in-applied-research",
    "href": "posts/039_27Mar_2025/index.html#why-these-problems-matter-in-applied-research",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "5 Why These Problems Matter in Applied Research",
    "text": "5 Why These Problems Matter in Applied Research\nIn some purely predictive settings, poor inference may be tolerable. In most applied research, however, regression models are used to:\n\ndraw causal conclusions,\ninform clinical or policy decisions,\nsupport regulatory submissions,\nor generate scientific knowledge.\n\nIn these contexts, stepwise regression is especially problematic.\n\n5.1 Clinical and Epidemiological Studies\nIn medical research, variable inclusion is often interpreted causally - even when analysts disclaim causal intent. A covariate that survives stepwise selection may be described as a ‚Äúrisk factor‚Äù or ‚Äúindependent predictor‚Äù, while excluded variables are implicitly treated as unimportant.\nBut this is deeply misleading. Stepwise regression does not distinguish between confounders, mediators, and colliders, and it frequently excludes clinically important variables simply because their effects are imprecisely estimated.\n\n\n5.2 Regulatory and Reporting Contexts\nIn regulatory settings, such as clinical trial reporting, transparency and reproducibility are paramount. Stepwise regression undermines both, because:\n\nthe analysis path is data-driven and difficult to justify prospectively,\nresults may not reproduce across datasets or populations,\nand inferential quantities lack a clear interpretation.\n\nFor these reasons, many regulatory guidelines explicitly discourage data-driven variable selection."
  },
  {
    "objectID": "posts/039_27Mar_2025/index.html#what-to-do-instead",
    "href": "posts/039_27Mar_2025/index.html#what-to-do-instead",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "6 What to Do Instead",
    "text": "6 What to Do Instead\nOk, we‚Äôve spent a fair amount of time talking about the problem. Let‚Äôs now discuss what we can actually do about it. The good news is that we are not short of alternatives. In fact, modern approaches are often simpler, more transparent, and more defensible.\nI am going to discuss these approaches in the context of ‚Äúmodel intent‚Äù - i.e.¬†what is the purpose for your regression model? This ultimately boils down to one of two options - explanation vs prediction. Model explanation and model prediction answer fundamentally different questions. Explanatory models aim to estimate and interpret the effect of variables, often with a causal lens, which makes pre-specification, subject-matter knowledge, and valid inference essential. An example research question that such a model could answer might be:\n\n‚ÄúWhat is the effect of DMT x on the risk of disease relapse, after adjusting for age, sex, smoking status, and baseline disease severity?‚Äù\n\ni.e.¬†we are interested primarily in the explanatory ‚Äòeffect‚Äô of a new DMT on disease relapse.\nPredictive models, by contrast, are judged by how well they perform on new data, not by whether their coefficients are interpretable or causally meaningful - in other words, we don‚Äôt really care about individual variable ‚Äòeffects‚Äô but rather just the overall predictive power of the model. Here, the commensurate research question takes a different form:\n\n‚ÄúUsing age, sex, smoking status, current therapy and disease severity data at baseline, what is an individual‚Äôs 5-year risk of disease relapse?‚Äù\n\ni.e.¬†we want to predict relapse risk but don‚Äôt really care about any individual pedictor.\n\n6.1 Explanation\n\n6.1.1 Pre-Specification and Subject-Matter Knowledge\nWhen we are interested in model explanation, the most robust approach is also the least glamorous: decide in advance (i.e.¬†prior to seeing the data) which variables belong in the model, and make that our one and only model.\nHere, variable inclusion should be guided by:\n\nscientific understanding,\ncausal reasoning,\nand study design.\n\nConsideration should be given to sythesising these justifications in a supporting causal diagram (DAG - see the previous post).\nNote that if a variable is a known confounder, it should be included regardless of its p-value. In contrast, if it is irrelevant to the scientific question, it should not be tested for inclusion.\nThis approach prioritises validity over convenience. We are very interested in understanding the characteristics of each individual predictor variable (i.e.¬†is it a potential confounder, collider, mediator, etc in the exposure -&gt; outcome relationship), and the interplay between all such variables. Again, this is where a causal diagram, even if we are not planning an explicit causal analysis, can be very helpful. But this is where the hard work ends in explanatory modelling. From a coding perspective, things are easier than ever - we just run whatever model we have decided on (and nothing more), comfortable in the fact that our p-values and confidence intervals are correct.\n\n\n6.1.2 Transparent Sensitivity Analyses\nIf, for whatever reason, pre-specification is not possible and variable selection cannot be avoided, resultant models should be treated as exploratory and accompanied by:\n\nsensitivity analyses,\nstability assessments,\nand clear disclaimers about inferential limitations.\n\nThis is still inferior to pre-specification, but far better than unqualified stepwise regression.\n\n\n\n6.2 Prediction\n\n6.2.1 Full Models with Shrinkage\nIn contrast, when we are interested in model prediction our goals change. We are simply after raw predictive performance and it matters less about the individual variables that comprise the model or how they were chosen. There is actually a fairly nice solution to the problem of variable selection in the context of prediction modelling, and that is ‚Äúregularisation‚Äù or ‚Äúpenalised regression‚Äù methods. These methods acknowledge uncertainty more honestly than stepwise procedures.\nRegularisation is central to prediction because it controls overfitting by shrinking model coefficients, trading a small amount of bias for a large reduction in variance and thus improving performance on new data. Ridge regression shrinks all coefficients toward zero and is particularly effective when many predictors have small, correlated effects. Lasso applies stronger shrinkage that can set some coefficients exactly to zero, producing sparse models that are easier to deploy but can be unstable when predictors are highly correlated. Elastic net combines ridge and lasso penalties, often giving the best predictive performance in practice by encouraging sparsity while retaining groups of correlated predictors.\nKeep reading to the end of this section to see an example of how penalised regression with Elastic net is used.\n\n\n6.2.2 Internal vs External Validation\nBecause we are ultimately interested in predictive power, once we have established a prediction model we should evaluate its performance. There are multiple ways to do this (see here for a good primer), with the main techniques being:\n\ncross-validation,\nbootstrap resampling,\nor, ideally, external validation datasets.\n\nThe general idea in each case is that we ‚Äútrain‚Äù our model on a dataset and then ‚Äútest‚Äù its predictive power on new data. The reason for this is that it is not uncommon when we create a model from a single dataset that the model usually fits the peculiarities of that dataset much better than one it hasn‚Äôt seen before (i.e.¬†it overfits). Thus, predictive performance will often be less on a ‚Äútest‚Äù compared to a ‚Äútrain‚Äù dataset. In cross-validation and bootstrapping we are using sleight of hand to create ‚Äútrain‚Äù and ‚Äútest‚Äù data from the only dataset we have - these are referred to as internal validation methods. While external validation is better because we can train the model on all of our data and then test on a completely unseen dataset, it is harder to achieve in practice for obvious reasons.\nWhatever the method of model performance evaluation, the focus now changes from p-values and confidence intervals to predictive accuracy and generalisability.\n\n\n6.2.3 Model Averaging\nSometimes we may end up with more than one candidate prediction model that have similar predictive power but are based on different modelling strategies (e.g.¬†lasso vs random forest vs GAM). Model averaging is an approach that accounts for uncertainty about model choice by combining predictions or estimates from multiple plausible prediction models rather than relying on a single selected model. Instead of treating one model as ‚Äúthe truth,‚Äù it weights each model according to a measure of support - such as predictive performance, information criteria, or posterior probability - and averages across them. This can improve predictive accuracy and reduce overconfidence, particularly when several models perform similarly well. Model averaging is most natural when there is genuine uncertainty among a discrete set of competing models, and it contrasts with single-model selection approaches that ignore this uncertainty.\n\n\n6.2.4 Elastic Net Example\nAlright, let‚Äôs put some of what we‚Äôve learned into practice. Let‚Äôs simulate some data and run both a stepwise regression and an elastic net, comparing the performance of each. For the simulation I will create a dataset with 500 observations and 20 predictors. I‚Äôll make the regression coefficients for the first 5 predictors non-zero (representing real effects) and set the coefficients for the last 15 to zero (representing noise). I‚Äôll also specify that all predictors are moderately correlated with each other (r = 0.4). With this data-generating process we can then meaningfully compare how stepwise regression and elastic net behave when only a few predictors truly matter (and hopefully find that the latter approach performs better).\n\n\nCode\nset.seed(123)\n\nn &lt;- 500\np &lt;- 20\n\nSigma &lt;- matrix(0.4, p, p)\ndiag(Sigma) &lt;- 1\n\nlibrary(MASS)\nX &lt;- mvrnorm(n, mu = rep(0, p), Sigma = Sigma)\n\nbeta &lt;- c(2, 2, 2, -1.5, -1.5, rep(0, p - 5))\ny &lt;- X %*% beta + rnorm(n, sd = 2)\n\ndat &lt;- data.frame(y, X)\ncolnames(dat) &lt;- c(\"y\", paste0(\"X\", 1:p))\n\n\nNow let‚Äôs create train and test datasets from the simulated data. To do this we‚Äôll randomly choose 70% of the observations for the train dataset and allocate the remaining 30% to the test dataset.\n\n\nCode\nset.seed(456)\nidx &lt;- sample(seq_len(n), size = 0.7 * n)\n\ntrain &lt;- dat[idx, ]\ntest  &lt;- dat[-idx, ]\n\n\nWe will now perform a classic stepwise regression using step() to automate the process. step() fits multiple models moving back and forth between a model with no predictors (intercept only model) and one with all 20 predictors (full model). At each stage it computes the AIC for each candidate model, moves to the model with the lowest AIC and then stops when no further addition or deletion of variables improves the AIC.\nWe can see that the first five predictors are correctly selected, but the algorithm also selects four noise predictors, two of which are deemed statistically significant.\n\n\nCode\nfull_mod &lt;- lm(y ~ ., data = train)\nnull_mod &lt;- lm(y ~ 1, data = train)\n  \nstep_mod &lt;- step(\n  null_mod,\n  scope = list(lower = null_mod, upper = full_mod),\n  direction = \"both\",\n  trace = FALSE\n)\n\nsummary(step_mod)\n\n\n\nCall:\nlm(formula = y ~ X1 + X3 + X5 + X2 + X4 + X15 + X10 + X14 + X18, \n    data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6759 -1.2016  0.0696  1.2556  5.7055 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.07313    0.10297   0.710   0.4781    \nX1           2.08653    0.11979  17.418   &lt;2e-16 ***\nX3           2.12815    0.13199  16.123   &lt;2e-16 ***\nX5          -1.61482    0.12589 -12.827   &lt;2e-16 ***\nX2           2.02325    0.12951  15.622   &lt;2e-16 ***\nX4          -1.58574    0.12656 -12.530   &lt;2e-16 ***\nX15          0.30294    0.13056   2.320   0.0209 *  \nX10         -0.32018    0.12951  -2.472   0.0139 *  \nX14          0.22121    0.13102   1.688   0.0922 .  \nX18         -0.20767    0.13261  -1.566   0.1183    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.907 on 340 degrees of freedom\nMultiple R-squared:  0.7858,    Adjusted R-squared:  0.7802 \nF-statistic: 138.6 on 9 and 340 DF,  p-value: &lt; 2.2e-16\n\n\nLet‚Äôs check the performance by using the model to predict on the test data, then calculating a common regression performance measure - the root mean square error (RMSE). The lower the RMSE, the better.\n\n\nCode\ny_pred_step &lt;- predict(step_mod, newdata = test)\nrmse_step &lt;- sqrt(mean((test$y - y_pred_step)^2))\nrmse_step\n\n\n[1] 2.018798\n\n\nNow let‚Äôs compare this to an elastic net, using the glmnet package, which fits penalised regression models (ridge, lasso, elastic net). We can break this code block down into a series of steps as follows:\n\nglmnet fits many elastic net models, each with:\n\nthe same predictors,\nthe same alpha = 0.5 (mix of ridge and lasso),\ndifferent values of lambda (penalty strength).\n\nIt then performs 10-fold cross-validation:\n\nthe training data are split into 10 folds,\neach fold is held out in turn,\nprediction error is estimated for each lambda.\n\nAnd finally, chooses the optimal penalty:\n\nlambda.min ‚Üí lowest cross-validated error,\nlambda.1se ‚Üí largest Œª within 1 SE of the minimum (simpler model).\n\n\nThis step answers: ‚ÄúHow much regularisation gives the best out-of-sample performance?‚Äù\n\n\nCode\nlibrary(glmnet)\n\n# glmnet requires predictors as a numeric matrix, not a data frame.\nX_train &lt;- as.matrix(train[, -1]) # contains all predictors but remove the outcome variable (y)\ny_train &lt;- train$y                # response\nX_test  &lt;- as.matrix(test[, -1])\ny_test  &lt;- test$y\n\ncv_enet &lt;- cv.glmnet(\n  X_train, y_train,\n  alpha = 0.5,                    # elastic net\n  nfolds = 10\n)\n\n# Extract the coefficients from the single elastic net model corresponding to the optimal Œª.\ncoef(cv_enet, s = \"lambda.min\")\n\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n              lambda.min\n(Intercept)  0.066848837\nX1           2.012777264\nX2           1.921988809\nX3           2.043667440\nX4          -1.469708904\nX5          -1.503814250\nX6           0.010503422\nX7          -0.111528143\nX8          -0.065850506\nX9          -0.080697600\nX10         -0.232749640\nX11          0.010956657\nX12          .          \nX13          0.120277174\nX14          0.166942224\nX15          0.237722152\nX16          0.030739719\nX17          0.058957699\nX18         -0.115478884\nX19         -0.046143403\nX20          0.004132131\n\n\nNote that even when the true data-generating process is sparse, elastic net does not necessarily recover the true set of predictors, particularly when predictors are correlated. This is not a failure: the elastic net objective is to minimise prediction error, not to identify the true model. At the penalty that optimises cross-validated performance, elastic net may retain many small coefficients because correlated noise variables still carry predictive information. In other words, if allowing small non-zero coefficients on ‚Äúnoise‚Äù predictors improves prediction even slightly, elastic net will do that.\nNow let‚Äôs compare the prediction error from both modelling approaches.\n\n\nCode\ny_pred_enet &lt;- predict(cv_enet, X_test, s = \"lambda.min\")\nrmse_enet &lt;- sqrt(mean((y_test - y_pred_enet)^2))\nrmse_enet\n\n\n[1] 1.999773\n\n\nCode\nc(\n  Stepwise_RMSE   = rmse_step,\n  ElasticNet_RMSE = rmse_enet\n)\n\n\n  Stepwise_RMSE ElasticNet_RMSE \n       2.018798        1.999773 \n\n\nThe elastic net is slightly better, but ultimately they‚Äôre pretty similar. But that doesn‚Äôt mean that this exercise has been a waste of time. Even when stepwise regression and elastic net achieve similar RMSE on a given test set, elastic net is preferable because it achieves this performance through explicit regularisation rather than discrete variable selection.\nPenalised models are:\n\nmore stable to sampling variation,\nhandle correlated predictors more gracefully,\nand control overfitting directly.\n\nThe apparent equivalence in point performance masks important differences in reliability and robustness, which become evident under resampling or repeated evaluation. Penalised regression does not eliminate sampling variability, but it responds to it smoothly. Unlike stepwise regression, which makes brittle include‚Äìexclude decisions, elastic net adjusts coefficients continuously, allowing predictive performance to remain stable even when individual coefficients fluctuate.\nAll of this is to say that if we ran these models many times across different datasets, penalised regression will still give us a relatively stable model and predictive performance - stepwise methods won‚Äôt."
  },
  {
    "objectID": "posts/039_27Mar_2025/index.html#a-pragmatic-takeaway",
    "href": "posts/039_27Mar_2025/index.html#a-pragmatic-takeaway",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "7 A Pragmatic Takeaway",
    "text": "7 A Pragmatic Takeaway\nStepwise regression persists because it is easy, familiar, and superficially tidy. But its apparent simplicity masks deep statistical flaws that undermine inference, reproducibility, and scientific credibility.\nModern alternatives - pre-specification for explanatory modelling; and shrinkage, validation ¬± model-averaging for prediction modelling - are not only more principled, they are often easier to explain and defend.\nThe question is no longer whether stepwise regression is flawed. The question is why we continue to use it when better tools are readily available.\nIf you want to read more about this topic I‚Äôd suggest having a look at the following papers:\n\nGreenland S. Invited commentary: variable selection versus shrinkage in the control of multiple confounders. Am J Epidemiol. 2008 Mar 1;167(5):523-9; discussion 530-1. doi: 10.1093/aje/kwm355. Epub 2008 Jan 27. Erratum in: Am J Epidemiol. 2008 May 1;167(9):1142. PMID: 18227100.\nWalter S, Tiemeier H. Variable selection: current practice in epidemiological studies. Eur J Epidemiol. 2009;24(12):733-6. doi: 10.1007/s10654-009-9411-2. Epub 2009 Dec 5. PMID: 19967429; PMCID: PMC2791468.\nHeinze G, Wallisch C, Dunkler D. Variable selection - A review and recommendations for the practicing statistician. Biom J. 2018 May;60(3):431-449. doi: 10.1002/bimj.201700067. Epub 2018 Jan 2. PMID: 29292533; PMCID: PMC5969114.\nSauerbrei W, Perperoglou A, Schmid M, Abrahamowicz M, Becher H, Binder H, Dunkler D, Harrell FE Jr, Royston P, Heinze G; for TG2 of the STRATOS initiative. State of the art in selection of variables and functional forms in multivariable analysis-outstanding issues. Diagn Progn Res. 2020 Apr 2;4:3. doi: 10.1186/s41512-020-00074-3. PMID: 32266321; PMCID: PMC7114804.\n\nWell that‚Äôs pretty much it for this post folks. Hopefully you can start to incorporate some of these techniques into your next regression modelling endeavour. See you next month."
  },
  {
    "objectID": "posts/038_27Feb_2026/index.html",
    "href": "posts/038_27Feb_2026/index.html",
    "title": "A Gentle Introduction to Directed Acyclic Graphs (DAGs)",
    "section": "",
    "text": "A good part of the research that we do in the unit is observational in nature. Rather than the highly controlled environments of laboratory-based experimental designs and randomised clinical trials, observational research is really the ‚ÄúWild West‚Äù cousin, where both its virtue and vice is in the fact that we gain knowledge by observing the ‚Äúchaos‚Äù of the natural world. Consequently, observational research lives and dies by assumptions. We assume certain variables cause others, assume some relationships matter and others don‚Äôt, and assume that statistical adjustment corresponds to causal control. Directed acyclic graphs (DAGs) force these assumptions out into the open.\nIn this post, we‚Äôll move beyond theory and show how to construct, visualise, and interrogate DAGs in R using the ggdag and daggity packages. Along the way, we‚Äôll revisit why DAGs are essential for observational research and how they clarify the roles of confounders, mediators, and colliders."
  },
  {
    "objectID": "posts/038_27Feb_2026/index.html#basic-concepts",
    "href": "posts/038_27Feb_2026/index.html#basic-concepts",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.1 Basic concepts",
    "text": "2.1 Basic concepts\nAnd this is where the fundamental idea of the sampling distribution comes into play. In a nutshell, the sampling distribution represents the theoretical distribution of a sample statistic that‚Äôs derived from repeatedly randomly sampling a population of interest. Now this is a thought exercise only - we don‚Äôt do it for obvious reasons in practice - but it allows us to make assumptions about the behaviour of the population parameter that we are trying to estimate. (Note - while I say ‚Äòtheoretical‚Äô above, these distributions can now be verified in silico but in the pre-computer era were confirmed empirically by a combination of mathematical derivation and physical simulation - i.e.¬†bootstrapping by hand!)\nIn this thought exercise we would have a population that we are interested in calculating a parameter for, and we‚Äôd draw a sample of observations from this population. We‚Äôd then calculate the corresponding sample statistic - let‚Äôs say it‚Äôs a mean value - and we‚Äôd plot that on a frequency histogram. We would then repeat that process many times, plotting each mean value along the way. The resulting plot would show the distribution of all the sample means - and this is what we call the sampling distribution of the sample statistic. It‚Äôs important to note that this is NOT the distribution of the data itself, but the distribution of a summary statistic derived from the data.\n\n\n\n\n\nNow, depending on the sample size, the shape of the underlying raw data distribution and the specific summary statistic we‚Äôre interested in, sampling distributions for many statistics often end up looking normal (or close to normal) in shape. And that allows us to leverage fairly simple normal distribution properties such as the mean and standard deviation (SD) to infer the population mean and it‚Äôs associated uncertainty. The mean should converge to the actual population parameter and the SD tells us about the uncertainty in the estimation of the parameter - and in fact is directly interpretable from the sampling distribution itself, as the standard error (SE). Once we have the SE it becomes trivial to calculate the 95% confidence interval (CI).\nThe point in telling you all of this is to highlight to you is that we can use just a single sample to form probabilistic statements about a population parameter, rather than needing to measure the entire population."
  },
  {
    "objectID": "posts/038_27Feb_2026/index.html#when-it-works",
    "href": "posts/038_27Feb_2026/index.html#when-it-works",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.2 When it works",
    "text": "2.2 When it works\nMost of the time the theory of parametric sampling distributions will work just fine for what you want to do. Probability density functions (PDF‚Äôs) that define the ‚Äòshape‚Äô of the sampling distribution have been derived for many sample statistics. These equations are sometimes referred to as ‚Äòclosed-form solutions‚Äô. Some PDF‚Äôs are fairly simple, other‚Äôs are almost intractably complex and some are simply unknown. But what is important about this, is that when you know the PDF for a sampling distribution, you can calculate exact 95% CI‚Äôs.\nWhen PDF‚Äôs become too difficult or are simply unknown, we can start to leverage approximations. And that probably explains why I haven‚Äôt needed to bootstrap much in my own work - the classic central limit theorem (CLT) does its job pretty well. The CLT basically states that you can have whatever shaped raw data distribution you want - flat, skewed, bi-modal - whatever, but when you then construct a sampling distribution from the resulting sample means, that distribution will be normal in shape (if you‚Äôve got a large enough sample).\nThe thing is, the logic of the central limit theorem translates fairly well to most other sample statistics of interest - for example, medians, proportions, correlations, regression coefficients, and more - through a concept called large-sample asymptotic normality. This approximation basically says that if our sample is large enough, the sampling distributions of these other parameters will likewise approach normality in terms of shape. What that means at the end of the day is that we can use fairly simple(-ish) equations for large sample approximations to estimate SE‚Äôs and 95% CI‚Äôs. The equations for the SE for the mean and proportion are shown below and I‚Äôm sure you‚Äôve seen these before.\n\\[ \\bar{x} \\pm 1.96^* \\frac{s}{\\sqrt{n}} \\hspace{2cm} \\widehat{p} \\pm 1.96^* \\sqrt{\\frac{\\widehat{p}(1 - \\widehat{p})}{n}} \\]"
  },
  {
    "objectID": "posts/038_27Feb_2026/index.html#when-it-doesnt",
    "href": "posts/038_27Feb_2026/index.html#when-it-doesnt",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.3 When it doesn‚Äôt",
    "text": "2.3 When it doesn‚Äôt\nClearly, there are situations that can occasionally arise when basic sampling theory might not be adequate for your analytic needs, otherwise we wouldn‚Äôt have this post. To my mind there are three main reasons why you might look further afield to a resampling method like the bootstrap to support your analyses.\n\nThe first is sample size. Asymptotic normal theory relies on a ‚Äòlarge‚Äô sample size to be accurate, and the bootstrap deals with smaller samples much better. However, it is itself not immune to small sample bias (when n becomes quite small - say &lt; 15). In such cases not much can save you unless you collect more data, so you might just need to rely on descriptive statistics only.\nThe second situation is where equations for the SE are either so complex as to be virtually intractable, or simply don‚Äôt exist (as described above). Here you can use the bootstrap to estimate the sampling distribution directly with relative ease.\nFinally, for sampling distributions that depart quite obviously from normality. Here the large-sample approximations just don‚Äôt work well, but you can use the bootstrap in these cases to actually capture that non-normal shape and apply it in valid inference."
  },
  {
    "objectID": "posts/038_27Feb_2026/index.html#why-is-it-called-the-bootstrap",
    "href": "posts/038_27Feb_2026/index.html#why-is-it-called-the-bootstrap",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.4 Why is it called the bootstrap?",
    "text": "2.4 Why is it called the bootstrap?\nAlright - let‚Äôs actually talk about The Bootstrap! There is some interesting history in how the bootstrap came to be called what it is, as it‚Äôs etymology isn‚Äôt from the statistical domain. The origins of the term are sometimes attributed to an 18th century work of fiction - The Surprising Adventures of Baron Munchausen - in which Baron Munchausen‚Äôs plan for getting himself (and his horse) out of a swamp was to pull himself out by his bootstraps. Curiously there appears to be no actual reference to his bootstraps in the story itself, where instead he uses his own hair (pigtails to be specific).\nIn any case, over time the term evolved to mean many things but with the overarching theme of ‚Äòperforming a near impossible task‚Äô, or ‚Äòdoing more with less‚Äô. It is not unheard of today in political discourse as a narrative for self-starting economic mobility - that is, ‚Äúif you just put in the hard work, you will eventually be successful‚Äù.\nIn statistics specifically, the bootstrap is used to mean that the population parameter we are interested in can be sufficiently defined by the sample of data that we have. In other words, ‚Äòthe sample ‚Äúpulls itself up by its bootstraps‚Äù‚Äô."
  },
  {
    "objectID": "posts/038_27Feb_2026/index.html#an-important-statistical-idea",
    "href": "posts/038_27Feb_2026/index.html#an-important-statistical-idea",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "2.5 An important statistical idea",
    "text": "2.5 An important statistical idea\nBefore I get to explaining what the bootstrap is in more detail, let me start by introducing a paper that was published in 2020 by two very well-known American statisticians - What are the most important statistical ideas of the past 50 years.\n\nCounterfactual causal inference\nBootstrapping and simulation-based inference\nOverparameterized models and regularization\nMultilevel models\nGeneric computation algorithms\nAdaptive decision analysis\nRobust inference\nExploratory data analysis\n\nEach of these ideas has existed in some form prior to the 1970s, both in the theoretical statistics literature and in the practice of various applied Ô¨Åelds. But the authors consider that each has developed enough in the past 50 years to have essentially become something new and in many instances this has been facilitated by the modern computing age, as some of these techniques just weren‚Äôt practical to apply before we had fast computers. We have some ideas that you might already be familiar with from traditional statistics - counterfactual inference, multilevel model, robust inference, exploratory data analysis; and perhaps others that might be less so because they fall more into the realm of data science and predictive analytics - overparameterised models, generic algorithms and decision analysis.\nGiven that bootstrapping is one of these important statistical ideas from the last 50 years, let‚Äôs now learn some more about it."
  },
  {
    "objectID": "posts/038_27Feb_2026/index.html#what-is-it",
    "href": "posts/038_27Feb_2026/index.html#what-is-it",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "3.1 What is it?",
    "text": "3.1 What is it?\nThe bootstrap method is a resampling technique that estimates the sampling distribution of a statistic by treating the original sample as a proxy for the population. Instead of drawing new samples from an unknown population, which is what we learned about earlier, we simulate this process by repeatedly drawing samples - with replacement - from our single observed sample. And this allows us to approximate the variability and properties of the statistic, based on the assumption that our single, observed sample is a good representation of the underlying population.\nIn other words, the bootstrap treats the original sample as a miniature, empirical population. Each bootstrap sample is the same size as the original and is created by sampling with replacement. This ‚Äúwith replacement‚Äù step is critical because it ensures each bootstrap sample is a unique combination of values from the original data, simulating the variability you‚Äôd expect from a new sample.\nSo we use the same steps here that I outlined earlier in constructing the true sampling distribution - that is, for each bootstrap sample (and we typically specify thousands of them), we calculate our summary statistic and plot these as a frequency histogram. The collection of all these bootstrap sample statistics forms the bootstrap sampling distribution, which then serves as an estimate of the true sampling distribution.\nWe can then calculate other important measures such as the SE and CI‚Äôs by reading the 2.5 and 97.5 percentile values directly off the plot. We don‚Äôt actually need to invoke any mathematical formulae as we previously did - and that‚Äôs because we have an actual distribution now rather than just a theoretical one."
  },
  {
    "objectID": "posts/038_27Feb_2026/index.html#bootstrap-sampling-distribution",
    "href": "posts/038_27Feb_2026/index.html#bootstrap-sampling-distribution",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "3.2 Bootstrap sampling distribution",
    "text": "3.2 Bootstrap sampling distribution\nRemember what our theoretical sampling distribution looked like? (scroll up if you don‚Äôt). Now when we look at our bootstrapped sampling distribution, there isn‚Äôt really much that‚Äôs changed. The main differences are that we‚Äôve substituted our only sample for our population and we‚Äôre now ‚Äòresampling‚Äô from that, rather than sampling from our population. Everything else basically stays the same. Note how we can easily extract the confidence limits ‚Äòempirically‚Äô, directly from the plot, by just ordering all the values from lowest to highest and taking the values at the 2.5 and 97.5 percentiles. These correspond to the lower and upper confidence limits, giving us 95% coverage for the true population parameter. Remember, the beauty of this method is that it doesn‚Äôt assume a specific distribution for the data, and that is extremely useful when the classical assumptions aren‚Äôt met."
  },
  {
    "objectID": "posts/038_27Feb_2026/index.html#sampling-distributions-reimagined",
    "href": "posts/038_27Feb_2026/index.html#sampling-distributions-reimagined",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "3.3 Sampling distributions reimagined",
    "text": "3.3 Sampling distributions reimagined\nThese aren‚Äôt my images but I thought I‚Äôd show them to you because it‚Äôs a tangible visual take on the same two concepts, using the global population, and I think if you‚Äôve been having some trouble following along, this should make things a lot clearer. The top picture shows the true (or theoretical) sampling distribution for a mean. We start off with all 7.6 billion people in the world and then take multiple samples from the population, calculating the mean in each sample and then plotting the distribution of those sample means.\nYou can also easily appreciate how the bootstrap sampling distribution differs, below that. We might still start off with our population, but it remains unrealised, and all we actually have is the one sample that we draw from it. Our bootstrap samples are then resamples of that one sample, with replacement. Note how in each bootstrap sample, one individual has been sampled twice - the grey person in the first, the purple in the second and the green in the third. But that‚Äôs fine and to be expected. We then calculate the mean in each resample and plot the distribution of those means.\n\nTheoretical\n\n\n\n\n\n\n\nBootstrap"
  },
  {
    "objectID": "posts/038_27Feb_2026/index.html#raw-data-distribution",
    "href": "posts/038_27Feb_2026/index.html#raw-data-distribution",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "5.1 Raw Data Distribution",
    "text": "5.1 Raw Data Distribution\nTo do this in R we‚Äôre going to use an inbuilt dataset, and in fact it doesn‚Äôt even matter what that is, so I‚Äôm not going to describe it here (details are in the code at the end of this post that will allow you to fully reproduce all analyses).\n\n\n\n\n\nThis is the raw data distribution for the variable we‚Äôll be bootstrapping the mean for, and it consists of 47 observations. You could argue that there‚Äôs a normal shape to it, but in all honesty, there probably aren‚Äôt enough data points to say that with certainty.\nWe‚Äôll now take this variable and we‚Äôll bootstrap it 1000 times - in other words we‚Äôll take 1000 resamples each of size 47, replacing each value in the event that it‚Äôs drawn. Then we‚Äôll calculate the mean value in each of those 1000 resamples."
  },
  {
    "objectID": "posts/038_27Feb_2026/index.html#sampling-distribution",
    "href": "posts/038_27Feb_2026/index.html#sampling-distribution",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "5.2 Sampling Distribution",
    "text": "5.2 Sampling Distribution\nWhen we construct a frequency histogram of those 1000 mean values, we see the following:\n\n\n\n\n\nThere are a couple of salient things to note:\n\nThe first is that the bootstrap sampling distribution is quite normal in shape, even though the raw data distribution might not have been.\nThe second point is that the mean of the original sample and the mean of all the bootstrapped means is virtually identical.\n\nThis is a good thing as it means that the sample mean is an unbiased estimator of the population mean. Another way of saying this is that ‚Äúif I were to repeat this study many times, the sample mean would, on average, hit the true population mean.‚Äù\n\nThe last thing to say about this plot is that if we wanted to obtain the bootstrapped confidence limits we could simply read off the values corresponding to the 2.5 and 97.5 percentiles from the plot. Of course, in practice you‚Äôd get your stats software to do this for you, but the point is that it‚Äôs quite easy to do and doesn‚Äôt involve any formulae."
  },
  {
    "objectID": "posts/038_27Feb_2026/index.html#comparison-of-95-cis",
    "href": "posts/038_27Feb_2026/index.html#comparison-of-95-cis",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "5.3 Comparison of 95% CI‚Äôs",
    "text": "5.3 Comparison of 95% CI‚Äôs\n\n\n\n\n\nAnd these are the 95% CI‚Äôs from the various methods. The first one is the ‚Äòtheoretical‚Äô - assuming normality and the others are derived from the boot.ci function after doing the bootstrapping procedure. Really, there‚Äôs not a lot to say about this - you can see that the coverage of all the CI‚Äôs is fairly similar, and that‚Äôs a good thing as it means you can can have increased confidence in the robustness of your results. (Note - the ‚Äòtheoretical‚Äô CI in this case for the mean is based off a t test which enables ‚Äòexact‚Äô CI‚Äôs to be calculated as the PDF of the sampling distribution is known. So while I say ‚Äòassuming normality‚Äô in this case it‚Äôs really an exact CI)."
  },
  {
    "objectID": "posts/038_27Feb_2026/index.html#raw-data-distribution-1",
    "href": "posts/038_27Feb_2026/index.html#raw-data-distribution-1",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "6.1 Raw Data Distribution",
    "text": "6.1 Raw Data Distribution\n\n\n\n\n\nOk - let‚Äôs now consider a different sample statistic that we might be interested in - the correlation coefficient. This is a scatterplot with overlaid density plot of the previous variable (on the x-axis) and a second variable (on the y-axis) from the same dataset. The marginal distribution of the second variable is far from normal as I‚Äôm sure you can appreciate, by looking at the density curve on the right side. When we look at the bivariate relationship in terms of the scatterplot itself, it‚Äôs not hard to imagine a negative relationship between the two variables."
  },
  {
    "objectID": "posts/038_27Feb_2026/index.html#sampling-distribution-1",
    "href": "posts/038_27Feb_2026/index.html#sampling-distribution-1",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "6.2 Sampling Distribution",
    "text": "6.2 Sampling Distribution\nIn contrast to the sample mean, the sampling distribution of the correlation coefficient does NOT have the same, simple, normal shape, straight out of the box. This statistic definitely relies on asymptotic normality based on having a large sample - larger than you would require for the sample mean.\n\n\n\n\n\nThis time when we construct a frequency histogram of those 1000 mean values, we get quite a positively skewed bootstrap sampling distribution, and there is no way we could argue this is normal in shape. The other observation that we can easily make is that the original sample correlation coefficient is different (more negative) to the mean of all the bootstrapped correlation coefficients.\nWhat this reflects is that the sample correlation coefficient is a biased estimator of the population correlation coefficient. And another way of saying this is that ‚Äúif I were to repeat this study many times, the sample correlation would, on average, consistently be closer to zero than the true population correlation.‚Äù Now, this bias is worse as the correlation approaches either plus or minus one, and with small sample sizes. The bias reduces as the sample size increases according to our large sample theory for asymptotic normality.\nAs with the sample mean, if we want to obtain empirical 95% CI‚Äôs we can just read off the corresponding values at the 2.5 and 97.5 percentiles."
  },
  {
    "objectID": "posts/038_27Feb_2026/index.html#comparison-of-95-cis-1",
    "href": "posts/038_27Feb_2026/index.html#comparison-of-95-cis-1",
    "title": "A Gentle Introduction to the Bootstrap",
    "section": "6.3 Comparison of 95% CI‚Äôs",
    "text": "6.3 Comparison of 95% CI‚Äôs\n\n\n\n\n\nConfidence interval coverage with the correlation coefficient is also a little different to what we previously saw with the sample mean. The theoretical and two of the bootstrap intervals - the percentile and the bias-corrected percentile are quite similar, whereas the remaining two bootstrap intervals - the normal and basic are quite different. OK, so what do we believe here? Well, the normal and basic intervals should really only be trusted when we‚Äôve got a large sample size and a well-behaved statistic - and you could make a good argument that we don‚Äôt really have either of those two conditions being met here. Therefore it‚Äôs either the percentile method or its bias-corrected variant and as I mentioned before the latter is probably the best method, in general, to choose. When we compare the bias-corrected interval to the theoretical interval, we can see that in fact they‚Äôre not that different, but the bias-corrected is a little more conservative (i.e.¬†the CI is wider) - which is always a good thing, I think, in quantifying uncertainty.\nSo at the end of the day, for these data, it‚Äôs good to be able to report both types of CI‚Äôs - theoretical and empirical from the bootstrap. And that‚Äôs because we know from the outset that we‚Äôre dealing with a sample statistic that might not conform to the theoretical assumptions for CI estimation as well as a ‚Äòbetter behaved‚Äô statistic like the sample mean."
  },
  {
    "objectID": "posts/038_27Feb_2026/index.html#introduction",
    "href": "posts/038_27Feb_2026/index.html#introduction",
    "title": "A Gentle Introduction to Directed Acyclic Graphs (DAGs)",
    "section": "",
    "text": "A good part of the research that we do in the unit is observational in nature. Rather than the highly controlled environments of laboratory-based experimental designs and randomised clinical trials, observational research is really the ‚ÄúWild West‚Äù cousin, where both its virtue and vice is in the fact that we gain knowledge by observing the ‚Äúchaos‚Äù of the natural world. Consequently, observational research lives and dies by assumptions. We assume certain variables cause others, assume some relationships matter and others don‚Äôt, and assume that statistical adjustment corresponds to causal control. Directed acyclic graphs (DAGs) force these assumptions out into the open.\nIn this post, we‚Äôll move beyond theory and show how to construct, visualise, and interrogate DAGs in R using the ggdag and daggity packages. Along the way, we‚Äôll revisit why DAGs are essential for observational research and how they clarify the roles of confounders, mediators, and colliders."
  },
  {
    "objectID": "posts/038_27Feb_2026/index.html#association-vs-causation",
    "href": "posts/038_27Feb_2026/index.html#association-vs-causation",
    "title": "A Gentle Introduction to Directed Acyclic Graphs (DAGs)",
    "section": "2 Association vs Causation",
    "text": "2 Association vs Causation\nBefore diving into DAGs, let‚Äôs briefly explore the idea that we often have unrealistic expectations for the intent of our work. In my early research career I was pulled up at least once by a journal reviewer (hopefully I never made the same mistake again) for describing my results as an ‚Äúeffect‚Äù, without the use of quotation marks in the manuscript at the time. I received a relatively lengthy screed in review chastising me that use of the term in that context implied causation between the exposure and outcome, and all I could really report was an association. The reviewer had a point and so I now never write ‚Äúeffect‚Äù without the quotation marks, hopefully implying my usage of the word as a colloquial proxy for association or correlation.\nPart of the problem is that it‚Äôs hard to disabuse ourselves of what we‚Äôre taught in statistics classes. When we specify something like the following regression model:\nlm(outcome ~ exposure + covariates, data = df)\nwe are implicitly making causal claims about: which variables precede exposure, which variables influence outcome, and which associations we want to block or preserve. But we need to stop thinking like that.\nWhile the ultimate goal of most observational research is to inform interventions by identifying ‚Äúcause and effect‚Äù, the vast majority of published analyses remain strictly association studies due to the inherent limitations of non-experimental data. Although our ingrained motivations as scientists want us to argue that one thing leads to another (i.e.¬†the exposure causes the outcome), it is important to keep in mind that because of the assumptions we must make, this is often an impossible goal. Establishing causation with sufficient evidence is not an all or nothing event, however. Think of it as a dial turning between association and causation, where the former represents the default position. The more assumptions you convincingly satisfy, the more the dial may be allowed to turn clockwise towards causation rather than just association in explaining the exposure -&gt; outcome relationship.\nNow, I‚Äôm not going to go through all of the assumptions necessary for causal inference in detail. But I will mention the first, and arguably the most important. That is the assumption of No Unobserved Confounding. In other words, we need to know that we have identified and accounted for all possible covariates that might distort our exposure -&gt; outcome relationship of interest. Only then, can we start to assert with a little more confidence that ‚Äúeffects‚Äù are indeed just that, allowing us to use language that is less neutral and more concordant with a causal process.\nAnd this is where a DAG can be extremely helpful."
  },
  {
    "objectID": "posts/038_27Feb_2026/index.html#what-are-directed-acyclic-graphs",
    "href": "posts/038_27Feb_2026/index.html#what-are-directed-acyclic-graphs",
    "title": "A Gentle Introduction to Directed Acyclic Graphs (DAGs)",
    "section": "3 What are Directed Acyclic Graphs?",
    "text": "3 What are Directed Acyclic Graphs?\nA Directed Acyclic Graph (DAG) is a visual map used to represent the assumed causal relationships between variables, and to identify confounding. They are relatively recent development in the field of causal inference, based on formal logic and machine learning research from the 1990‚Äôs. In the modern era, DAGs are critical in much observational research and I think it is now safe to say that you won‚Äôt see your manuscript accepted to a reputable journal if you plan to conduct an explicit causal analysis, without one.\nThe following is an example DAG taken from a recent paper visualising all theoretical variable pathways in the proposed relationship between various chronic lung diseases and lung cancer.\n\n\n\n\n\nThis is a fairly complex DAG, so don‚Äôt be put off by it. Indeed, while this figure is visually arresting, at their core every DAG contains only two basic structures:\n\nThe variables in the postulated data-generating process (DGP), each represented by a ‚Äúnode‚Äù on the diagram, and:\nThe causal relationships (or pathways) in the postulated DGP, each represented by an arrow from the cause variable to the caused variable (i.e.¬†effect).\n\nThus, a DAG is:\n\nDirected: arrows indicate causal direction between two variables,\nAcyclic: arrows can never form a closed/feedback loop (e.g., A‚ÜíB‚ÜíC‚ÜíA). This reflects the reality that a cause must precede its effect in time; you cannot be your own ancestor.\nGraphical: variables are nodes, relationships are paths.\n\nWith this knowledge and the context of the above example then, one could make statements such as:\n\n‚ÄúLung disease ‚Äòcauses‚Äô lung cancer‚Äù, or:\n‚ÄúAge ‚Äòcauses‚Äô lung disease‚Äù, or:\n‚ÄúSmoking ‚Äòcauses‚Äô lung cancer‚Äù.\n\nSome such statements we may not know with certainty to be true, but this is ok. In fact, an arrow in a DAG is almost always a hypothesised assumption rather than a proven fact. One of the most powerful features of a DAG is that it functions as a ‚Äúhonesty check‚Äù in that it forces you to move your private mental model into a public, visual space where others can critique it.\n\n\nCode\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(tidyverse)\n\n# Define node coordinates\ndag_coords &lt;- tibble::tribble(\n  ~name, ~x,  ~y,\n  \"C\",    0,   1,    # Confounder at top\n  \"E\",   -1,   0,    # Exposure at bottom left\n  \"O\",    1,   0     # Outcome at bottom right\n)\n\n# Creae DAG\ndag &lt;- dagify(\n  E ~ C,\n  O ~ C,\n  O ~ E,\n  labels = c(\n    C = \"Confounder\",\n    E = \"Exposure\",\n    O = \"Outcome\"),\n  coords = dag_coords,\n  exposure = \"E\",\n  outcome = \"O\")\n\n# Plot automatically (using ggdag)\nset.seed(124) # Set and check label positions don't obscure arrowheads\ndag |&gt;\n  ggdag(text = FALSE, use_labels = \"label\") +\n  theme_dag() +\n  expand_plot(expand_x = expansion(c(.5, .5)))\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(124)\ndag |&gt;\n  ggdag_paths(text = FALSE, use_labels = \"label\", shadow = T) +\n  theme_dag() +\n  expand_plot(expand_x = expansion(c(.5, .5)))\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(124)\ndag |&gt;\n  ggdag_adjustment_set(text = FALSE, use_labels = \"label\", shadow = T) +\n  theme_dag() +\n  expand_plot(expand_x = expansion(c(.5, .5)))\n\n\n\n\n\n\n\n\n\nIn DAGs:\nadjusting for confounders blocks biasing paths,\nadjusting for mediators changes the estimand,\nadjusting for colliders creates bias.\nR makes these ideas concrete.\nSecond, the arrow just tells us that one variable causes another. It doesn‚Äôt say anything about whether that causal effect is positive or negative. All we have on the diagram is that the outcome of the coin flip affects your cake status."
  },
  {
    "objectID": "posts/038_27Feb_2026/index.html#the-basics",
    "href": "posts/038_27Feb_2026/index.html#the-basics",
    "title": "A Gentle Introduction to Directed Acyclic Graphs (DAGs)",
    "section": "3 The Basics",
    "text": "3 The Basics\n\n3.1 What are Directed Acyclic Graphs?\nA Directed Acyclic Graph (DAG) is a visual map used to represent the assumed causal relationships between variables, and to identify confounding. They are relatively recent development in the field of causal inference, based on formal logic and machine learning research from the 1990‚Äôs. In the modern era, DAGs are critical in much observational research and I think it is now safe to say that you won‚Äôt see your manuscript accepted to a reputable journal if you plan to conduct an explicit causal analysis, without one.\nThe following is an example DAG taken from a recent paper visualising all theoretical variable pathways in the proposed relationship between various chronic lung diseases and lung cancer.\n\n\n\n\n\nThis is a fairly complex DAG, so don‚Äôt be put off by it. Indeed, while this figure is visually arresting, at their core every DAG contains only two basic structures:\n\nThe variables in the postulated data-generating process (DGP), each represented by a ‚Äúnode‚Äù on the diagram, and:\nThe causal relationships (or pathways) in the postulated DGP, each represented by an arrow from the cause variable to the caused variable (i.e.¬†effect).\n\nThus, a DAG is:\n\nDirected: arrows indicate causal direction between two variables,\nAcyclic: arrows can never form a closed/feedback loop (e.g., A‚ÜíB‚ÜíC‚ÜíA). This reflects the reality that a cause must precede its effect in time; you cannot be your own ancestor.\nGraphical: variables are nodes, relationships are paths.\n\nWith this knowledge and the context of the above example then, one could make statements such as:\n\n‚ÄúLung disease ‚Äòcauses‚Äô lung cancer‚Äù, or:\n‚ÄúAge ‚Äòcauses‚Äô lung disease‚Äù, or:\n‚ÄúSmoking ‚Äòcauses‚Äô lung cancer‚Äù.\n\nSome such statements we may not know with certainty to be true, but this is ok. In fact, an arrow in a DAG is almost always a hypothesised assumption rather than a proven fact. One of the most powerful features of a DAG is that it functions as a ‚Äúhonesty check‚Äù in that it forces you to move your private mental model into a public, visual space where others can critique it.\n\n\n3.2 The Building Block Paths of a DAG\nJust as there are only two structural components to a DAG, there are only three types of paths (i.e.¬†sequence of arrows) that you need to think about. Once you have a handle on these you are well on your way to not only being able to interpret a DAG, but construct one of your own. The building block paths are:\n\nChains:\n\nA ‚Üí B ‚Üí C\n\nForks:\n\nA ‚Üê B ‚Üí C\n\nInverted Forks (Colliders):\n\nA ‚Üí B ‚Üê C\n\n\nLet‚Äôs visualise each of these with some R code and the ggdag and daggity packages.\n\n\nCode\nlibrary(ggdag)\nlibrary(dagitty)\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(kableExtra)\n\n# Define node coordinates\ndag_coords &lt;- tibble::tribble(\n  ~name, ~x,  ~y,\n  \"C\",    0,   1,    # Confounder/Collider/Mediator at top\n  \"E\",   -1,   0,    # Exposure at bottom left\n  \"O\",    1,   0     # Outcome at bottom right\n)\n\n# Encode DAGs for each path type\nChain &lt;- dagify(\n  C ~ E,\n  O ~ C,\n  coords = dag_coords,\n  exposure = \"E\",\n  outcome = \"O\")\n\nFork &lt;- dagify(\n  E ~ C,\n  O ~ C,\n  coords = dag_coords,\n  exposure = \"E\",\n  outcome = \"O\")\n\nCollider &lt;- dagify(\n  C ~ E,\n  C ~ O,\n  coords = dag_coords,\n  exposure = \"E\",\n  outcome = \"O\")\n\n# Merge plots\ndag_flows &lt;- map(list(Chain = Chain, Fork = Fork, Collider = Collider), tidy_dagitty) |&gt;\n  map(\"data\") |&gt;\n  list_rbind(names_to = \"dag\") |&gt;\n  mutate(dag = factor(dag, levels = c(\"Chain\", \"Fork\", \"Collider\")))\n\n# Add labels based on dag type and node name\ndag_flows &lt;- dag_flows |&gt;\n  mutate(label = case_when(\n    dag == \"Chain\" & name == \"C\" ~ \"Mediator\",\n    dag == \"Fork\" & name == \"C\" & xend == -1 ~ \"Confounder\", # specify xend to stop 2 confounder labels\n    dag == \"Collider\" & name == \"C\" ~ \"Collider\",\n    name == \"E\" ~ \"Exposure\",\n    name == \"O\" ~ \"Outcome\",\n    TRUE ~ \"\"\n  ))\n\n# Plot\nset.seed(131) # Set and check label positions don't obscure arrowheads\ndag_flows |&gt;\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges(edge_width = 1) +\n  geom_dag_point() +\n  geom_label_repel(aes(label = label),\n                 size = 3.5,\n                 box.padding = 0.5,\n                 point.padding = 0.5,\n                 fill = \"white\",\n                 label.padding = unit(0.25, \"lines\"),\n                 label.size = 0.25) +\n  facet_wrap(~dag) +\n  expand_plot(\n    expand_x = expansion(c(0.2, 0.2)),\n    expand_y = expansion(c(0.2, 0.2))\n  ) +\n  theme_dag()\n\n\n\n\n\n\n\n\n\nAs you can see there are some fundamental similarities and differences in each path type. The exposure and outcome are common to all - it‚Äôs the junction variable that sits between these that determine the path behaviour (i.e.¬†whether a causal effect is transmitted and in what direction). Let‚Äôs briefly discuss these variables as understanding their influence on a path is fundamental to understanding a DAG.\nA confounder is a variable that causally affects both the exposure and the outcome. Because it creates a non-causal association between exposure and outcome, failing to adjust for a confounder leads to biased effect estimates. For example, when studying the effect of coffee consumption on heart disease, smoking is a confounder: smokers tend to drink more coffee, and smoking independently increases heart disease risk. Adjusting for smoking blocks this path and helps isolate the causal effect of coffee.\nIn contrast, a mediator lies on the causal pathway from exposure to outcome and represents part of the mechanism by which the exposure exerts its effect. Mediators are not sources of bias; they are part of the effect itself. Adjusting for a mediator therefore changes the scientific question by removing indirect effects. For instance, in a study of physical activity and cardiovascular mortality, blood pressure is a mediator: physical activity lowers blood pressure, which in turn reduces mortality risk. If you adjust for blood pressure, you no longer estimate the total effect of physical activity, but only the effect that does not operate through blood pressure.\nA collider is a variable that is caused by two (or more) other variables. Unlike confounders, colliders do not create bias unless you condition (i.e.¬†adjust) for them - at which point they induce a spurious association between their causes. For example, suppose you study the relationship between genetic risk and occupational exposure among hospital patients, where hospital admission is influenced by both genetics and exposure. Hospital admission is a collider: restricting the analysis to hospitalised patients (i.e.¬†conditioning on the collider) creates an artificial association between genetic risk and exposure that does not exist in the general population.\nOK, that‚Äôs all a bit to take in - what are the practical takeaways you might ask. Let‚Äôs imagine you are looking at the DAG you have just created trying to decide which variables you should adjust for in your regression model. You want to estimate the causal effect of the exposure on your outcome of interest. There are three basic rules to follow:\n\nDO adjust for confounders (common causes of exposure and outcome)\nDO NOT adjust for colliders (common eÔ¨Äects of exposure and outcome)\nDO NOT adjust for mediators (variables on the causal pathway)*\n\n(* In a mediation analysis, you DO adjust for the mediator to estimate the direct eÔ¨Äect of the exposure on the outcome adjusted for the mediator).\nWe will put this into practice with an example shortly, but before we do, it‚Äôs important to first introduce a couple more DAG-related concepts.\n\n\n3.3 Backdoor, Frontdoor, Open and Closed Paths\nWe‚Äôve already established that paths describe the routes by which causation can flow between an exposure and an outcome. But now let‚Äôs flesh that idea out in a little more detail. A backdoor path is any path that connects exposure to outcome and begins with an arrow into the exposure (for example, Exposure ‚Üê Confounder ‚Üí Outcome). These paths represent non-causal sources of association - typically confounding - and, if left open, they bias causal effect estimates. A frontdoor path, by contrast, starts with an arrow out of the exposure (Exposure ‚Üí ‚Ä¶ ‚Üí Outcome) and represents the causal effect you are trying to estimate.\n\nThe fundamental goal when interpreting a DAG is therefore to block all backdoor paths while leaving frontdoor paths intact.\n\nPaths can also be open or closed. Whether a path is open or closed determines whether it transmits causation. A path is open by default unless it is blocked by conditioning on a non-collider (such as a confounder) or by the presence of a collider that is not conditioned on. In other words, if you have a collider in your path, that path is already closed and there is nothing further you need to do. In this case, adjusting for the collider will re-open the path. Above I gave you three rules to follow for whether to adjust for a junction variable or not - let me now explain why:\n\nConditioning on a confounder closes a backdoor path and removes bias;\n\n\nConditioning on a mediator closes part of a frontdoor path and changes the estimand;\n\n\nConditioning on a collider opens a path that was previously closed and induces bias.\n\nInterpreting a DAG means systematically listing all backdoor paths between exposure and outcome, identifying which variables close those paths without opening new ones, and using that minimal set for adjustment. It‚Äôs in this way that DAGs provide a clear, causal logic for deciding what to adjust for and what to avoid, independent of statistical significance or model fit.\nLet‚Äôs now illustrate all of these concepts with a simulated example of a research question and its associated DAG."
  },
  {
    "objectID": "posts/038_27Feb_2026/index.html#a-realistic-epidemiologic-example",
    "href": "posts/038_27Feb_2026/index.html#a-realistic-epidemiologic-example",
    "title": "A Gentle Introduction to Directed Acyclic Graphs (DAGs)",
    "section": "4 A Realistic Epidemiologic Example",
    "text": "4 A Realistic Epidemiologic Example\nThe example we‚Äôre going to use today is based on actual research investigating the effect of air pollution on cardiovascular disease - the question we are asking is: ‚ÄúWhat is the causal effect of long-term PM2.5 exposure on cardiovascular mortality?‚Äù For the sake of brevity, I am not going to discuss the background literature. While I haven‚Äôt drawn on any particular piece of research in creating this example, if you are interested in reading more about the topic, this would be a good place to start.\n\n\n\n\n\n\nNote\n\n\n\nPM2.5 refers to particulate matter in air that is less than 2.5 Œºm or less in diameter. The reason that small particulate matter is significant is that it can be inhaled into the lungs and lead to adverse health effects.\n\n\nWe‚Äôve spent a bit of time thinking about this research question in terms of all potential variables that might be involved in the relationship between PM2.5 exposure, its causal risk effect for cardiovascular mortality, and their interplay with each other. In the end we‚Äôve come up with the following list of variables, which thankfully, we also have data available for:\n\nPM25: air pollution exposure\nMortality: cardiovascular death\nAge\nSmoking\nSES: socioeconomic status\nCVD: pre-existing cardiovascular disease\nHealthcare: healthcare utilisation\n\n\n4.1 Encoding the Causal Assumptions and DAG Visualisation\nHere is the resulting DAG which represents the encoding of our assumptions about the potential relationships between exposure, outcome and all other secondary variables. It may look imposing, but let‚Äôs break it down. We start out by listing all possible causations between pairs of variables - for example, we might postulate that age ‚Äúcauses‚Äù PM25, age ‚Äúcauses‚Äù mortality, SES ‚Äúcauses‚Äù PM25, and so on. Once we have exhausted all possible relationships we can specify the DAG as shown in the code below, whereby age ‚Äúcauses‚Äù PM25 is encoded in the dagify() function as pm25 ~ age, using the identical syntax to what we write when specifying a regression model. Plotting the DAG then becomes a trivial exercise.\n\n\nCode\n# Encode DAG\nairpollution_dag &lt;- dagify(\n  pm25 ~ age,\n  mortality ~ age,\n  pm25 ~ ses,\n  smoking ~ ses, \n  healthcare ~ ses,\n  cvd ~ smoking,\n  mortality ~ smoking,\n  cvd ~ pm25,\n  mortality ~ cvd,\n  healthcare ~ cvd,\n  labels = c(\n    pm25 = \"PM25\",\n    mortality = \"Mortality\",\n    age = \"Age\",\n    ses = \"SES\",\n    smoking = \"Smoking\",\n    healthcare = \"Healthcare\",\n    cvd = \"CVD\"),\n  #coords = dag_coords,\n  exposure = \"pm25\",\n  outcome = \"mortality\")\n\n# Plot DAG\nset.seed(137)\nairpollution_dag |&gt; \n  ggdag(text = FALSE, use_labels = \"label\", stylized = TRUE) +\n  theme_dag() +\n  expand_plot(expand_x = expansion(c(.5, .5)))\n\n\n\n\n\n\n\n\n\n\n\n4.2 Interpreting the DAG\nWe then want to interpret the DAG so we can make some logical attempt to justify what variables to include, exclude, adjust and not adjust for in our regression model. To do this, at least initially, it is helpful to enumerate all possible paths contained in the DAG that exist between the exposure (PM25) and the outcome (mortality).\nThere are two ways to do this - the easy way and the hard way. The hard way involves manually tracing out every path based on the DAG and writing them down. For obvious reasons this is error-prone and subject to missing some of the paths, but it can be helpful as a learning exercise. The easy way is as simple as asking the daggity package to do it for you. In the code block below, you‚Äôll see that I have extended this functionality by writing a function to also output ‚Äústatus‚Äù (path open vs closed) and ‚Äútype‚Äù (frontdoor vs backdoor) information. Running this on the current DAG object generates the following table:\n\n\nCode\n# Function to enumerate all paths, status and type\nextract_dag_paths &lt;- function(dag) {\n  require(dagitty)\n  require(dplyr)\n  require(purrr)\n  \n  # Get all paths between exposure and outcome\n  all_paths &lt;- paths(dag)\n  \n  # Create dataframe\n  path_df &lt;- data.frame(\n    path = all_paths$paths,\n    stringsAsFactors = FALSE\n  )\n  \n  # Add open/closed status\n  path_df$status &lt;- ifelse(all_paths$open, \"open\", \"closed\")\n  \n  # Determine path type\n  path_df$type &lt;- map(path_df$path, function(p) {\n    # Check if path contains any &lt;- arrows (backdoor indicator)\n    if (grepl(\"&lt;-\", p)) {\n      return(\"backdoor\")\n    } else {\n      # All arrows point forward\n      return(\"frontdoor\")\n    }\n  })\n  \n  # Reorder columns for clarity\n  path_df &lt;- path_df |&gt; \n    arrange(desc(type), desc(status)) |&gt; \n    mutate(id = row_number()) |&gt; \n    select(id, path, status, type) \n\n  \n  return(path_df)\n}\n\n# Print\npath_summary &lt;- extract_dag_paths(airpollution_dag)\nkable(path_summary)\n\n\n\n\n\n\n\n\n\n\n\nid\npath\nstatus\ntype\n\n\n\n\n1\npm25 -&gt; cvd -&gt; mortality\nopen\nfrontdoor\n\n\n2\npm25 &lt;- age -&gt; mortality\nopen\nbackdoor\n\n\n3\npm25 &lt;- ses -&gt; smoking -&gt; cvd -&gt; mortality\nopen\nbackdoor\n\n\n4\npm25 &lt;- ses -&gt; smoking -&gt; mortality\nopen\nbackdoor\n\n\n5\npm25 -&gt; cvd -&gt; healthcare &lt;- ses -&gt; smoking -&gt; mortality\nclosed\nbackdoor\n\n\n6\npm25 -&gt; cvd &lt;- smoking -&gt; mortality\nclosed\nbackdoor\n\n\n7\npm25 &lt;- ses -&gt; healthcare &lt;- cvd -&gt; mortality\nclosed\nbackdoor\n\n\n8\npm25 &lt;- ses -&gt; healthcare &lt;- cvd &lt;- smoking -&gt; mortality\nclosed\nbackdoor\n\n\n\n\n\n\nThis tells us that we have 8 paths at play between the exposure and the outcome. It also tells us straight off the bat that we don‚Äôt need to worry about 4 of these - the ones that are ‚Äúclosed‚Äù (id‚Äôs 5-8). The reason these are already closed is because a collider is present in each - Healthcare in paths 5, 7 and 8, and CVD in path 6. This leaves us with 4 open paths - and we can visualise these quite easily with the ggdag_paths() function in the following way:\n\n\nCode\nset.seed(137)\nairpollution_dag |&gt; \n  ggdag_paths(text = FALSE, use_labels = \"label\", stylized = TRUE) +\n  expand_plot(expand_x = expansion(c(.5, .5)))\n\n\n\n\n\n\n\n\n\n\n\n4.3 Identifying the Minimally Sufficient Adjustment Set\nOf these 4 open paths, the first is a frontdoor path (and thus our main path of interest in determining the causal effect), and the remainder are backdoor paths. We therefore need to find a way to close paths 2 - 4, and the easiest way to do that is to condition on a confounder. Looking at those paths a natural choice would be to adjust for Age (path 2) and SES (paths 3 and 4) as these are confounders. Handily, there‚Äôs an algorithmic way to check this as well - by using the ggdag_adjustment_set() function as follows:\n\n\nCode\nset.seed(137)\nairpollution_dag |&gt; \n  ggdag_adjustment_set(text = FALSE, use_labels = \"label\", shadow = TRUE, stylized = TRUE,\n    exposure = \"pm25\",\n    outcome  = \"mortality\") +\n  theme_dag()\n\n\n\n\n\n\n\n\n\nSo what does this tell us? Indeed {Age, SES} is one potential adjustment set, but there is also another - {Age, Smoking}. Which should we choose? Well, this is is a case of daggity telling us what can work, but we should use our own causal reasoning to decide what should work more appropriately. From dagitty‚Äôs point of view: ‚ÄúGiven the assumptions encoded in this DAG, either of these sets is sufficient to identify the causal effect.‚Äù But that doesn‚Äôt mean that each set is equally defensible scientifically.\nThe first set blocks all backdoor paths without conditioning on anything downstream of exposure. This is the cleanest and most interpretable adjustment set, and it corresponds to the total causal effect one sets out to estimate. In contrast, the second set includes a variable (Smoking) that is a descendant of SES). This set works only because conditioning on Smoking incidentally blocks the same backdoor path that SES also blocks - so while the second set is formally sufficient, it is causally inferior.\nWhen dagitty gives you multiple valid sets, you can apply this hierarchy to hopefully make a more reasoned decision:\n\nPrefer pre-exposure variables over post-exposure or downstream variables\nPrefer true common causes over descendants of common causes\nAvoid mediators and their descendants if estimating total effects\nAvoid colliders or ‚Äúselection‚Äù variables even if they appear in valid sets\nChoose the smallest, simplest set that aligns with the scientific question\nUnder these principles, {Age, SES} clearly dominates any alternative."
  },
  {
    "objectID": "posts/038_27Feb_2026/index.html#some-final-thoughts",
    "href": "posts/038_27Feb_2026/index.html#some-final-thoughts",
    "title": "A Gentle Introduction to Directed Acyclic Graphs (DAGs)",
    "section": "5 Some Final Thoughts",
    "text": "5 Some Final Thoughts\n\n5.1 Why Not Adjust for Everything?\nAs you can tell, there‚Äôs a lot of work in justifying your causal model. While it may be tempting to adopt a ‚Äúkitchen sink‚Äù approach by adjusting for every available variable to avoid model misspecification, this strategy often backfires. First, over-adjusting leads to statistical inefficiency; by consuming precious degrees of freedom, you dilute the model‚Äôs power and risk overfitting, especially in smaller datasets or when variables are highly correlated. Second, and more critically, blindly adding covariates can introduce bias rather than remove it. If you inadvertently adjust for a collider you can backdoor paths and induce bias as I previously mentioned. Ultimately, a model packed with irrelevant or harmful variables obscures the true relationship and can lead to fundamentally flawed causal conclusions.\n\n\n5.2 What DAGs Don‚Äôt Do\nWhile DAGs are indispensable for mapping causal structures, they are not a panacea for the inherent risks of observational research. Most importantly, a DAG is only as good as the assumptions it visualises; if your graph is missing a crucial arrow or node, it cannot magically guarantee causal identification or alert you to the presence of unmeasured confounding that might still be biasing your results. Furthermore, a DAG is a qualitative tool that guides model selection, but it does not replace the need for rigorous sensitivity analyses to test how robust your findings are to potential ‚Äúhidden‚Äù biases or alternative structures. Ultimately, a DAG is not a machine that produces a ‚Äúcorrect‚Äù answer, but rather a logical framework that ensures your statistical modeling choices - specifically which variables you choose to include or exclude - are strictly coherent with the specific causal question you are attempting to answer.\n\n\n5.3 Should You Include a DAG in Your Next Paper?\nMaybe the answer to this question should be yes.\nI don‚Äôt think we‚Äôre at the stage where journals are requiring DAGs be supplied as part of a submission, but it is becoming increasingly common for reviewers/editors to ask for one in review if they aren‚Äôt convinced a specified model is correct. When a reviewer asks, ‚ÄúWhy didn‚Äôt you control for Variable X?‚Äù, being able to point to a DAG and say, ‚ÄúVariable X is a collider (or mediator) according to current domain knowledge, and adjusting for it would introduce bias,‚Äù is a much stronger rebuttal than simply stating it wasn‚Äôt available in the data.\nEven when not required, DAGs:\n\njustify confounder selection,\nexpose inappropriate adjustment,\nclarify estimands,\nand reduce reviewer disagreement.\n\nIt is not a stretch to say that a single DAG can replace pages of methodological explanation.\n\n\n5.4 Really, The End!\nIf you are doing observational research and making causal claims, you already have a DAG in your head. Writing it down and then visualising it - using tools like dagitty and ggdag - makes those assumptions visible, testable, and discussable. It could be argued that in modern epidemiology and biostatistics, DAGs are not an optional extra. They are part of responsible study design.\nI hope you didn‚Äôt find this post too heavy going. See you next month!"
  },
  {
    "objectID": "posts/041_29May_2025/index.html",
    "href": "posts/041_29May_2025/index.html",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "",
    "text": "As a clinician/researcher you might not be aware that data-driven variable selection methods are widely regarded within the statistical community as the the equivalent of Facebook‚Äôs ‚ÄòCrazy Uncle‚Äô - an individual who is misinformed, opinionated and someone we just can‚Äôt easily shed from our lives.\nSo what‚Äôs the parallel I‚Äôm trying to draw, you might ask?\nWell, despite decades of criticism in the statistical literature, stepwise regression (as a form of data-driven variable selection) continues to appear in academic papers, regulatory submissions, and industry analyses, when it is universally recognised as plain wrong. Despite that, stepwise regression remains one of the most commonly taught and used model-building techniques in applied statistics. Indeed, I was taught these methods during my Masters of Biostatistics 10 years ago. The appeal is obvious: when faced with many candidate predictors and limited sample size, stepwise procedures promise an automated, objective way to ‚Äúlet the data decide‚Äù which variables matter.\n\n\n\n\n\n\nNote\n\n\n\nWhen I refer to ‚Äúdata-driven‚Äù or ‚Äústepwise‚Äù I am meaning any decision process leading to the addition or removal of candidate variables from a regression model that is based on data-derived statistics such as significance, R-squared, Akaike Information Criteria (AIC), etc‚Ä¶\n\n\nSo then, what‚Äôs the problem?\nWell, there are many. The most notable I will summarise from page 68 of Frank Harrell‚Äôs modern statistical classic - Regression Modelling Strategies:\n\nThe R-squared or even adjusted R-squared values of the end model are biased high.\nThe F and Chi-square test statistics of the final model do not have the claimed distribution.\nThe standard errors of coefficient estimates are biased low and confidence intervals for effects and predictions are falsely narrow.\nThe p values are too small (there are severe multiple comparison problems in addition to problems 2. and 3.) and do not have the proper meaning, and it is difficult to correct for this.\nThe regression coefficients are biased high in absolute value and need shrinkage but this is rarely done.\nVariable selection is made arbitrary by collinearity.\nIt allows us to not think about the problem.\n\nThe net effect of the above is that we may end up making claims, assertions or clinical decisions based on incorrect evidence.\nIn this post I will attempt to make the case that stepwise regression is not just suboptimal, but seriously flawed when used for inference, explanation, or decision-making. The problems are not subtle or academic; they directly undermine the validity, reproducibility, and interpretability of results. I will outline why stepwise regression fails, why these failures are amplified in applied research (particularly clinical and epidemiological settings), and what modern, defensible alternatives look like in practice.\nThe goal is not to shame analysts who use stepwise regression ‚Äî I too am guilty of the multitude of sins I am about to discuss ‚Äî but to provide a clear roadmap towards best practice."
  },
  {
    "objectID": "posts/041_29May_2025/index.html#introduction",
    "href": "posts/041_29May_2025/index.html#introduction",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "",
    "text": "As a clinician/researcher you might not be aware that data-driven variable selection methods are widely regarded within the statistical community as the the equivalent of Facebook‚Äôs ‚ÄòCrazy Uncle‚Äô - an individual who is misinformed, opinionated and someone we just can‚Äôt easily shed from our lives.\nSo what‚Äôs the parallel I‚Äôm trying to draw, you might ask?\nWell, despite decades of criticism in the statistical literature, stepwise regression (as a form of data-driven variable selection) continues to appear in academic papers, regulatory submissions, and industry analyses, when it is universally recognised as plain wrong. Despite that, stepwise regression remains one of the most commonly taught and used model-building techniques in applied statistics. Indeed, I was taught these methods during my Masters of Biostatistics 10 years ago. The appeal is obvious: when faced with many candidate predictors and limited sample size, stepwise procedures promise an automated, objective way to ‚Äúlet the data decide‚Äù which variables matter.\n\n\n\n\n\n\nNote\n\n\n\nWhen I refer to ‚Äúdata-driven‚Äù or ‚Äústepwise‚Äù I am meaning any decision process leading to the addition or removal of candidate variables from a regression model that is based on data-derived statistics such as significance, R-squared, Akaike Information Criteria (AIC), etc‚Ä¶\n\n\nSo then, what‚Äôs the problem?\nWell, there are many. The most notable I will summarise from page 68 of Frank Harrell‚Äôs modern statistical classic - Regression Modelling Strategies:\n\nThe R-squared or even adjusted R-squared values of the end model are biased high.\nThe F and Chi-square test statistics of the final model do not have the claimed distribution.\nThe standard errors of coefficient estimates are biased low and confidence intervals for effects and predictions are falsely narrow.\nThe p values are too small (there are severe multiple comparison problems in addition to problems 2. and 3.) and do not have the proper meaning, and it is difficult to correct for this.\nThe regression coefficients are biased high in absolute value and need shrinkage but this is rarely done.\nVariable selection is made arbitrary by collinearity.\nIt allows us to not think about the problem.\n\nThe net effect of the above is that we may end up making claims, assertions or clinical decisions based on incorrect evidence.\nIn this post I will attempt to make the case that stepwise regression is not just suboptimal, but seriously flawed when used for inference, explanation, or decision-making. The problems are not subtle or academic; they directly undermine the validity, reproducibility, and interpretability of results. I will outline why stepwise regression fails, why these failures are amplified in applied research (particularly clinical and epidemiological settings), and what modern, defensible alternatives look like in practice.\nThe goal is not to shame analysts who use stepwise regression ‚Äî I too am guilty of the multitude of sins I am about to discuss ‚Äî but to provide a clear roadmap towards best practice."
  },
  {
    "objectID": "posts/041_29May_2025/index.html#why-stepwise-regression-persists",
    "href": "posts/041_29May_2025/index.html#why-stepwise-regression-persists",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "2 Why Stepwise Regression Persists",
    "text": "2 Why Stepwise Regression Persists\nLet‚Äôs get the elephant in the room out of the way. If there are so many issues with stepwise regression why is it still so commonly used in the analysis of clinical data? Well, I don‚Äôt think there is a single answer to this question, but rather several potential factors that probably interplay in maintaining its veneer of contemporary methodological relevance:\n\nHistorical inertia: it has been taught for decades and appears in older textbooks.\nFear of change: researchers may worry that journal editors/reviewers do not appreciate new approaches.\nCognitive appeal: produces a single, seemingly parsimonious model.\nEase of implementation: many statistical packages make stepwise selection easy and prominent.\nMisplaced objectivity: automated procedures appear neutral, even when they enforce arbitrary thresholds on our data.\n\nThese justifications, when considered on an individual basis, become hard to defend. But before we look at what we can do instead, let‚Äôs first delve a little deeper into the specific problems associated with stepwise regression."
  },
  {
    "objectID": "posts/041_29May_2025/index.html#what-is-stepwise-regression",
    "href": "posts/041_29May_2025/index.html#what-is-stepwise-regression",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "3 What is Stepwise Regression?",
    "text": "3 What is Stepwise Regression?\nMost of you already know what I‚Äôm talking about, but let‚Äôs recap the basics for those who don‚Äôt.\nStepwise regression refers to a family of automated variable selection procedures applied to regression models. The most common variants are:\n\nForward selection: start with no predictors, then add variables one at a time based on some criterion (often the smallest p-value or largest improvement in AIC).\nBackward elimination: start with all candidate predictors, then remove variables that fail to meet a significance threshold.\nBidirectional (stepwise) selection: alternate between adding and removing variables at each step.\n\nThese procedures are typically driven by hypothesis tests (e.g.¬†p &lt; 0.05) or information criteria (AIC, BIC), and they terminate once no further steps improve the chosen criterion. The end result is a single model containing a data-driven subset of the original pool of available candidate predictor variables."
  },
  {
    "objectID": "posts/041_29May_2025/index.html#the-core-statistical-problems",
    "href": "posts/041_29May_2025/index.html#the-core-statistical-problems",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "4 The Core Statistical Problems",
    "text": "4 The Core Statistical Problems\nAlthough I mentioned these at the outset of this post (based on Harrell‚Äôs text), let‚Äôs flesh some of them out in more detail now.\n\n4.1 Invalid Inference After Selection\nPerhaps the most fundamental problem is that standard inferential quantities are wrong after stepwise selection. When using data-driven variable selection:\n\nP-values, confidence intervals, and standard errors are computed as if the model had been specified in advance.\n\nBut in reality, the data were used at least twice: once to select variables, and again to estimate their effects.\nThis reuse of data leads to:\n\nunderestimated standard errors,\noverly narrow confidence intervals,\nand inflated statistical significance.\n\nYou may not be aware that statistical inference methods using p values, standard errors and confidence intervals were designed to be valid when applied once to a pre-specified model, not iteratively reapplied to a model where chance partly informs the decision at each step. This invalidates the nominal properties of a statistical test, giving us as researchers, a false sense of certainty.\n\n\n4.2 Multiple Testing in Disguise\nStepwise regression performs a large number of implicit hypothesis tests while pretending to conduct only a few. Each step involves testing multiple candidate variables, but no adjustment is made for multiplicity.\nViewed correctly, stepwise selection is a form of uncorrected multiple testing with a stopping rule. The resulting family-wise Type I error rate can be dramatically higher than the nominal level, especially when predictors are correlated.\nThis means that ‚Äústatistically significant‚Äù predictors selected by stepwise methods are often artifacts of chance rather than real signals.\n\n\n4.3 Overfitting and Optimism\nStepwise procedures are designed to optimise in-sample fit. As a result, they tend to overfit, especially when:\n\nthe number of predictors is large relative to the sample size,\npredictors are correlated,\nor the true signal-to-noise ratio is modest.\n\nOverfitted models appear impressive in the training data but perform poorly on new data. This optimism is rarely quantified or acknowledged when stepwise regression is used.\n\n\n4.4 Model Instability\nOne of the most damaging properties of stepwise regression is instability. Small perturbations in the data - e.g.¬†removing a few observations, changing a significance threshold, or using a different random split - can produce entirely different selected models.\nThis instability arises because stepwise selection operates near decision boundaries. When predictors are correlated or effects are weak, tiny changes can flip inclusion decisions.\nAn unstable model cannot be trusted for interpretation, explanation, or clinical decision-making.\n\n\n4.5 Biased Coefficient Estimates\nEven when a predictor truly has an effect, stepwise regression tends to overestimate its magnitude if it is selected. This phenomenon - sometimes called the ‚Äúwinner‚Äôs curse‚Äù - occurs because variables are selected given they look large in the sample. The result is that reported effect sizes tend to be exaggerated, with subsequent studies often failing to replicate the findings."
  },
  {
    "objectID": "posts/041_29May_2025/index.html#why-these-problems-matter-in-applied-research",
    "href": "posts/041_29May_2025/index.html#why-these-problems-matter-in-applied-research",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "5 Why These Problems Matter in Applied Research",
    "text": "5 Why These Problems Matter in Applied Research\nIn some purely predictive settings, poor inference may be tolerable. In most applied research, however, regression models are used to:\n\ndraw causal conclusions,\ninform clinical or policy decisions,\nsupport regulatory submissions,\nor generate scientific knowledge.\n\nIn these contexts, stepwise regression is especially problematic.\n\n5.1 Clinical and Epidemiological Studies\nIn medical research, variable inclusion is often interpreted causally - even when analysts disclaim causal intent. A covariate that survives stepwise selection may be described as a ‚Äúrisk factor‚Äù or ‚Äúindependent predictor‚Äù, while excluded variables are implicitly treated as unimportant.\nBut this is deeply misleading. Stepwise regression does not distinguish between confounders, mediators, and colliders, and it frequently excludes clinically important variables simply because their effects are imprecisely estimated.\n\n\n5.2 Regulatory and Reporting Contexts\nIn regulatory settings, such as clinical trial reporting, transparency and reproducibility are paramount. Stepwise regression undermines both, because:\n\nthe analysis path is data-driven and difficult to justify prospectively,\nresults may not reproduce across datasets or populations,\nand inferential quantities lack a clear interpretation.\n\nFor these reasons, many regulatory guidelines explicitly discourage data-driven variable selection."
  },
  {
    "objectID": "posts/041_29May_2025/index.html#what-to-do-instead",
    "href": "posts/041_29May_2025/index.html#what-to-do-instead",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "6 What to Do Instead",
    "text": "6 What to Do Instead\nOk, we‚Äôve spent a fair amount of time talking about the problem. Let‚Äôs now discuss what we can actually do about it. The good news is that we are not short of alternatives. In fact, modern approaches are often simpler, more transparent, and more defensible.\nI am going to discuss these approaches in the context of ‚Äúmodel intent‚Äù - i.e.¬†what is the purpose for your regression model? This ultimately boils down to one of two options - explanation vs prediction. Model explanation and model prediction answer fundamentally different questions. Explanatory models aim to estimate and interpret the effect of variables, often with a causal lens, which makes pre-specification, subject-matter knowledge, and valid inference essential. An example research question that such a model could answer might be:\n\n‚ÄúWhat is the effect of DMT x on the risk of disease relapse, after adjusting for age, sex, smoking status, and baseline disease severity?‚Äù\n\ni.e.¬†we are interested primarily in the explanatory ‚Äòeffect‚Äô of a new DMT on disease relapse.\nPredictive models, by contrast, are judged by how well they perform on new data, not by whether their coefficients are interpretable or causally meaningful - in other words, we don‚Äôt really care about individual variable ‚Äòeffects‚Äô but rather just the overall predictive power of the model. Here, the commensurate research question takes a different form:\n\n‚ÄúUsing age, sex, smoking status, current therapy and disease severity data at baseline, what is an individual‚Äôs 5-year risk of disease relapse?‚Äù\n\ni.e.¬†we want to predict relapse risk but don‚Äôt really care about any individual pedictor.\n\n6.1 Explanation\n\n6.1.1 Pre-Specification and Subject-Matter Knowledge\nWhen we are interested in model explanation, the most robust approach is also the least glamorous: decide in advance (i.e.¬†prior to seeing the data) which variables belong in the model, and make that our one and only model.\nHere, variable inclusion should be guided by:\n\nscientific understanding,\ncausal reasoning,\nand study design.\n\nConsideration should be given to sythesising these justifications in a supporting causal diagram (DAG - see the previous post).\nNote that if a variable is a known confounder, it should be included regardless of its p-value. In contrast, if it is irrelevant to the scientific question, it should not be tested for inclusion.\nThis approach prioritises validity over convenience. We are very interested in understanding the characteristics of each individual predictor variable (i.e.¬†is it a potential confounder, collider, mediator, etc in the exposure -&gt; outcome relationship), and the interplay between all such variables. Again, this is where a causal diagram, even if we are not planning an explicit causal analysis, can be very helpful. But this is where the hard work ends in explanatory modelling. From a coding perspective, things are easier than ever - we just run whatever model we have decided on (and nothing more), comfortable in the fact that our p-values and confidence intervals are correct.\n\n\n6.1.2 Transparent Sensitivity Analyses\nIf, for whatever reason, pre-specification is not possible and variable selection cannot be avoided, resultant models should be treated as exploratory and accompanied by:\n\nsensitivity analyses,\nstability assessments,\nand clear disclaimers about inferential limitations.\n\nThis is still inferior to pre-specification, but far better than unqualified stepwise regression.\n\n\n\n6.2 Prediction\n\n6.2.1 Full Models with Shrinkage\nIn contrast, when we are interested in model prediction our goals change. We are simply after raw predictive performance and it matters less about the individual variables that comprise the model or how they were chosen. There is actually a fairly nice solution to the problem of variable selection in the context of prediction modelling, and that is ‚Äúregularisation‚Äù or ‚Äúpenalised regression‚Äù methods. These methods acknowledge uncertainty more honestly than stepwise procedures.\nRegularisation is central to prediction because it controls overfitting by shrinking model coefficients, trading a small amount of bias for a large reduction in variance and thus improving performance on new data. Ridge regression shrinks all coefficients toward zero and is particularly effective when many predictors have small, correlated effects. Lasso applies stronger shrinkage that can set some coefficients exactly to zero, producing sparse models that are easier to deploy but can be unstable when predictors are highly correlated. Elastic net combines ridge and lasso penalties, often giving the best predictive performance in practice by encouraging sparsity while retaining groups of correlated predictors.\nKeep reading to the end of this section to see an example of how penalised regression with Elastic net is used.\n\n\n6.2.2 Internal vs External Validation\nBecause we are ultimately interested in predictive power, once we have established a prediction model we should evaluate its performance. There are multiple ways to do this (see here for a good primer), with the main techniques being:\n\ncross-validation,\nbootstrap resampling,\nor, ideally, external validation datasets.\n\nThe general idea in each case is that we ‚Äútrain‚Äù our model on a dataset and then ‚Äútest‚Äù its predictive power on new data. The reason for this is that it is not uncommon when we create a model from a single dataset that the model usually fits the peculiarities of that dataset much better than one it hasn‚Äôt seen before (i.e.¬†it overfits). Thus, predictive performance will often be less on a ‚Äútest‚Äù compared to a ‚Äútrain‚Äù dataset. In cross-validation and bootstrapping we are using sleight of hand to create ‚Äútrain‚Äù and ‚Äútest‚Äù data from the only dataset we have - these are referred to as internal validation methods. While external validation is better because we can train the model on all of our data and then test on a completely unseen dataset, it is harder to achieve in practice for obvious reasons.\nWhatever the method of model performance evaluation, the focus now changes from p-values and confidence intervals to predictive accuracy and generalisability.\n\n\n6.2.3 Model Averaging\nSometimes we may end up with more than one candidate prediction model that have similar predictive power but are based on different modelling strategies (e.g.¬†lasso vs random forest vs GAM). Model averaging is an approach that accounts for uncertainty about model choice by combining predictions or estimates from multiple plausible prediction models rather than relying on a single selected model. Instead of treating one model as ‚Äúthe truth,‚Äù it weights each model according to a measure of support - such as predictive performance, information criteria, or posterior probability - and averages across them. This can improve predictive accuracy and reduce overconfidence, particularly when several models perform similarly well. Model averaging is most natural when there is genuine uncertainty among a discrete set of competing models, and it contrasts with single-model selection approaches that ignore this uncertainty.\n\n\n6.2.4 Elastic Net Example\nAlright, let‚Äôs put some of what we‚Äôve learned into practice. Let‚Äôs simulate some data and run both a stepwise regression and an elastic net, comparing the performance of each. For the simulation I will create a dataset with 500 observations and 20 predictors. I‚Äôll make the regression coefficients for the first 5 predictors non-zero (representing real effects) and set the coefficients for the last 15 to zero (representing noise). I‚Äôll also specify that all predictors are moderately correlated with each other (r = 0.4). With this data-generating process we can then meaningfully compare how stepwise regression and elastic net behave when only a few predictors truly matter (and hopefully find that the latter approach performs better).\n\n\nCode\nset.seed(123)\n\nn &lt;- 500\np &lt;- 20\n\nSigma &lt;- matrix(0.4, p, p)\ndiag(Sigma) &lt;- 1\n\nlibrary(MASS)\nX &lt;- mvrnorm(n, mu = rep(0, p), Sigma = Sigma)\n\nbeta &lt;- c(2, 2, 2, -1.5, -1.5, rep(0, p - 5))\ny &lt;- X %*% beta + rnorm(n, sd = 2)\n\ndat &lt;- data.frame(y, X)\ncolnames(dat) &lt;- c(\"y\", paste0(\"X\", 1:p))\n\n\nNow let‚Äôs create train and test datasets from the simulated data. To do this we‚Äôll randomly choose 70% of the observations for the train dataset and allocate the remaining 30% to the test dataset.\n\n\nCode\nset.seed(456)\nidx &lt;- sample(seq_len(n), size = 0.7 * n)\n\ntrain &lt;- dat[idx, ]\ntest  &lt;- dat[-idx, ]\n\n\nWe will now perform a classic stepwise regression using step() to automate the process. step() fits multiple models moving back and forth between a model with no predictors (intercept only model) and one with all 20 predictors (full model). At each stage it computes the AIC for each candidate model, moves to the model with the lowest AIC and then stops when no further addition or deletion of variables improves the AIC.\nWe can see that the first five predictors are correctly selected, but the algorithm also selects four noise predictors, two of which are deemed statistically significant.\n\n\nCode\nfull_mod &lt;- lm(y ~ ., data = train)\nnull_mod &lt;- lm(y ~ 1, data = train)\n  \nstep_mod &lt;- step(\n  null_mod,\n  scope = list(lower = null_mod, upper = full_mod),\n  direction = \"both\",\n  trace = FALSE\n)\n\nsummary(step_mod)\n\n\n\nCall:\nlm(formula = y ~ X1 + X3 + X5 + X2 + X4 + X15 + X10 + X14 + X18, \n    data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6759 -1.2016  0.0696  1.2556  5.7055 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.07313    0.10297   0.710   0.4781    \nX1           2.08653    0.11979  17.418   &lt;2e-16 ***\nX3           2.12815    0.13199  16.123   &lt;2e-16 ***\nX5          -1.61482    0.12589 -12.827   &lt;2e-16 ***\nX2           2.02325    0.12951  15.622   &lt;2e-16 ***\nX4          -1.58574    0.12656 -12.530   &lt;2e-16 ***\nX15          0.30294    0.13056   2.320   0.0209 *  \nX10         -0.32018    0.12951  -2.472   0.0139 *  \nX14          0.22121    0.13102   1.688   0.0922 .  \nX18         -0.20767    0.13261  -1.566   0.1183    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.907 on 340 degrees of freedom\nMultiple R-squared:  0.7858,    Adjusted R-squared:  0.7802 \nF-statistic: 138.6 on 9 and 340 DF,  p-value: &lt; 2.2e-16\n\n\nLet‚Äôs check the performance by using the model to predict on the test data, then calculating a common regression performance measure - the root mean square error (RMSE). The lower the RMSE, the better.\n\n\nCode\ny_pred_step &lt;- predict(step_mod, newdata = test)\nrmse_step &lt;- sqrt(mean((test$y - y_pred_step)^2))\nrmse_step\n\n\n[1] 2.018798\n\n\nNow let‚Äôs compare this to an elastic net, using the glmnet package, which fits penalised regression models (ridge, lasso, elastic net). We can break this code block down into a series of steps as follows:\n\nglmnet fits many elastic net models, each with:\n\nthe same predictors,\nthe same alpha = 0.5 (mix of ridge and lasso),\ndifferent values of lambda (penalty strength).\n\nIt then performs 10-fold cross-validation:\n\nthe training data are split into 10 folds,\neach fold is held out in turn,\nprediction error is estimated for each lambda.\n\nAnd finally, chooses the optimal penalty:\n\nlambda.min ‚Üí lowest cross-validated error,\nlambda.1se ‚Üí largest Œª within 1 SE of the minimum (simpler model).\n\n\nThis step answers: ‚ÄúHow much regularisation gives the best out-of-sample performance?‚Äù\n\n\nCode\nlibrary(glmnet)\n\n# glmnet requires predictors as a numeric matrix, not a data frame.\nX_train &lt;- as.matrix(train[, -1]) # contains all predictors but remove the outcome variable (y)\ny_train &lt;- train$y                # response\nX_test  &lt;- as.matrix(test[, -1])\ny_test  &lt;- test$y\n\ncv_enet &lt;- cv.glmnet(\n  X_train, y_train,\n  alpha = 0.5,                    # elastic net\n  nfolds = 10\n)\n\n# Extract the coefficients from the single elastic net model corresponding to the optimal Œª.\ncoef(cv_enet, s = \"lambda.min\")\n\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n              lambda.min\n(Intercept)  0.066848837\nX1           2.012777264\nX2           1.921988809\nX3           2.043667440\nX4          -1.469708904\nX5          -1.503814250\nX6           0.010503422\nX7          -0.111528143\nX8          -0.065850506\nX9          -0.080697600\nX10         -0.232749640\nX11          0.010956657\nX12          .          \nX13          0.120277174\nX14          0.166942224\nX15          0.237722152\nX16          0.030739719\nX17          0.058957699\nX18         -0.115478884\nX19         -0.046143403\nX20          0.004132131\n\n\nNote that even when the true data-generating process is sparse, elastic net does not necessarily recover the true set of predictors, particularly when predictors are correlated. This is not a failure: the elastic net objective is to minimise prediction error, not to identify the true model. At the penalty that optimises cross-validated performance, elastic net may retain many small coefficients because correlated noise variables still carry predictive information. In other words, if allowing small non-zero coefficients on ‚Äúnoise‚Äù predictors improves prediction even slightly, elastic net will do that.\nNow let‚Äôs compare the prediction error from both modelling approaches.\n\n\nCode\ny_pred_enet &lt;- predict(cv_enet, X_test, s = \"lambda.min\")\nrmse_enet &lt;- sqrt(mean((y_test - y_pred_enet)^2))\nrmse_enet\n\n\n[1] 1.999773\n\n\nCode\nc(\n  Stepwise_RMSE   = rmse_step,\n  ElasticNet_RMSE = rmse_enet\n)\n\n\n  Stepwise_RMSE ElasticNet_RMSE \n       2.018798        1.999773 \n\n\nThe elastic net is slightly better, but ultimately they‚Äôre pretty similar. But that doesn‚Äôt mean that this exercise has been a waste of time. Even when stepwise regression and elastic net achieve similar RMSE on a given test set, elastic net is preferable because it achieves this performance through explicit regularisation rather than discrete variable selection.\nPenalised models are:\n\nmore stable to sampling variation,\nhandle correlated predictors more gracefully,\nand control overfitting directly.\n\nThe apparent equivalence in point performance masks important differences in reliability and robustness, which become evident under resampling or repeated evaluation. Penalised regression does not eliminate sampling variability, but it responds to it smoothly. Unlike stepwise regression, which makes brittle include‚Äìexclude decisions, elastic net adjusts coefficients continuously, allowing predictive performance to remain stable even when individual coefficients fluctuate.\nAll of this is to say that if we ran these models many times across different datasets, penalised regression will still give us a relatively stable model and predictive performance - stepwise methods won‚Äôt."
  },
  {
    "objectID": "posts/041_29May_2025/index.html#a-pragmatic-takeaway",
    "href": "posts/041_29May_2025/index.html#a-pragmatic-takeaway",
    "title": "The Perils of Stepwise Regression - And what to do instead",
    "section": "7 A Pragmatic Takeaway",
    "text": "7 A Pragmatic Takeaway\nStepwise regression persists because it is easy, familiar, and superficially tidy. But its apparent simplicity masks deep statistical flaws that undermine inference, reproducibility, and scientific credibility.\nModern alternatives - pre-specification for explanatory modelling; and shrinkage, validation ¬± model-averaging for prediction modelling - are not only more principled, they are often easier to explain and defend.\nThe question is no longer whether stepwise regression is flawed. The question is why we continue to use it when better tools are readily available.\nIf you want to read more about this topic I‚Äôd suggest having a look at the following papers:\n\nGreenland S. Invited commentary: variable selection versus shrinkage in the control of multiple confounders. Am J Epidemiol. 2008 Mar 1;167(5):523-9; discussion 530-1. doi: 10.1093/aje/kwm355. Epub 2008 Jan 27. Erratum in: Am J Epidemiol. 2008 May 1;167(9):1142. PMID: 18227100.\nWalter S, Tiemeier H. Variable selection: current practice in epidemiological studies. Eur J Epidemiol. 2009;24(12):733-6. doi: 10.1007/s10654-009-9411-2. Epub 2009 Dec 5. PMID: 19967429; PMCID: PMC2791468.\nHeinze G, Wallisch C, Dunkler D. Variable selection - A review and recommendations for the practicing statistician. Biom J. 2018 May;60(3):431-449. doi: 10.1002/bimj.201700067. Epub 2018 Jan 2. PMID: 29292533; PMCID: PMC5969114.\nSauerbrei W, Perperoglou A, Schmid M, Abrahamowicz M, Becher H, Binder H, Dunkler D, Harrell FE Jr, Royston P, Heinze G; for TG2 of the STRATOS initiative. State of the art in selection of variables and functional forms in multivariable analysis-outstanding issues. Diagn Progn Res. 2020 Apr 2;4:3. doi: 10.1186/s41512-020-00074-3. PMID: 32266321; PMCID: PMC7114804.\n\nWell that‚Äôs pretty much it for this post folks. Hopefully you can start to incorporate some of these techniques into your next regression modelling endeavour. See you next month."
  }
]