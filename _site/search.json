[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSurvival Analysis - Under the Hood\n\n\n\n\n\n\ncode\n\n\nconcept\n\n\nsurvival\n\n\n\nIf you find survival analysis confusing - this post is for you.\n\n\n\n\n\nNov 1, 2024\n\n\n28 min\n\n\n\n\n\n\n\n\n\n\n\n\nBreaking Free From ‘The Cult of P’\n\n\n\n\n\n\nconcept\n\n\nfun\n\n\n\nA reminder not to hang all your hopes and dreams on one insignificant number (see what I did there)\n\n\n\n\n\nOct 18, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nEpi. 101 Lesson - Confounding\n\n\n\n\n\n\nanalysis\n\n\ncode\n\n\nconcept\n\n\nmodelling\n\n\n\nConfounding - how statistical adjustment relates to stratification.\n\n\n\n\n\nOct 4, 2024\n\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\n\nBiostats Book Club\n\n\n\n\n\n\nfun\n\n\nresources\n\n\n\nRule # 1. You do not talk about Biostats Book Club.\n\n\n\n\n\nSep 6, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\ntidylog - Console Messaging in R\n\n\n\n\n\n\ncode\n\n\n\nFinally get some feedback about what your data manipulations are actually doing.\n\n\n\n\n\nAug 23, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nLogarithms and Why They’re Important in Statistics\n\n\n\n\n\n\ncode\n\n\nconcept\n\n\nvisualisation\n\n\n\nHaving to deal with logarithms may send a shiver down your spine, but they’re not as hard as you may think\n\n\n\n\n\nAug 9, 2024\n\n\n21 min\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression - Under the Hood\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nmodelling\n\n\nvisualisation\n\n\nprobability\n\n\n\nSee how log-odds, odds and probability are all simply versions of each other - and fundamental to logistic regression.\n\n\n\n\n\nJul 26, 2024\n\n\n32 min\n\n\n\n\n\n\n\n\n\n\n\n\nAcademic Research - “What’s in a Title?”\n\n\n\n\n\n\nfun\n\n\n\n“That which we call a paper by any other title would have as much impact?”\n\n\n\n\n\nJun 14, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nReshaping Data - Think Before you Pivot\n\n\n\n\n\n\nconcept\n\n\ncode\n\n\n\nSome common-use scenarios for reshaping your data from wide to long format\n\n\n\n\n\nMay 31, 2024\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nOrdinal Logistic Regression for Ordinal Outcomes?\n\n\n\n\n\n\nanalysis\n\n\nconcept\n\n\ncode\n\n\nlogistic\n\n\nmodelling\n\n\n\nThe proportional odds model is the best choice when we are regressing an ordinal outcome.\n\n\n\n\n\nMay 17, 2024\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\ngtsummary - Your New Go-To for Tables\n\n\n\n\n\n\ncode\n\n\npresentation\n\n\n\nCreate summary and regression tables in a flash.\n\n\n\n\n\nMay 3, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nEverything is a Linear Model\n\n\n\n\n\n\nanalysis\n\n\nconcept\n\n\ncode\n\n\nmodelling\n\n\n\nThe t-test and linear model with one grouping variable are two sides of the same coin.\n\n\n\n\n\nApr 19, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nDon’t be Scared of Splines\n\n\n\n\n\n\nanalysis\n\n\nconcept\n\n\ncode\n\n\nmodelling\n\n\nvisualisation\n\n\n\nRestricted cubic splines give you the ultimate flexiblity in modelling continuous predictors.\n\n\n\n\n\nApr 5, 2024\n\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\n\nEasily view your data by a grouping variable\n\n\n\n\n\n\ncode\n\n\n\nUse by() to view your data by a grouping variable.\n\n\n\n\n\nMar 22, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nSimpson’s Paradox - Contextualised for Research Students\n\n\n\n\n\n\nconcept\n\n\ncode\n\n\nvisualisation\n\n\n\nSimpson’s Paradox is essentially an extreme form of confounding, characterised by a reversal of the effect estimate sign.\n\n\n\n\n\nMar 8, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nImmortal time bias - “The fallacy that never dies” (Part 2)\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nmodelling\n\n\nsurvival\n\n\nvisualisation\n\n\n\nLet’s investigate immortal time bias with a coded example.\n\n\n\n\n\nFeb 23, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nImmortal time bias - “The fallacy that never dies” (Part 1)\n\n\n\n\n\n\nconcept\n\n\nsurvival\n\n\n\nLearn why this misclassification of time in a survival analysis can seriously bias your results.\n\n\n\n\n\nFeb 16, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nPut your ggplot on steroids\n\n\n\n\n\n\nvisualisation\n\n\ncode\n\n\n\nPlotly adds some interactivity and can help clarify your data.\n\n\n\n\n\nFeb 2, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nA Very Merry Christmas\n\n\n\n\n\n\nfun\n\n\ncode\n\n\nvisualisation\n\n\n\nHo, Ho, Ho\n\n\n\n\n\nDec 8, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nIt pays to think like a Bayesian\n\n\n\n\n\n\npuzzle\n\n\nprobability\n\n\nBayesian\n\n\n\nBayesian reasoning is more in line with how we process chance in everyday life.\n\n\n\n\n\nDec 1, 2023\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nInteractions (effect modifiers) are important - don’t ignore them\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nmodelling\n\n\nlogistic\n\n\n\nKeep an open mind to interactions in your next model.\n\n\n\n\n\nNov 24, 2023\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nStats Tips - Welcome\n\n\n\n\n\n\nnews\n\n\n\nWelcome to what I hope can become a useful stats resource.\n\n\n\n\n\nNov 22, 2023\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/007_08Mar_2024/index.html",
    "href": "posts/007_08Mar_2024/index.html",
    "title": "Simpson’s Paradox - Contextualised for Research Students",
    "section": "",
    "text": "A thought exercise this week in possible ways to interpret the following plot. This comes from hypothetical (simulated) data that purports to assess research students productivity (measured by how quickly they can type) as a function of the number of standard cups of coffee they drink in a day.\n\n\n\n\n\n\nNote\n\n\n\nI am no way suggesting that these findings apply to MSNI research students who no doubt remain highly productive throughout their research careers.\n\n\n\n\nCode\nlibrary(bayestestR)\nlibrary(ggplot2)\nlibrary(dplyr)\n# Simulate some Simpson's paradox style data \nset.seed(253445)\ndat &lt;- simulate_simpson(\n  n = 50,\n  r = 0.5,\n  groups = 3,\n  difference = 2,\n  group_prefix = \"G_\"\n)\n# A couple of variable manipulations to get the variables to look the way I want\ndat$V2 &lt;- (dat$V2*10)+90\ndat$V1 &lt;- (dat$V1+1)/2\n# Rename groups\ndat &lt;- dat |&gt; \n  mutate(Group = case_when(Group == \"G_1\" ~ \"Honours\",\n                           Group == \"G_2\" ~ \"Masters\",\n                           Group == \"G_3\" ~ \"PhD\")) |&gt; \n  rename(`Student Type` = Group)\n# Plot aggregated data\nggplot(dat, aes(x = V1, y = V2)) + \n  geom_point(size = 3) + \n  geom_smooth(method = \"lm\", se = F) +\n  ggtitle(\"Research students - Typing speed as a function of coffee consumption\") +\n  xlab(\"Number of (standard) cups of coffee\") + ylab(\"Words Per Minute (WPM)\") +\n  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, by = 20)) +\n  scale_x_continuous(limits = c(0, 5), breaks = seq(0, 5, by = 1)) +\n  theme_bw(base_size = 18)\n\n\n\n\n\n\n\n\n\nIf they define typing speed as total words/total time elapsed then the time spent buying/making coffee, holding cups and drinking would inflate the denominator without the numerator increasing and make it appear that typing speed has decreased as more time is spent not typing. Would be very hard to get the speed back up just by typing faster in that scenario. So clearly the workplace should be providing each worker with a pre-made, heated camelbak filled with coffee each day to improve productivity. Hands free coffee drinking! (Robb - former Research Student)\nThe best response of the week has to go to Robb for this very well considered and justified attempt to have the bosses supply one of these (I’m assuming this is what you really meant Robb):\n\n\n\nNew MSNI Coffee Facilities?\n\n\nSo what is this scatterplot showing? Well there appears to be a relationship between coffee drinking and typing speed - drinking more cups of coffee seems to be associated with reduced typing speed (note that I am not suggesting that drinking more coffee causes reduced typing speed, merely that there is a correlation). Fitting a regression model to these data produces a best-fitting line with a negative slope or coefficient. It is interesting though - is this direction of ‘effect’ what one might reasonably expect? Perhaps, but my intuition would be that drinking more coffee might naturally correlate with a faster typing ability (don’t forget I’ve made up the data to suit the story I’m telling - I can only guess as to whether these associations are real or not).\nSo what else could be going on to produce the pattern that you see? Well, you might then naturally think that there is some other ‘lurking’ variable (i.e. a confounder) that is distorting or masking the true relationship between coffee consumption and typing speed leading to the association that we actually observe. And, you’d be right…\nIt just so turns out that the program that the research student is enrolled in (Honours, Masters, PhD) is an important factor in teasing apart the coffee -&gt; typing speed association. If we now condition on or control for Student Type we see a completely different picture regarding that association. As before, we can similarly fit regression lines to these three subgroups. For each Student Type drinking more coffee is now associated with a faster typing speed (the slope/coefficient of those lines are now positive), but we can also observe some other interesting findings. On average, Honours students drink the fewest cups of coffee and have the fastest typing speed - they are fresh and motivated. In contrast, PhD students drink the most coffee and have the slowest typing speed. My take on this (and again I’m sure this doesn’t apply to MSNI students), is that by the time you’ve become a PhD student, you have more caffeine coursing through your system than actual blood, but in fact this does little to help your productivity which is more impaired by your sheer exhaustion, increasing cynicism towards academic life and typing-related repetitive strain injury (maybe this is just a realisation of my own PhD experience 🤔).\n\n\nCode\nggplot(dat, aes(x = V1, y = V2, color = `Student Type`)) + \n  geom_point(size = 3) + \n  geom_smooth(method = \"lm\", se = F) +\n  ggtitle(\"Research students - Typing speed as a function of coffee consumption\") +\n  xlab(\"Number of (standard) cups of coffee\") + ylab(\"Words Per Minute (WPM)\") +\n  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, by = 20)) +\n  scale_x_continuous(limits = c(0, 5), breaks = seq(0, 5, by = 1)) + \n  theme_bw(base_size = 18) +\n  theme(legend.position = c(1,1), legend.justification = c(1.1,1.1))\n\n\n\n\n\n\n\n\n\nYou might have come across Simpson’s Paradox in your statistical reading - it’s a fairly common epidemiological bias but it can have important implications for the interpretation of evidence from observational studies, including yours. And so to that end it’s not purely an exercise for academic interest. Simpson’s Paradox is a version of Lord’s Paradox (differentiated by whether exposure and outcome variables are categorical, or continuous, or a combination of both) but at the end of the day they are both a type of Reversal Paradox. Regardless of the variable type, a common characteristic is shared in the Reversal Paradox: the association between two variables can be reversed, diminished, or enhanced when another variable (confounder) is statistically controlled for.\n\n\n\n\n\n\nImportant\n\n\n\nObserved associations at the aggregated level - when important underlying group structures aren’t realised - or worse still, ignored - can potentially reverse when the data are disaggregated and those underlying group structures are considered in the analysis. In other words, the observed association across all groups can be quite different to that within each group. This is Simpson’s Paradox in a nutshell.\n\n\nA canonical example of Simpson’s Paradox is the relationship between body mass and longevity across different species of animals. In general, across all animal species, larger animals (elephants, whales, etc) tend to live longer than smaller animals (rodents, birds, etc). There are of course exceptions to that rule - I’m looking at you Mr Tortoise. But when you look within species, an inverse correlation typically exists - being heavier tends to be associated with a shorter life.\nThese examples highlight the importance of understanding your data and the research questions you are asking. Prior knowledge of potential variable relationships (measured and unmeasured) and underlying causal theory should be the primary considerations guiding you in the modelling of your data. Simply following variable selection techniques based on statistical criteria can still lead to models that are consistent and replicable, but also very easily lead to erroneous conclusions because you haven’t considered a pesky ‘lurking’ factor that can leave you with the equivalent of statistical and scientific egg on your face."
  },
  {
    "objectID": "posts/100__2024/index.html",
    "href": "posts/100__2024/index.html",
    "title": "Logistic regression under the hood",
    "section": "",
    "text": "Code\n# Recreate data from Ophthalmic statistics note 11: logistic regression.\n# Original source: A comparison of several methods of macular hole measurement using optical coherence tomography, and their value in predicting anatomical and visual outcomes.\n\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(ggmagnify)\nlibrary(emmeans)\n\n# Simulate data ----\nn &lt;- 1000                    # don't change this unless necessary (plots might be fragile)\nset.seed(1234)\nx  &lt;-  rnorm(n, 486, 142)    # generate macular hole inner opening data with mean 486 and sd = 152\nz  &lt;-  10.89 - 0.016 * x     # generate variable that is linear combination of intercept = 10.89 and coefficient for macular hole -0.016 (logit scale)\npr  &lt;-  1/(1 + exp(-z))      # generate probabilities from this\ny  &lt;-  rbinom(n, 1, pr)      # generate outcome variable as a function of those probabilities\n\n# Create dataframe from these:\ndf &lt;-  data.frame(y = y, x = x, z = z, pr = pr)\ndf &lt;- df |&gt; \n  filter(x &gt; 100) # only include those with thickness &gt; 100\n\n# Logistic regression model ----\n# Rescale x to 1 unit = 100 microns instead of 1 micron\nsummary(mod_logistic &lt;- glm(y ~ I(x/100), data = df, family = \"binomial\"))\n\n\n\nCall:\nglm(formula = y ~ I(x/100), family = \"binomial\", data = df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  10.3501     0.7456   13.88   &lt;2e-16 ***\nI(x/100)     -1.5045     0.1212  -12.42   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 773.74  on 989  degrees of freedom\nResidual deviance: 494.67  on 988  degrees of freedom\nAIC: 498.67\n\nNumber of Fisher Scoring iterations: 6\n\n\nCode\n# Emmeans of logodds at 600 and 700 microns\nemmeans_df &lt;- data.frame(emmeans(mod_logistic, ~ x, at = list(x = c(600, 700))))\n# Create df of emmeans and corresponding odds and probs, for plotting\nemmeans_df &lt;- emmeans_df |&gt; \n  select(x, emmean) |&gt; \n  rename(logodds = emmean) |&gt; \n  mutate(odds = round(exp(logodds), 3),\n         probs = round(plogis(logodds), 2),\n         logodds = round(logodds, 3))\n\n# Predictions ----\n# Create new df to predict on new values of x\nnew_dat &lt;- data.frame(x = seq(from = 0, to = 1200, length.out = 100))\n# Predict new fitted values and SE's on logodds scale\npred_logodds &lt;- predict(mod_logistic, newdata = new_dat, type = \"link\", se = TRUE)\nnew_dat &lt;- cbind(new_dat, pred_logodds)\n# Create new df of predictions\npredictions &lt;- new_dat |&gt; \n  rename(pred_logodds_est = fit) |&gt; \n  mutate(pred_logodds_LL = pred_logodds_est - (1.96 * se.fit),\n         pred_logodds_UL = pred_logodds_est + (1.96 * se.fit)) |&gt; \n  select(-c(se.fit, residual.scale))\n# Predict new fitted values and SE's on odds scale\npredictions &lt;- predictions |&gt; \n  mutate(pred_odds_est = exp(pred_logodds_est),\n         pred_odds_LL = exp(pred_logodds_LL),\n         pred_odds_UL = exp(pred_logodds_UL))\n# Predict new fitted values and SE's on probability scale\npred_probs &lt;- predict(mod_logistic, newdata = new_dat, type = \"response\", se = TRUE)\nnew_dat &lt;- cbind(new_dat[1], pred_probs)\nnew_dat &lt;- new_dat |&gt; \n  mutate(pred_probs_LL = fit - (1.96 * se.fit),\n         pred_probs_UL = fit + (1.96 * se.fit))\n# Add predicted probs and CIs to predictions df\npredictions &lt;- cbind(predictions, \n                     pred_probs_est = new_dat$fit, \n                     pred_probs_LL = new_dat$pred_probs_LL,\n                     pred_probs_UL = new_dat$pred_probs_UL)\n\n# Reformat plots slightly for ggarrange ----\np3a &lt;- ggplot(predictions, aes(x = x, y = pred_logodds_est)) + \n  geom_ribbon(aes(ymin = pred_logodds_LL, ymax = pred_logodds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = x, y = y), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1150, y = 6, label = \"log-odds\", size = 10) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-100, 100), breaks = seq(-100, 100, by = 2)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(-8, 8)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(x, logodds),\n                            label = emmeans_df$logodds, \n                            nudge_x = c(-50, 50), nudge_y = c(4, -4),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  theme_bw(base_size = 25) +\n  ylab(\"\") +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\n\np4a &lt;- ggplot(predictions, aes(x = x, y = pred_odds_est)) + \n  geom_ribbon(aes(ymin = pred_odds_LL, ymax = pred_odds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = x, y = y), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1150, y = 6000, label = \"odds\", size = 10) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(seq(0, 1000000, by = 1000))) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(0, 7000)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ylab(\"\") +\n  theme_bw(base_size = 25) +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\np4a_inset &lt;- p4a +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(1,2,3,4,5, seq(0, 1000000, by = 1000))) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(x, odds),\n                            label = emmeans_df$odds, \n                            nudge_x = c(-50, 50), nudge_y = c(-1, 2),\n                            color = \"red\", segment.size = 0.2, size = 5)\np4a &lt;- p4a + geom_magnify(from = c(xmin = 500, xmax = 1000, ymin = 0, ymax = 5), \n                          to = c(xmin = 465, xmax = 1010, ymin = 1000, ymax = 5000), \n                          shadow = T, axes = \"y\", plot = p4a_inset)\n\np5a &lt;- ggplot(predictions, aes(x = x, y = pred_probs_est)) + \n  geom_ribbon(aes(ymin = pred_probs_LL, ymax = pred_probs_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = x, y = y), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1150, y = 0.8, label = \"probability\", size = 10) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(x, probs),\n                            label = emmeans_df$probs, \n                            nudge_x = c(-50, 50), nudge_y = c(-0.1, 0.1),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  ylab(\"\") + xlab(\"Macular hole thickness\") +\n  theme_bw(base_size = 25)\nggarrange(p3a, p4a, p5a, align = \"v\", ncol = 1, heights = c(1,1,1.2))"
  },
  {
    "objectID": "posts/008_22Mar_2024/index.html",
    "href": "posts/008_22Mar_2024/index.html",
    "title": "Easily view your data by a grouping variable",
    "section": "",
    "text": "It is easy enough to view a dataframe in RStudio by opening the dataframe in the viewer or printing the dataframe (or part of it) to the console. However, this can be messy if you want to quickly identify data by a grouping variable (usually the patient id). The by() function can help you to do this. Let’s illustrate its utility with the sleepstudy dataset from the lme4 package. To start with I’ll print the data for the first 3 subjects as one might.\nsleepstudy |&gt; as_tibble() |&gt; print(n = 30)\n\n\nCode\nlibrary(lme4)\nlibrary(dplyr)\n# Load data\ndata(\"sleepstudy\")\nsleepstudy |&gt; as_tibble() |&gt; print(n = 30)\n\n\n# A tibble: 180 × 3\n   Reaction  Days Subject\n      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  \n 1     250.     0 308    \n 2     259.     1 308    \n 3     251.     2 308    \n 4     321.     3 308    \n 5     357.     4 308    \n 6     415.     5 308    \n 7     382.     6 308    \n 8     290.     7 308    \n 9     431.     8 308    \n10     466.     9 308    \n11     223.     0 309    \n12     205.     1 309    \n13     203.     2 309    \n14     205.     3 309    \n15     208.     4 309    \n16     216.     5 309    \n17     214.     6 309    \n18     218.     7 309    \n19     224.     8 309    \n20     237.     9 309    \n21     199.     0 310    \n22     194.     1 310    \n23     234.     2 310    \n24     233.     3 310    \n25     229.     4 310    \n26     220.     5 310    \n27     235.     6 310    \n28     256.     7 310    \n29     261.     8 310    \n30     248.     9 310    \n# ℹ 150 more rows\n\n\nBut we can do this better with:\nby(sleepstudy, sleepstudy$PATIENT_ID, identity)[1:3]\nNote that the [1:3] indicates the range of group indices that you want to view.\n\n\nCode\nby(sleepstudy, sleepstudy$Subject, identity)[1:3]\n\n\n$`308`\n   Reaction Days Subject\n1  249.5600    0     308\n2  258.7047    1     308\n3  250.8006    2     308\n4  321.4398    3     308\n5  356.8519    4     308\n6  414.6901    5     308\n7  382.2038    6     308\n8  290.1486    7     308\n9  430.5853    8     308\n10 466.3535    9     308\n\n$`309`\n   Reaction Days Subject\n11 222.7339    0     309\n12 205.2658    1     309\n13 202.9778    2     309\n14 204.7070    3     309\n15 207.7161    4     309\n16 215.9618    5     309\n17 213.6303    6     309\n18 217.7272    7     309\n19 224.2957    8     309\n20 237.3142    9     309\n\n$`310`\n   Reaction Days Subject\n21 199.0539    0     310\n22 194.3322    1     310\n23 234.3200    2     310\n24 232.8416    3     310\n25 229.3074    4     310\n26 220.4579    5     310\n27 235.4208    6     310\n28 255.7511    7     310\n29 261.0125    8     310\n30 247.5153    9     310\n\n\nIf you want to take this a step further, you can generalise this with a function that will allow you to quickly view the data in any range that you want, without having to continually copy and paste that line of code. Just call the function with your dataframe and group id names and the range of group indices that you want to view (interestingly while writing this function I worked out you don’t even need the by() function to achieve the same result).\nprint_groups(sleepstudy, Subject, 1, 3)\n\n\nCode\n# Create function\nprint_groups &lt;- function(df, id, index1, index2) {\n  df &lt;- data.frame(df)\n  ids_all &lt;-  unique(eval(substitute(id), df))\n  ids_range &lt;- ids_all[index1:index2]\n  if (index1 &lt;= length(ids_all) & index2 &lt;= length(ids_all)) {\n    for (id2 in ids_range) {\n      cat(paste0(\"id = \", id2, \"\\n\"))\n      print(df[eval(substitute(id), df) %in% id2,])\n      cat(\"----------------------------\\n\\n\")\n    }\n  } else {\n    print(\"There aren't that many groups in your dataset\")\n  }\n}\n\n# Use function\nprint_groups(sleepstudy, Subject, 1, 3)\n\n\nid = 308\n   Reaction Days Subject\n1  249.5600    0     308\n2  258.7047    1     308\n3  250.8006    2     308\n4  321.4398    3     308\n5  356.8519    4     308\n6  414.6901    5     308\n7  382.2038    6     308\n8  290.1486    7     308\n9  430.5853    8     308\n10 466.3535    9     308\n----------------------------\n\nid = 309\n   Reaction Days Subject\n11 222.7339    0     309\n12 205.2658    1     309\n13 202.9778    2     309\n14 204.7070    3     309\n15 207.7161    4     309\n16 215.9618    5     309\n17 213.6303    6     309\n18 217.7272    7     309\n19 224.2957    8     309\n20 237.3142    9     309\n----------------------------\n\nid = 310\n   Reaction Days Subject\n21 199.0539    0     310\n22 194.3322    1     310\n23 234.3200    2     310\n24 232.8416    3     310\n25 229.3074    4     310\n26 220.4579    5     310\n27 235.4208    6     310\n28 255.7511    7     310\n29 261.0125    8     310\n30 247.5153    9     310\n----------------------------\n\n\nAnd there you have it!"
  },
  {
    "objectID": "posts/001_24Nov2023/index.html",
    "href": "posts/001_24Nov2023/index.html",
    "title": "Interactions (effect modifiers) are important - don’t ignore them",
    "section": "",
    "text": "Code\nlibrary(knitr)\nlibrary(quarto)\nlibrary(emmeans)\nlibrary(flextable)\nlibrary(ggplot2)\nsuppressPackageStartupMessages(library(gtsummary))\nopts_chunk$set(echo = T,\n               cache = F,\n               prompt = F,\n               tidy = F,\n               message = F,\n               warning = F)"
  },
  {
    "objectID": "posts/001_24Nov2023/index.html#the-question",
    "href": "posts/001_24Nov2023/index.html#the-question",
    "title": "Interactions (effect modifiers) are important - don’t ignore them",
    "section": "1 The Question",
    "text": "1 The Question\n\nRecall that the question this week was to choose between:\nA) Age is a confounder in the relationship between sex and hospitalisation from car crash.\nB) Age is an effect modifier in the relationship between sex and hospitalisation from car crash.\nusing the data supplied below."
  },
  {
    "objectID": "posts/001_24Nov2023/index.html#the-answer",
    "href": "posts/001_24Nov2023/index.html#the-answer",
    "title": "Interactions (effect modifiers) are important - don’t ignore them",
    "section": "2 The Answer",
    "text": "2 The Answer\n\nThe answer is B. Younger male drivers tend to be more stupid and injure themselves more seriously than their female counterparts. Upon reaching a suitable age of maturity their risk of hospitalisation reduces to about the same (if we assume the 95% CI for the 8% reduction includes 0) as that for females.\nWhen a third variable plays a role in the association between an exposure and an outcome it may act as a confounder OR effect modifier (AND sometimes both).\nA simple confounder (e.g. age) will show the same exposure -&gt; outcome association across all of its categories (e.g. same risk ratio in &lt; 40 yrs and ≥ 40 yrs)\nAn effect modifier will show different magnitudes of association across its categories (e.g. the risk ratio will differ in those &lt; 40 yrs and ≥ 40 yrs).\nStratification is the simplest form of exploring and adjusting for confounding/interaction effects (used before we had all this computing power).\nSubgroups of data are created for each category of confounder/effect modifier and estimates of interest (mean differences, risk ratios, etc) calculated in each.\nThese can then be combined in a weighted manner to give an overall (adjusted) estimate if NO effect modification is present.\nThis is equivalent to including the third variable as a covariate in our regression model (we now use models rather than stratification methods).\nSimple inclusion in the regression model (using + in R) FORCES the exposure -&gt; outcome association to be the same across all categories of the effect modifier even if in reality it’s not.\n+ assumes confounding ONLY and NO effect modification.\nA problem arises, however, when the third variable is more an effect modifier, rather than confounder.\nIf we suspect effect modification is present, we need to include this third variable in the model as an interaction term (using * in R)\nThis will allow the exposure -&gt; outcome association to differ across categories of the effect modifier.\n* assumes effect modification is present.\nThis is a more flexible model specification, than simply ‘adjusting’ for a variable.\nInterpretation is a little more involved (always happy to help with this) but the point is it’s important not to blindly assume a third variable can only ever be a confounder.\nIf effect modification is present, you need to know about it.\nIt is simple to test for effect modification in R, Stata, etc. Include the interaction term and then drop it if not clinically/statistically significant at some level.\nI have included some R output below showing the equivalence of stratification and modelling approaches to interaction effects.\n\nBefore we get to that - a simple set of guidelines for how to think about crude vs stratified associations:\n\nEquivalence of model-derived crude estimate\nRecall that the aggregated data that the crude estimate is calculated from is:\n\n\nCode\ndat_agg &lt;- data.frame(sex = c(\"Male\", \"Female\"),\n                      hospitalised = as.numeric(c(1330, 798)), \n                      not_hospitalised = as.numeric(c(7018,6400)))\ndat_agg\n\n\n\n\n\n\nsex\nhospitalised\nnot_hospitalised\n\n\n\n\nMale\n1330\n7018\n\n\nFemale\n798\n6400\n\n\n\n\n\n\nTo estimate this model in R we essentially run a logistic regression but instead of outputting an odds ratio, we calculate a risk ratio by specifying a log rather than the default logit link. We will also use the aggregate model specification, as we don’t have the individual-level data.\nThe model-derived crude risk ratio for sex = 1.44 (95% CI 1.32, 1.56; p &lt; 0.001). This is very close to the estimate that we initially calculated manually from the 2 x 2 table (1.45).\n\n\nCode\nmod_crude &lt;- glm(cbind(hospitalised, not_hospitalised) ~ sex, data = dat_agg, family = binomial(link = \"log\"))\ntbl_regression(mod_crude, exp = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nRR1\n95% CI1\np-value\n\n\n\n\nsex\n\n\n\n\n\n\n\n\n    Female\n—\n—\n\n\n\n\n    Male\n1.44\n1.32, 1.56\n&lt;0.001\n\n\n\n1 RR = Relative Risk, CI = Confidence Interval"
  },
  {
    "objectID": "posts/001_24Nov2023/index.html#lets-introduce-age-40-vs-40-as-a-third-variable",
    "href": "posts/001_24Nov2023/index.html#lets-introduce-age-40-vs-40-as-a-third-variable",
    "title": "Interactions (effect modifiers) are important - don’t ignore them",
    "section": "3 Let’s introduce age (<40 vs ≥ 40) as a third variable",
    "text": "3 Let’s introduce age (&lt;40 vs ≥ 40) as a third variable\n\n\nCode\ndat_disagg &lt;- data.frame(sex = c(\"Male\", \"Female\", \"Male\", \"Female\"),\n                         age = c(\"&lt; 40\", \"&lt; 40\", \"≥ 40\", \"≥ 40\"),\n                         hospitalised = as.numeric(c(966, 460, 364, 348)), \n                         not_hospitalised = as.numeric(c(3146, 3000, 3872, 3400)))\ndat_disagg\n\n\n\n\n\n\nsex\nage\nhospitalised\nnot_hospitalised\n\n\n\n\nMale\n&lt; 40\n966\n3146\n\n\nFemale\n&lt; 40\n460\n3000\n\n\nMale\n≥ 40\n364\n3872\n\n\nFemale\n≥ 40\n348\n3400\n\n\n\n\n\n\n\n3.1 Age as a confounder\nFor now, let’s just assume age is a confounder in the association between sex and hospitalisation risk. The model formulation in R is then:\nmod_adj &lt;- glm(cbind(hospitalised, not_hospitalised) ~ sex + age, data = dat_agg, family = binomial(link = \"log\"))\nand the risk ratios we get are:\n\n\nCode\nmod_confound &lt;- glm(cbind(hospitalised, not_hospitalised) ~ sex + age, data = dat_disagg, family = binomial(link = \"log\"))\ntbl_regression(mod_confound, exp = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nRR1\n95% CI1\np-value\n\n\n\n\nsex\n\n\n\n\n\n\n\n\n    Female\n—\n—\n\n\n\n\n    Male\n1.43\n1.32, 1.55\n&lt;0.001\n\n\nage\n\n\n\n\n\n\n\n\n    &lt; 40\n—\n—\n\n\n\n\n    ≥ 40\n0.47\n0.43, 0.51\n&lt;0.001\n\n\n\n1 RR = Relative Risk, CI = Confidence Interval\n\n\n\n\n\n\n\n\nSo, what we are seeing here is that the magnitude of association between sex and hospitalisation risk is averaged (in a weighted way) over both categories of age to produce one effect estimate sex = 1.43 (95% CI 1.32, 1.55; p &lt; 0.001). This just so happens to be almost the same as the crude estimate when you ignore age altogether.\nThe effect for age in this model is such that whatever your sex, there is about a 53% reduction in the risk of hospitalisation if you are over 40 vs under 40. Note that in the stratification approach, you aren’t able to calculate an effect for age because you are stratifying by it (essentially treating it as a nuisance variable).\n\n\n3.2 Age as an effect modifier\nNow, let’s correctly model age as an effect modifier in the association between sex and hospitalisation risk. The model formulation in R is then (note the * operator):\nmod_adj &lt;- glm(cbind(hospitalised, not_hospitalised) ~ sex * age, data = dat_agg, family = binomial(link = \"log\"))\nand the risk ratios we get are:\n\n\nCode\nmod_interact &lt;- glm(cbind(hospitalised, not_hospitalised) ~ sex * age, data = dat_disagg, family = binomial(link = \"log\"))\ntbl_regression(mod_interact, exp = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nRR1\n95% CI1\np-value\n\n\n\n\nsex\n\n\n\n\n\n\n\n\n    Female\n—\n—\n\n\n\n\n    Male\n1.77\n1.60, 1.96\n&lt;0.001\n\n\nage\n\n\n\n\n\n\n\n\n    &lt; 40\n—\n—\n\n\n\n\n    ≥ 40\n0.70\n0.61, 0.80\n&lt;0.001\n\n\nsex * age\n\n\n\n\n\n\n\n\n    Male * ≥ 40\n0.52\n0.44, 0.62\n&lt;0.001\n\n\n\n1 RR = Relative Risk, CI = Confidence Interval\n\n\n\n\n\n\n\n\nNote how the p value for the interaction term is very low - this would be a good indicator that the model fits the data better with the interaction term present than without it (i.e. assuming age as a confounder only).\nAs I mentioned earlier, the model interpretation with an interaction present does become a little more complicated, but let’s break this down (note that I use “effect” in a non-causal way):\n\nThe coefficient for sex = 1.77 (95% CI 1.60, 1.96; p &lt; 0.001). The represents the “effect” of sex (being male relative to female) on hospitalisation risk at the reference level of age, which in this case is the under 40 yrs group. So, for those under 40, there is about a 77% increased risk for males relative to females.\nThe coefficient for age = 0.70 (95% CI 0.61, 0.80; p &lt; 0.001). This represents the “effect” of age (being older than 40 yrs relative to younger than 40 yrs) on hospitalisation risk at the reference level of sex, which in this case is female. So, for females, there is about a 30% risk reduction in the need for hospitalisation for older relative to younger drivers.\nThe coefficient for the interaction term: sex * age = 0.52 (95% CI 0.44, 0.62; p &lt; 0.001). This represents the multiplicative increase in the magnitude of association for males over 40 yrs."
  },
  {
    "objectID": "posts/001_24Nov2023/index.html#effect-modification-means-more-associations-to-estimate",
    "href": "posts/001_24Nov2023/index.html#effect-modification-means-more-associations-to-estimate",
    "title": "Interactions (effect modifiers) are important - don’t ignore them",
    "section": "4 Effect modification means more associations to estimate",
    "text": "4 Effect modification means more associations to estimate\nIn this specific case, when you treat age as a confounder, the model produces two risk ratios - one for sex and one for age. However, when you treat age as an effect modifier, there are now four possible risk ratios to estimate (if you care about age more than it being a “nuisance” variable to control for). These are:\n\nThe effect of being male in younger individuals.\nThe effect of being male in older individuals.\nThe effect of being older in females.\nThe effect of being older in males.\n\nYou can easily enough work these out manually by multiplying the respective reference coefficients with the interaction coefficient. The risk ratios for each of the above would then be:\n\n1.77 (we can just read this one straight off the model output)\n1.77 x 0.52 = 0.92\n0.70 (again we can just read this one straight off)\n0.70 x 0.52 = 0.36\n\nNote that the effects for 1. and 2. are very similar to what we calculated straight from the 2 x 2 tables (1.84 and 0.92, respectively - as previously mentioned, the effects for 3. and 4. aren’t able to be calculated for the stratifying variable)."
  },
  {
    "objectID": "posts/001_24Nov2023/index.html#emmeans-should-be-your-new-best-friend",
    "href": "posts/001_24Nov2023/index.html#emmeans-should-be-your-new-best-friend",
    "title": "Interactions (effect modifiers) are important - don’t ignore them",
    "section": "5 Emmeans should be your new best friend",
    "text": "5 Emmeans should be your new best friend\nPerhaps I am preaching to the converted, but if you don’t know what the emmeans package and specific function in R does, then you should learn about it (the equivalent function in Stata is margins).\nhttps://cran.r-project.org/web/packages/emmeans/index.html\nhttps://aosmith.rbind.io/2019/03/25/getting-started-with-emmeans/\nemmeans does a lot of things, but perhaps its workhorse function is to allow you to take a model and calculate adjusted predictions (either at set values of covariates, or by ‘averaging’ over them). In this case, we can very easily use emmeans to reproduce the manual calculations we just did.\n\n\nCode\nemmeans(mod_interact, ~ sex + age, type = \"response\") |&gt; \n  data.frame() |&gt; \n  flextable() |&gt; \n  colformat_double(digits = 3, na_str = \"N/A\") |&gt;\n  set_table_properties(layout = \"autofit\") |&gt; \n  height(height = 1, unit = \"cm\") |&gt; \n  hrule(rule = \"atleast\", part = \"header\") |&gt; \n  align(align = \"center\", part = \"body\") |&gt;\n  bg(bg = \"white\", part = \"all\") |&gt; \n  flextable::font(fontname = \"Consolas\", part = \"all\") |&gt;\n  theme_vanilla()\n\n\nPredicted Probabilities of HospitalisationsexageprobSEdfasymp.LCLasymp.UCLFemale&lt; 400.1330.006 Inf0.1220.145Male&lt; 400.2350.007 Inf0.2220.248Female≥ 400.0930.005 Inf0.0840.103Male≥ 400.0860.004 Inf0.0780.095\n\n\nSpecifying type = \"response\" in the emmeans call indicates that we want to calculate the outcome on the probability (i.e. risk) scale. It is simple enough to plot these predicted probabilities using the emmip function in emmeans.\n\n\nCode\nemmip(mod_interact, age ~ sex, type = \"response\") + \n  theme_bw(base_size = 18)\n\n\n\n\n\n\n\n\n\nTo get the risk ratios we have been working with until now, we simply add the pairs(rev = T) function to the call:\n\n\nCode\nemmeans(mod_interact, ~ sex + age, type = \"response\") |&gt; pairs(rev = T) |&gt; \n  data.frame() |&gt; \n  flextable() |&gt; \n  colformat_double(digits = 3, na_str = \"N/A\") |&gt;\n  set_table_properties(layout = \"autofit\") |&gt; \n  height(height = 1, unit = \"cm\") |&gt; \n  hrule(rule = \"atleast\", part = \"header\") |&gt; \n  align(align = \"center\", part = \"body\") |&gt;\n  bg(bg = \"white\", part = \"all\") |&gt; \n  flextable::font(fontname = \"Consolas\", part = \"all\") |&gt;\n  theme_vanilla()\n\n\nAll Pairwise Risk RatioscontrastratioSEdfnullz.ratiop.valueMale &lt; 40 / Female &lt; 401.7670.091 Inf1.00011.0030.000Female ≥ 40 / Female &lt; 400.6980.047 Inf1.000-5.3560.000Female ≥ 40 / Male &lt; 400.3950.023 Inf1.000-15.9230.000Male ≥ 40 / Female &lt; 400.6460.043 Inf1.000-6.5820.000Male ≥ 40 / Male &lt; 400.3660.021 Inf1.000-17.4990.000Male ≥ 40 / Female ≥ 400.9250.066 Inf1.000-1.0830.700\n\n\nNote that this gives us two extra comparisons we might not really want (the 3rd and 4th lines of the output) as it estimates every single pairwise comparison. We can get a bit fancier and customise the emmeans output to give us only what we want:\n\n\nCode\nemm &lt;- emmeans(mod_interact, ~ sex + age, type = \"response\") # save the estimated risks\ncustom &lt;- list(`The effect of being male in younger individuals` = c(-1,1,0,0),\n               `The effect of being male in older individuals` = c(0,0,-1,1),\n               `The effect of being older in females` = c(-1,0,1,0),\n               `The effect of being older in males` = c(0,-1,0,1)) # create custom grid of RR's to estimate\ncontrast(emm, custom) |&gt; \n  summary(infer = T) |&gt; \n  data.frame() |&gt; \n  flextable() |&gt; \n  colformat_double(digits = 3, na_str = \"N/A\") |&gt;\n  set_table_properties(layout = \"autofit\") |&gt; \n  height(height = 1, unit = \"cm\") |&gt; \n  hrule(rule = \"atleast\", part = \"header\") |&gt; \n  align(align = \"center\", part = \"body\") |&gt;\n  bg(bg = \"white\", part = \"all\") |&gt; \n  flextable::font(fontname = \"Consolas\", part = \"all\") |&gt;\n  theme_vanilla()\n\n\nCustom Pairwise Risk RatioscontrastratioSEdfasymp.LCLasymp.UCLnullz.ratiop.valueThe effect of being male in younger individuals1.7670.091 Inf1.5971.9561.00011.0030.000The effect of being male in older individuals0.9250.066 Inf0.8041.0651.000-1.0830.279The effect of being older in females0.6980.047 Inf0.6120.7961.000-5.3560.000The effect of being older in males0.3660.021 Inf0.3270.4091.000-17.4990.000\n\n\nNote, that these match the manual calculations pretty well.\nPlease take some time to learn about emmeans (or margins in Stata). It will make your life so much easier if you plan to have a career in research (and don’t always have access to a statistician)."
  },
  {
    "objectID": "posts/009_05Apr_2024/index.html",
    "href": "posts/009_05Apr_2024/index.html",
    "title": "Don’t be Scared of Splines",
    "section": "",
    "text": "Have you heard the term restricted cubic spline (RCS) and thought ‘that’s just seems too hard but I should learn about it one day’ and then stuck to modelling your continuous predictor as you always do, assuming it has a linear relationship with the outcome? Well I hope that by the end of this post you have a better basic understanding of what RCS’s actually are, how they give you so much more flexibility in your modelling toolkit, and above all else, how they are really not that hard to use.\nWhen you want to explore the association between a continuous predictor variable and an outcome (of any form really - binary, count, continuous) you have choices to make about how you parameterise that predictor. In fact, one could consider a hierarchy of such choices that range from downright egregious through to ‘we’ll just do it how it’s always done’ through to what is becoming thought more of these days as ‘best-practice’ in statistical modelling. These approaches include:\n\nCategorising the predictor - Egregious.\nAssuming the predictor has a linear relationship with the outcome (or its link function if the outcome is not continuous) - ‘We’ll just do it how it’s always done’.\nAssuming the predictor has a non-linear relationship with the outcome and using a piece-wise model (segmented regression) to model smaller segments of the data where linearity does in fact hold - A better alternative than assuming linearity.\nAssuming the predictor has a non-linear relationship with the outcome and using polynomial (e.g. quadratic/cubic/etc) regression - Again, a better alternative than assuming linearity.\nAssuming the predictor has a non-linear relationship with the outcome and using RCS’s - arguably ‘best practice’.\n\n\n\n\n\n\n\nImportant\n\n\n\nDon’t forget it’s always a good idea to plot your data first to visualise the relationship (using a lowess smoother helps). There is no need to worry about non-linearity if in fact the relationship between your predictor and outcome isn’t - just model it as linear and you’re done.\n\n\nNow, having made that point, let me follow by saying that this is relatively easy when you have a continuous predictor and a continuous outcome. But what do you do with other outcome types - for example, in the case of a binary outcome (successes/failures) your outcome just consists of a bunch of 0's and 1's. This is more challenging to visualise but it is possible. Here, a good way to visualise your observed data is to ‘bin’ the predictor into discrete categories (arbitrarily decided by you), counting the number of successes out of the total in each bin - this will give the proportion of successes - i.e. the observed probability of success in each bin. You then convert each proportion into its equivalent logit (log-odds) and plot this (on the Y axis) against the mid-point of each bin of the predictor on the X axis. If the best-fitting line in that plot is approximately linear, then the assumption of linearity of the continuous predictor with the binary outcome is upheld.\nWell I did say this was more challenging…\nUltimately, visualise your data where it’s easy enough to do so (which will primarily be the continuous predictor vs continuous outcome case). But when this isn’t so straightforward or practical, as in the binary outcome case above, we can in lieu use model-fitting statistics to help us decide whether incorporating non-linear predictor terms in our model is of value or not. In the basic comparison we fit two models - one assuming linearity and one assuming non-linearity (via one of the other methods) and let either some information criterion (e.g. the AIC), or a likelihood ratio test guide us as to the better fit. So it is certainly still possible to incorporate non-linear terms in a statistical model without having first plotted that data. I’ll illustrate this shortly.\nNow let’s have a look at some simulated data that demonstrates a non-linear relationship of the predictor with the outcome, and how these different approaches may be applied to model this."
  },
  {
    "objectID": "posts/003_08Dec_2023/index.html",
    "href": "posts/003_08Dec_2023/index.html",
    "title": "A Very Merry Christmas",
    "section": "",
    "text": "This figure was made using ggplot2 and while I can’t take credit for coming up with the idea (source), I have added a couple of flourishes including the animated lights.\nI hope everyone has a safe, happy and enjoyable holiday period.\n\n\nCode\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(gifski)\nlibrary(extrafont)\nloadfonts()\n\n# Read in the base Christmas tree data\nChristmasTree &lt;- read.csv(\"https://raw.githubusercontent.com/t-redactyl/Blog-posts/master/Christmas%20tree%20base%20data.csv\")\n\n# Change tree colour\nChristmasTree$Tree.Colour[ChristmasTree$Tree.Colour == \"#143306\"] &lt;- \"green4\"\n\n# Generate the \"lights\"\nDesired.Lights &lt;- 100\nTotal.Lights &lt;- sum(round(Desired.Lights * 0.35) + round(Desired.Lights * 0.20) + \n                    round(Desired.Lights * 0.17) + round(Desired.Lights * 0.13) +\n                    round(Desired.Lights * 0.10) + round(Desired.Lights * 0.05))\n\nLights &lt;- data.frame(Lights.X = c(round(runif(round(Desired.Lights * 0.35), 4, 18), 0),\n                                  round(runif(round(Desired.Lights * 0.20), 5, 17), 0),\n                                  round(runif(round(Desired.Lights * 0.17), 6, 16), 0),\n                                  round(runif(round(Desired.Lights * 0.13), 7, 15), 0),\n                                  round(runif(round(Desired.Lights * 0.10), 8, 14), 0),\n                                  round(runif(round(Desired.Lights * 0.05), 10, 12), 0)))\nLights$Lights.Y &lt;- c(round(runif(round(Desired.Lights * 0.35), 4, 6), 0),\n                     round(runif(round(Desired.Lights * 0.20), 7, 8), 0),\n                     round(runif(round(Desired.Lights * 0.17), 9, 10), 0),\n                     round(runif(round(Desired.Lights * 0.13), 11, 12), 0),\n                     round(runif(round(Desired.Lights * 0.10), 13, 14), 0),\n                     round(runif(round(Desired.Lights * 0.05), 15, 17), 0))\nLights$Lights.Colour &lt;- c(round(runif(Total.Lights, 1, 3), 0))\n\n# Generate the \"baubles\"\nBaubles &lt;- data.frame(Bauble.X = c(6, 9, 15, 17, 5, 13, 16, 7, 10, 14, 7, 9, 11, 14, 8, 14, 9, 12, 11, 12, 14, 11, 17, 10))\nBaubles$Bauble.Y &lt;- c(4, 5, 4, 4, 5, 5, 5, 6, 6, 6, 8, 8, 8, 8, 10, 10, 11, 11, 12, 13, 10, 16, 7, 14)\nBaubles$Bauble.Colour &lt;- factor(c(1, 2, 2, 3, 2, 3, 1, 3, 1, 1, 1, 2, 1, 2, 3, 3, 2, 1, 3, 2, 1, 3, 3, 1))\nBaubles$Bauble.Size &lt;- c(6, 18, 6, 6, 12, 6, 12, 12, 12, 6, 6, 6, 18, 18, 18, 12, 18, 6, 6, 12, 12, 18, 18, 12)\n\n# Generate the plot\np &lt;- ggplot() + \n  geom_tile(data = ChristmasTree, aes(x = Tree.X, y = Tree.Y, fill = Tree.Colour)) +\n  scale_fill_identity() + \n  geom_point(data = Lights, aes(x = Lights.X, y = Lights.Y), color = \"lightgoldenrodyellow\", shape = 8) +\n  geom_point(data = Baubles, aes(x = Bauble.X, y = Bauble.Y, colour = Bauble.Colour), size = Baubles$Bauble.Size, shape = 16) +\n  scale_colour_manual(values = c(\"firebrick2\", \"gold\", \"blue3\")) +\n  scale_size_area(max_size = 12) +\n  theme_bw() +\n  scale_x_continuous(breaks = NULL) + \n  scale_y_continuous(breaks = NULL) +\n  geom_segment(aes(x = 2.5, xend = 4.5, y = 1.5, yend = 1.5), colour = \"blueviolet\", size = 2) +\n  geom_segment(aes(x = 5.5, xend = 8.5, y = 1.5, yend = 1.5), colour = \"dodgerblue3\", size = 2) +\n  geom_segment(aes(x = 13.5, xend = 16.5, y = 1.5, yend = 1.5), colour = \"blueviolet\", size = 2) +\n  geom_segment(aes(x = 17.5, xend = 19.5, y = 1.5, yend = 1.5), colour = \"dodgerblue3\", size = 2) +\n  geom_segment(aes(x = 3.5, xend = 3.5, y = 0.5, yend = 2.5), colour = \"blueviolet\", size = 2) +\n  geom_segment(aes(x = 7.0, xend = 7.0, y = 0.5, yend = 2.5), colour = \"dodgerblue3\", size = 2) +\n  geom_segment(aes(x = 15.0, xend = 15.0, y = 0.5, yend = 2.5), colour = \"blueviolet\", size = 2) +\n  geom_segment(aes(x = 18.5, xend = 18.5, y = 0.5, yend = 2.5), colour = \"dodgerblue3\", size = 2) +\n  annotate(\"text\", x = 11, y = 20, label = \"Merry Christmas!\",family = \"Luminari\", color = \"white\", size = 12) +\n  transition_states(states=Lights.Colour, transition_length = 0, state_length = 0.0001) +\n  labs(x = \"\", y = \"\") +\n  theme(legend.position = \"none\") +\n  theme(panel.background = element_rect(fill = 'midnightblue', colour = \"yellow\"))\n\n# Animate\nanimate(p, nframe = 20, fps = 20)"
  },
  {
    "objectID": "posts/004_02Feb_2024/index.html",
    "href": "posts/004_02Feb_2024/index.html",
    "title": "Put your ggplot on steroids",
    "section": "",
    "text": "Welcome back to Stats Tips for 2024 - hope you managed a nice break.\nIt’s a short one today. If you didn’t already now it existed, check out plotly for taking your ggplots to the next level.\nSometimes it can be extremely helpful to quickly link discrete elements of a plot to the corresponding observation/s in your dataframe. For example, you have a suspected outlier in a scatterplot and you want to know which individual that belongs to. Or, you have an unavoidably busy plot; for example, plotting the predictions from a mixed model for longitudinal data overlaid on the observed data for comparison. In these cases it’s nearly impossible to discern the origin of the plotted data. In both use-case scenarios (and many more), plotly can help.\nIn this example of the latter use-case, we are going to use data from a built-in dataset in the lme4 package. The sleepstudy data looks at reaction times over time in sleep-deprived individuals. For the sake of the exercise we will fit a mixed model with reaction time (ms) as the outcome, time (days) as a fixed-effect and time (days) and individual as random-effects. So this is a random slopes model allowing the ‘effect’ of sleep-deprivation on reaction time to vary over time for each individual. We fit the model and view a few lines of the dataframe which now contains the fixed (mod_pred_fix) and random (mod_pred_ran) predictions.\n\n\nCode\nlibrary(lme4)\nlibrary(ggplot2)\nlibrary(plotly)\n# Load data\ndata(\"sleepstudy\")\n# Model\nmod &lt;- lmer(Reaction ~ Days + (Days | Subject), data = sleepstudy)\n# Predict\nsleepstudy$mod_pred_fix &lt;- predict(mod, re.form = NA) # predict fixed effects\nsleepstudy$mod_pred_ran &lt;- predict(mod) # predict random effects\n# View data\nhead(sleepstudy, 10)\n\n\n\n\n\n\nReaction\nDays\nSubject\nmod_pred_fix\nmod_pred_ran\n\n\n\n\n249.5600\n0\n308\n251.4051\n253.6637\n\n\n258.7047\n1\n308\n261.8724\n273.3299\n\n\n250.8006\n2\n308\n272.3397\n292.9962\n\n\n321.4398\n3\n308\n282.8070\n312.6624\n\n\n356.8519\n4\n308\n293.2742\n332.3287\n\n\n414.6901\n5\n308\n303.7415\n351.9950\n\n\n382.2038\n6\n308\n314.2088\n371.6612\n\n\n290.1486\n7\n308\n324.6761\n391.3275\n\n\n430.5853\n8\n308\n335.1434\n410.9937\n\n\n466.3535\n9\n308\n345.6107\n430.6600\n\n\n\n\n\n\nWe can then plot the data interactively by simply ‘wrapping’ the ggplot object in a plotly call. If you hover over a data point you can easily identify which individual it belongs to as well as the observed reaction time. Similarly, by hovering over one of the random slopes you will see the predicted reaction time and the individual that corresponds to.\nYou won’t want to do this for every plot you make but it does provide a simple way to make some of your more complex visualisations using ggplot that bit more useful (and fun!) in helping to understand your data.\n\n\nCode\n# Plot\np &lt;- sleepstudy |&gt;\n    ggplot(aes(x = Days, y = Reaction, color = factor(Subject))) +\n    geom_line(aes(x = Days, y = mod_pred_ran)) +\n    geom_line(aes(x = Days, y = mod_pred_fix), linewidth = 2, color = \"blue\") +\n    geom_point(alpha = 0.5) +\n    xlab(\"Time (days)\") + ylab(\"Reaction Time (ms)\") +\n    guides(color = \"none\") +\n    theme_bw(base_size = 15)\nggplotly(p)"
  },
  {
    "objectID": "posts/005_16Feb_2024/index.html",
    "href": "posts/005_16Feb_2024/index.html",
    "title": "Immortal time bias - “The fallacy that never dies” (Part 1)",
    "section": "",
    "text": "In 2001, a paper published in the Annals of Internal Medicine reported that Oscar winners had a longer life expectancy - by about 4 years - compared to their less successful peers. The authors conclusions were that:\n“The association of high status with increased longevity that prevails in the public also extends to celebrities, contributes to a large survival advantage, and is partially explained by factors related to success.”\nThe study received widespread attention in the media, with one future Oscar winner acknowledging the work in her acceptance speech.\nThe problem was that the reported survival advantage was illusory, and the reason for this was an invalid analysis that is not alone in the literature. As in any simple time-to-event analysis, two groups may be compared in their respective ‘survival’ times. In this study, subjects were first classified as winners or non-winners and observation time counted as their time alive. The error in this case was to consider winning status time-fixed (A), when in reality it is time-varying (B). By naively assuming it is time-fixed, we are erroneously attributing the time that a winner was in fact a loser prior to getting their gong, to their winning observation time.\n\nThis creates a distortion or bias in the exposure/treatment -&gt; outcome association, usually in a direction that overestimates the benefit of the exposure/treatment. When proper methods are then employed, the perceived benefits are reduced or sometimes reversed. And in fact that is exactly what was found when a re-analysis of the data was conducted in 2006 - the survival advantage was calculated to be closer to 1 year and not deemed statistically significant.\nIn general, the observation time prior to the exposure/treatment commencing (for those exposed/treated) is considered ‘immortal’, because the subject cannot experience the outcome during this period as they have yet to receive the exposure/treatment. If you are like me, this fairly classic description of immortal time hurts my brain and so I just like to simply think of it as the period that a person’s observation time has been misclassified.\nObservational research that involves time-to-event outcomes is particularly prone to immortal time bias and central to the problem is the specification of ‘time zero’ - i.e. when does the clock start? There are several examples of study design choices that can lead you down the wrong analysis path if you are not careful, and an especially pertinent one in this field is drawing contrasts between treated and untreated patients (hint: a patient is not ‘treated’ for the duration of their observation time if they were only on treatment for the last 10% of that time).\nSo, what’s the solution?\nTime-varying covariates\nI will illustrate their use in an example in the next post."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Stats Tips - Welcome",
    "section": "",
    "text": "Hi Everyone,\nI’m not sure how much everyone’s getting out of the stats tips that I put on WhatsApp on Fridays, but maybe some of it is helpful. For those of you who are interested, I thought that if I was going to do this on a semi-regular basis, I might as well turn it into a resource. So I am having a go at a blog-style format for posting these tips. It also means I can more easily illustrate concepts where needed with code, etc. And it also helps to not overload your WhatsApp with a bunch of text. Some posts will be short and some will be longer and I may not be able to put something up every week, depending on workload, but will do my best. I hope it’s something people find useful.\nSome general housekeeping:\n\nYou can view and copy the code as blocks just before each set of output (there will be a Copy to Clipboard button at the top right of each code block); or by clicking the &lt;/&gt; Code button at the top right of the page, then View Source and copying the entire block.\nThere is a light/dark mode toggle on the top right of the page, depending on how you like to view your internet.\nFeel free to add any comments/questions to a post and/or provide general feedback."
  },
  {
    "objectID": "posts/002_01Dec_2023/index.html",
    "href": "posts/002_01Dec_2023/index.html",
    "title": "It pays to think like a Bayesian",
    "section": "",
    "text": "Recall that the question this week was to choose between:\nA) Switch to another door.\nB) Stay with your original door.\nC) It doesn’t matter if you switch or stay."
  },
  {
    "objectID": "posts/002_01Dec_2023/index.html#the-question",
    "href": "posts/002_01Dec_2023/index.html#the-question",
    "title": "It pays to think like a Bayesian",
    "section": "",
    "text": "Recall that the question this week was to choose between:\nA) Switch to another door.\nB) Stay with your original door.\nC) It doesn’t matter if you switch or stay."
  },
  {
    "objectID": "posts/002_01Dec_2023/index.html#the-answer",
    "href": "posts/002_01Dec_2023/index.html#the-answer",
    "title": "It pays to think like a Bayesian",
    "section": "The Answer",
    "text": "The Answer\nThe answer is that you should switch doors, and in fact if you do switch, you double your chances of winning the car - from 33.3% to 66.7%.\nThis is known as the Monty Hall problem and when it was first posed in a magazine column in 1975 managed to confuse readers to the extent that even mathematicians were writing in to the magazine to claim that answer was in fact wrong and staying with the originally chosen door was the better strategy for success.\nThe simplest way that I can explain this is that you start out with a 33.3% chance of winning the car and those probabilities don’t change once you lock in your selection and Monty offers you another chance to choose (i.e. the probabilities don’t change to 50/50 once Monty reveals what’s behind one of the doors).\n\nIf you stay\nIf you choose the correct door to start with (for which there is a 33.3% chance), staying will result in you ending up with the car (winning).\nIf you choose the incorrect door to start with (for which there is a 66.7% chance), staying will necessarily result in you ending up with a goat (losing).\n\n\nIf you switch\nIf you choose the correct door to start with (for which there is a 33.3% chance), switching will result in you ending up with a goat (losing).\nIf you choose the incorrect door to start with (for which there is a 66.7% chance), switching will necessarily result in you ending up with the car, because Monty has to pick the only other losing door to open (winning).\nStaying is associated with a 33.3% success rate, whereas switching doubles your chance of success to 66.7%.\nStop here if equations give you the equivalent of the aftermath of eating Mexican food. What I have done below is show how we can arrive at the same answer using an analytical approach when our logic/intuition fails. You may not want to venture that far…\nWhat I think is cool about this problem is that while the result might seem counterintuitive to how we naturally process chance, using Bayesian reasoning provides a formulaic way to get at the right answer. This again uses conditional probabilities as I introduced them a few weeks ago. Bayesian thinking is about utilising prior knowledge in conjunction with new data to improve or update our knowledge (whereas the Frequentist approach to statistics doesn’t care so much about prior knowledge and instead just uses the data at hand).\n\n\n\n\n\n\nImportant Concept\n\n\n\nBayesian reasoning enables the analysis of data under the light of prior knowledge.\n\n\nBayes Theorem can be written as:\n\\[\nPr(\\theta | data) = \\frac{Pr(data | \\theta) Pr(\\theta)}{Pr(data)}\n\\]\nwhere \\(\\theta\\) could be a particular parameter or hypothesis.\nHere:\n\\(Pr(data | \\theta)\\) is the likelihood function (the data, or what we measure)\n\\(Pr(\\theta)\\) is the prior probability of our hypothesis (prior knowledge before we the measurement)\n\\(Pr(data)\\) is the prior probability of the data\n\\(Pr(\\theta | data)\\) is the posterior probability of our hypothesis (i.e. “in light of the data”)\nOn the Bayesian/Frequentist topic, note that \\(Pr(data | \\theta)\\) is what null-hypothesis significance testing (NHST) encapsulates and this is a Frequentist concept. Whenever we calculate a p value we are asking:\n\n“What is the probability of this new data (or data even more extreme) occurring by chance given the null hypothesis is true?”\n\nBut really, what we want to know most of the time is the opposite:\n\n“What is the probability of the null hypothesis being true given this new data?”\n\nThat is a Bayesian concept and is answered with \\(Pr(\\theta | data)\\). Maybe we should become more Bayesian in how we handle our research…\nAnyway, excuse the digression. We can generalise Bayes Theorem to the Monty Hall problem as:\n\\[\nPr(\\text{car behind door x} | \\text{Monty opens door y}) = \\frac{Pr(\\text{Monty opens door y} | \\text{car behind door x}) Pr(\\text{car behind door x})}{Pr(\\text{Monty opens door y})}\n\\]\nFor the sake of the exercise, let x = door 1 and y = door 3.\nSo we are interested in the probability the car is behind door 1 (that means we picked door 1) when Monty opens door 3 to reveal a goat.\nIn calculating the different components of Bayes Theorem, we first need to enumerate the various probabilities.\n\\(Pr(\\text{car behind door 1}) = Pr(\\text{car behind door 2}) = Pr(\\text{car behind door 3}) = 33.3\\%\\)\nThese are the prior probabilities.\nThen:\n\\(Pr(\\text{Monty opens door 3} | \\text{car behind door 1}) = 50\\%\\)\nMonty can only pick doors 2 or 3, as we picked door 1.\n\\(Pr(\\text{Monty opens door 3} | \\text{car behind door 2}) = 100\\%\\)\nMonty can only pick door 3, as we picked door 1 and he doesn’t want to reveal the car behind door 2.\n\\(Pr(\\text{Monty opens door 3} | \\text{car behind door 3}) = 0\\%\\)\nMonty won’t reveal the car as part of his playing rules.\nThese are the likelihoods or the data.\nThe \\(Pr(\\text{Monty opens door 3})\\) is a little trickier to calculate. Here we don’t need to worry about the car being behind any specific door, only that Monty won’t reveal it. Intuitively, this would be \\(50\\%\\) as he only has two doors to choose from. But you can also work this out by summing the product of each of the prior probabilities and the evidence:\n\\(Pr(\\text{Monty opens door 3}) = (0.33 * 0.5) + (0.33 * 1) + (0.33 * 0) = 0.5\\)\nFinally, we can get to working out the posterior probabilities of the car being behind each door given Monty opens door 3. We use Bayes Theorem as shown above to do this.\n\\[\nPr(\\text{car behind door 1} | \\text{Monty opens door 3}) = \\frac{Pr(\\text{Monty opens door 3} | \\text{car behind door 1}) Pr(\\text{car behind door 1})}{Pr(\\text{Monty opens door 3})}\n\\] \\[\n= \\frac{0.5 * 0.33}{0.5} = 33.3\\%\n\\] Likewise:\n\\[\nPr(\\text{car behind door 2} | \\text{Monty opens door 3})  = \\frac{1 * 0.33}{0.5} = 66.7\\%\n\\] and\n\\[\nPr(\\text{car behind door 3} | \\text{Monty opens door 3})  = \\frac{0 * 0.33}{0.5} = 0\\%\n\\]\nRemember, we initially chose door 1. So, when Monty opens door 3 (this could have been door 2 - we just needed to pick a door for the exercise), we double our chances of winning by switching to door 2. And this is how Bayesian reasoning can come to the rescue when our own intuition fails.\nFurther explanation can be found here if you remain unconvinced:\nhttps://en.wikipedia.org/wiki/Monty_Hall_problem\nhttps://statisticsbyjim.com/fun/monty-hall-problem/"
  },
  {
    "objectID": "posts/006_23Feb_2024/index.html",
    "href": "posts/006_23Feb_2024/index.html",
    "title": "Immortal time bias - “The fallacy that never dies” (Part 2)",
    "section": "",
    "text": "In the last post I introduced the concept of immortal time bias and how it can distort associations in your survival analysis, if you naively misclassify unexposed/untreated observation time as exposed/treated. This week I am going to illustrate the concept with some data and R code. It would have been good to analyse the Oscar Winner’s data but as I could not locate that anywhere online, we are instead going to look at one of the first studies in which immortal time bias was subsequently recognised to be a problem.\nThe work came out of Stanford University in the early 1970s and assessed the survival benefit of potential heart transplant recipients. In the analysis, the event of interest was death and the primary treatment was heart transplantation - so survival amongst transplant recipients was compared to that amongst accepted patients into the program that did not end up receiving a transplant. Treatment was initially considered time-fixed and the patients divided into two groups - ‘ever transplanted’ vs ‘never transplanted’. Survival time amongst recipients was found to be longer than those who didn’t receive transplantation.\nThe immortal time bias here involves the waiting time of those patients who survived to make it to the transplant. Because this portion of the observation time was classified as exposed to transplantation instead of unexposed, it offered a guaranteed survival time to the transplanted group. The result of this misclassification was to produce an artificial increase in the mortality rate of the reference group, thus suggesting a benefit of heart transplant surgery. In a later reanalysis of the data, the apparent survival benefit of the transplanted group disappeared when the immortal time was properly accounted for by a time-dependent analysis."
  },
  {
    "objectID": "posts/006_23Feb_2024/index.html#load-data",
    "href": "posts/006_23Feb_2024/index.html#load-data",
    "title": "Immortal time bias - “The fallacy that never dies” (Part 2)",
    "section": "1 Load data",
    "text": "1 Load data\nAs this study is considered a canonical example of immortal time bias, the data comes built into R’s survival package. We can load the data and inspect the relevant jasa dataframe as below.\n\n\nCode\nlibrary(survival)\nlibrary(survminer)\nlibrary(gtsummary)\nlibrary(dplyr)\ndata(heart, package = \"survival\")\nhead(jasa)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbirth.dt\naccept.dt\ntx.date\nfu.date\nfustat\nsurgery\nage\nfutime\nwait.time\ntransplant\nmismatch\nhla.a2\nmscore\nreject\n\n\n\n\n1937-01-10\n1967-11-15\nNA\n1968-01-03\n1\n0\n30.84463\n49\nNA\n0\nNA\nNA\nNA\nNA\n\n\n1916-03-02\n1968-01-02\nNA\n1968-01-07\n1\n0\n51.83573\n5\nNA\n0\nNA\nNA\nNA\nNA\n\n\n1913-09-19\n1968-01-06\n1968-01-06\n1968-01-21\n1\n0\n54.29706\n15\n0\n1\n2\n0\n1.11\n0\n\n\n1927-12-23\n1968-03-28\n1968-05-02\n1968-05-05\n1\n0\n40.26283\n38\n35\n1\n3\n0\n1.66\n0\n\n\n1947-07-28\n1968-05-10\nNA\n1968-05-27\n1\n0\n20.78576\n17\nNA\n0\nNA\nNA\nNA\nNA\n\n\n1913-11-08\n1968-06-13\nNA\n1968-06-15\n1\n0\n54.59548\n2\nNA\n0\nNA\nNA\nNA\nNA\n\n\n\n\n\n\nThe variables that we’re going to use are:\n\nfustat - the ‘event’ variable; 0 = alive, 1 = dead at the end of follow-up.\nfutime - the primary ‘time’ variable; time (days) from acceptance into the transplant program until death or censoring.\nwait.time - the secondary ‘time’ variable; time (days) from acceptance into the transplant program until receiving a heart if transplanted (NA for those who never underwent transplant surgery).\ntransplant - the ‘treatment/exposure’ variable; 0 = did not receive heart, 1 = received heart."
  },
  {
    "objectID": "posts/006_23Feb_2024/index.html#visualise-individual-survival-trajectories",
    "href": "posts/006_23Feb_2024/index.html#visualise-individual-survival-trajectories",
    "title": "Immortal time bias - “The fallacy that never dies” (Part 2)",
    "section": "2 Visualise individual survival trajectories",
    "text": "2 Visualise individual survival trajectories\nUsing a bit of ggplot2 magic, we can now plot the individual observation times for the 103 patients in the study. Note that I have stratified observation time by transplant status (orange for the period a patient remains untransplanted and blue for the period following a transplant).\n\n\nCode\n# Create 'id' variable\njasa$id &lt;- seq(1:dim(jasa)[1])\n# Replace wait.time with futime if didn't undergo transplant\njasa$wait.time[is.na(jasa$wait.time)] &lt;- jasa$futime[is.na(jasa$wait.time)]\n# Plot\njasa |&gt;\n  ggplot(aes(x = id, y = futime)) +\n  geom_linerange(aes(ymin = 0, ymax = wait.time), color = \"#E7B800\", linewidth = 1) +\n  geom_linerange(aes(ymin = wait.time, ymax = futime), color = \"#2E9FDF\", linewidth = 1) +\n  geom_point(aes(shape = factor(fustat)), stroke = 1, cex = 1, color = \"black\") +\n  scale_shape_manual(values = c(1, 3), labels = c(\"Censored\", \"Died\"), name = \"Outcome\") +\n  annotate(\"text\", x = 95, y = 1400, label = \"Observation time = yellow - untransplanted\", size = 5, color = \"#E7B800\") +\n  annotate(\"text\", x = 92, y = 1380, label = \"Observation time = blue - post-transplant\", size = 5, color = \"#2E9FDF\") +\n  ggtitle(\"Survival Trajectories for Heart Transplant Patients\") +   \n  ylab(\"Time (days)\") +\n  xlab(\"Patient Number\") + \n  coord_flip() + \n  theme_bw(base_size = 20) +\n  theme(axis.text.y = element_text(size = 15))"
  },
  {
    "objectID": "posts/006_23Feb_2024/index.html#naive-analysis-assuming-treatment-status-is-time-fixed",
    "href": "posts/006_23Feb_2024/index.html#naive-analysis-assuming-treatment-status-is-time-fixed",
    "title": "Immortal time bias - “The fallacy that never dies” (Part 2)",
    "section": "3 Naive analysis assuming treatment status is time-fixed",
    "text": "3 Naive analysis assuming treatment status is time-fixed\n\n3.1 Visualise survival curves\nPlotting the Kaplan-Meier survival curves are easy by first saving the survfit object:\nfit &lt;- survfit(Surv(futime, fustat) ~ transplant, data = jasa)\nand then passing this ggsurvplot which does a nicer job of plotting survival data then using R’s base functions. Note that we ignore wait.time and only specify futime in our fit function. This is because we are assuming if a patient was transplanted, the entire duration of their observation period was considered as such.\n\n\nCode\nfit_naive &lt;- survfit(Surv(futime, fustat) ~ transplant, data = jasa)\nggsurvplot(fit_naive,\n          risk.table = TRUE,\n          risk.table.col = \"strata\",\n          linetype = \"strata\",\n          surv.median.line = \"hv\",\n          ggtheme = theme_bw(base_size = 20),\n          palette = c(\"#E7B800\", \"#2E9FDF\"))\n\n\n\n\n\n\n\n\n\n\n\n3.2 Cox model\nFitting a Cox model is also simple with:\nmod_naive &lt;- coxph(Surv(futime, fustat) ~ transplant, data = jasa)\n\n\nCode\nmod_naive &lt;- coxph(Surv(futime, fustat) ~ transplant, data = jasa)\ntbl_regression(mod_naive, exp = T)\n\n\n\n\n\n\n\n\nCharacteristic\nHR1\n95% CI1\np-value\n\n\n\n\ntransplant\n0.27\n0.17, 0.43\n&lt;0.001\n\n\n\n1 HR = Hazard Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nThis gives a HR = 0.27 (95% CI 0.17, 0.43; p &lt; 0.001) indicating that there is about a 73% reduction in the risk of death with transplantation. Pretty effective, right?"
  },
  {
    "objectID": "posts/006_23Feb_2024/index.html#correct-analysis-assuming-treatment-status-is-time-varying",
    "href": "posts/006_23Feb_2024/index.html#correct-analysis-assuming-treatment-status-is-time-varying",
    "title": "Immortal time bias - “The fallacy that never dies” (Part 2)",
    "section": "4 Correct analysis assuming treatment status is time-varying",
    "text": "4 Correct analysis assuming treatment status is time-varying\nUp until now we have just used the data as it’s been presented to us. Each patient has a single observation with all information about them contained in that row of data. However, to perform the correct time-dependent analysis we first need to construct a time-varying version of the treatment (i.e. transplant) variable. This data format is known as ‘counting process’ and in the general case involves creating potentially multiple rows of data for each patient with each row corresponding to a different exposure/treatment period of that patients observation time. In this specific example, we will create an additional row of data for transplanted patients splitting time at the point of transplant, so that the first row contains the time from acceptance into the transplant program to the point of transplant, and the second row contains the time from transplant to either death or censoring. We specify this in ‘start, stop’ format rather than the duration of the interval itself. We will use the tmerge function to do this, although a little bit of manual programming can also achieve the same result.\n\n\nCode\n# Create subset of data selecting relevant variables\njasa_subset &lt;- jasa |&gt; \n  select(id, wait.time, futime, fustat, transplant)\n# Can't have an end time of 0 (one obs) - change this to 0.5\njasa_subset$futime[jasa_subset$futime == 0] &lt;- 0.5\n# Create dataframe in counting process format\njasa_cp &lt;- tmerge(data1 = jasa_subset |&gt; select(id, futime, fustat), \n                  data2 = jasa_subset |&gt; select(id, futime, fustat, wait.time, transplant), \n                  id = id, \n                  death = event(futime, fustat),\n                  transplant = tdc(wait.time)) |&gt; \n            select(-c(futime, fustat))\n\n\nRemember that the original data looked like:\n\n\nCode\nhead(jasa_subset, 7)\n\n\n\n\n\n\nid\nwait.time\nfutime\nfustat\ntransplant\n\n\n\n\n1\n49\n49\n1\n0\n\n\n2\n5\n5\n1\n0\n\n\n3\n0\n15\n1\n1\n\n\n4\n35\n38\n1\n1\n\n\n5\n17\n17\n1\n0\n\n\n6\n2\n2\n1\n0\n\n\n7\n50\n674\n1\n1\n\n\n\n\n\n\nAnd the newly created dataframe in counting process format:\n\n\nCode\nhead(jasa_cp, 9)\n\n\n\n\n\n\nid\ntstart\ntstop\ndeath\ntransplant\n\n\n\n\n1\n0\n49\n1\n0\n\n\n2\n0\n5\n1\n0\n\n\n3\n0\n15\n1\n1\n\n\n4\n0\n35\n0\n0\n\n\n4\n35\n38\n1\n1\n\n\n5\n0\n17\n1\n0\n\n\n6\n0\n2\n1\n0\n\n\n7\n0\n50\n0\n0\n\n\n7\n50\n674\n1\n1\n\n\n\n\n\n\nNote the new ‘start, stop’ time variables. We have also renamed fustat to death for a more intuitive name. In this small data subset, Subject’s 3, 4 and 7 underwent a transplant, but only the latter two had both unexposed and exposed time periods during observation (Subject 3 was transplanted at the beginning of their observation), hence each subject now has two rows of data.\n\n4.1 Visualise survival curves\nPlotting the Kaplan-Meier survival curves for the data in this correct format reveals a vastly different result to that which we viewed earlier. There is now almost no separation in the curves.\n\n\nCode\nfit_correct &lt;- survfit(Surv(tstart, tstop, death) ~ transplant, data = jasa_cp)\nggsurvplot(fit_correct,\n          risk.table = TRUE,\n          risk.table.col = \"strata\",\n          linetype = \"strata\",\n          surv.median.line = \"hv\",\n          ggtheme = theme_bw(base_size = 20),\n          palette = c(\"#E7B800\", \"#2E9FDF\"))\n\n\n\n\n\n\n\n\n\n\n\n4.2 Cox model\nCommensurately, the output of the Cox model now gives a HR = 1.13 (95% CI 0.63, 2.04; p = 0.7) indicating that there is about a 13% increase in the risk of death with transplantation - but this could be as much as a 104% increase or even a 37% decrease. That is, we can’t be confident the observed effect didn’t occur just by chance. Clearly, this tells a different story to the naive analysis we previously conducted.\n\n\nCode\nmod_correct &lt;- coxph(Surv(tstart, tstop, death) ~ transplant, data = jasa_cp)\ntbl_regression(mod_correct, exp = T)\n\n\n\n\n\n\n\n\nCharacteristic\nHR1\n95% CI1\np-value\n\n\n\n\ntransplant\n1.13\n0.63, 2.04\n0.7\n\n\n\n1 HR = Hazard Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nThe lesson here is to always think about whether your exposure or treatment changes over the course of an individual’s observation time, and if it does, to account for that in your survival model by constructing a time-varying covariate."
  },
  {
    "objectID": "posts/009_05Apr_2024/index.html#categorising-the-predictor",
    "href": "posts/009_05Apr_2024/index.html#categorising-the-predictor",
    "title": "Don’t be Scared of Splines",
    "section": "3.1 Categorising the Predictor",
    "text": "3.1 Categorising the Predictor\nI’m not even really going to talk about this - it’s very rarely a good thing to categorise a continuous variable. The loss of information and power and introduction of spurious threshold effects (e.g., by grouping 20- to 29-year-olds in one category and 30- to 39-year-olds in another, we create the impression that 20- and 29-year-olds tend to be more alike than 29- and 30-year-olds) are just some of the reasons why this is a bad idea."
  },
  {
    "objectID": "posts/009_05Apr_2024/index.html#lowess-smoother",
    "href": "posts/009_05Apr_2024/index.html#lowess-smoother",
    "title": "Don’t be Scared of Splines",
    "section": "2.1 Lowess Smoother",
    "text": "2.1 Lowess Smoother\nI mentioned using a lowess smoother above to help visualise any potential association in your data. Lowess regression is a type of non-parametric regression method that fits a smooth curve to your data by calculating a weighted average of Y across a moving span (or window) of X. It is a great initial exploratory method for looking at your data. If we fit a lowess curve to these data, we can see the following:\n\n\nCode\n# Plot lowess\nggplot(dat, aes(x = x, y = y)) +\n  geom_point(size = 3) +\n  geom_smooth(method = \"loess\", se = F, linewidth = 2, color = \"#1F77B4FF\") + \n  xlab(\"x\") + ylab(\"y\") + \n  theme_bw(base_size = 20)"
  },
  {
    "objectID": "posts/009_05Apr_2024/index.html#assuming-linearity",
    "href": "posts/009_05Apr_2024/index.html#assuming-linearity",
    "title": "Don’t be Scared of Splines",
    "section": "3.2 Assuming Linearity",
    "text": "3.2 Assuming Linearity\nIf you bothered to plot the data you would know the association between X and Y was non-linear. But plenty of people don’t bother to do this and just go ahead and fit a model under the assumption of linearity. If we erroneously did this, the best-fitting regression line would appear as in the following plot. Clearly, for these data, this is a bad modelling choice leading one to think there is no association at all between X and Y.\n\n\nCode\n# Model\nmod1 &lt;- lm(y ~ x, data = dat)\n# Predict Y from model\ndat$mod1_pred &lt;- predict(mod1, dat)\n# Plot linear\nggplot(dat, aes(x = x, y = mod1_pred)) +\n  geom_line(linewidth = 2, color = \"deeppink\") + \n  geom_point(data = dat, aes(x = x, y = y), size = 3) +\n  xlab(\"x\") + ylab(\"y\") + \n  theme_bw(base_size = 20)"
  },
  {
    "objectID": "posts/009_05Apr_2024/index.html#assuming-non-linearity---segmented-regression",
    "href": "posts/009_05Apr_2024/index.html#assuming-non-linearity---segmented-regression",
    "title": "Don’t be Scared of Splines",
    "section": "3.3 Assuming Non-Linearity - Segmented Regression",
    "text": "3.3 Assuming Non-Linearity - Segmented Regression\nLet’s now look at a piecewise or linear spline model. Another name for this is segmented regression - a method in which the predictor is partitioned into intervals and a separate line segment is fit to each interval. Essentially, we are fitting multiple, linked linear regression models. To do this we need to first decide where sensible threshold/s exist in the data for us to partition the predictor, allowing approximate linearity within those partitions. In this case, if we look back at the lowess plot, the vertex of the curve represents a reasonable threshold and so we might decide to use a predictor cut-point at X = 50.\nI won’t go into the details of the parameterisation (it’s there in the code), but if we fit such a model and then make model predictions from that, we get the following plot. Clearly this is a much better representation of the actual trend in the data, compared to assuming a linear relationship.\n\n\nCode\n# Create a new variable corresponding to change in slope (using 50 as threshold)\ndat &lt;- dat |&gt; \n  mutate(x50 = (x - 50) * (x &gt;= 50))  # will be 0 if x &lt; 50\n# Model\nmod2 &lt;- lm(y ~ x + x50, data = dat)\n# Predict Y from model\ndat$mod2_pred &lt;- predict(mod2, dat)\n# Plot piecewise\nggplot(dat, aes(x = x, y = mod2_pred)) +\n  geom_line(linewidth = 2, color = \"chartreuse\") + \n  geom_point(data = dat, aes(x = x, y = y), size = 3) +\n  xlab(\"x\") + ylab(\"y\") + \n  theme_bw(base_size = 20)"
  },
  {
    "objectID": "posts/009_05Apr_2024/index.html#assuming-non-linearity---polynomial-regression",
    "href": "posts/009_05Apr_2024/index.html#assuming-non-linearity---polynomial-regression",
    "title": "Don’t be Scared of Splines",
    "section": "3.4 Assuming Non-Linearity - Polynomial Regression",
    "text": "3.4 Assuming Non-Linearity - Polynomial Regression\nPolynomial regression takes this idea further by allowing smoothness to be incorporated into the modelling of the non-linearity. It is a form of regression analysis in which the association between X and Y is modelled as an nth degree polynomial in X. It is important to keep in mind that while the model fits a non-linear curve to the data, the statistical estimation of the model is still considered linear (in the unknown parameters). This differentiates this and models with RCS splines from true non-linear models. See here for some further explanation.\nSo now let’s fit a quadratic model to these data. It doesn’t look like too bad a fit either, does it.\n\n\nCode\n# Model - quadratic\nmod3 &lt;- lm(y ~ x + I(x^2), data = dat)\n# Predict Y from models\ndat$mod3_pred &lt;- predict(mod3, dat)\n# Plot piecewise\nggplot(dat, aes(x = x, y = mod3_pred)) +\n  geom_line(linewidth = 2, color = \"chocolate1\") + \n  geom_point(data = dat, aes(x = x, y = y), size = 3) +\n  xlab(\"x\") + ylab(\"y\") + \n  theme_bw(base_size = 20)"
  },
  {
    "objectID": "posts/009_05Apr_2024/index.html#assuming-non-linearity---rcss",
    "href": "posts/009_05Apr_2024/index.html#assuming-non-linearity---rcss",
    "title": "Don’t be Scared of Splines",
    "section": "3.5 Assuming Non-Linearity - RCS’s",
    "text": "3.5 Assuming Non-Linearity - RCS’s\n\n3.5.1 The Basics of RCS’s\nI’d like to make an important first point in that RCS’s can be applied in any statistical model that linearly relates a predictor to an outcome. We have been emphasising continuous (predictor) vs continuous (outcome) associations because these are the simplest to conceptualise. But the same applies to any generalised linear or survival model.\n\n\n\n\n\n\nNote\n\n\n\nGeneralised linear models use link functions to linearise a predictor on the link scale and it is that scale that we are interested in knowing whether the predictor has an approximately linear relationship with the outcome to ensure model assumptions are met. E.g. for binary outcomes we want to know if the association of the continuous predictor and the logit (log-odds) of the outcome is linear or not. We don’t so much care about the association of the continuous predictor with the odds or the probability of the outcome because we know these to be non-linear. I will hopefully elaborate on this idea in a future post.\n\n\nThe intuition behind RCS’s is that the continuous predictor is broken into multiple intervals at locations called knots and for each interval the association between the predictor and the outcome is estimated separately. The association within each interval can be estimated with increasing complexity - from the linear splines that we have already explored in segmented regression, to cubic splines (polynomials) as we describe them here. I won’t go into detail of the underlying maths because it does get complicated, but a series of spline basis functions are used to ‘build’ the resulting cubic spline within each interval (please see the papers below for more detail.) The cubic splines within each interval are restricted in a couple of ways: firstly, adjacent splines join smoothly at knot locations because their slopes are constrained to be equal at these boundaries, and secondly, the spline functions are constrained to be linear in the tails (i.e., before the ﬁrst and after the last knot)\n\n\n3.5.2 Fitting RCS’s to the Current Data\nThere are multiple packages in R that allow you to fit RCS’s to your data but my go to is the ns() function in the splines package. I would encourage you to look at the papers listed below if you want to explore alternatives. Using ns(), the way to specify a RCS term on the relevant predictor in your model is really quite simple. The main argument that you need to specify is the degrees of freedom (df) which is equivalent to the number of different intervals that you want to model in your predictor-outcome relationship. This also corresponds to the number of RCS coefficients in your model output. However, as alluded to above, this DOES NOT represent the number of internal knots that the model uses under the hood to achieve this - which is always one less. So the take home here is that if you want to model 4 intervals of your predictor for which you feel the association differs with your outcome, you specify df = 4 which signals to ns() that it needs to define 3 internal knots.\nNow the placement of the knots can also be specified, but to be honest I’ve never felt the need to do this. For the most part things seem to work fine with ns() default placement of knots at quantiles of the distribution of the predictor. For example, if you specified df = 4, then 3 internal knots would be placed at the 25th, 50th and 75th percentiles of the distribution of X.\nSo the basic form of a linear model with an ns() term included is then:\nlm(y ~ ns(x, df = 4), data = dat)\nand if you wish to check at what actual values of your predictor ns() has placed the knots, you can use:\nattr(ns(x, df = 4), \"knots\")\nFor an interesting comparison, we are now going to fit 4 models with RCS’s:\n\n2 knots (df = 3)\n3 knots (df = 4)\n4 knots (df = 5)\n20 knots (df = 21)\n\nThe resultant model predictions are shown below.\n\n\nCode\n# Model - rcs with 2 knots\nmod4a &lt;- lm(y ~ ns(x, df = 3), data = dat)\n# Model - rcs with 3 knots\nmod4b &lt;- lm(y ~ ns(x, 4), data = dat)\n# Model - rcs with 4 knots\nmod4c &lt;- lm(y ~ ns(x, 5), data = dat)\n# Model - rcs with 20 knots\nmod4d &lt;- lm(y ~ ns(x, 21), data = dat)\n# Predict Y from models\ndat$mod4a_pred &lt;- predict(mod4a, dat)\ndat$mod4b_pred &lt;- predict(mod4b, dat)\ndat$mod4c_pred &lt;- predict(mod4c, dat)\ndat$mod4d_pred &lt;- predict(mod4d, dat)\n# Convert to long format for easier plotting\ndat_long &lt;- dat |&gt; \n  select(1:2, 7:10) |&gt; \n  pivot_longer(3:6)\n# Plot rcs \nggplot() +\n  geom_line(data = dat_long, aes(x = x, y = value, color = name), linewidth = 2) + \n  geom_point(data = dat, aes(x = x, y = y), size = 3) +\n  scale_color_paletteer_d(\"ggsci::category20_d3\", name = \"RCS - # of knots\", labels = c(\"2\", \"3\", \"4\", \"20\")) +\n  xlab(\"x\") + ylab(\"y\") + \n  theme_bw(base_size = 20) +\n  theme(legend.position = c(1,1), legend.justification = c(2.8,1.1))\n\n\n\n\n\n\n\n\n\nWhat can we gather from this. Well there is no doubt some subjectivity to the interpretation of these plots, but my take is that the RCS with 2 internal knots is actually very similar to the quadratic (polynomial) model - these are both quite ‘smoothed’. We then start to see a little more flexibility in the model fit for the models with 3 and 4 internal knots - and really I’d be hard-pushed to say they’re that different. We can unequivocally say, though, that the model with 20 internal knots looks like it’s picking up a lot of noise in the data - this is a classic case of model over-fitting and we want to avoid this as much as possible. The main issue with overfit models is that they appear to work very well with the data that they were fit to - the predictions are excellent! But the catch is that the model has been fit to the idiosyncrasies of that specific dataset and consequently doesn’t generalise well to any other dataset that you might want to test your model on - for these new data the predictions are now terrible! We should always keep this in mind when we are formulating models to fit to our data, irrespective of whether we are using RCS’s or not.\nWhen we are using RCS’s though, for most applications, three to five internal knots strike a nice balance between complicating the model needlessly and fitting data pleasingly. For these data, it would not be unreasonable to suggest that the models with 3 or 4 internal knots capture the fit nicely (one could argue that the 2-knot model is a little underfit, and of course the 20 knot model is grossly overfit). So, from purely eyeballing the plots, I would tend to settle on the 3-knot model.\n\n\n3.5.3 Model Comparisons\nWe can add some statistical rigour to this intuition by calculating model-fit statistics, and I have done this using the AIC for all the models we have considered in this post. When we use the AIC to help decide on model fit we are looking for (relatively) lower (i.e. more negative numbers). When we calculate these we can see some interesting results. The model assuming linearity is comparatively a terrible fit (AIC = 227). The piecewise model isn’t too bad though (AIC = -23) if we are looking to the RCS models as a gold standard. The quadratic model has a comparatively poorer fit (AIC = 31) and this is actually fairly similar to the 2-knot RCS model (AIC = 11 - remember we said they looked similar). The 3-knot and greater RCS models seem to perform the best, but this is really a case of diminishing returns. We can fairly justify either a 3- (AIC = -34) or 4-knot (AIC = -38) model and I don’t think any reasonable reviewer would criticise you for either choice.\n\n\nCode\n# AIC for each model\nmods_aic &lt;- data.frame(AIC(mod1, mod2, mod3, mod4a, mod4b, mod4c, mod4d))\nmods_aic &lt;- tibble::rownames_to_column(mods_aic, var = \"Model\")\nmods_aic &lt;- mods_aic |&gt; \n  select(-df) |&gt; \n  mutate(Model = case_when(Model == \"mod1\" ~ \"Linear Regression\",\n                           Model == \"mod2\" ~ \"Segmented Regression\",\n                           Model == \"mod3\" ~ \"Polynomial Regression\",\n                           Model == \"mod4a\" ~ \"RCS - 2 knots\",\n                           Model == \"mod4b\" ~ \"RCS - 3 knots\",\n                           Model == \"mod4c\" ~ \"RCS - 4 knots\",\n                           Model == \"mod4d\" ~ \"RCS - 20 knots\"))\nmods_aic\n\n\n\n\n\n\nModel\nAIC\n\n\n\n\nLinear Regression\n226.55549\n\n\nSegmented Regression\n-22.98867\n\n\nPolynomial Regression\n30.77121\n\n\nRCS - 2 knots\n10.55193\n\n\nRCS - 3 knots\n-33.66342\n\n\nRCS - 4 knots\n-38.49374\n\n\nRCS - 20 knots\n-34.52662\n\n\n\n\n\n\n\n\n3.5.4 Interpretation and Presentation of your Results\nOk, so we’re nearly done. You’ve done the hard work of recognising your predictor has a non-linear relationship with your outcome, assessed multiple approaches to modelling that non-linearity and settled on a RCS with 3 knots on the predictor as the best-fitting model. You excitedly run your model and get the following output:\n\n\nCode\n# Model - rcs with 3 knots\nmod4b &lt;- lm(y ~ ns(x, 4), data = dat)\n# Format model results in a table\nmod4b |&gt; gtsummary::tbl_regression()\n\n\n\n\n\n\nModel Output\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nns(x, 4)\n\n\n\n\n\n\n\n\n    ns(x, 4)1\n-2.8\n-3.0, -2.6\n&lt;0.001\n\n\n    ns(x, 4)2\n-0.69\n-0.89, -0.50\n&lt;0.001\n\n\n    ns(x, 4)3\n-1.2\n-1.6, -0.82\n&lt;0.001\n\n\n    ns(x, 4)4\n0.53\n0.34, 0.71\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nWTH?! What does that all mean? Paul, what are you doing to me? I just want the simple output that I’m used to where I can say that a one-unit change in my predictor corresponds to some change in my outcome!\nI apologise for my facetiousness - no doubt you have cottoned on to the fact that you when you specifically model a non-linear association between two variables, there is no longer any constancy in the relationship between the two variables. A one-unit change in X will give a different change in Y depending on the values of X.\nSo, what to do? First recognise that the RCS coefficients presented to you in a regression output are essentially useless from an interpretation point of view. You can’t easily use these to describe the association between X and Y in reporting your results.\nThe general approach to interpretation and reporting of results in the presence of non-linearity in a regression model is to pick salient values (biological, clinical) of X to predict model-estimated values of Y. You can quite easily calculate model-estimated means and differences using our friend emmeans which I described to you in an earlier post.\nFor illustrative purposes, let’s say we’re interested in knowing the model-estimated values of Y corresponding to X values of 20, 30, 50 and 60. We can estimate these values and differences (contrasts) of interest using emmeans.\n\n\nCode\n# Plot rcs \nggplot() +\n  geom_line(data = dat, aes(x = x, y = mod4b_pred), color = \"#EE8635\", linewidth = 2) + \n  geom_point(data = dat, aes(x = x, y = y), size = 3) +\n  geom_vline(xintercept = c(20,30,50,60), color = \"red\", linetype = \"dotted\", linewidth = 1) +\n  scale_x_continuous(limits = c(0, 100), breaks = c(20, 30, 50, 60)) +\n  xlab(\"x\") + ylab(\"y\") + \n  theme_bw(base_size = 20)\n\n\n\n\n\n\n\n\n\nCode\n# emmeans\nemmeans(mod4b, ~ x, at = list(x = c(20,30,50,60))) |&gt; \n  data.frame() |&gt; \n  select(-df) |&gt; \n  rename(\"X\" = \"x\",\n         \"Emmean (Y)\" = \"emmean\",\n         \"95% C.I. (lower)\" = \"lower.CL\",\n         \"95% C.I. (upper)\" = \"upper.CL\") |&gt; \n  flextable() |&gt; \n  colformat_double(j = c(2:5), digits = 3, na_str = \"N/A\") |&gt;\n  set_table_properties(layout = \"autofit\") |&gt; \n  height(height = 1, unit = \"cm\") |&gt; \n  hrule(rule = \"atleast\", part = \"header\") |&gt; \n  align(align = \"center\", part = \"body\") |&gt;\n  bg(bg = \"white\", part = \"all\") |&gt; \n  flextable::font(fontname = \"Consolas\", part = \"all\") |&gt;\n  theme_vanilla()\n\n\nEstimated Marginal MeansXEmmean (Y)SE95% C.I. (lower)95% C.I. (upper)200.3050.0420.2220.38830-0.2480.040-0.328-0.16850-1.0120.043-1.098-0.92560-0.7650.036-0.837-0.693\n\n\n\n\nCode\n# contrasts\nemm &lt;- emmeans(mod4b, ~ x, at = list(x = c(20,30,50,60)))\ncustom &lt;- list(`Change in Y corresponding to change in X from 20 to 30` = c(-1,1,0,0),\n               `Change in Y corresponding to change in X from 50 to 60` = c(0,0,-1,1))\ncontrast(emm, custom) |&gt; \n  summary(infer = T) |&gt; \n  data.frame() |&gt; \n  select(c(-df, -t.ratio)) |&gt; \n  rename(\"Contrast\" = \"contrast\",\n         \"Estimate\" = \"estimate\",\n         \"95% C.I. (lower)\" = \"lower.CL\",\n         \"95% C.I. (upper)\" = \"upper.CL\",\n         \"p\" = \"p.value\") |&gt; \n  flextable() |&gt; \n  colformat_double(digits = 3, na_str = \"N/A\") |&gt;\n  set_table_properties(layout = \"autofit\") |&gt; \n  height(height = 1, unit = \"cm\") |&gt; \n  hrule(rule = \"atleast\", part = \"header\") |&gt; \n  align(align = \"center\", part = \"body\") |&gt;\n  bg(bg = \"white\", part = \"all\") |&gt; \n  flextable::font(fontname = \"Consolas\", part = \"all\") |&gt;\n  theme_vanilla()\n\n\nContrasts of Estimated Marginal MeansContrastEstimateSE95% C.I. (lower)95% C.I. (upper)pChange in Y corresponding to change in X from 20 to 30-0.5530.019-0.592-0.5150.000Change in Y corresponding to change in X from 50 to 600.2470.0260.1960.2970.000\n\n\nThat’s probably enough on RCS’s for now - this post has turned out longer than I initially anticipated. I hope you have found it helpful and above all, found some motivation to using RCS’s in your modelling endeavours if you are not already.\n\n\n3.5.5 Extra Reading\nIf you want to dive a little deeper into RCS’s than we’ve done here, I can thoroughly recommend the following 3 papers:\nModeling non-linear relationships in epidemiological data: The application and interpretation of spline models\nCubic splines to model relationships between continuous variables and outcomes: a guide for clinicians\nA review of spline function procedures in R"
  },
  {
    "objectID": "posts/010_19Apr_2024/index.html",
    "href": "posts/010_19Apr_2024/index.html",
    "title": "Everything is a Linear Model",
    "section": "",
    "text": "I want to share with you a secret - maybe you already know it. It took me a while into my statistical learnings to realise this and since then I’ve seen people write about it (see here and here for examples). But the basic idea is that many of the common statistical tests that we use (e.g. t-test, ANOVA, etc) are really nothing more than variations on the general linear model that we’re all accustomed to:\n\\[ y = ax + b \\]\nThe former are specific-use tests, whereas the latter is an ‘umbrella’ model that can be broadly adapted to accomplish each of the same tasks - perhaps there’s something to be said for learning just one set of syntax. Let me illustrate this to you with one example using the two-sample t-test. We’ll use the genderweight dataset from the datarium package in R which consists of the bodyweights of 40 subjects (20 males, 20 females). We’re interested in working out whether there is a gender difference. A look at the data shows:\n\n\nCode\nlibrary(ggplot2)\ndata(\"genderweight\", package = \"datarium\")\nhead(genderweight, 10)\n\n\n\n\n\n\nid\ngroup\nweight\n\n\n\n\n1\nF\n61.58587\n\n\n2\nF\n64.55486\n\n\n3\nF\n66.16888\n\n\n4\nF\n59.30860\n\n\n5\nF\n64.85825\n\n\n6\nF\n65.01211\n\n\n7\nF\n62.85052\n\n\n8\nF\n62.90674\n\n\n9\nF\n62.87110\n\n\n10\nF\n62.21992\n\n\n\n\n\n\n\n1 Plot the Data\nIt’s always helpful to first plot the data:\n\n\nCode\nggplot(genderweight, aes(x = group, y = weight)) +\n  geom_jitter(size = 3, width = 0.05) +\n  scale_y_continuous(limits = c(50, 100), breaks = seq(50, 100, by = 10)) +\n  stat_summary(fun = mean, \n               geom = \"errorbar\", \n               aes(ymax = after_stat(y), ymin = after_stat(y)), \n               width = 0.25) +\n  theme_bw(base_size = 20)\n\n\n\n\n\n\n\n\n\n\n\n2 Two-Sample t-Test\nNow, we can run our standard t-test as follows (by default, computing the Welch version of the test which does not assume the same variances in each group). In words, we are asking to test the difference in weight by group (i.e. males vs females).\nt.test(weight ~ group, data = genderweight)\n\n\nCode\nt.test(weight ~ group, data = genderweight)\n\n\n\n    Welch Two Sample t-test\n\ndata:  weight by group\nt = -20.791, df = 26.872, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n95 percent confidence interval:\n -24.53135 -20.12353\nsample estimates:\nmean in group F mean in group M \n       63.49867        85.82612 \n\n\nThis output tells us that the mean weight in females and males is 63.5 kg and 85.8 kg, respectively. Furthermore, the 95% C.I. for the difference (note that is does not give us the actual difference) in those two weights is -24.5, -20.1 and as the interval does not contain 0 this is statistically significant (as also reflected in the p-value).\n\n\n3 Linear Model\nNow, the equivalent linear model (i.e. linear regression) in R is simply:\nsummary(lm(weight ~ group, data = genderweight))\n\n\nCode\nsummary(lm(weight ~ group, data = genderweight))\n\n\n\nCall:\nlm(formula = weight ~ group, data = genderweight)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8163 -1.3647 -0.4869  1.3980  9.2365 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  63.4987     0.7593   83.62   &lt;2e-16 ***\ngroupM       22.3274     1.0739   20.79   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.396 on 38 degrees of freedom\nMultiple R-squared:  0.9192,    Adjusted R-squared:  0.9171 \nF-statistic: 432.3 on 1 and 38 DF,  p-value: &lt; 2.2e-16\n\n\nThe output is slightly different but the information contained is almost the same. (Intercept) represents the mean weight in the reference category of the group variable (in this case females). groupM represents the difference in means between females and males (22.3 kg). Note that the 95% C.I.’s aren’t presented as part of this standard output, but we can obtain that information easily enough with:\nconfint(lm(weight ~ group, data = genderweight))\n\n\nCode\nconfint(lm(weight ~ group, data = genderweight))\n\n\n               2.5 %   97.5 %\n(Intercept) 61.96145 65.03589\ngroupM      20.15349 24.50140\n\n\nNote the slight difference in the 95% C.I.’s to that obtained from the t-test. The general linear model, by assumption, assumes homogeneity of variances among the two groups.\nFinally, if you would prefer to know the actual mean values of each group as well, it’s possible to amend the lm call slightly by removing the intercept term. This gives:\nsummary(lm(weight ~ group - 1, data = genderweight))\n\n\nCode\nsummary(lm(weight ~ group - 1, data = genderweight))\n\n\n\nCall:\nlm(formula = weight ~ group - 1, data = genderweight)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8163 -1.3647 -0.4869  1.3980  9.2365 \n\nCoefficients:\n       Estimate Std. Error t value Pr(&gt;|t|)    \ngroupF  63.4987     0.7593   83.62   &lt;2e-16 ***\ngroupM  85.8261     0.7593  113.03   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.396 on 38 degrees of freedom\nMultiple R-squared:  0.9981,    Adjusted R-squared:  0.998 \nF-statistic:  9884 on 2 and 38 DF,  p-value: &lt; 2.2e-16\n\n\nThe two-sample t-test is just one example of a special case of the general linear model. The first link I provided above contains a neat pdf describing many other special cases and I would encourage you to have a look at these. While you might still use these specific tests in your day to day work, it is nonetheless helpful to broaden your statistical knowledge in the realisation that the general linear model is fundamental to all of these."
  },
  {
    "objectID": "posts/013_31May_2024/index.html",
    "href": "posts/013_31May_2024/index.html",
    "title": "Reshaping Data - Think Before you Pivot",
    "section": "",
    "text": "In your day-to-day data analysis work you will probably find yourself at some point needing to reshape data, and this is usually to suit an analytical need. Reshaping is changing the rectangular structure of the columns and rows in your dataset without altering the content. Data comes in two basic shapes: wide and long."
  },
  {
    "objectID": "posts/011_03May_2024/index.html",
    "href": "posts/011_03May_2024/index.html",
    "title": "gtsummary - Your New Go-To for Tables",
    "section": "",
    "text": "I thought I should bring this excellent package to your attention if you weren’t aware that it exists, as I have taken gtsummary somewhat for granted over the last few years since it first appeared on CRAN. I’m prompted in part due to a research student having to recently remake several “Table 1” - style tables (following a data change) in manuscript preparation for submission and they were going to redo this manually. When they realised what gtsummary could do in terms of saving them time, I think they were fairly impressed. So today, I’m just going to show you a couple of basic functionalities of this package. It is extremely extensible and if you can’t find answers for your own customisation needs on the homepage or vignette, I have found googling the issue often brings an answer. The developer is also quite active on stackoverflow.com. The homepage can be found at:\nhttps://www.danieldsjoberg.com/gtsummary/index.html\nWe going to use a publicly available MS dataset, so if you want to run the code yourself you will first need to download the data from:\nBrain MRI dataset of multiple sclerosis with consensus manual lesion segmentation and patient meta information\nThis dataset contains the demographic and clinical data on 60 patients (MRI data in accompanying datasets available at link).\n\n1 Load and Inspect the Data\nLet’s have a look at the first few lines:\n\n\nCode\nhead(dat, 10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nGender\nAge\nAge.of.onset\nEDSS\nDoes.the.time.difference.between.MRI.acquisition.and.EDSS…two.months\nTypes.of.Medicines\nPresenting.Symptom\nDose.the.patient.has.Co.moroidity\nPyramidal\nCerebella\nBrain.stem\nSensory\nSphincters\nVisual\nMental\nSpeech\nMotor.System\nSensory.System\nCoordination\nGait\nBowel.and.bladder.function\nMobility\nMental.State\nOptic.discs\nFields\nNystagmus\nOcular.Movement\nSwallowing\n\n\n\n\n1\nF\n56\n43\n3.0\nNo\nGelenia\nMotor\nNo\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n2\nF\n29\n19\n1.5\nNo\nGelenia\nSensory\nNo\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n3\nF\n15\n8\n4.0\nNo\nTysabri\nMotor\nNo\n1\n1\n0\n0\n0\n1\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n4\nF\n24\n20\n6.0\nNo\nTysabri\nSensory\nNo\n1\n1\n1\n0\n1\n0\n0\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n5\nF\n33\n31\n0.0\nNo\nAvonex\nPain\nNo\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n6\nF\n44\n40\n5.0\nNo\nAvonex\nMotor\nNo\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n1\n0\n0\n0\n0\n1\n0\n0\n\n\n7\nM\n43\n40\n3.5\nNo\nBetaferon\nMotor & Visual\nNo\n0\n1\n0\n0\n0\n0\n1\n0\n1\n1\n1\n0\n0\n1\n0\n1\n0\n0\n0\n0\n\n\n8\nF\n32\n30\n1.0\nNo\nGelenia\nVisual\nNo\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n9\nF\n36\n33\n6.0\nNo\nGelenia\nMotore\nNo\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n10\nF\n39\n35\n3.0\nNo\nBetaferon\nMotor & Behavioural\nNo\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n2 Summary Table\nLet’s say you want to create a summary table showing descriptive statistics of the various demographic and clinical characteristics, stratified by DMT (Types.of.Medicines). In the first instance, this can be a basic call of tbl_summary() specifying Types.of.Medicines as the stratifying variable. We want to specify medians (IQR) and n’s (%’s) as the summary statistics.\n\n\nCode\nlibrary(gtsummary)\ndat |&gt; \n  select(-ID) |&gt; \n  tbl_summary(\n    by = Types.of.Medicines,\n    statistic = list(all_continuous() ~ \"{median} ({p25},{p75})\",\n                     all_categorical() ~ \"{n}/{N} ({p}%)\"),\n    digits = all_continuous() ~ 1) |&gt; \n  add_overall()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOverall, N = 601\nAvonex, N = 51\nBetaferon, N = 241\nGelenia, N = 91\nRebif, N = 141\nTysabri, N = 81\n\n\n\n\nGender\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    F\n46/60 (77%)\n5/5 (100%)\n15/24 (63%)\n9/9 (100%)\n10/14 (71%)\n7/8 (88%)\n\n\n    M\n13/60 (22%)\n0/5 (0%)\n8/24 (33%)\n0/9 (0%)\n4/14 (29%)\n1/8 (13%)\n\n\n    N\n1/60 (1.7%)\n0/5 (0%)\n1/24 (4.2%)\n0/9 (0%)\n0/14 (0%)\n0/8 (0%)\n\n\nAge\n33.0 (20.0,42.3)\n24.0 (23.0,33.0)\n37.5 (23.8,43.0)\n42.0 (36.0,52.0)\n32.5 (18.5,38.0)\n20.5 (15.0,24.3)\n\n\nAge.of.onset\n30.5 (19.8,40.0)\n20.0 (20.0,31.0)\n35.0 (23.0,41.0)\n40.0 (30.0,42.0)\n31.0 (18.5,37.0)\n17.0 (16.3,21.3)\n\n\nEDSS\n2.0 (1.0,3.5)\n1.5 (1.0,4.0)\n2.3 (1.0,3.1)\n3.0 (1.5,3.0)\n1.3 (1.0,2.4)\n3.0 (1.4,4.3)\n\n\nDoes.the.time.difference.between.MRI.acquisition.and.EDSS...two.months\n26/60 (43%)\n0/5 (0%)\n10/24 (42%)\n3/9 (33%)\n11/14 (79%)\n2/8 (25%)\n\n\nPresenting.Symptom\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Balance\n4/60 (6.7%)\n0/5 (0%)\n2/24 (8.3%)\n0/9 (0%)\n2/14 (14%)\n0/8 (0%)\n\n\n    Balance &Motor\n1/60 (1.7%)\n0/5 (0%)\n0/24 (0%)\n0/9 (0%)\n0/14 (0%)\n1/8 (13%)\n\n\n    Motor\n10/60 (17%)\n1/5 (20%)\n3/24 (13%)\n1/9 (11%)\n3/14 (21%)\n2/8 (25%)\n\n\n    Motor & Behavioural\n1/60 (1.7%)\n0/5 (0%)\n1/24 (4.2%)\n0/9 (0%)\n0/14 (0%)\n0/8 (0%)\n\n\n    Motor & Sensory\n1/60 (1.7%)\n0/5 (0%)\n1/24 (4.2%)\n0/9 (0%)\n0/14 (0%)\n0/8 (0%)\n\n\n    Motor & Visual\n2/60 (3.3%)\n0/5 (0%)\n2/24 (8.3%)\n0/9 (0%)\n0/14 (0%)\n0/8 (0%)\n\n\n    Motore\n1/60 (1.7%)\n0/5 (0%)\n0/24 (0%)\n1/9 (11%)\n0/14 (0%)\n0/8 (0%)\n\n\n    Pain\n1/60 (1.7%)\n1/5 (20%)\n0/24 (0%)\n0/9 (0%)\n0/14 (0%)\n0/8 (0%)\n\n\n    Sensory\n19/60 (32%)\n0/5 (0%)\n8/24 (33%)\n3/9 (33%)\n7/14 (50%)\n1/8 (13%)\n\n\n    Sensory & Visual\n1/60 (1.7%)\n0/5 (0%)\n1/24 (4.2%)\n0/9 (0%)\n0/14 (0%)\n0/8 (0%)\n\n\n    Sensory & Motor\n1/60 (1.7%)\n0/5 (0%)\n1/24 (4.2%)\n0/9 (0%)\n0/14 (0%)\n0/8 (0%)\n\n\n    Sensory & Visual\n1/60 (1.7%)\n0/5 (0%)\n0/24 (0%)\n1/9 (11%)\n0/14 (0%)\n0/8 (0%)\n\n\n    Sensory & Visual ,Balance , Motor, Sexual,Fatigue\n1/60 (1.7%)\n0/5 (0%)\n1/24 (4.2%)\n0/9 (0%)\n0/14 (0%)\n0/8 (0%)\n\n\n    Sensory &Motor\n1/60 (1.7%)\n0/5 (0%)\n0/24 (0%)\n0/9 (0%)\n0/14 (0%)\n1/8 (13%)\n\n\n    Visual\n14/60 (23%)\n3/5 (60%)\n4/24 (17%)\n2/9 (22%)\n2/14 (14%)\n3/8 (38%)\n\n\n    Visual & Balance\n1/60 (1.7%)\n0/5 (0%)\n0/24 (0%)\n1/9 (11%)\n0/14 (0%)\n0/8 (0%)\n\n\nDose.the.patient.has.Co.moroidity\n13/60 (22%)\n0/5 (0%)\n8/24 (33%)\n3/9 (33%)\n2/14 (14%)\n0/8 (0%)\n\n\nPyramidal\n31/60 (52%)\n2/5 (40%)\n14/24 (58%)\n5/9 (56%)\n4/14 (29%)\n6/8 (75%)\n\n\nCerebella\n17/60 (28%)\n1/5 (20%)\n8/24 (33%)\n2/9 (22%)\n3/14 (21%)\n3/8 (38%)\n\n\nBrain.stem\n5/60 (8.3%)\n1/5 (20%)\n1/24 (4.2%)\n0/9 (0%)\n1/14 (7.1%)\n2/8 (25%)\n\n\nSensory\n18/60 (30%)\n1/5 (20%)\n8/24 (33%)\n3/9 (33%)\n3/14 (21%)\n3/8 (38%)\n\n\nSphincters\n9/60 (15%)\n0/5 (0%)\n5/24 (21%)\n0/9 (0%)\n2/14 (14%)\n2/8 (25%)\n\n\nVisual\n17/60 (28%)\n3/5 (60%)\n6/24 (25%)\n2/9 (22%)\n2/14 (14%)\n4/8 (50%)\n\n\nMental\n2/60 (3.3%)\n0/5 (0%)\n2/24 (8.3%)\n0/9 (0%)\n0/14 (0%)\n0/8 (0%)\n\n\nSpeech\n6/60 (10%)\n0/5 (0%)\n4/24 (17%)\n0/9 (0%)\n1/14 (7.1%)\n1/8 (13%)\n\n\nMotor.System\n35/60 (58%)\n3/5 (60%)\n14/24 (58%)\n5/9 (56%)\n6/14 (43%)\n7/8 (88%)\n\n\nSensory.System\n19/60 (32%)\n0/5 (0%)\n8/24 (33%)\n4/9 (44%)\n4/14 (29%)\n3/8 (38%)\n\n\nCoordination\n17/60 (28%)\n2/5 (40%)\n6/24 (25%)\n2/9 (22%)\n2/14 (14%)\n5/8 (63%)\n\n\nGait\n17/60 (28%)\n2/5 (40%)\n7/24 (29%)\n1/9 (11%)\n4/14 (29%)\n3/8 (38%)\n\n\nBowel.and.bladder.function\n9/60 (15%)\n1/5 (20%)\n2/24 (8.3%)\n1/9 (11%)\n3/14 (21%)\n2/8 (25%)\n\n\nMobility\n4/60 (6.7%)\n0/5 (0%)\n2/24 (8.3%)\n1/9 (11%)\n1/14 (7.1%)\n0/8 (0%)\n\n\nMental.State\n3/60 (5.0%)\n0/5 (0%)\n2/24 (8.3%)\n0/9 (0%)\n1/14 (7.1%)\n0/8 (0%)\n\n\nOptic.discs\n22/60 (37%)\n2/5 (40%)\n8/24 (33%)\n3/9 (33%)\n4/14 (29%)\n5/8 (63%)\n\n\nFields\n0/60 (0%)\n0/5 (0%)\n0/24 (0%)\n0/9 (0%)\n0/14 (0%)\n0/8 (0%)\n\n\nNystagmus\n7/60 (12%)\n1/5 (20%)\n3/24 (13%)\n2/9 (22%)\n0/14 (0%)\n1/8 (13%)\n\n\nOcular.Movement\n2/60 (3.3%)\n0/5 (0%)\n0/24 (0%)\n1/9 (11%)\n0/14 (0%)\n1/8 (13%)\n\n\nSwallowing\n3/60 (5.0%)\n0/5 (0%)\n3/24 (13%)\n0/9 (0%)\n0/14 (0%)\n0/8 (0%)\n\n\n\n1 n/N (%); Median (25%,75%)\n\n\n\n\n\n\n\n\nIn fact, that’s a pretty good start. However, we think that including the column frequency as the denominator in every cell is just clutter, so let’s remove that. We’ll also include an argument for reporting missingness if any exists. Additionally, we want to tidy up some of the variable names - I’ll just do Age, Age.of.onset and the somewhat convoluted Does.the.time.difference.between.MRI.acquisition.and.EDSS...two.months for now. In fact, for the latter we’ll make it a short name and include a footnote to expand on the variable description.\n\n\nCode\ndat |&gt; \n  select(-ID) |&gt; \n  tbl_summary(\n    by = Types.of.Medicines,\n    statistic = list(all_continuous() ~ \"{median} ({p25},{p75})\",\n                     all_categorical() ~ \"{n} ({p}%)\"),\n    digits = all_continuous() ~ 1,\n    missing_text = \"(Missing)\",\n    label = c(Age ~ \"Age, yrs - median (IQR)\",\n              Age.of.onset ~ \"Age onset, yrs - median (IQR)\",\n              Does.the.time.difference.between.MRI.acquisition.and.EDSS...two.months ~ \"Time difference &lt; 2 months\")) |&gt; \n    modify_table_styling(columns = label,\n                         rows = label == \"Time difference &lt; 2 months\",\n                         footnote = \"Does the time difference between MRI acquisition and EDSS &lt; two months\") |&gt; \n  add_overall()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOverall, N = 601\nAvonex, N = 51\nBetaferon, N = 241\nGelenia, N = 91\nRebif, N = 141\nTysabri, N = 81\n\n\n\n\nGender\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    F\n46 (77%)\n5 (100%)\n15 (63%)\n9 (100%)\n10 (71%)\n7 (88%)\n\n\n    M\n13 (22%)\n0 (0%)\n8 (33%)\n0 (0%)\n4 (29%)\n1 (13%)\n\n\n    N\n1 (1.7%)\n0 (0%)\n1 (4.2%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\nAge, yrs - median (IQR)\n33.0 (20.0,42.3)\n24.0 (23.0,33.0)\n37.5 (23.8,43.0)\n42.0 (36.0,52.0)\n32.5 (18.5,38.0)\n20.5 (15.0,24.3)\n\n\nAge onset, yrs - median (IQR)\n30.5 (19.8,40.0)\n20.0 (20.0,31.0)\n35.0 (23.0,41.0)\n40.0 (30.0,42.0)\n31.0 (18.5,37.0)\n17.0 (16.3,21.3)\n\n\nEDSS\n2.0 (1.0,3.5)\n1.5 (1.0,4.0)\n2.3 (1.0,3.1)\n3.0 (1.5,3.0)\n1.3 (1.0,2.4)\n3.0 (1.4,4.3)\n\n\nTime difference &lt; 2 months2\n26 (43%)\n0 (0%)\n10 (42%)\n3 (33%)\n11 (79%)\n2 (25%)\n\n\nPresenting.Symptom\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Balance\n4 (6.7%)\n0 (0%)\n2 (8.3%)\n0 (0%)\n2 (14%)\n0 (0%)\n\n\n    Balance &Motor\n1 (1.7%)\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n1 (13%)\n\n\n    Motor\n10 (17%)\n1 (20%)\n3 (13%)\n1 (11%)\n3 (21%)\n2 (25%)\n\n\n    Motor & Behavioural\n1 (1.7%)\n0 (0%)\n1 (4.2%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\n    Motor & Sensory\n1 (1.7%)\n0 (0%)\n1 (4.2%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\n    Motor & Visual\n2 (3.3%)\n0 (0%)\n2 (8.3%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\n    Motore\n1 (1.7%)\n0 (0%)\n0 (0%)\n1 (11%)\n0 (0%)\n0 (0%)\n\n\n    Pain\n1 (1.7%)\n1 (20%)\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\n    Sensory\n19 (32%)\n0 (0%)\n8 (33%)\n3 (33%)\n7 (50%)\n1 (13%)\n\n\n    Sensory & Visual\n1 (1.7%)\n0 (0%)\n1 (4.2%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\n    Sensory & Motor\n1 (1.7%)\n0 (0%)\n1 (4.2%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\n    Sensory & Visual\n1 (1.7%)\n0 (0%)\n0 (0%)\n1 (11%)\n0 (0%)\n0 (0%)\n\n\n    Sensory & Visual ,Balance , Motor, Sexual,Fatigue\n1 (1.7%)\n0 (0%)\n1 (4.2%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\n    Sensory &Motor\n1 (1.7%)\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n1 (13%)\n\n\n    Visual\n14 (23%)\n3 (60%)\n4 (17%)\n2 (22%)\n2 (14%)\n3 (38%)\n\n\n    Visual & Balance\n1 (1.7%)\n0 (0%)\n0 (0%)\n1 (11%)\n0 (0%)\n0 (0%)\n\n\nDose.the.patient.has.Co.moroidity\n13 (22%)\n0 (0%)\n8 (33%)\n3 (33%)\n2 (14%)\n0 (0%)\n\n\nPyramidal\n31 (52%)\n2 (40%)\n14 (58%)\n5 (56%)\n4 (29%)\n6 (75%)\n\n\nCerebella\n17 (28%)\n1 (20%)\n8 (33%)\n2 (22%)\n3 (21%)\n3 (38%)\n\n\nBrain.stem\n5 (8.3%)\n1 (20%)\n1 (4.2%)\n0 (0%)\n1 (7.1%)\n2 (25%)\n\n\nSensory\n18 (30%)\n1 (20%)\n8 (33%)\n3 (33%)\n3 (21%)\n3 (38%)\n\n\nSphincters\n9 (15%)\n0 (0%)\n5 (21%)\n0 (0%)\n2 (14%)\n2 (25%)\n\n\nVisual\n17 (28%)\n3 (60%)\n6 (25%)\n2 (22%)\n2 (14%)\n4 (50%)\n\n\nMental\n2 (3.3%)\n0 (0%)\n2 (8.3%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\nSpeech\n6 (10%)\n0 (0%)\n4 (17%)\n0 (0%)\n1 (7.1%)\n1 (13%)\n\n\nMotor.System\n35 (58%)\n3 (60%)\n14 (58%)\n5 (56%)\n6 (43%)\n7 (88%)\n\n\nSensory.System\n19 (32%)\n0 (0%)\n8 (33%)\n4 (44%)\n4 (29%)\n3 (38%)\n\n\nCoordination\n17 (28%)\n2 (40%)\n6 (25%)\n2 (22%)\n2 (14%)\n5 (63%)\n\n\nGait\n17 (28%)\n2 (40%)\n7 (29%)\n1 (11%)\n4 (29%)\n3 (38%)\n\n\nBowel.and.bladder.function\n9 (15%)\n1 (20%)\n2 (8.3%)\n1 (11%)\n3 (21%)\n2 (25%)\n\n\nMobility\n4 (6.7%)\n0 (0%)\n2 (8.3%)\n1 (11%)\n1 (7.1%)\n0 (0%)\n\n\nMental.State\n3 (5.0%)\n0 (0%)\n2 (8.3%)\n0 (0%)\n1 (7.1%)\n0 (0%)\n\n\nOptic.discs\n22 (37%)\n2 (40%)\n8 (33%)\n3 (33%)\n4 (29%)\n5 (63%)\n\n\nFields\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\nNystagmus\n7 (12%)\n1 (20%)\n3 (13%)\n2 (22%)\n0 (0%)\n1 (13%)\n\n\nOcular.Movement\n2 (3.3%)\n0 (0%)\n0 (0%)\n1 (11%)\n0 (0%)\n1 (13%)\n\n\nSwallowing\n3 (5.0%)\n0 (0%)\n3 (13%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\n\n1 n (%); Median (25%,75%)\n\n\n2 Does the time difference between MRI acquisition and EDSS &lt; two months\n\n\n\n\n\n\n\n\nIf you want to save the created table, you can do this in one of two ways. The first is save it directly as a .docx file which should work most of the time. However, if you notice any formatting issues, change the save target file extension to .html, then open that in Word and you should be ok as well. An important point is to first save the table in your R script to an object - e.g.\ntbl &lt;- dat |&gt; tbl_summary(...\nThe command to save the table as a Word (or html file is then):\ngt::gtsave(as_gt(tbl), filename = \"summary_table.docx\", path = \"...your_path.../\")\n\n\n3 Regression Table\ngtsummary’s other strength is in making regression tables, and the relevant workhorse function here is tbl_regression().\nLet’s say we’re interested in the association between Age onset and the presence of Sensory symptoms (I don’t really know whether this makes sense or not but it’s just to run a regression). The outcome variable here is binary, so we’ll need to specify a logistic regression model. We can do that as follows in R and we obtain the standard (fairly bland from the point of view of presentation/collaboration) ouput:\n\n\nCode\nmod &lt;- glm(Sensory ~ Age.of.onset, family = 'binomial', data = dat)\nsummary(mod)\n\n\n\nCall:\nglm(formula = Sensory ~ Age.of.onset, family = \"binomial\", data = dat)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -1.75743    0.87101  -2.018   0.0436 *\nAge.of.onset  0.02987    0.02641   1.131   0.2581  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 73.304  on 59  degrees of freedom\nResidual deviance: 71.994  on 58  degrees of freedom\nAIC: 75.994\n\nNumber of Fisher Scoring iterations: 4\n\n\nLet’s pretty this up by passing the model results through tbl_regression():\n\n\nCode\nmod |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\nCharacteristic\nlog(OR)1\n95% CI1\np-value\n\n\n\n\nAge.of.onset\n0.03\n-0.02, 0.08\n0.3\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nNot bad, but we’d like the output to be in terms of odds-ratios rather than log odds-ratios. That’s actually quite simple to do:\n\n\nCode\nmod |&gt; \n  tbl_regression(exponentiate = T)\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nAge.of.onset\n1.03\n0.98, 1.09\n0.3\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nWhat if you want to include some model summary fit-statistics:\n\n\nCode\nmod |&gt; \n  tbl_regression(exponentiate = T) |&gt; \n  add_glance_source_note()\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    Age.of.onset\n1.03\n0.98, 1.09\n0.3\n  \n  \n    \n      Null deviance = 73.3; Null df = 59.0; Log-likelihood = -36.0; AIC = 76.0; BIC = 80.2; Deviance = 72.0; Residual df = 58; No. Obs. = 60\n    \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nThat’s all great, but I’ve just noticed that the predictor variable isn’t formatted so well, so let’s change that.\n\n\nCode\nmod |&gt; \n  tbl_regression(exponentiate = T,\n                 label = c(Age.of.onset ~ \"Age onset\")) |&gt; \n  add_glance_source_note()\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    Age onset\n1.03\n0.98, 1.09\n0.3\n  \n  \n    \n      Null deviance = 73.3; Null df = 59.0; Log-likelihood = -36.0; AIC = 76.0; BIC = 80.2; Deviance = 72.0; Residual df = 58; No. Obs. = 60\n    \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\ntbl_regression() supports almost any model you can throw at it.\n\n\n4 Last Word\nI hope you find both of these functions useful in your day-to-day coding and data analysis - they are great additions to your R toolkit, not only for their time-saving capabilities, but also the fantastic improvements to the visual style of results formatting that you can achieve, for which base R often falls far short."
  },
  {
    "objectID": "posts/012_17May_2024/index.html",
    "href": "posts/012_17May_2024/index.html",
    "title": "Ordinal Logistic Regression for Ordinal Outcomes?",
    "section": "",
    "text": "It has become clear to me over the years that there is usually no one correct way to kick a statistical goal - several approaches might all converge to the same “best” answer. To that end it’s also often difficult to identify an unequivocally incorrect way to do something. So data analysis is not black and white - there are shades of grey (not 50 though). However, there are some approaches that are well-intentioned but ill-considered and some that are performed purely as shortcuts - whether that be for the sake of time or simplicity. Both conditions can lead to results and their interpretation that are misleading at best.\nWhere am I heading with this? Well, today’s post is based on something as fundamental as the scale that we use to measure or record our data on. It might be good at this point to take a few moments to look over the following link (there is no point in me re-inventing the wheel as Harvey Motulsky - the author of Prism - explains things so eloquently):\nMeasurement/Variable Scales\nThis page describes a ‘hierarchy’ of measurement/variable scales (nominal -&gt; ordinal -&gt; interval -&gt; ratio) as well as their differences. If you’re unfamiliar with these concepts, it would be worthwhile brushing up on them as they are integral to the discussion that follows."
  },
  {
    "objectID": "posts/012_17May_2024/index.html#the-data",
    "href": "posts/012_17May_2024/index.html#the-data",
    "title": "Ordinal Logistic Regression for Ordinal Outcomes?",
    "section": "3.1 The Data",
    "text": "3.1 The Data\nWe first need to load some data to experiment on, and we’ll use the same publicly available MS dataset that we used in the last post. So if you want to run the code yourself you will first need to download the data from:\nBrain MRI dataset of multiple sclerosis with consensus manual lesion segmentation and patient meta information\nThis dataset contains the demographic and clinical data on 60 patients (MRI data in accompanying datasets available at link)."
  },
  {
    "objectID": "posts/012_17May_2024/index.html#load-and-inspect-the-data",
    "href": "posts/012_17May_2024/index.html#load-and-inspect-the-data",
    "title": "Ordinal Logistic Regression for Ordinal Outcomes?",
    "section": "3.1 Load and Inspect the Data",
    "text": "3.1 Load and Inspect the Data\nWe first need to load some data to experiment on, and we’ll use the same publicly available MS dataset that we used in the last post. So if you want to run the code yourself you will first need to download the data from:\nBrain MRI dataset of multiple sclerosis with consensus manual lesion segmentation and patient meta information\nThis dataset contains the demographic and clinical data on 60 patients (MRI data in accompanying datasets available at link).\nLet’s have a look at the first few lines:\n\n\nCode\nhead(dat, 10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nGender\nAge\nAge.of.onset\nEDSS\nDoes.the.time.difference.between.MRI.acquisition.and.EDSS…two.months\nTypes.of.Medicines\nPresenting.Symptom\nDose.the.patient.has.Co.moroidity\nPyramidal\nCerebella\nBrain.stem\nSensory\nSphincters\nVisual\nMental\nSpeech\nMotor.System\nSensory.System\nCoordination\nGait\nBowel.and.bladder.function\nMobility\nMental.State\nOptic.discs\nFields\nNystagmus\nOcular.Movement\nSwallowing\n\n\n\n\n1\nF\n56\n43\n3.0\nNo\nGelenia\nMotor\nNo\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n2\nF\n29\n19\n1.5\nNo\nGelenia\nSensory\nNo\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n3\nF\n15\n8\n4.0\nNo\nTysabri\nMotor\nNo\n1\n1\n0\n0\n0\n1\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n4\nF\n24\n20\n6.0\nNo\nTysabri\nSensory\nNo\n1\n1\n1\n0\n1\n0\n0\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n5\nF\n33\n31\n0.0\nNo\nAvonex\nPain\nNo\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n6\nF\n44\n40\n5.0\nNo\nAvonex\nMotor\nNo\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n1\n0\n0\n0\n0\n1\n0\n0\n\n\n7\nM\n43\n40\n3.5\nNo\nBetaferon\nMotor & Visual\nNo\n0\n1\n0\n0\n0\n0\n1\n0\n1\n1\n1\n0\n0\n1\n0\n1\n0\n0\n0\n0\n\n\n8\nF\n32\n30\n1.0\nNo\nGelenia\nVisual\nNo\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n9\nF\n36\n33\n6.0\nNo\nGelenia\nMotore\nNo\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n10\nF\n39\n35\n3.0\nNo\nBetaferon\nMotor & Behavioural\nNo\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n\n\n\n\nLet’s say we want to regress EDSS on Sensory (the presence of sensory symptoms). In the first instance it’s always good to visualise the data, so let’s do that by using some boxplots. It appears that sensory symptoms are associated with higher EDSS - eyeballing the plot suggests that the median EDSS is ~ 1.5 in the absence of Sensory symptoms and ~ 3 when Sensory symptoms are present.\n\n\nCode\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(gtsummary) \nlibrary(ordinal)\nlibrary(ggeffects)\nggplot(dat, aes(x = factor(Sensory), y = EDSS)) +\n  geom_boxplot() +\n  geom_dotplot(binaxis = 'y', stackdir = 'center', position = position_dodge(1), dotsize = 0.8) +\n  xlab(\"Sensory Symptoms\") +\n  theme_bw(base_size = 20)"
  },
  {
    "objectID": "posts/012_17May_2024/index.html#linear-regression",
    "href": "posts/012_17May_2024/index.html#linear-regression",
    "title": "Ordinal Logistic Regression for Ordinal Outcomes?",
    "section": "3.2 Linear Regression",
    "text": "3.2 Linear Regression\nOne way to model the association between the two variables would be to simply treat EDSS as a numeric variable. That’s very straightforward and I’m sure you’ve run a linear regression model before. I’ll use the gtsummary package that I highlighted in the last post to format the results.\n\n\nCode\nmod1 &lt;- lm(EDSS ~ Sensory, data = dat)\nmod1 |&gt; tbl_regression(intercept = T)\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n2.0\n1.5, 2.5\n&lt;0.001\n\n\nSensory\n0.93\n0.05, 1.8\n0.038\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nWhile we are now estimating means and not medians, the model results are mostly in line with trend observed in the boxplot. The EDSS in the reference group (no symptoms) is on average, 2.0, and the difference in EDSS in the group with symptoms is an additional 0.93 units (i.e. 2.9 on average).\nThere is nothing too difficult about that - if you are willing to accept that the difference between any two equally spaced EDSS scores has about the same clinical impact on the patient, and you are willing to accept the assumptions that go along with linear regression."
  },
  {
    "objectID": "posts/012_17May_2024/index.html#ordinal-logistic-regression",
    "href": "posts/012_17May_2024/index.html#ordinal-logistic-regression",
    "title": "Ordinal Logistic Regression for Ordinal Outcomes?",
    "section": "3.3 Ordinal Logistic Regression",
    "text": "3.3 Ordinal Logistic Regression\nAn alternative approach to model the association between the two variables is to treat EDSS as an ordinal variable. Ordinal logistic regression (using the proportional-odds or the cumulative-logit model) may be used with an outcome variable that consists of three or more categories to model the cumulative probability of falling in any particular category or those below, versus all categories above. In other words we are considering cumulative probabilities up to a threshold, thereby making the whole range of ordinal categories binary at that threshold. Another way to think about the cumulative-logit model is that it essentially consists of a set of binary logistic regression models for each possible binary dichotomisation (i.e. threshold/cut-point) of the ordinal outcome. For example, in the simplest case of an (ordered) three-category outcome, there are two possible thresholds and thus two possible binary logistic regression models. The proportional-odds model allows a comparison of category 1 vs categories 2-3, and simultaneously categories 1-2 vs 3, producing a kind of averaged or summary odds ratio reflecting one overall ‘effect’ estimate (if one traditionally considered ‘important’ assumption is met - more on this soon).\nConceptually, the proportional-odds model is based on the idea of a continuous latent outcome - think of a normal distribution. While we cannot observe or measure this construct, the actual observed ordered categories are mapped to this latent continuous variable and used in the model estimation process. In addition to the ‘effect’ estimates for each predictor in the model, a series of intercepts (which represent the thresholds or cutpoints) are also typically reported, depending on your software. Let’s actually run one of these models now.\nThere are a couple of packages in R that estimate proportional-odds models and today we’ll use the ordinal package. The essential function here is clm(). An important thing to note with these models is that your outcome has to be formatted as a factor. The basic specification is simple and in line with typical R models:\nclm(EDSS ~ Sensory, data = dat, link = \"logit\")\nWe will format our results using the excellent gtsummary package I introduced you to in the last post.\n\n\nCode\ndat$EDSS &lt;- factor(dat$EDSS) # format EDSS as factor\nmod2 &lt;- clm(EDSS ~ factor(Sensory), data = dat, link = \"logit\")\nmod2 |&gt; tbl_regression(exponentiate = T, intercept = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\n0|0.5\n0.12\n\n\n&lt;0.001\n\n\n0.5|1\n0.23\n\n\n&lt;0.001\n\n\n1|1.5\n0.81\n\n\n0.5\n\n\n1.5|2\n1.36\n\n\n0.3\n\n\n2|2.5\n1.82\n\n\n0.062\n\n\n2.5|3\n2.12\n\n\n0.022\n\n\n3|3.5\n3.91\n\n\n&lt;0.001\n\n\n3.5|4\n5.62\n\n\n&lt;0.001\n\n\n4|4.5\n14.6\n\n\n&lt;0.001\n\n\n4.5|5\n18.0\n\n\n&lt;0.001\n\n\n5|6\n47.7\n\n\n&lt;0.001\n\n\nfactor(Sensory)\n\n\n\n\n\n\n\n\n    0\n—\n—\n\n\n\n\n    1\n3.08\n1.18, 8.31\n0.024\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nSo let’s not worry too much about the intercepts and instead just focus on the OR for the predictor of interest (Sensory). The basic interpretation of this is that sensory symptoms are associated with higher EDSS - specifically, there is an ~ 3-fold increase in the odds of having a higher EDSS vs a lower EDSS at any of the possible cutpoints that result in a hypothetical dichotomisation of the data. An example of this might be a group of patients with EDSS above 2 vs below 2, or equivalently a group of patients with EDSS above 5 vs below 5. Note that we are getting the same direction of effect that we observed with a simple linear regression (really, we shouldn’t have expected anything else).\nNow, let’s talk about model assumptions. Compared to the linear regression model, the proportional-odds model is relatively assumption-free. This actually makes it an alternative modelling option even for continuous (interval) outcomes when these fail to meet the standard assumptions (mostly to do with model residuals). This has been described here. There is one assumption, however, that has historically been deemed important to the validity of the proportional-odds model, and that is aptly called the proportional odds assumption. In a nutshell, violation of this assumption suggests that the association of predictor variable with the ordinal outcome depends on the level (category) of the outcome. Hence, the assumption of a single summary OR then supposedly becomes untenable.\nThere are a couple of ways to test the proportional-odds assumption and the simplest is with the Brant-Wald test. A p-value of less than 0.05 on this test — particularly on the Omnibus plus at least one of the variables (if you have multiple predictors) — should be interpreted as a failure of the proportional odds assumption.\n\n\nCode\nlibrary(gofcat)\nbrant.test(mod2)\n\n\n\nBrant Test:\n                    chi-sq   df   pr(&gt;chi)  \nOmnibus               17.3   10      0.068 .\nfactor(Sensory)1      17.3   10      0.068 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nH0: Proportional odds assumption holds\n\n\nThis is a very simple model, so I would have hoped that the test would pass. But I must say that I have not had a lot of luck in my own experience - proportional-odds fail by this test more often than they pass. It does seem well known, however, that the test can falsely reject the null hypothesis that the assumption is satisfied, leading to an incorrect conclusion that the analysis is invalid. On this topic, I am willing to pay more attention to what Frank Harrell on his blog (not that I am using it as a get-out-of-jail-free card):\nViolation of Proportional Odds is Not Fatal\nOne last thing before we finish this section. I’m an advocate of not just stopping at the model results. Model coefficients tell you something, for sure - the ‘effect’ on the outcome of a one-unit change in the predictor. But that doesn’t really tell you what the predicted values of the outcome (for given values of the predictor) actually are. So I always think it’s a good idea to plot the model predictions. Let’s do that now with the following code:\n\n\nCode\n# Create new dataframe to predict on\nnewdat &lt;- data.frame(Sensory = c(0, 1)) |&gt; \n  mutate(Sensory = factor(Sensory))\n# Predict on the linear (log-odds) scale\nmod2predict  &lt;- cbind(newdat, predict(mod2, newdat, interval = T, type = \"prob\"))\n# Put estimated probabilities into long format\ndat_long_est &lt;- mod2predict |&gt;\n  pivot_longer(2:13,\n    names_to = \"outcome_val\",\n    values_to = \"pred_prob\") |&gt; \n  select(Sensory, outcome_val, pred_prob)\n# Put estimated lower CI into long format\ndat_long_lowerci &lt;- mod2predict |&gt;\n  pivot_longer(14:25,\n    names_to = \"outcome_val\",\n    values_to = \"pred_lowerci\") |&gt; \n  select(Sensory, outcome_val, pred_lowerci)\n# Put estimated upper CI into long format\ndat_long_upperci &lt;- mod2predict |&gt;\n  pivot_longer(26:37,\n    names_to = \"outcome_val\",\n    values_to = \"pred_upperci\") |&gt; \n  select(Sensory, outcome_val, pred_upperci)\n# cbind together\ndat_long &lt;- cbind(dat_long_est, dat_long_lowerci[3], dat_long_upperci[3])\n# Create EDSS variable from outcome_val\ndat_long &lt;- dat_long |&gt; \n  mutate(EDSS = as.numeric(str_sub(outcome_val, 5, -1)))\n# Plot\nggplot(dat_long, aes(x = EDSS, y = pred_prob)) + \n  geom_point(aes(color = Sensory), position = position_dodge(width = 0.4), size = 2) +\n  geom_errorbar(aes(ymin = pred_lowerci, ymax = pred_upperci, color = Sensory), position = position_dodge(width = 0.4), width = 0.4, linewidth = 0.8) + \n  scale_x_continuous(limits = c(0, 6), breaks = seq(0, 6, by = 0.5)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.1)) +\n  scale_color_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n  xlab(\"EDSS\") + ylab(\"Predicted Probability\") +\n  theme_bw(base_size = 20)\n\n\n\n\n\n\n\n\n\nThis shows the predicted probability of having each level of EDSS, stratified by the presence or absence of sensory symptoms. Compared to being symptom-free, the presence of sensory symptoms are associated with higher predicted probabilities as one’s EDSS increases."
  },
  {
    "objectID": "posts/013_31May_2024/index.html#primary-transformations",
    "href": "posts/013_31May_2024/index.html#primary-transformations",
    "title": "Logistic Regression - Under the Hood",
    "section": "3.1 Primary Transformations",
    "text": "3.1 Primary Transformations\nIf we have the odds of an event, then we can transform that to a probability - or a risk, which is another way to refer to the chance of an outcome - by dividing the odds by the odds plus 1.\n\\[\\text{p} = \\frac{\\text{odds}}{1 + \\text{odds}}\\]\nIf instead we have the probability of an event, we can transform that to the equivalent odds of the event, by dividing the probability by its complement (that is 1 minus the probability).\n\\[\\text{odds} = \\frac{\\text{p}}{1 - \\text{p}}\\]\nAnd the log-odds is simply taking the natural logarithm of the odds - there is nothing difficult about that.\n\\[\\text{log-odds} = \\text{log(odds)}\\]\nSo these three are what I consider primary transformations and we can cycle between them quite easily with a calculator."
  },
  {
    "objectID": "posts/013_31May_2024/index.html#secondary-transformations",
    "href": "posts/013_31May_2024/index.html#secondary-transformations",
    "title": "Logistic Regression - Under the Hood",
    "section": "3.2 Secondary Transformations",
    "text": "3.2 Secondary Transformations\nThere are additional transformations that you can make and while these aren’t quite as straightforward, they are even more fundamental to logistic regression, which ultimately uses the log-odds as the scale to estimate its parameters on.\nThe first is that we can get to a probability directly from the log-odds by taking the exponent of the log-odds divided by the exponent of the log-odds plus one.\n\\[\\text{p} = \\frac{e^{\\text{log-odds}}}{1 + e^{\\text{log-odds}}}\\]\nAnd as you can see, that’s really no different to the first equation above because the exponent of the log-odds is simply the odds (the reverse transformation of logging a number is to take the exponent of that logged number). i.e.\n\\[\\text{odds} = e^{\\text{log-odds}}\\]\nNow, the final transformation, and the one that’s most relevant to logistic regression shows the relationship between the log-odds and probability and is known as the logit function (red box) - and we’ll talk more about this in a minute.\n\\[\\text{log-odds} = \\text{log}\\Bigl(\\frac{\\text{p}}{1 - \\text{p}}\\Bigr)\\]"
  },
  {
    "objectID": "posts/013_31May_2024/index.html#applied-to-the-research-question",
    "href": "posts/013_31May_2024/index.html#applied-to-the-research-question",
    "title": "Logistic Regression - Under the Hood",
    "section": "3.3 Applied to the Research Question",
    "text": "3.3 Applied to the Research Question\n\n\nCode\n# Recreate data from Ophthalmic statistics note 11: logistic regression.\n# Original source: A comparison of several methods of macular hole measurement using optical coherence tomography, and their value in predicting anatomical and visual outcomes.\n\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(ggmagnify)\nlibrary(emmeans)\n\n# Simulate data ----\nn &lt;- 1000                    # don't change this unless necessary (plots might be fragile)\nset.seed(1234)\nx  &lt;-  rnorm(n, 486, 142)    # generate macular hole inner opening data with mean 486 and sd = 152\nz  &lt;-  10.89 - 0.016 * x     # generate variable that is linear combination of intercept = 10.89 and coefficient for macular hole -0.016 (logit scale)\npr  &lt;-  1/(1 + exp(-z))      # generate probabilities from this\ny  &lt;-  rbinom(n, 1, pr)      # generate outcome variable as a function of those probabilities\n\n# Create dataframe from these:\ndf &lt;-  data.frame(y = y, x = x, z = z, pr = pr)\ndf &lt;- df |&gt; \n  filter(x &gt; 100) # only include those with thickness &gt; 100\n\n# Logistic regression model ----\n# Rescale x to 1 unit = 100 microns instead of 1 micron\nsummary(mod_logistic &lt;- glm(y ~ I(x/100), data = df, family = \"binomial\"))\n\n\n\nCall:\nglm(formula = y ~ I(x/100), family = \"binomial\", data = df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  10.3501     0.7456   13.88   &lt;2e-16 ***\nI(x/100)     -1.5045     0.1212  -12.42   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 773.74  on 989  degrees of freedom\nResidual deviance: 494.67  on 988  degrees of freedom\nAIC: 498.67\n\nNumber of Fisher Scoring iterations: 6\n\n\nCode\n# Emmeans of logodds at 600 and 700 microns\nemmeans_df &lt;- data.frame(emmeans(mod_logistic, ~ x, at = list(x = c(600, 700))))\n# Create df of emmeans and corresponding odds and probs, for plotting\nemmeans_df &lt;- emmeans_df |&gt; \n  select(x, emmean) |&gt; \n  rename(logodds = emmean) |&gt; \n  mutate(odds = round(exp(logodds), 3),\n         probs = round(plogis(logodds), 2),\n         logodds = round(logodds, 3))\n\n# Predictions ----\n# Create new df to predict on new values of x\nnew_dat &lt;- data.frame(x = seq(from = 0, to = 1200, length.out = 100))\n# Predict new fitted values and SE's on logodds scale\npred_logodds &lt;- predict(mod_logistic, newdata = new_dat, type = \"link\", se = TRUE)\nnew_dat &lt;- cbind(new_dat, pred_logodds)\n# Create new df of predictions\npredictions &lt;- new_dat |&gt; \n  rename(pred_logodds_est = fit) |&gt; \n  mutate(pred_logodds_LL = pred_logodds_est - (1.96 * se.fit),\n         pred_logodds_UL = pred_logodds_est + (1.96 * se.fit)) |&gt; \n  select(-c(se.fit, residual.scale))\n# Predict new fitted values and SE's on odds scale\npredictions &lt;- predictions |&gt; \n  mutate(pred_odds_est = exp(pred_logodds_est),\n         pred_odds_LL = exp(pred_logodds_LL),\n         pred_odds_UL = exp(pred_logodds_UL))\n# Predict new fitted values and SE's on probability scale\npred_probs &lt;- predict(mod_logistic, newdata = new_dat, type = \"response\", se = TRUE)\nnew_dat &lt;- cbind(new_dat[1], pred_probs)\nnew_dat &lt;- new_dat |&gt; \n  mutate(pred_probs_LL = fit - (1.96 * se.fit),\n         pred_probs_UL = fit + (1.96 * se.fit))\n# Add predicted probs and CIs to predictions df\npredictions &lt;- cbind(predictions, \n                     pred_probs_est = new_dat$fit, \n                     pred_probs_LL = new_dat$pred_probs_LL,\n                     pred_probs_UL = new_dat$pred_probs_UL)\n\n# Reformat plots slightly for ggarrange ----\np3a &lt;- ggplot(predictions, aes(x = x, y = pred_logodds_est)) + \n  geom_ribbon(aes(ymin = pred_logodds_LL, ymax = pred_logodds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = x, y = y), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1150, y = 6, label = \"log-odds\", size = 10) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-100, 100), breaks = seq(-100, 100, by = 2)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(-8, 8)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(x, logodds),\n                            label = emmeans_df$logodds, \n                            nudge_x = c(-50, 50), nudge_y = c(4, -4),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  theme_bw(base_size = 25) +\n  ylab(\"\") +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\n\np4a &lt;- ggplot(predictions, aes(x = x, y = pred_odds_est)) + \n  geom_ribbon(aes(ymin = pred_odds_LL, ymax = pred_odds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = x, y = y), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1150, y = 6000, label = \"odds\", size = 10) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(seq(0, 1000000, by = 1000))) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(0, 7000)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ylab(\"\") +\n  theme_bw(base_size = 25) +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\np4a_inset &lt;- p4a +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(1,2,3,4,5, seq(0, 1000000, by = 1000))) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(x, odds),\n                            label = emmeans_df$odds, \n                            nudge_x = c(-50, 50), nudge_y = c(-1, 2),\n                            color = \"red\", segment.size = 0.2, size = 5)\np4a &lt;- p4a + geom_magnify(from = c(xmin = 500, xmax = 1000, ymin = 0, ymax = 5), \n                          to = c(xmin = 465, xmax = 1010, ymin = 1000, ymax = 5000), \n                          shadow = T, axes = \"y\", plot = p4a_inset)\n\np5a &lt;- ggplot(predictions, aes(x = x, y = pred_probs_est)) + \n  geom_ribbon(aes(ymin = pred_probs_LL, ymax = pred_probs_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = x, y = y), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1150, y = 0.8, label = \"probability\", size = 10) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(x, probs),\n                            label = emmeans_df$probs, \n                            nudge_x = c(-50, 50), nudge_y = c(-0.1, 0.1),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  ylab(\"\") + xlab(\"Macular hole thickness\") +\n  theme_bw(base_size = 25)\nggarrange(p3a, p4a, p5a, align = \"v\", ncol = 1, heights = c(1,1,1.2))"
  },
  {
    "objectID": "posts/013_31May_2024/index.html#probability-vs-odds-as-applied-to-the-research-question",
    "href": "posts/013_31May_2024/index.html#probability-vs-odds-as-applied-to-the-research-question",
    "title": "Logistic Regression - Under the Hood",
    "section": "3.3 Probability vs Odds as Applied to the Research Question",
    "text": "3.3 Probability vs Odds as Applied to the Research Question\nLet’s go back now to our original research question and specifically look at the difference between the probability and odds of an event - I want to solidify this concept in your minds. For most of us probability (or chance) is much more intuitive than the notion of odds. If I think about the likelihood of it being a wet day, I think there’s (for example) an 80% chance of it raining, rather than 4 to 1 odds in favour of it raining. However, if you like to wager on the horses than you may actually be quite comfortable with thinking in terms of odds - as those are the terms bets are placed in.\nIn the following I’ve simulated some data from the original paper (based on the summary statistics and regression model provided). Obviously macular hole size is a continuous variable, but for the sake of this illustration I am going to dichotomise it at it’s approximate mean value (480 microns). From this we can draw up a 2 x 2 contingency table of the number of successful and failed macular hole repairs as a function of the dichotomised version of macular hole size.\nWhen we talk about probabilities we focus on the proportion of successes out of the total for each row. So, the probability (or risk) of successful repair for those patients with a macular hole size less than 480 microns is therefore 479/487 or about 0.98. That’s in contrast to the probability of successful repair in those patients with a size greater than 480 microns - where the chance of success is 380/503 or 0.76 - so lower as one would expect. You can then take a ratio of those two probabilities or risks to give the classic risk ratio and that equals 1.3 which tells you that the chance of successful repair is 30% greater if you have a smaller macular hole at initial presentation.\nNow let’s look at the equivalent odds. When we talk about odds we focus on the ratio of successes to failures and don’t really worry about the totals. So the odds of successful repair for those patients with a smaller initial hole is 479/8 or 60 to 1. The odds of success with a larger hole are a lot more modest - 380/123 or about 3 to 1. We can then calculate the classic odds ratio which works out at about 20, meaning the odds of successful repair is about 20 times greater in those with thinner initial holes.\nNote that both of these effect measures are commensurate - they point in exactly the same direction - that you fare better with a smaller hole at diagnosis. They are just different ways of expressing the same thing."
  },
  {
    "objectID": "posts/013_31May_2024/index.html#probability-vs-odds-and-as-applied-to-the-research-question",
    "href": "posts/013_31May_2024/index.html#probability-vs-odds-and-as-applied-to-the-research-question",
    "title": "Logistic Regression - Under the Hood",
    "section": "3.3 Probability vs Odds (and as applied to the Research Question)",
    "text": "3.3 Probability vs Odds (and as applied to the Research Question)\nLet’s go back now to our original research question and specifically look at the difference between the probability and odds of an event - I want to solidify this concept in your minds. For most of us probability (or chance) is much more intuitive than the notion of odds. If I think about the likelihood of it being a wet day, I think there’s (for example) an 80% chance of it raining, rather than 4 to 1 odds in favour of it raining. However, if you like to wager on the horses than you may actually be quite comfortable with thinking in terms of odds - as those are the terms bets are placed in.\nIn the following I’ve simulated some data from the original paper (based on the summary statistics and regression model provided). Obviously macular hole size is a continuous variable, but for the sake of this illustration I am going to dichotomise it at it’s approximate mean value (480 microns). From this we can draw up a 2 x 2 contingency table of the number of successful and failed macular hole repairs as a function of the dichotomised version of macular hole size.\nWhen we talk about probabilities we focus on the proportion of successes out of the total for each row. So, the probability (or risk) of successful repair for those patients with a macular hole size less than 480 microns is therefore 479/487 or about 0.98. That’s in contrast to the probability of successful repair in those patients with a size greater than 480 microns - where the chance of success is 380/503 or 0.76 - so lower as one would expect. You can then take a ratio of those two probabilities or risks to give the classic risk ratio and that equals 1.3 which tells you that the chance of successful repair is 30% greater if you have a smaller macular hole at initial presentation.\nNow let’s look at the equivalent odds. When we talk about odds we focus on the ratio of successes to failures and don’t really worry about the totals. So the odds of successful repair for those patients with a smaller initial hole is 479/8 or 60 to 1. The odds of success with a larger hole are a lot more modest - 380/123 or about 3 to 1. We can then calculate the classic odds ratio which works out at about 20, meaning the odds of successful repair is about 20 times greater in those with thinner initial holes.\nNote that both of these effect measures are commensurate - they point in exactly the same direction - that you fare better with a smaller hole at diagnosis. They are just different ways of expressing the same thing."
  },
  {
    "objectID": "posts/013_31May_2024/index.html#the-logit-link-function-and-why-log-odds-are-actually-important",
    "href": "posts/013_31May_2024/index.html#the-logit-link-function-and-why-log-odds-are-actually-important",
    "title": "Logistic Regression - Under the Hood",
    "section": "4.1 The Logit Link Function (and why log-odds are actually important)",
    "text": "4.1 The Logit Link Function (and why log-odds are actually important)\nThe log-odds or the logit function is pivotal to how logistic regression works. The logit function is a type of link function and there’s really nothing mysterious or difficult about this. A link function is just a function of the mean of Y - in other words a transformation of the mean of Y - and we use this link function as the outcome instead of Y itself.\n\\[\\text{logit(p)} = \\text{log}\\Bigl(\\frac{\\text{p}}{1 - \\text{p}}\\Bigr)\\] The logistic model then becomes:\n\\[\\text{Y} = \\text{logit(p)} = \\beta_0 \\; + \\; \\beta_1x\\]\nThis allows the outcome to become continuous and unbounded and the association between Y and X to become linear, therefore satisfying those fundamental assumptions of the linear model that I mentioned earlier.\nThus, the logit is a type of transformation that links or maps probability values from \\((0, 1)\\) to real numbers \\({\\displaystyle (-\\infty ,+\\infty )}\\). Now, on the log-odds/logit scale the effect of a one-unit change in X on Y becomes constant once again."
  },
  {
    "objectID": "posts/013_31May_2024/index.html#wth-log-oddsoddsprobabilities---youre-confusing-me---pick-one-please",
    "href": "posts/013_31May_2024/index.html#wth-log-oddsoddsprobabilities---youre-confusing-me---pick-one-please",
    "title": "Logistic Regression - Under the Hood",
    "section": "4.2 WTH? log-odds/odds/probabilities - You’re confusing me - pick one please!!",
    "text": "4.2 WTH? log-odds/odds/probabilities - You’re confusing me - pick one please!!\nIt’s important to remember that logistic regression is really about probabilities, not odds. Probabilities are more intuitive than odds (unless you like to put a bet on the horses), so why use odds? Well, model results have historically been expressed in terms of odds and odds ratios for mathematical convenience. As it is for the log-odds scale, the ‘effect’ of a one-unit change in X on Y is also constant on the odds scale (but as a ratio of two odds, not as a difference in two log-odds). So an odds ratio describes a constant ‘effect’ of a predictor on the odds of an outcome, irrespective of the value of the predictor. This is not the case on the probability scale, where both differences and ratios vary per unit change depending on the value of the predictor (I will illustrate this later).\nThe TLDR:\n\nModel is estimated (under the hood) on the log-odds scale to satisfy modelling assumptions.\nModel results are displayed on the odds scale (as odds ratios) for their constancy.\nProbabilities are calculated post-hoc with not much extra effort."
  },
  {
    "objectID": "posts/013_31May_2024/index.html#back-to-the-research-question",
    "href": "posts/013_31May_2024/index.html#back-to-the-research-question",
    "title": "Logistic Regression - Under the Hood",
    "section": "4.3 Back to the Research Question",
    "text": "4.3 Back to the Research Question\nLet’s go back to the original research question for the final time and now illustrate these concepts in that context. The regression equation for surgical repair success as a function of macular hole size that was estimated in our paper is shown here.\n\\[\\text{logit(p)} = 10.89 - 0.016 \\; \\text{x} \\;\\text{macular hole size}\\;(\\mu m)\\]\nBasically, the log-odds of successful repair can be predicted by calculating the result of 10.89 minus the product of 0.016 and whatever the macular hole size is in microns.\n\n4.3.1 Simulate Some Data\nWe can simulate some data from that regression equation - essentially reverse engineering fake data from the estimated best fit equation on the actual data. Why do this? Well, it gives us some sense of what the actual data may have looked like that the researchers collected empirically. We can then do all sorts of cool things like refit a model, generate model predictions, etc. Data simulation is something I am still learning more about but it opens up a world of possibilities in your analytical work.\nFor this simulation I used a normal distribution with a mean and SD for macular hole size of 486 and 142, respectively (the numbers came from the original research paper). The code to show how to reproduce this in R is included below but you can do something similar in Stata and in most other scriptable statistical software packages.\n\n\nCode\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(emmeans)\nlibrary(gtsummary)\nlibrary(ggmagnify)\nlibrary(ggpubr)\n# Simulation\nn &lt;- 1000                                          # set number of obs to simulate\nset.seed(1234)                                     # set seed for reproducibility\nmac_hole_size  &lt;-  rnorm(n, 486, 142)              # generate macular hole inner opening data with mean 486 and sd = 152\nlogodds_success  &lt;-  10.89 - 0.016 * mac_hole_size # generate variable that is linear combination of intercept = 10.89 and coefficient for macular hole -0.016 (logit scale)\nodds_success  &lt;-  exp(logodds_success)             # exponentiate logodds to get odds\nprob_success  &lt;-  odds_success/(1 + odds_success)  # generate probabilities from this\ny_success  &lt;-  rbinom(n, 1, prob_success)          # generate outcome variable as a function of those probabilities\ndf &lt;-  data.frame(cbind(mac_hole_size,             # combine into dataframe\n                        logodds_success,\n                        odds_success,\n                        prob_success, \n                        y_success))\ndf &lt;- df |&gt; \n  filter(mac_hole_size &gt; 100)                       # only include those with size &gt; 100\n\n\nEssentially, we generate some random values of macular hole size based on the specified distribution and then plug those values in to the regression equation and calculate the various outcome measures. The resultant data look like (first 20 rows shown):\n\n\nCode\nhead(df, 20) |&gt; \n  kable(align = \"c\", digits = 2) |&gt; \n  kable_styling() |&gt; \n  row_spec(1, bold = T, background = \"skyblue1\") |&gt; \n  row_spec(20, bold = T, background = \"lightgoldenrod1\")\n\n\n\n\n\nmac_hole_size\nlogodds_success\nodds_success\nprob_success\ny_success\n\n\n\n\n314.60\n5.86\n349.48\n1.00\n1\n\n\n525.39\n2.48\n11.99\n0.92\n1\n\n\n639.99\n0.65\n1.92\n0.66\n1\n\n\n152.91\n8.44\n4644.44\n1.00\n1\n\n\n546.94\n2.14\n8.49\n0.89\n1\n\n\n557.86\n1.96\n7.13\n0.88\n1\n\n\n404.39\n4.42\n83.08\n0.99\n1\n\n\n408.38\n4.36\n77.94\n0.99\n1\n\n\n405.85\n4.40\n81.16\n0.99\n1\n\n\n359.61\n5.14\n170.06\n0.99\n1\n\n\n418.24\n4.20\n66.57\n0.99\n1\n\n\n344.23\n5.38\n217.53\n1.00\n1\n\n\n375.77\n4.88\n131.32\n0.99\n1\n\n\n495.15\n2.97\n19.44\n0.95\n1\n\n\n622.25\n0.93\n2.54\n0.72\n1\n\n\n470.34\n3.36\n28.92\n0.97\n1\n\n\n413.44\n4.28\n71.88\n0.99\n1\n\n\n356.61\n5.18\n178.44\n0.99\n1\n\n\n367.12\n5.02\n150.82\n0.99\n1\n\n\n829.05\n-2.37\n0.09\n0.09\n0\n\n\n\n\n\n\n\n\nLet’s pick a couple of data points to examine more closely.\nIf we look at the first row (blue), the macular hole size is about 315 microns and this translates to a log-odds of successful repair of 5.9 (calculated straight from the regression equation), an odds of successful repair of about 350 to 1 (transformation calculation) and a probability of successful repair of 0.997 (transformation calculation). We can then generate the actual outcome status using that probability as a random variable in a binomial distribution. So in this case a relatively low macular hole size translates to a successful repair, as we might have expected.\nNow if we look at the last row of data (yellow), we see something different. Here we have a simulated size of 829 microns and this translates to a log-odds of successful repair of -2.4, an odds of 1 to 10 against and a probability of less than 0.1. Not surprisingly, the outcome status in this case is simulated as a failure.\n\n\n4.3.2 Plot the Observed Data\nWhat if we now plot the observed data, that is, plot the outcome status as a function of macular hole size?\nI’m a big advocate of always plotting your data first - before you calculate any descriptive statistics and certainly before running any statistical tests. In fact I’m not being melodramatic when I say that plotting your data can save you from looking like an analytical fool when you’re potentially ill-founded assumptions aren’t realised in the actual data. A good visualisation can reveal these kinds of problems instantly.\nAfter that big recommendation, we might be left feeling slightly deflated when we see that plotting binary data doesn’t appear that informative - I mean where’s the nice scatter plot and imaginary trend line that we’re used to thinking about?\n\n\nCode\n# Plot observed data\nggplot(df, aes(mac_hole_size, y_success)) + \n  geom_point(size = 4, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 1)) +\n  xlab(\"Macular hole size\") + ylab(\"Surgical repair success\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\nWell it’s not entirely helpless - we can see that there’s more successes skewed towards smaller macular holes and more failures skewed towards larger macular holes, so that does tell us something useful. But there are better ways to plot binary data and I’ll take you through this shortly.\n\n\n4.3.3 Fit Linear Model (Naive Approach)\nWhat if we fit a standard regression line to binary data, effectively treating Y as continuous? Well people have done this in the past, and in some parts of the data analysis world, still do. It kind of works, but it’s not the best tool for the job and we can certainly do better. The best fit line is negative suggesting that as macular hole size increases, repair success decreases, so it is in line with what we know to be true. Note that I have rescaled macular hole size so that the resulting coefficient isn’t so small (now 1 ‘unit’ = 100 microns instead of 1 micron).\n\n\nCode\n# Linear model\nmod_linear &lt;- lm(y_success ~ I(mac_hole_size/100), data = df) \nmod_linear |&gt; \n  tbl_regression(intercept = T)\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n1.5\n1.4, 1.5\n&lt;0.001\n\n\nI(mac_hole_size/100)\n-0.12\n-0.14, -0.11\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nCode\n# Plot data with regression line\nggplot(df, aes(mac_hole_size, y_success)) + \n  geom_point(size = 2, alpha = 0.1) +\n  geom_abline(slope = coef(mod_linear)[[\"I(mac_hole_size/100)\"]]/100, \n              intercept = coef(mod_linear)[[\"(Intercept)\"]], \n              color = \"cornflowerblue\", linewidth = 1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 1)) +\n  xlab(\"Macular hole size\") + ylab(\"Surgical repair success\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.4 Problem - Predictions are made Outside the Probability Domain\nThe problem with naively fitting a standard linear model is that because we are know treating Y as unbounded and continuous, the model no longer knows that the outcome has to be constrained between 0 and 1, and predictions can easily fall outside this range. And that’s exactly what we can see here - a macular hole size of 100 microns (blue) predicts outcome success at 1.35 and a size of 1200 microns (yellow) predicts outcome success at -0.02. So fitting a standard linear model to binary data is really not a good approach.\n\n\nCode\n# Create new df to predict on new values of x\nnew_dat &lt;- data.frame(mac_hole_size = seq(from = 0, to = 1200, by = 50))\n# Predict new fitted values\npred &lt;- cbind(new_dat, prediction = predict(mod_linear, newdata = new_dat))\n# Display predictions\npred |&gt; \n  kable(align = \"c\", digits = 2) |&gt; \n  kable_styling() |&gt; \n  row_spec(3, bold = T, background = \"skyblue1\") |&gt; \n  row_spec(25, bold = T, background = \"lightgoldenrod1\")\n\n\n\n\n\nmac_hole_size\nprediction\n\n\n\n\n0\n1.47\n\n\n50\n1.41\n\n\n100\n1.35\n\n\n150\n1.29\n\n\n200\n1.22\n\n\n250\n1.16\n\n\n300\n1.10\n\n\n350\n1.04\n\n\n400\n0.98\n\n\n450\n0.91\n\n\n500\n0.85\n\n\n550\n0.79\n\n\n600\n0.73\n\n\n650\n0.66\n\n\n700\n0.60\n\n\n750\n0.54\n\n\n800\n0.48\n\n\n850\n0.42\n\n\n900\n0.35\n\n\n950\n0.29\n\n\n1000\n0.23\n\n\n1050\n0.17\n\n\n1100\n0.10\n\n\n1150\n0.04\n\n\n1200\n-0.02\n\n\n\n\n\n\n\n\n\n4.3.5 Plot the ‘Binned’ Observed Data\nSo I said above that there was a better way to plot binary data in an effort to make the visualisation more information, and there is. Instead of just plotting the raw data - just the 0‘s and 1’s - you can create ’bins’ of data based on (some fairly arbitrary) thresholds of the predictor you are plotting against and then plot the proportion of successes within each bin. Here I created ‘bins’ of data for each 50 \\(\\mu m\\)’s of macular hole size and then calculated the mean success rate (i.e. the proportion of 1’s) within each bin.\n\n\nCode\n# Create bins of data for each 50 microns of macular hole size\ndf$bins &lt;- cut(df$mac_hole_size, breaks = seq(0, 1000, by = 50))\n# Calculate means (number of successful repairs divided by total number of repairs) in each bin\ndf &lt;- df |&gt; \n  group_by(bins) |&gt; \n  mutate(mean_y = mean(y_success))\n# Plot observed (averaged) data \nggplot(df, aes(as.numeric(bins)*50-25, mean_y)) + \n  geom_point(size = 2) +\n  geom_line(color = \"orange\", linewidth = 1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Proportion\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\nNow we are starting to see the classic sigmoid shape of the relationship between probability and a continuous predictor.\n\n\n4.3.6 Fit a Logistic Model to the Simulated Data\nNow, let’s finally fit a logistic model to these simulated data.\n\n\nCode\n# Logistic model\nmod_logistic &lt;- glm(y_success ~ I(mac_hole_size/100), data = df, family = \"binomial\")\nmod_logistic |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\nCharacteristic\nlog(OR)1\n95% CI1\np-value\n\n\n\n\nI(mac_hole_size/100)\n-1.5\n-1.8, -1.3\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nOn the model estimation log-odds scale, the parameter estimate for the effect of macular hole size is -1.5, meaning for every 100 micron increase in macular hole size, the log-odds of success decreases by 1.5 units. All stats software will also provide you with parameter estimates as odds ratios, because that’s how we’re accustomed to reporting effects. And the equivalent odds ratio in this case is 0.22. In other words, for every 100 micron increase in macular hole size there is about a 78% reduction in the odds of success.\n\n\nCode\nmod_logistic |&gt; \n  tbl_regression(exponentiate = T)\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nI(mac_hole_size/100)\n0.22\n0.17, 0.28\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nNote that the odds ratio coefficient is a simple transformation of the log-odds coefficient (as we described earlier.) That is:\n\\[e^{-1.5} = 0.22\\]\n\n\n4.3.7 Plot Predicted log-odds of Surgical Repair Success\nNow we can plot the predicted log-odds of surgical repair success. Note that this is a straight line effectively fitted to our binary outcome, and the reason again that we can do this is because the binary outcome has been ‘remapped’ to log-odds and the log-odds exist on an unbounded and continuous scale. So this is a valid plot, unlike that before where we plotted the predictions treating Y as if it were a continuous outcome (I’m not necessarily suggesting this is a useful plot, just that it’s valid).\n\n\nCode\n# Predictions\n# Emmeans of logodds at 600 and 700 microns\nemmeans_df &lt;- data.frame(emmeans(mod_logistic, ~ mac_hole_size, at = list(mac_hole_size = c(600, 700, 800))))\n# Create df of emmeans and corresponding odds and probs, for plotting\nemmeans_df &lt;- emmeans_df |&gt; \n  select(mac_hole_size, emmean) |&gt; \n  rename(logodds = emmean) |&gt; \n  mutate(odds = round(exp(logodds), 3),\n         probs = round(plogis(logodds), 2),\n         logodds = round(logodds, 3))\n\n# Predictions ----\n# Create new df to predict on new values of x\nnew_dat &lt;- data.frame(mac_hole_size = seq(from = 0, to = 1200, length.out = 100))\n# Predict new fitted values and SE's on logodds scale\npred_logodds &lt;- predict(mod_logistic, newdata = new_dat, type = \"link\", se = TRUE)\nnew_dat &lt;- cbind(new_dat, pred_logodds)\n# Create new df of predictions\npredictions &lt;- new_dat |&gt; \n  rename(pred_logodds_est = fit) |&gt; \n  mutate(pred_logodds_LL = pred_logodds_est - (1.96 * se.fit),\n         pred_logodds_UL = pred_logodds_est + (1.96 * se.fit)) |&gt; \n  select(-c(se.fit, residual.scale))\n# Predict new fitted values and SE's on odds scale\npredictions &lt;- predictions |&gt; \n  mutate(pred_odds_est = exp(pred_logodds_est),\n         pred_odds_LL = exp(pred_logodds_LL),\n         pred_odds_UL = exp(pred_logodds_UL))\n# Predict new fitted values and SE's on probability scale\npred_probs &lt;- predict(mod_logistic, newdata = new_dat, type = \"response\", se = TRUE)\nnew_dat &lt;- cbind(new_dat[1], pred_probs)\nnew_dat &lt;- new_dat |&gt; \n  mutate(pred_probs_LL = fit - (1.96 * se.fit),\n         pred_probs_UL = fit + (1.96 * se.fit))\n# Add predicted probs and CIs to predictions df\npredictions &lt;- cbind(predictions, \n                     pred_probs_est = new_dat$fit, \n                     pred_probs_LL = new_dat$pred_probs_LL,\n                     pred_probs_UL = new_dat$pred_probs_UL)\n# Plot predicted log-odds\nggplot(predictions, aes(x = mac_hole_size, y = pred_logodds_est)) + \n  geom_ribbon(aes(ymin = pred_logodds_LL, ymax = pred_logodds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-100, 100), breaks = seq(-100, 100, by = 2)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(-8, 8)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted log-odds\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.8 Plot Predicted Odds of Surgical Repair Success\nWe can also plot the predicted odds of surgical repair success. Note that this best-fitting line is not linear, but instead looks like an exponential plot. We established previously that odds exist on a scale from 0 to \\(+\\infty\\) and that’s what we can appeciate here. Remember that this is just another transformation of the log-odds and the same trend is seen - as hole size increases, the odds of success decline.\n\n\nCode\n# Plot predicted odds\nggplot(predictions, aes(x = mac_hole_size, y = pred_odds_est)) + \n  geom_ribbon(aes(ymin = pred_odds_LL, ymax = pred_odds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1000000), breaks = seq(0, 1000000, by = 1000)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(0, 7000)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted odds\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.9 Plot Predicted Probabilities of Surgical Repair Success\nAnd we can also plot the predicted probabilities of surgical repair success. Again this is just another transformation of the log-odds or the odds. Note the classic sigmoid shape which emulates what we saw earlier when plotting the binned raw data. To my mind this plot is more useful than either plots of predicted log-odds or odds, as it tells us the likely chance of successful repair conditional on the initial macular hole size.\n\n\nCode\n# Plot predicted probabilities\nggplot(predictions, aes(x = mac_hole_size, y = pred_probs_est)) + \n  geom_ribbon(aes(ymin = pred_probs_LL, ymax = pred_probs_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted probability\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.10 Plot Observed and Predicted Probabilities of Surgical Repair Success\nAnd in fact what you should ideally find if you plot both your observed and predicted probabilities is that they are quite similar, reflecting that the model actually fits the data quite well. Of course, in this case the convergence of observed and predicted probabilities is unsurprising given that we know the data-generating proces - i.e. we fitted a model to data that came from a model. But, in general, this is a good way to assess your model fit.\n\n\nCode\n# Plot observed and predicted data\nggplot(df, aes(as.numeric(bins)*50-25, mean_y)) + \n  geom_point(size = 2) +\n  geom_line(aes(color = \"observed\"), linewidth = 1) +\n  geom_line(data = predictions, aes(x = mac_hole_size, y = pred_probs_est, color = \"predicted\"), linewidth = 1) +\n  scale_color_manual(name = \"\", values = c(\"predicted\" = \"cornflowerblue\", \"observed\" = \"orange\")) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Probability\") +\n  theme_bw(base_size = 25) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n4.3.11 Putting It All Together\nWhat I want to try and reinforce in the figure below is how the log-odds, the odds and the probability of the outcome are all transformations of each other.\nIn logistic regression we are ultimately interested in the probability of the outcome occurring. But we use the log-odds or logit link function to map probabilities onto a linear real-number range. This means that the change in Y for each unit increase in X remains the same and we can see that in the top plot. So the log-odds coefficient that our stats software gives us is constant across all values of macular hole size. This represents the difference in the log-odds of success for each unit change in X.\nOne unit increase in log-odds:\n\n600 -&gt; 700 \\(\\mu m\\)\n= -0.181 - 1.323 = -1.504\n\n700 -&gt; 800 \\(\\mu m\\)\n= -1.686 - -0.181 = -1.505\n\n\nThe middle plot shows the equivalent odds of success across the same range of macular hole size. Here the difference in Y is no longer constant for each unit increase in X, but the commensurate ratio is constant - and this gives us the odds ratio. This means that the odds ratio is constant across all values of macular hole size.\nOne unit increase in odds:\n\n600 -&gt; 700 \\(\\mu m\\)\n= 0.834/3.756 = 0.22\n\n700 -&gt; 800 \\(\\mu m\\)\n= 0.185/0.834 = 0.22\n\n\nFinally the lower plot shows the equivalent probabilities of success and now there is no constancy in either unit differences or ratios. So, if we are interested in expressing our logistic regression model results as differences or ratios of predicted probabilities, we need to make sure that we qualify the macular hole size that a particular predicted probability corresponds to, as this will change across the range of X.\nOne unit increase in probability:\n\n600 -&gt; 700 \\(\\mu m\\)\n= 0.45 - 0.79 = -0.34\n= 0.45/0.79 = 0.57\n\n700 -&gt; 800 \\(\\mu m\\)\n= 0.16 - 0.45 = -0.29\n= 0.16/0.45 = 0.36\n\n\n\n\nCode\n# Reformat plots slightly for ggarrange ----\np1 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_logodds_est)) + \n  geom_ribbon(aes(ymin = pred_logodds_LL, ymax = pred_logodds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1150, y = 6, label = \"log-odds\", size = 10) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  annotate(\"text\", x = 1110, y = 3.5, label = \"unit differences = constant\", size = 5) +\n  scale_y_continuous(limits = c(-100, 100), breaks = seq(-100, 100, by = 2)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(-8, 8)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, logodds),\n                            label = emmeans_df$logodds, \n                            nudge_x = c(50, 50, 50), nudge_y = c(4, 4, 4),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  theme_bw(base_size = 20) +\n  ylab(\"\") +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\n\np2 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_odds_est)) + \n  geom_ribbon(aes(ymin = pred_odds_LL, ymax = pred_odds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1180, y = 6000, label = \"odds\", size = 10) +\n  annotate(\"text\", x = 1136, y = 5000, label = \"unit ratios = constant\", size = 5) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(seq(0, 1000000, by = 1000))) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(0, 7000)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ylab(\"\") +\n  theme_bw(base_size = 20) +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\np2_inset &lt;- p2 +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(1,2,3,4,5, seq(0, 1000000, by = 1000))) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, odds),\n                            label = emmeans_df$odds, \n                            nudge_x = c(50, 50, 50), nudge_y = c(0.5, 1, 1),\n                            color = \"red\", segment.size = 0.2, size = 5)\np2 &lt;- p2 + geom_magnify(from = c(xmin = 500, xmax = 1000, ymin = 0, ymax = 5), \n                          to = c(xmin = 470, xmax = 1010, ymin = 1000, ymax = 5000), \n                          shadow = T, axes = \"y\", plot = p2_inset)\n\np3 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_probs_est)) + \n  geom_ribbon(aes(ymin = pred_probs_LL, ymax = pred_probs_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1140, y = 0.8, label = \"probability\", size = 10) +\n  annotate(\"text\", x = 1075, y = 0.65, label = \"unit differences/ratios = not constant\", size = 5) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, probs),\n                            label = emmeans_df$probs, \n                            nudge_x = c(50, 50, 50), nudge_y = c(0.1, 0.1, 0.1),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  ylab(\"\") + xlab(\"Macular hole size\") +\n  theme_bw(base_size = 20)\nggarrange(p1, p2, p3, align = \"v\", ncol = 1, heights = c(1,1,1.2))\n\n\n\n\n\n\n\n\n\n\n\n4.3.12 Key points\nThis has been a long post but we are finally at the end. I hope that reading this has demystified what goes on under the hood in logistic regression as it’s really not that difficult once you understand the purpose of the link function.\nThese are the main summary points:\n\nLogistic regression is a type of generalised linear model that allows the estimation of associations of potential predictors with a binary outcome.\nThe logit link function transforms the probabilities of a binary outcome variable to a continuous, unbounded scale (log-odds). Standard linear modelling methods can then be applied to estimate the model.\nLog-odds, odds and probabilities are all simple transformations of one another.\nThe model is estimated on the log-odds scale where the association between the predictor and the log-odds of the outcome is linear, and the unit difference is constant.\nResults may be reported in terms of odds or probabilities:\n\nOn the odds scale the association between the predictor and the odds of the outcome is non-linear, but the unit ratio is constant.\nOn the probability scale the association between the predictor and the probability of the outcome is non-linear, and the unit difference/ratio is not constant.\nThis means probability estimates (risk differences of ratios) depend on the value of the predictor and this needs to be specified."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html",
    "href": "posts/014_14Jun_2024/index.html",
    "title": "Academic Research - “What’s in a Title?”",
    "section": "",
    "text": "So that was a pretty poor attempt to adapt a famous adage from Romeo and Juliet - “What’s in a name? That which we call a rose by any other name would smell just as sweet” - for my own expository purposes.\nBefore taking a short mid-year break (and also because I didn’t have time to prepare anything formal this week), I thought we could have a little fun in taking a look at some clever, bizarre, and in some cases laugh-out-loud titles that academic researchers have published with over the years.\nBut we can’t have fun without a little science as well. So, is there actually anything to be gained by giving your research paper a less serious title? Interestingly, there is a recent preprint paper out there that has attempted to assess this relationship - specifically between humour in article titles and the all-important citation impact.\nIf this title is funny, will you cite me? Citation impacts of humour and other features of article titles in ecology and evolution (and the Nature opinion piece of the same paper).\nThe TL;DR is that articles with humourous titles tend to be cited less, but article ‘importance’ was considered to be a confounder in this association, and once adjustment was made for that, articles with humourous titles were in fact cited more. If you’re interested, have a read of the paper. I’m not that convinced as the authors used self-citation as a proxy for their definition of importance and I’m sure there are better ways to assess this.\nBut anyway, now that the science is out of the way, let’s get to the fun bit. These are a collection of papers that I’ve come across over the years and no doubt some will be familiar to you, but please indulge me anyway. Many of these have used clever wordplay or puns, but some are also serious (and unfortunate) titles.\nI am going to try my best to broadly group them by some linking theme, if that is even possible…."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#case-1-one-name-column-one-value-column",
    "href": "posts/014_14Jun_2024/index.html#case-1-one-name-column-one-value-column",
    "title": "Reshaping Data - Think Before you Pivot",
    "section": "2.1 Case 1: One Name Column, One Value Column",
    "text": "2.1 Case 1: One Name Column, One Value Column\nThis is the most common and most straight-forward application of reshaping to long that you might be required to perform. In this case we have multiple value columns in wide format that we want to reshape to one name column and one value column. The good news is that we have already done this in the example above. The code is also shown but just to revisit that briefly for clarity:\n\ndf_long &lt;- df_wide |&gt; \n  pivot_longer(cols = month_0:month_3,\n               names_to = \"month\",\n               names_prefix = \"month_\",\n               values_to = \"bp\")\n\nSo we take the df_wide dataframe and ‘pipe’ it to the pivot_longer() function where we specify that we want to take the columns from (and including) month_0 to month_3, assigning those column names as category labels in the new month name variable, while also placing each corresponding BP measurement into the new bp value variable. The names_prefix argument is optional but was used here to strip out the somewhat redundant month_ text from each column name prior to labelling. You could certainly leave this in if you wanted and the result would then be:\n\n\n\n\n\nid\nmonth\nbp\n\n\n\n\n1\nmonth_0\n136.79\n\n\n1\nmonth_1\n170.78\n\n\n1\nmonth_2\n130.11\n\n\n1\nmonth_3\n136.40\n\n\n2\nmonth_0\n99.08\n\n\n2\nmonth_1\n110.87\n\n\n2\nmonth_2\n118.15\n\n\n2\nmonth_3\n141.49\n\n\n\n\n\nBut I’m sure you’d agree that the results looks cleaner without all that unnecessary repetition."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#case-2-multiple-name-columns-one-value-column",
    "href": "posts/014_14Jun_2024/index.html#case-2-multiple-name-columns-one-value-column",
    "title": "Reshaping Data - Think Before you Pivot",
    "section": "2.2 Case 2: Multiple Name Columns, One Value Column",
    "text": "2.2 Case 2: Multiple Name Columns, One Value Column\nLet’s now extend this idea a little. Imagine that in addition to BP measurements, subjects also had their weight measured at the same time points (simulated with a mean of 70 kg and SD 15 kg). Now we have data that could potentially look like:\n\n\nCode\nid &lt;- seq(1:5)\nfor(i in 0:3){\n  var_name_bp &lt;- paste0(\"month_\",i,\"_bp\")\n  assign(var_name_bp, rnorm(5, 130, 20))\n  var_name_wt &lt;- paste0(\"month_\",i,\"_wt\")\n  assign(var_name_wt, rnorm(5, 70, 15))\n}\ndf_wide &lt;- data.frame(cbind(id, month_0_bp, month_0_wt, month_1_bp, month_1_wt, month_2_bp, month_2_wt, month_3_bp, month_3_wt))\ndf_wide |&gt;  \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nmonth_0_bp\nmonth_0_wt\nmonth_1_bp\nmonth_1_wt\nmonth_2_bp\nmonth_2_wt\nmonth_3_bp\nmonth_3_wt\n\n\n\n\n1\n139.56\n64.44\n142.76\n88.21\n94.20\n31.81\n123.68\n110.41\n\n\n2\n159.89\n58.74\n136.44\n58.65\n137.26\n60.98\n135.68\n56.16\n\n\n3\n118.89\n62.49\n101.90\n56.53\n103.91\n54.20\n129.39\n91.74\n\n\n4\n106.44\n90.90\n138.15\n64.54\n126.27\n73.29\n120.39\n59.81\n\n\n5\n141.39\n74.20\n109.42\n62.05\n114.40\n51.03\n73.31\n82.06\n\n\n\n\n\nWhat to do here?\nActually, some thought is required at this point as there are two potential paths you could go down and it all depends on what you want to achieve. Let’s assume that you want to put all measurement values in one column. Once you have decided on this final form, the code is not challenging. We will necessarily end up with two names columns instead of just one, one for time (month) and one for the all the clinical measures (BP and weight). The main changes to the code are to now supply two new variable names to the names_to argument as well as tell the function how to source the new category labels with the names_sep argument. This will split the currently wide variable names at the second _ (after stripping out the redundant month_ text) and use the number as the month label and the type of measurement as the clinical measure label.\n\ndf_long &lt;- df_wide |&gt; \n  pivot_longer(cols = month_0_bp:month_3_wt,\n               names_to = c(\"month\", \"clinical_measure\"),\n               names_prefix = \"month_\",\n               names_sep = \"_\",\n               values_to = \"value\")\n\nand the data looks like:\n\n\nCode\ndf_long |&gt;  \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nid\nmonth\nclinical_measure\nvalue\n\n\n\n\n1\n0\nbp\n139.56\n\n\n1\n0\nwt\n64.44\n\n\n1\n1\nbp\n142.76\n\n\n1\n1\nwt\n88.21\n\n\n1\n2\nbp\n94.20\n\n\n1\n2\nwt\n31.81\n\n\n1\n3\nbp\n123.68\n\n\n1\n3\nwt\n110.41\n\n\n2\n0\nbp\n159.89\n\n\n2\n0\nwt\n58.74\n\n\n2\n1\nbp\n136.44\n\n\n2\n1\nwt\n58.65\n\n\n2\n2\nbp\n137.26\n\n\n2\n2\nwt\n60.98\n\n\n2\n3\nbp\n135.68\n\n\n2\n3\nwt\n56.16\n\n\n3\n0\nbp\n118.89\n\n\n3\n0\nwt\n62.49\n\n\n3\n1\nbp\n101.90\n\n\n3\n1\nwt\n56.53\n\n\n3\n2\nbp\n103.91\n\n\n3\n2\nwt\n54.20\n\n\n3\n3\nbp\n129.39\n\n\n3\n3\nwt\n91.74\n\n\n4\n0\nbp\n106.44\n\n\n4\n0\nwt\n90.90\n\n\n4\n1\nbp\n138.15\n\n\n4\n1\nwt\n64.54\n\n\n4\n2\nbp\n126.27\n\n\n4\n2\nwt\n73.29\n\n\n4\n3\nbp\n120.39\n\n\n4\n3\nwt\n59.81\n\n\n5\n0\nbp\n141.39\n\n\n5\n0\nwt\n74.20\n\n\n5\n1\nbp\n109.42\n\n\n5\n1\nwt\n62.05\n\n\n5\n2\nbp\n114.40\n\n\n5\n2\nwt\n51.03\n\n\n5\n3\nbp\n73.31\n\n\n5\n3\nwt\n82.06\n\n\n\n\n\nI tend to think of this as a complete reshaping to long format."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#case-3-multiple-name-columns-multiple-value-columns",
    "href": "posts/014_14Jun_2024/index.html#case-3-multiple-name-columns-multiple-value-columns",
    "title": "Reshaping Data - Think Before you Pivot",
    "section": "2.3 Case 3: Multiple Name Columns, Multiple Value Columns",
    "text": "2.3 Case 3: Multiple Name Columns, Multiple Value Columns\nBut what if didn’t want to do this and instead wanted the values of BP and weight to appear in their own columns - a partial reshaping to long format if you like.\n\ndf_long &lt;- df_wide |&gt; \n  pivot_longer(cols = month_0_bp:month_3_wt,\n               names_to = c(\"month\", \".value\"),\n               names_prefix = \"month_\",\n               names_sep = \"_\")\n\nand the data now looks like:\n\n\nCode\ndf_long |&gt;  \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nid\nmonth\nbp\nwt\n\n\n\n\n1\n0\n121.12\n89.31\n\n\n1\n1\n147.56\n62.52\n\n\n1\n2\n124.86\n90.79\n\n\n1\n3\n119.11\n68.30\n\n\n2\n0\n146.37\n77.34\n\n\n2\n1\n128.50\n73.84\n\n\n2\n2\n143.65\n77.60\n\n\n2\n3\n149.05\n75.75\n\n\n3\n0\n101.35\n77.29\n\n\n3\n1\n139.16\n95.57\n\n\n3\n2\n143.61\n70.25\n\n\n3\n3\n110.51\n67.72\n\n\n4\n0\n135.15\n83.72\n\n\n4\n1\n114.41\n49.70\n\n\n4\n2\n116.78\n64.98\n\n\n4\n3\n143.78\n54.83\n\n\n5\n0\n117.96\n57.93\n\n\n5\n1\n160.39\n65.14\n\n\n5\n2\n127.80\n39.26\n\n\n5\n3\n102.54\n74.54"
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#case-3-one-name-columns-multiple-value-columns",
    "href": "posts/014_14Jun_2024/index.html#case-3-one-name-columns-multiple-value-columns",
    "title": "Reshaping Data - Think Before you Pivot",
    "section": "2.3 Case 3: One Name Columns, Multiple Value Columns",
    "text": "2.3 Case 3: One Name Columns, Multiple Value Columns\nBut what if didn’t want to do this and instead wanted the values of BP and weight to appear in their own columns - a partial reshaping to long format if you like. To my mind this is probably a more useful long format than what we considered in the last example, although there may be some niche use-case scenarios that require data to be in that format for analysis (they just elude me right now).\nSo let’s now assume that you want separate columns of values for each type of measurement. Now we will end up with one name column and two value columns - one for BP and one for weight. The general form of the code doesn’t change a lot in this case - the main thing being that we replace “clinical_measure” in the names_to argument with a special term .value which indicates that the pivoted (new) columns will be split by the text after the second _ in the currently wide column names - i.e. taking on the value of bp or wt. In those two new columns the corresponding measurement values will be placed. The code looks like:\n\ndf_long &lt;- df_wide |&gt; \n  pivot_longer(cols = month_0_bp:month_3_wt,\n               names_to = c(\"month\", \".value\"),\n               names_prefix = \"month_\",\n               names_sep = \"_\")\n\nand the data now looks like:\n\n\nCode\ndf_long |&gt;  \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nid\nmonth\nbp\nwt\n\n\n\n\n1\n0\n125.77\n58.28\n\n\n1\n1\n151.33\n60.70\n\n\n1\n2\n148.67\n53.45\n\n\n1\n3\n90.00\n58.16\n\n\n2\n0\n155.15\n51.05\n\n\n2\n1\n138.30\n70.95\n\n\n2\n2\n131.13\n73.50\n\n\n2\n3\n94.79\n64.58\n\n\n3\n0\n109.26\n45.20\n\n\n3\n1\n150.97\n79.06\n\n\n3\n2\n126.63\n25.21\n\n\n3\n3\n136.61\n55.40\n\n\n4\n0\n171.49\n71.59\n\n\n4\n1\n109.88\n88.51\n\n\n4\n2\n98.91\n64.93\n\n\n4\n3\n164.71\n83.97\n\n\n5\n0\n92.08\n81.82\n\n\n5\n1\n129.54\n67.93\n\n\n5\n2\n141.80\n77.97\n\n\n5\n3\n150.33\n88.55"
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#case-3-one-name-column-multiple-value-columns",
    "href": "posts/014_14Jun_2024/index.html#case-3-one-name-column-multiple-value-columns",
    "title": "Reshaping Data - Think Before you Pivot",
    "section": "2.3 Case 3: One Name Column, Multiple Value Columns",
    "text": "2.3 Case 3: One Name Column, Multiple Value Columns\nBut what if didn’t want to do this and instead wanted the values of BP and weight to appear in their own columns - a partial reshaping to long format if you like. To my mind this is probably a more useful long format than what we considered in the last example, although there may be some niche use-case scenarios that require data to be in that format for analysis (they just elude me right now).\nSo let’s now assume that you want separate columns of values for each type of measurement. Now we will end up with one name column and two value columns - one for BP and one for weight. The general form of the code doesn’t change a lot in this case - the main thing being that we replace “clinical_measure” in the names_to argument with a special term .value which indicates that the pivoted (new) columns will be split by the text after the second _ in the currently wide column names - i.e. taking on the value of bp or wt (so it’s not necessary to specify a values_to term this time around). In those two new columns the corresponding measurement values will be placed. The code looks like:\n\ndf_long &lt;- df_wide |&gt; \n  pivot_longer(cols = month_0_bp:month_3_wt,\n               names_to = c(\"month\", \".value\"),\n               names_prefix = \"month_\",\n               names_sep = \"_\")\n\nand the data now looks like:\n\n\nCode\ndf_long |&gt;  \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nid\nmonth\nbp\nwt\n\n\n\n\n1\n0\n139.56\n64.44\n\n\n1\n1\n142.76\n88.21\n\n\n1\n2\n94.20\n31.81\n\n\n1\n3\n123.68\n110.41\n\n\n2\n0\n159.89\n58.74\n\n\n2\n1\n136.44\n58.65\n\n\n2\n2\n137.26\n60.98\n\n\n2\n3\n135.68\n56.16\n\n\n3\n0\n118.89\n62.49\n\n\n3\n1\n101.90\n56.53\n\n\n3\n2\n103.91\n54.20\n\n\n3\n3\n129.39\n91.74\n\n\n4\n0\n106.44\n90.90\n\n\n4\n1\n138.15\n64.54\n\n\n4\n2\n126.27\n73.29\n\n\n4\n3\n120.39\n59.81\n\n\n5\n0\n141.39\n74.20\n\n\n5\n1\n109.42\n62.05\n\n\n5\n2\n114.40\n51.03\n\n\n5\n3\n73.31\n82.06"
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#primary-transformations",
    "href": "posts/014_14Jun_2024/index.html#primary-transformations",
    "title": "Logistic Regression - Under the Hood",
    "section": "3.1 Primary Transformations",
    "text": "3.1 Primary Transformations\nIf we have the odds of an event, then we can transform that to a probability - or a risk, which is another way to refer to the chance of an outcome - by dividing the odds by the odds plus 1.\n\\[\\text{p} = \\frac{\\text{odds}}{1 + \\text{odds}}\\]\nIf instead we have the probability of an event, we can transform that to the equivalent odds of the event, by dividing the probability by its complement (that is 1 minus the probability).\n\\[\\text{odds} = \\frac{\\text{p}}{1 - \\text{p}}\\]\nAnd the log-odds is simply taking the natural logarithm of the odds - there is nothing difficult about that.\n\\[\\text{log-odds} = \\text{log(odds)}\\]\nSo these three are what I consider primary transformations and we can cycle between them quite easily with a calculator."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#secondary-transformations",
    "href": "posts/014_14Jun_2024/index.html#secondary-transformations",
    "title": "Logistic Regression - Under the Hood",
    "section": "3.2 Secondary Transformations",
    "text": "3.2 Secondary Transformations\nThere are additional transformations that you can make and while these aren’t quite as straightforward, they are even more fundamental to logistic regression, which ultimately uses the log-odds as the scale to estimate its parameters on.\nThe first is that we can get to a probability directly from the log-odds by taking the exponent of the log-odds divided by the exponent of the log-odds plus one.\n\\[\\text{p} = \\frac{e^{\\text{log-odds}}}{1 + e^{\\text{log-odds}}}\\]\nAnd as you can see, that’s really no different to the first equation above because the exponent of the log-odds is simply the odds (the reverse transformation of logging a number is to take the exponent of that logged number). i.e.\n\\[\\text{odds} = e^{\\text{log-odds}}\\]\nNow, the final transformation, and the one that’s most relevant to logistic regression shows the relationship between the log-odds and probability and is known as the logit function (red box) - and we’ll talk more about this in a minute.\n\\[\\text{log-odds} = \\text{log}\\Bigl(\\frac{\\text{p}}{1 - \\text{p}}\\Bigr)\\]"
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#probability-vs-odds-and-as-applied-to-the-research-question",
    "href": "posts/014_14Jun_2024/index.html#probability-vs-odds-and-as-applied-to-the-research-question",
    "title": "Logistic Regression - Under the Hood",
    "section": "3.3 Probability vs Odds (and as applied to the Research Question)",
    "text": "3.3 Probability vs Odds (and as applied to the Research Question)\nLet’s go back now to our original research question and specifically look at the difference between the probability and odds of an event - I want to solidify this concept in your minds. For most of us probability (or chance) is much more intuitive than the notion of odds. If I think about the likelihood of it being a wet day, I think there’s (for example) an 80% chance of it raining, rather than 4 to 1 odds in favour of it raining. However, if you like to wager on the horses than you may actually be quite comfortable with thinking in terms of odds - as those are the terms bets are placed in.\nIn the following I’ve simulated some data from the original paper (based on the summary statistics and regression model provided). Obviously macular hole size is a continuous variable, but for the sake of this illustration I am going to dichotomise it at it’s approximate mean value (480 microns). From this we can draw up a 2 x 2 contingency table of the number of successful and failed macular hole repairs as a function of the dichotomised version of macular hole size.\nWhen we talk about probabilities we focus on the proportion of successes out of the total for each row. So, the probability (or risk) of successful repair for those patients with a macular hole size less than 480 microns is therefore 479/487 or about 0.98. That’s in contrast to the probability of successful repair in those patients with a size greater than 480 microns - where the chance of success is 380/503 or 0.76 - so lower as one would expect. You can then take a ratio of those two probabilities or risks to give the classic risk ratio and that equals 1.3 which tells you that the chance of successful repair is 30% greater if you have a smaller macular hole at initial presentation.\nNow let’s look at the equivalent odds. When we talk about odds we focus on the ratio of successes to failures and don’t really worry about the totals. So the odds of successful repair for those patients with a smaller initial hole is 479/8 or 60 to 1. The odds of success with a larger hole are a lot more modest - 380/123 or about 3 to 1. We can then calculate the classic odds ratio which works out at about 20, meaning the odds of successful repair is about 20 times greater in those with thinner initial holes.\nNote that both of these effect measures are commensurate - they point in exactly the same direction - that you fare better with a smaller hole at diagnosis. They are just different ways of expressing the same thing."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#the-logit-link-function-and-why-log-odds-are-actually-important",
    "href": "posts/014_14Jun_2024/index.html#the-logit-link-function-and-why-log-odds-are-actually-important",
    "title": "Logistic Regression - Under the Hood",
    "section": "4.1 The Logit Link Function (and why log-odds are actually important)",
    "text": "4.1 The Logit Link Function (and why log-odds are actually important)\nThe log-odds or the logit function is pivotal to how logistic regression works. The logit function is a type of link function and there’s really nothing mysterious or difficult about this. A link function is just a function of the mean of Y - in other words a transformation of the mean of Y - and we use this link function as the outcome instead of Y itself.\n\\[\\text{logit(p)} = \\text{log}\\Bigl(\\frac{\\text{p}}{1 - \\text{p}}\\Bigr)\\] The logistic model then becomes:\n\\[\\text{Y} = \\text{logit(p)} = \\beta_0 \\; + \\; \\beta_1x\\]\nThis allows the outcome to become continuous and unbounded and the association between Y and X to become linear, therefore satisfying those fundamental assumptions of the linear model that I mentioned earlier.\nThus, the logit is a type of transformation that links or maps probability values from \\((0, 1)\\) to real numbers \\({\\displaystyle (-\\infty ,+\\infty )}\\). Now, on the log-odds/logit scale the effect of a one-unit change in X on Y becomes constant once again."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#wth-log-oddsoddsprobabilities---youre-confusing-me---pick-one-please",
    "href": "posts/014_14Jun_2024/index.html#wth-log-oddsoddsprobabilities---youre-confusing-me---pick-one-please",
    "title": "Logistic Regression - Under the Hood",
    "section": "4.2 WTH? log-odds/odds/probabilities - You’re confusing me - pick one please!!",
    "text": "4.2 WTH? log-odds/odds/probabilities - You’re confusing me - pick one please!!\nIt’s important to remember that logistic regression is really about probabilities, not odds. Probabilities are more intuitive than odds (unless you like to put a bet on the horses), so why use odds? Well, model results have historically been expressed in terms of odds and odds ratios for mathematical convenience. As it is for the log-odds scale, the ‘effect’ of a one-unit change in X on Y is also constant on the odds scale (but as a ratio of two odds, not as a difference in two log-odds). So an odds ratio describes a constant ‘effect’ of a predictor on the odds of an outcome, irrespective of the value of the predictor. This is not the case on the probability scale, where both differences and ratios vary per unit change depending on the value of the predictor (I will illustrate this later).\nThe TLDR:\n\nModel is estimated (under the hood) on the log-odds scale to satisfy modelling assumptions.\nModel results are displayed on the odds scale (as odds ratios) for their constancy.\nProbabilities are calculated post-hoc with not much extra effort."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#back-to-the-research-question",
    "href": "posts/014_14Jun_2024/index.html#back-to-the-research-question",
    "title": "Logistic Regression - Under the Hood",
    "section": "4.3 Back to the Research Question",
    "text": "4.3 Back to the Research Question\nLet’s go back to the original research question for the final time and now illustrate these concepts in that context. The regression equation for surgical repair success as a function of macular hole size that was estimated in our paper is shown here.\n\\[\\text{logit(p)} = 10.89 - 0.016 \\; \\text{x} \\;\\text{macular hole size}\\;(\\mu m)\\]\nBasically, the log-odds of successful repair can be predicted by calculating the result of 10.89 minus the product of 0.016 and whatever the macular hole size is in microns.\n\n4.3.1 Simulate Some Data\nWe can simulate some data from that regression equation - essentially reverse engineering fake data from the estimated best fit equation on the actual data. Why do this? Well, it gives us some sense of what the actual data may have looked like that the researchers collected empirically. We can then do all sorts of cool things like refit a model, generate model predictions, etc. Data simulation is something I am still learning more about but it opens up a world of possibilities in your analytical work.\nFor this simulation I used a normal distribution with a mean and SD for macular hole size of 486 and 142, respectively (the numbers came from the original research paper). The code to show how to reproduce this in R is included below but you can do something similar in Stata and in most other scriptable statistical software packages.\n\n\nCode\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(emmeans)\nlibrary(gtsummary)\nlibrary(ggmagnify)\nlibrary(ggpubr)\n# Simulation\nn &lt;- 1000                                          # set number of obs to simulate\nset.seed(1234)                                     # set seed for reproducibility\nmac_hole_size  &lt;-  rnorm(n, 486, 142)              # generate macular hole inner opening data with mean 486 and sd = 152\nlogodds_success  &lt;-  10.89 - 0.016 * mac_hole_size # generate variable that is linear combination of intercept = 10.89 and coefficient for macular hole -0.016 (logit scale)\nodds_success  &lt;-  exp(logodds_success)             # exponentiate logodds to get odds\nprob_success  &lt;-  odds_success/(1 + odds_success)  # generate probabilities from this\ny_success  &lt;-  rbinom(n, 1, prob_success)          # generate outcome variable as a function of those probabilities\ndf &lt;-  data.frame(cbind(mac_hole_size,             # combine into dataframe\n                        logodds_success,\n                        odds_success,\n                        prob_success, \n                        y_success))\ndf &lt;- df |&gt; \n  filter(mac_hole_size &gt; 100)                       # only include those with size &gt; 100\n\n\nEssentially, we generate some random values of macular hole size based on the specified distribution and then plug those values in to the regression equation and calculate the various outcome measures. The resultant data look like (first 20 rows shown):\n\n\nCode\nhead(df, 20) |&gt; \n  kable(align = \"c\", digits = 2) |&gt; \n  kable_styling() |&gt; \n  row_spec(1, bold = T, background = \"skyblue1\") |&gt; \n  row_spec(20, bold = T, background = \"lightgoldenrod1\")\n\n\n\n\n\nmac_hole_size\nlogodds_success\nodds_success\nprob_success\ny_success\n\n\n\n\n314.60\n5.86\n349.48\n1.00\n1\n\n\n525.39\n2.48\n11.99\n0.92\n1\n\n\n639.99\n0.65\n1.92\n0.66\n1\n\n\n152.91\n8.44\n4644.44\n1.00\n1\n\n\n546.94\n2.14\n8.49\n0.89\n1\n\n\n557.86\n1.96\n7.13\n0.88\n1\n\n\n404.39\n4.42\n83.08\n0.99\n1\n\n\n408.38\n4.36\n77.94\n0.99\n1\n\n\n405.85\n4.40\n81.16\n0.99\n1\n\n\n359.61\n5.14\n170.06\n0.99\n1\n\n\n418.24\n4.20\n66.57\n0.99\n1\n\n\n344.23\n5.38\n217.53\n1.00\n1\n\n\n375.77\n4.88\n131.32\n0.99\n1\n\n\n495.15\n2.97\n19.44\n0.95\n1\n\n\n622.25\n0.93\n2.54\n0.72\n1\n\n\n470.34\n3.36\n28.92\n0.97\n1\n\n\n413.44\n4.28\n71.88\n0.99\n1\n\n\n356.61\n5.18\n178.44\n0.99\n1\n\n\n367.12\n5.02\n150.82\n0.99\n1\n\n\n829.05\n-2.37\n0.09\n0.09\n0\n\n\n\n\n\n\n\n\nLet’s pick a couple of data points to examine more closely.\nIf we look at the first row (blue), the macular hole size is about 315 microns and this translates to a log-odds of successful repair of 5.9 (calculated straight from the regression equation), an odds of successful repair of about 350 to 1 (transformation calculation) and a probability of successful repair of 0.997 (transformation calculation). We can then generate the actual outcome status using that probability as a random variable in a binomial distribution. So in this case a relatively low macular hole size translates to a successful repair, as we might have expected.\nNow if we look at the last row of data (yellow), we see something different. Here we have a simulated size of 829 microns and this translates to a log-odds of successful repair of -2.4, an odds of 1 to 10 against and a probability of less than 0.1. Not surprisingly, the outcome status in this case is simulated as a failure.\n\n\n4.3.2 Plot the Observed Data\nWhat if we now plot the observed data, that is, plot the outcome status as a function of macular hole size?\nI’m a big advocate of always plotting your data first - before you calculate any descriptive statistics and certainly before running any statistical tests. In fact I’m not being melodramatic when I say that plotting your data can save you from looking like an analytical fool when you’re potentially ill-founded assumptions aren’t realised in the actual data. A good visualisation can reveal these kinds of problems instantly.\nAfter that big recommendation, we might be left feeling slightly deflated when we see that plotting binary data doesn’t appear that informative - I mean where’s the nice scatter plot and imaginary trend line that we’re used to thinking about?\n\n\nCode\n# Plot observed data\nggplot(df, aes(mac_hole_size, y_success)) + \n  geom_point(size = 4, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 1)) +\n  xlab(\"Macular hole size\") + ylab(\"Surgical repair success\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\nWell it’s not entirely helpless - we can see that there’s more successes skewed towards smaller macular holes and more failures skewed towards larger macular holes, so that does tell us something useful. But there are better ways to plot binary data and I’ll take you through this shortly.\n\n\n4.3.3 Fit Linear Model (Naive Approach)\nWhat if we fit a standard regression line to binary data, effectively treating Y as continuous? Well people have done this in the past, and in some parts of the data analysis world, still do. It kind of works, but it’s not the best tool for the job and we can certainly do better. The best fit line is negative suggesting that as macular hole size increases, repair success decreases, so it is in line with what we know to be true. Note that I have rescaled macular hole size so that the resulting coefficient isn’t so small (now 1 ‘unit’ = 100 microns instead of 1 micron).\n\n\nCode\n# Linear model\nmod_linear &lt;- lm(y_success ~ I(mac_hole_size/100), data = df) \nmod_linear |&gt; \n  tbl_regression(intercept = T)\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n1.5\n1.4, 1.5\n&lt;0.001\n\n\nI(mac_hole_size/100)\n-0.12\n-0.14, -0.11\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nCode\n# Plot data with regression line\nggplot(df, aes(mac_hole_size, y_success)) + \n  geom_point(size = 2, alpha = 0.1) +\n  geom_abline(slope = coef(mod_linear)[[\"I(mac_hole_size/100)\"]]/100, \n              intercept = coef(mod_linear)[[\"(Intercept)\"]], \n              color = \"cornflowerblue\", linewidth = 1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 1)) +\n  xlab(\"Macular hole size\") + ylab(\"Surgical repair success\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.4 Problem - Predictions are made Outside the Probability Domain\nThe problem with naively fitting a standard linear model is that because we are know treating Y as unbounded and continuous, the model no longer knows that the outcome has to be constrained between 0 and 1, and predictions can easily fall outside this range. And that’s exactly what we can see here - a macular hole size of 100 microns (blue) predicts outcome success at 1.35 and a size of 1200 microns (yellow) predicts outcome success at -0.02. So fitting a standard linear model to binary data is really not a good approach.\n\n\nCode\n# Create new df to predict on new values of x\nnew_dat &lt;- data.frame(mac_hole_size = seq(from = 0, to = 1200, by = 50))\n# Predict new fitted values\npred &lt;- cbind(new_dat, prediction = predict(mod_linear, newdata = new_dat))\n# Display predictions\npred |&gt; \n  kable(align = \"c\", digits = 2) |&gt; \n  kable_styling() |&gt; \n  row_spec(3, bold = T, background = \"skyblue1\") |&gt; \n  row_spec(25, bold = T, background = \"lightgoldenrod1\")\n\n\n\n\n\nmac_hole_size\nprediction\n\n\n\n\n0\n1.47\n\n\n50\n1.41\n\n\n100\n1.35\n\n\n150\n1.29\n\n\n200\n1.22\n\n\n250\n1.16\n\n\n300\n1.10\n\n\n350\n1.04\n\n\n400\n0.98\n\n\n450\n0.91\n\n\n500\n0.85\n\n\n550\n0.79\n\n\n600\n0.73\n\n\n650\n0.66\n\n\n700\n0.60\n\n\n750\n0.54\n\n\n800\n0.48\n\n\n850\n0.42\n\n\n900\n0.35\n\n\n950\n0.29\n\n\n1000\n0.23\n\n\n1050\n0.17\n\n\n1100\n0.10\n\n\n1150\n0.04\n\n\n1200\n-0.02\n\n\n\n\n\n\n\n\n\n4.3.5 Plot the ‘Binned’ Observed Data\nSo I said above that there was a better way to plot binary data in an effort to make the visualisation more information, and there is. Instead of just plotting the raw data - just the 0‘s and 1’s - you can create ’bins’ of data based on (some fairly arbitrary) thresholds of the predictor you are plotting against and then plot the proportion of successes within each bin. Here I created ‘bins’ of data for each 50 \\(\\mu m\\)’s of macular hole size and then calculated the mean success rate (i.e. the proportion of 1’s) within each bin.\n\n\nCode\n# Create bins of data for each 50 microns of macular hole size\ndf$bins &lt;- cut(df$mac_hole_size, breaks = seq(0, 1000, by = 50))\n# Calculate means (number of successful repairs divided by total number of repairs) in each bin\ndf &lt;- df |&gt; \n  group_by(bins) |&gt; \n  mutate(mean_y = mean(y_success))\n# Plot observed (averaged) data \nggplot(df, aes(as.numeric(bins)*50-25, mean_y)) + \n  geom_point(size = 2) +\n  geom_line(color = \"orange\", linewidth = 1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Proportion\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\nNow we are starting to see the classic sigmoid shape of the relationship between probability and a continuous predictor.\n\n\n4.3.6 Fit a Logistic Model to the Simulated Data\nNow, let’s finally fit a logistic model to these simulated data.\n\n\nCode\n# Logistic model\nmod_logistic &lt;- glm(y_success ~ I(mac_hole_size/100), data = df, family = \"binomial\")\nmod_logistic |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\nCharacteristic\nlog(OR)1\n95% CI1\np-value\n\n\n\n\nI(mac_hole_size/100)\n-1.5\n-1.8, -1.3\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nOn the model estimation log-odds scale, the parameter estimate for the effect of macular hole size is -1.5, meaning for every 100 micron increase in macular hole size, the log-odds of success decreases by 1.5 units. All stats software will also provide you with parameter estimates as odds ratios, because that’s how we’re accustomed to reporting effects. And the equivalent odds ratio in this case is 0.22. In other words, for every 100 micron increase in macular hole size there is about a 78% reduction in the odds of success.\n\n\nCode\nmod_logistic |&gt; \n  tbl_regression(exponentiate = T)\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nI(mac_hole_size/100)\n0.22\n0.17, 0.28\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nNote that the odds ratio coefficient is a simple transformation of the log-odds coefficient (as we described earlier.) That is:\n\\[e^{-1.5} = 0.22\\]\n\n\n4.3.7 Plot Predicted log-odds of Surgical Repair Success\nNow we can plot the predicted log-odds of surgical repair success. Note that this is a straight line effectively fitted to our binary outcome, and the reason again that we can do this is because the binary outcome has been ‘remapped’ to log-odds and the log-odds exist on an unbounded and continuous scale. So this is a valid plot, unlike that before where we plotted the predictions treating Y as if it were a continuous outcome (I’m not necessarily suggesting this is a useful plot, just that it’s valid).\n\n\nCode\n# Predictions\n# Emmeans of logodds at 600 and 700 microns\nemmeans_df &lt;- data.frame(emmeans(mod_logistic, ~ mac_hole_size, at = list(mac_hole_size = c(600, 700, 800))))\n# Create df of emmeans and corresponding odds and probs, for plotting\nemmeans_df &lt;- emmeans_df |&gt; \n  select(mac_hole_size, emmean) |&gt; \n  rename(logodds = emmean) |&gt; \n  mutate(odds = round(exp(logodds), 3),\n         probs = round(plogis(logodds), 2),\n         logodds = round(logodds, 3))\n\n# Predictions ----\n# Create new df to predict on new values of x\nnew_dat &lt;- data.frame(mac_hole_size = seq(from = 0, to = 1200, length.out = 100))\n# Predict new fitted values and SE's on logodds scale\npred_logodds &lt;- predict(mod_logistic, newdata = new_dat, type = \"link\", se = TRUE)\nnew_dat &lt;- cbind(new_dat, pred_logodds)\n# Create new df of predictions\npredictions &lt;- new_dat |&gt; \n  rename(pred_logodds_est = fit) |&gt; \n  mutate(pred_logodds_LL = pred_logodds_est - (1.96 * se.fit),\n         pred_logodds_UL = pred_logodds_est + (1.96 * se.fit)) |&gt; \n  select(-c(se.fit, residual.scale))\n# Predict new fitted values and SE's on odds scale\npredictions &lt;- predictions |&gt; \n  mutate(pred_odds_est = exp(pred_logodds_est),\n         pred_odds_LL = exp(pred_logodds_LL),\n         pred_odds_UL = exp(pred_logodds_UL))\n# Predict new fitted values and SE's on probability scale\npred_probs &lt;- predict(mod_logistic, newdata = new_dat, type = \"response\", se = TRUE)\nnew_dat &lt;- cbind(new_dat[1], pred_probs)\nnew_dat &lt;- new_dat |&gt; \n  mutate(pred_probs_LL = fit - (1.96 * se.fit),\n         pred_probs_UL = fit + (1.96 * se.fit))\n# Add predicted probs and CIs to predictions df\npredictions &lt;- cbind(predictions, \n                     pred_probs_est = new_dat$fit, \n                     pred_probs_LL = new_dat$pred_probs_LL,\n                     pred_probs_UL = new_dat$pred_probs_UL)\n# Plot predicted log-odds\nggplot(predictions, aes(x = mac_hole_size, y = pred_logodds_est)) + \n  geom_ribbon(aes(ymin = pred_logodds_LL, ymax = pred_logodds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-100, 100), breaks = seq(-100, 100, by = 2)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(-8, 8)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted log-odds\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.8 Plot Predicted Odds of Surgical Repair Success\nWe can also plot the predicted odds of surgical repair success. Note that this best-fitting line is not linear, but instead looks like an exponential plot. We established previously that odds exist on a scale from 0 to \\(+\\infty\\) and that’s what we can appeciate here. Remember that this is just another transformation of the log-odds and the same trend is seen - as hole size increases, the odds of success decline.\n\n\nCode\n# Plot predicted odds\nggplot(predictions, aes(x = mac_hole_size, y = pred_odds_est)) + \n  geom_ribbon(aes(ymin = pred_odds_LL, ymax = pred_odds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1000000), breaks = seq(0, 1000000, by = 1000)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(0, 7000)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted odds\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.9 Plot Predicted Probabilities of Surgical Repair Success\nAnd we can also plot the predicted probabilities of surgical repair success. Again this is just another transformation of the log-odds or the odds. Note the classic sigmoid shape which emulates what we saw earlier when plotting the binned raw data. To my mind this plot is more useful than either plots of predicted log-odds or odds, as it tells us the likely chance of successful repair conditional on the initial macular hole size.\n\n\nCode\n# Plot predicted probabilities\nggplot(predictions, aes(x = mac_hole_size, y = pred_probs_est)) + \n  geom_ribbon(aes(ymin = pred_probs_LL, ymax = pred_probs_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted probability\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.10 Plot Observed and Predicted Probabilities of Surgical Repair Success\nAnd in fact what you should ideally find if you plot both your observed and predicted probabilities is that they are quite similar, reflecting that the model actually fits the data quite well. Of course, in this case the convergence of observed and predicted probabilities is unsurprising given that we know the data-generating proces - i.e. we fitted a model to data that came from a model. But, in general, this is a good way to assess your model fit.\n\n\nCode\n# Plot observed and predicted data\nggplot(df, aes(as.numeric(bins)*50-25, mean_y)) + \n  geom_point(size = 2) +\n  geom_line(aes(color = \"observed\"), linewidth = 1) +\n  geom_line(data = predictions, aes(x = mac_hole_size, y = pred_probs_est, color = \"predicted\"), linewidth = 1) +\n  scale_color_manual(name = \"\", values = c(\"predicted\" = \"cornflowerblue\", \"observed\" = \"orange\")) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Probability\") +\n  theme_bw(base_size = 25) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n4.3.11 Putting It All Together\nWhat I want to try and reinforce in the figure below is how the log-odds, the odds and the probability of the outcome are all transformations of each other.\nIn logistic regression we are ultimately interested in the probability of the outcome occurring. But we use the log-odds or logit link function to map probabilities onto a linear real-number range. This means that the change in Y for each unit increase in X remains the same and we can see that in the top plot. So the log-odds coefficient that our stats software gives us is constant across all values of macular hole size. This represents the difference in the log-odds of success for each unit change in X.\nOne unit increase in log-odds:\n\n600 -&gt; 700 \\(\\mu m\\)\n= -0.181 - 1.323 = -1.504\n\n700 -&gt; 800 \\(\\mu m\\)\n= -1.686 - -0.181 = -1.505\n\n\nThe middle plot shows the equivalent odds of success across the same range of macular hole size. Here the difference in Y is no longer constant for each unit increase in X, but the commensurate ratio is constant - and this gives us the odds ratio. This means that the odds ratio is constant across all values of macular hole size.\nOne unit increase in odds:\n\n600 -&gt; 700 \\(\\mu m\\)\n= 0.834/3.756 = 0.22\n\n700 -&gt; 800 \\(\\mu m\\)\n= 0.185/0.834 = 0.22\n\n\nFinally the lower plot shows the equivalent probabilities of success and now there is no constancy in either unit differences or ratios. So, if we are interested in expressing our logistic regression model results as differences or ratios of predicted probabilities, we need to make sure that we qualify the macular hole size that a particular predicted probability corresponds to, as this will change across the range of X.\nOne unit increase in probability:\n\n600 -&gt; 700 \\(\\mu m\\)\n= 0.45 - 0.79 = -0.34\n= 0.45/0.79 = 0.57\n\n700 -&gt; 800 \\(\\mu m\\)\n= 0.16 - 0.45 = -0.29\n= 0.16/0.45 = 0.36\n\n\n\n\nCode\n# Reformat plots slightly for ggarrange ----\np1 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_logodds_est)) + \n  geom_ribbon(aes(ymin = pred_logodds_LL, ymax = pred_logodds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1150, y = 6, label = \"log-odds\", size = 10) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  annotate(\"text\", x = 1110, y = 3.5, label = \"unit differences = constant\", size = 5) +\n  scale_y_continuous(limits = c(-100, 100), breaks = seq(-100, 100, by = 2)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(-8, 8)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, logodds),\n                            label = emmeans_df$logodds, \n                            nudge_x = c(50, 50, 50), nudge_y = c(4, 4, 4),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  theme_bw(base_size = 20) +\n  ylab(\"\") +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\n\np2 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_odds_est)) + \n  geom_ribbon(aes(ymin = pred_odds_LL, ymax = pred_odds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1180, y = 6000, label = \"odds\", size = 10) +\n  annotate(\"text\", x = 1136, y = 5000, label = \"unit ratios = constant\", size = 5) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(seq(0, 1000000, by = 1000))) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(0, 7000)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ylab(\"\") +\n  theme_bw(base_size = 20) +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\np2_inset &lt;- p2 +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(1,2,3,4,5, seq(0, 1000000, by = 1000))) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, odds),\n                            label = emmeans_df$odds, \n                            nudge_x = c(50, 50, 50), nudge_y = c(0.5, 1, 1),\n                            color = \"red\", segment.size = 0.2, size = 5)\np2 &lt;- p2 + geom_magnify(from = c(xmin = 500, xmax = 1000, ymin = 0, ymax = 5), \n                          to = c(xmin = 470, xmax = 1010, ymin = 1000, ymax = 5000), \n                          shadow = T, axes = \"y\", plot = p2_inset)\n\np3 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_probs_est)) + \n  geom_ribbon(aes(ymin = pred_probs_LL, ymax = pred_probs_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1140, y = 0.8, label = \"probability\", size = 10) +\n  annotate(\"text\", x = 1075, y = 0.65, label = \"unit differences/ratios = not constant\", size = 5) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, probs),\n                            label = emmeans_df$probs, \n                            nudge_x = c(50, 50, 50), nudge_y = c(0.1, 0.1, 0.1),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  ylab(\"\") + xlab(\"Macular hole size\") +\n  theme_bw(base_size = 20)\nggarrange(p1, p2, p3, align = \"v\", ncol = 1, heights = c(1,1,1.2))\n\n\n\n\n\n\n\n\n\n\n\n4.3.12 Key points\nThis has been a long post but we are finally at the end. I hope that reading this has demystified what goes on under the hood in logistic regression as it’s really not that difficult once you understand the purpose of the link function.\nThese are the main summary points:\n\nLogistic regression is a type of generalised linear model that allows the estimation of associations of potential predictors with a binary outcome.\nThe logit link function transforms the probabilities of a binary outcome variable to a continuous, unbounded scale (log-odds). Standard linear modelling methods can then be applied to estimate the model.\nLog-odds, odds and probabilities are all simple transformations of one another.\nThe model is estimated on the log-odds scale where the association between the predictor and the log-odds of the outcome is linear, and the unit difference is constant.\nResults may be reported in terms of odds or probabilities:\n\nOn the odds scale the association between the predictor and the odds of the outcome is non-linear, but the unit ratio is constant.\nOn the probability scale the association between the predictor and the probability of the outcome is non-linear, and the unit difference/ratio is not constant.\nThis means probability estimates (risk differences of ratios) depend on the value of the predictor and this needs to be specified."
  },
  {
    "objectID": "posts/013_31May_2024/index.html#case-1-one-name-column-one-value-column",
    "href": "posts/013_31May_2024/index.html#case-1-one-name-column-one-value-column",
    "title": "Reshaping Data - Think Before you Pivot",
    "section": "2.1 Case 1: One Name Column, One Value Column",
    "text": "2.1 Case 1: One Name Column, One Value Column\nThis is the most common and most straight-forward application of reshaping to long that you might be required to perform. In this case we have multiple value columns in wide format that we want to reshape to one name column and one value column. The good news is that we have already done this in the example above. The code is also shown but just to revisit that briefly for clarity:\n\ndf_long &lt;- df_wide |&gt; \n  pivot_longer(cols = month_0:month_3,\n               names_to = \"month\",\n               names_prefix = \"month_\",\n               values_to = \"bp\")\n\nSo we take the df_wide dataframe and ‘pipe’ it to the pivot_longer() function where we specify that we want to take the columns from (and including) month_0 to month_3, assigning those column names as category labels in the new month name variable, while also placing each corresponding BP measurement into the new bp value variable. The names_prefix argument is optional but was used here to strip out the somewhat redundant month_ text from each column name prior to labelling. You could certainly leave this in if you wanted and the result would then be:\n\n\n\n\n\nid\nmonth\nbp\n\n\n\n\n1\nmonth_0\n88.29\n\n\n1\nmonth_1\n145.65\n\n\n1\nmonth_2\n100.05\n\n\n1\nmonth_3\n132.61\n\n\n2\nmonth_0\n134.50\n\n\n2\nmonth_1\n140.99\n\n\n2\nmonth_2\n115.06\n\n\n2\nmonth_3\n157.35\n\n\n\n\n\nBut I’m sure you’d agree that the results looks cleaner without all that unnecessary repetition."
  },
  {
    "objectID": "posts/013_31May_2024/index.html#case-2-multiple-name-columns-one-value-column",
    "href": "posts/013_31May_2024/index.html#case-2-multiple-name-columns-one-value-column",
    "title": "Reshaping Data - Think Before you Pivot",
    "section": "2.2 Case 2: Multiple Name Columns, One Value Column",
    "text": "2.2 Case 2: Multiple Name Columns, One Value Column\nLet’s now extend this idea a little. Imagine that in addition to BP measurements, subjects also had their weight measured at the same time points (simulated with a mean of 70 kg and SD 15 kg). Now we have data that could potentially look like:\n\n\nCode\nid &lt;- seq(1:5)\nfor(i in 0:3){\n  var_name_bp &lt;- paste0(\"month_\",i,\"_bp\")\n  assign(var_name_bp, rnorm(5, 130, 20))\n  var_name_wt &lt;- paste0(\"month_\",i,\"_wt\")\n  assign(var_name_wt, rnorm(5, 70, 15))\n}\ndf_wide &lt;- data.frame(cbind(id, month_0_bp, month_0_wt, month_1_bp, month_1_wt, month_2_bp, month_2_wt, month_3_bp, month_3_wt))\ndf_wide |&gt;  \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nmonth_0_bp\nmonth_0_wt\nmonth_1_bp\nmonth_1_wt\nmonth_2_bp\nmonth_2_wt\nmonth_3_bp\nmonth_3_wt\n\n\n\n\n1\n168.71\n70.94\n110.54\n62.65\n141.74\n64.22\n117.43\n42.37\n\n\n2\n99.66\n86.30\n103.43\n69.65\n148.09\n69.11\n139.75\n60.50\n\n\n3\n122.44\n72.88\n120.12\n68.22\n105.90\n83.36\n146.80\n70.25\n\n\n4\n116.68\n52.53\n136.44\n70.32\n123.79\n71.43\n88.53\n51.79\n\n\n5\n122.52\n59.85\n145.43\n80.02\n157.58\n60.87\n137.32\n91.14\n\n\n\n\n\nWhat to do here?\nActually, some thought is required at this point as there are two potential paths you could go down and it all depends on what you want to achieve. Let’s assume that you want to put all measurement values in one column. Once you have decided on this final form, the code is not challenging. We will necessarily end up with two names columns instead of just one, one for time (month) and one for the all the clinical measures (BP and weight). The main changes to the code are to now supply two new variable names to the names_to argument as well as tell the function how to source the new category labels with the names_sep argument. This will split the currently wide variable names at the second _ (after stripping out the redundant month_ text) and use the number as the month label and the type of measurement as the clinical measure label.\n\ndf_long &lt;- df_wide |&gt; \n  pivot_longer(cols = month_0_bp:month_3_wt,\n               names_to = c(\"month\", \"clinical_measure\"),\n               names_prefix = \"month_\",\n               names_sep = \"_\",\n               values_to = \"value\")\n\nand the data looks like:\n\n\nCode\ndf_long |&gt;  \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nid\nmonth\nclinical_measure\nvalue\n\n\n\n\n1\n0\nbp\n168.71\n\n\n1\n0\nwt\n70.94\n\n\n1\n1\nbp\n110.54\n\n\n1\n1\nwt\n62.65\n\n\n1\n2\nbp\n141.74\n\n\n1\n2\nwt\n64.22\n\n\n1\n3\nbp\n117.43\n\n\n1\n3\nwt\n42.37\n\n\n2\n0\nbp\n99.66\n\n\n2\n0\nwt\n86.30\n\n\n2\n1\nbp\n103.43\n\n\n2\n1\nwt\n69.65\n\n\n2\n2\nbp\n148.09\n\n\n2\n2\nwt\n69.11\n\n\n2\n3\nbp\n139.75\n\n\n2\n3\nwt\n60.50\n\n\n3\n0\nbp\n122.44\n\n\n3\n0\nwt\n72.88\n\n\n3\n1\nbp\n120.12\n\n\n3\n1\nwt\n68.22\n\n\n3\n2\nbp\n105.90\n\n\n3\n2\nwt\n83.36\n\n\n3\n3\nbp\n146.80\n\n\n3\n3\nwt\n70.25\n\n\n4\n0\nbp\n116.68\n\n\n4\n0\nwt\n52.53\n\n\n4\n1\nbp\n136.44\n\n\n4\n1\nwt\n70.32\n\n\n4\n2\nbp\n123.79\n\n\n4\n2\nwt\n71.43\n\n\n4\n3\nbp\n88.53\n\n\n4\n3\nwt\n51.79\n\n\n5\n0\nbp\n122.52\n\n\n5\n0\nwt\n59.85\n\n\n5\n1\nbp\n145.43\n\n\n5\n1\nwt\n80.02\n\n\n5\n2\nbp\n157.58\n\n\n5\n2\nwt\n60.87\n\n\n5\n3\nbp\n137.32\n\n\n5\n3\nwt\n91.14\n\n\n\n\n\nI tend to think of this as a complete reshaping to long format."
  },
  {
    "objectID": "posts/013_31May_2024/index.html#case-3-one-name-column-multiple-value-columns",
    "href": "posts/013_31May_2024/index.html#case-3-one-name-column-multiple-value-columns",
    "title": "Reshaping Data - Think Before you Pivot",
    "section": "2.3 Case 3: One Name Column, Multiple Value Columns",
    "text": "2.3 Case 3: One Name Column, Multiple Value Columns\nBut what if didn’t want to do this and instead wanted the values of BP and weight to appear in their own columns - a partial reshaping to long format if you like. To my mind this is probably a more useful long format than what we considered in the last example, although there may be some niche use-case scenarios that require data to be in that format for analysis (they just elude me right now).\nSo let’s now assume that you want separate columns of values for each type of measurement. Now we will end up with one name column and two value columns - one for BP and one for weight. The general form of the code doesn’t change a lot in this case - the main thing being that we replace “clinical_measure” in the names_to argument with a special term .value which indicates that the pivoted (new) columns will be split by the text after the second _ in the currently wide column names - i.e. taking on the value of bp or wt (so it’s not necessary to specify a values_to term this time around). In those two new columns the corresponding measurement values will be placed. The code looks like:\n\ndf_long &lt;- df_wide |&gt; \n  pivot_longer(cols = month_0_bp:month_3_wt,\n               names_to = c(\"month\", \".value\"),\n               names_prefix = \"month_\",\n               names_sep = \"_\")\n\nand the data now looks like:\n\n\nCode\ndf_long |&gt;  \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nid\nmonth\nbp\nwt\n\n\n\n\n1\n0\n168.71\n70.94\n\n\n1\n1\n110.54\n62.65\n\n\n1\n2\n141.74\n64.22\n\n\n1\n3\n117.43\n42.37\n\n\n2\n0\n99.66\n86.30\n\n\n2\n1\n103.43\n69.65\n\n\n2\n2\n148.09\n69.11\n\n\n2\n3\n139.75\n60.50\n\n\n3\n0\n122.44\n72.88\n\n\n3\n1\n120.12\n68.22\n\n\n3\n2\n105.90\n83.36\n\n\n3\n3\n146.80\n70.25\n\n\n4\n0\n116.68\n52.53\n\n\n4\n1\n136.44\n70.32\n\n\n4\n2\n123.79\n71.43\n\n\n4\n3\n88.53\n51.79\n\n\n5\n0\n122.52\n59.85\n\n\n5\n1\n145.43\n80.02\n\n\n5\n2\n157.58\n60.87\n\n\n5\n3\n137.32\n91.14"
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html",
    "href": "posts/015_28Jun_2024/index.html",
    "title": "Academic Research - “What’s in a Title?”",
    "section": "",
    "text": "So that was a pretty poor attempt to adapt a famous adage from Romeo and Juliet - “What’s in a name? That which we call a rose by any other name would smell just as sweet” - for my own expository purposes.\nBefore taking a short mid-year break (and also because I didn’t have time to prepare anything formal this week), I thought we could have a little fun in taking a look at some clever, bizarre, and in some cases laugh-out-loud titles that academic researchers have published with over the years.\nBut we can’t have fun without a little science as well. So, is there actually anything to be gained by giving your research paper a less serious title? Interestingly, there is a recent preprint paper out there that has attempted to assess this relationship - specifically between humour in article titles and the all-important citation impact.\nIf this title is funny, will you cite me? Citation impacts of humour and other features of article titles in ecology and evolution (and the Nature opinion piece of the same paper).\nThe TL;DR is that articles with humourous titles tend to be cited less, but article ‘importance’ was considered to be a confounder in this association, and once adjustment was made for that, articles with humourous titles were in fact cited more. If you’re interested, have a read of the paper. I’m not that convinced as the authors used self-citation as a proxy for their definition of importance and I’m sure there are better ways to assess this.\nBut anyway, now that the science is out of the way, let’s get to the fun bit. These are a collection of papers that I’ve come across over the years and no doubt some will be familiar to you, but please indulge me anyway. Many of these have used clever wordplay or puns, but some are also serious (and unfortunate) titles.\nI am going to try my best to broadly group them by some linking theme, if that is even possible…."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#primary-transformations",
    "href": "posts/015_28Jun_2024/index.html#primary-transformations",
    "title": "Academic Research - “What’s in a Title”",
    "section": "3.1 Primary Transformations",
    "text": "3.1 Primary Transformations\nIf we have the odds of an event, then we can transform that to a probability - or a risk, which is another way to refer to the chance of an outcome - by dividing the odds by the odds plus 1.\n\\[\\text{p} = \\frac{\\text{odds}}{1 + \\text{odds}}\\]\nIf instead we have the probability of an event, we can transform that to the equivalent odds of the event, by dividing the probability by its complement (that is 1 minus the probability).\n\\[\\text{odds} = \\frac{\\text{p}}{1 - \\text{p}}\\]\nAnd the log-odds is simply taking the natural logarithm of the odds - there is nothing difficult about that.\n\\[\\text{log-odds} = \\text{log(odds)}\\]\nSo these three are what I consider primary transformations and we can cycle between them quite easily with a calculator."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#secondary-transformations",
    "href": "posts/015_28Jun_2024/index.html#secondary-transformations",
    "title": "Academic Research - “What’s in a Title”",
    "section": "3.2 Secondary Transformations",
    "text": "3.2 Secondary Transformations\nThere are additional transformations that you can make and while these aren’t quite as straightforward, they are even more fundamental to logistic regression, which ultimately uses the log-odds as the scale to estimate its parameters on.\nThe first is that we can get to a probability directly from the log-odds by taking the exponent of the log-odds divided by the exponent of the log-odds plus one.\n\\[\\text{p} = \\frac{e^{\\text{log-odds}}}{1 + e^{\\text{log-odds}}}\\]\nAnd as you can see, that’s really no different to the first equation above because the exponent of the log-odds is simply the odds (the reverse transformation of logging a number is to take the exponent of that logged number). i.e.\n\\[\\text{odds} = e^{\\text{log-odds}}\\]\nNow, the final transformation, and the one that’s most relevant to logistic regression shows the relationship between the log-odds and probability and is known as the logit function (red box) - and we’ll talk more about this in a minute.\n\\[\\text{log-odds} = \\text{log}\\Bigl(\\frac{\\text{p}}{1 - \\text{p}}\\Bigr)\\]"
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#probability-vs-odds-and-as-applied-to-the-research-question",
    "href": "posts/015_28Jun_2024/index.html#probability-vs-odds-and-as-applied-to-the-research-question",
    "title": "Academic Research - “What’s in a Title”",
    "section": "3.3 Probability vs Odds (and as applied to the Research Question)",
    "text": "3.3 Probability vs Odds (and as applied to the Research Question)\nLet’s go back now to our original research question and specifically look at the difference between the probability and odds of an event - I want to solidify this concept in your minds. For most of us probability (or chance) is much more intuitive than the notion of odds. If I think about the likelihood of it being a wet day, I think there’s (for example) an 80% chance of it raining, rather than 4 to 1 odds in favour of it raining. However, if you like to wager on the horses than you may actually be quite comfortable with thinking in terms of odds - as those are the terms bets are placed in.\nIn the following I’ve simulated some data from the original paper (based on the summary statistics and regression model provided). Obviously macular hole size is a continuous variable, but for the sake of this illustration I am going to dichotomise it at it’s approximate mean value (480 microns). From this we can draw up a 2 x 2 contingency table of the number of successful and failed macular hole repairs as a function of the dichotomised version of macular hole size.\nWhen we talk about probabilities we focus on the proportion of successes out of the total for each row. So, the probability (or risk) of successful repair for those patients with a macular hole size less than 480 microns is therefore 479/487 or about 0.98. That’s in contrast to the probability of successful repair in those patients with a size greater than 480 microns - where the chance of success is 380/503 or 0.76 - so lower as one would expect. You can then take a ratio of those two probabilities or risks to give the classic risk ratio and that equals 1.3 which tells you that the chance of successful repair is 30% greater if you have a smaller macular hole at initial presentation.\nNow let’s look at the equivalent odds. When we talk about odds we focus on the ratio of successes to failures and don’t really worry about the totals. So the odds of successful repair for those patients with a smaller initial hole is 479/8 or 60 to 1. The odds of success with a larger hole are a lot more modest - 380/123 or about 3 to 1. We can then calculate the classic odds ratio which works out at about 20, meaning the odds of successful repair is about 20 times greater in those with thinner initial holes.\nNote that both of these effect measures are commensurate - they point in exactly the same direction - that you fare better with a smaller hole at diagnosis. They are just different ways of expressing the same thing."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#the-logit-link-function-and-why-log-odds-are-actually-important",
    "href": "posts/015_28Jun_2024/index.html#the-logit-link-function-and-why-log-odds-are-actually-important",
    "title": "Academic Research - “What’s in a Title”",
    "section": "4.1 The Logit Link Function (and why log-odds are actually important)",
    "text": "4.1 The Logit Link Function (and why log-odds are actually important)\nThe log-odds or the logit function is pivotal to how logistic regression works. The logit function is a type of link function and there’s really nothing mysterious or difficult about this. A link function is just a function of the mean of Y - in other words a transformation of the mean of Y - and we use this link function as the outcome instead of Y itself.\n\\[\\text{logit(p)} = \\text{log}\\Bigl(\\frac{\\text{p}}{1 - \\text{p}}\\Bigr)\\] The logistic model then becomes:\n\\[\\text{Y} = \\text{logit(p)} = \\beta_0 \\; + \\; \\beta_1x\\]\nThis allows the outcome to become continuous and unbounded and the association between Y and X to become linear, therefore satisfying those fundamental assumptions of the linear model that I mentioned earlier.\nThus, the logit is a type of transformation that links or maps probability values from \\((0, 1)\\) to real numbers \\({\\displaystyle (-\\infty ,+\\infty )}\\). Now, on the log-odds/logit scale the effect of a one-unit change in X on Y becomes constant once again."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#wth-log-oddsoddsprobabilities---youre-confusing-me---pick-one-please",
    "href": "posts/015_28Jun_2024/index.html#wth-log-oddsoddsprobabilities---youre-confusing-me---pick-one-please",
    "title": "Academic Research - “What’s in a Title”",
    "section": "4.2 WTH? log-odds/odds/probabilities - You’re confusing me - pick one please!!",
    "text": "4.2 WTH? log-odds/odds/probabilities - You’re confusing me - pick one please!!\nIt’s important to remember that logistic regression is really about probabilities, not odds. Probabilities are more intuitive than odds (unless you like to put a bet on the horses), so why use odds? Well, model results have historically been expressed in terms of odds and odds ratios for mathematical convenience. As it is for the log-odds scale, the ‘effect’ of a one-unit change in X on Y is also constant on the odds scale (but as a ratio of two odds, not as a difference in two log-odds). So an odds ratio describes a constant ‘effect’ of a predictor on the odds of an outcome, irrespective of the value of the predictor. This is not the case on the probability scale, where both differences and ratios vary per unit change depending on the value of the predictor (I will illustrate this later).\nThe TLDR:\n\nModel is estimated (under the hood) on the log-odds scale to satisfy modelling assumptions.\nModel results are displayed on the odds scale (as odds ratios) for their constancy.\nProbabilities are calculated post-hoc with not much extra effort."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#back-to-the-research-question",
    "href": "posts/015_28Jun_2024/index.html#back-to-the-research-question",
    "title": "Academic Research - “What’s in a Title”",
    "section": "4.3 Back to the Research Question",
    "text": "4.3 Back to the Research Question\nLet’s go back to the original research question for the final time and now illustrate these concepts in that context. The regression equation for surgical repair success as a function of macular hole size that was estimated in our paper is shown here.\n\\[\\text{logit(p)} = 10.89 - 0.016 \\; \\text{x} \\;\\text{macular hole size}\\;(\\mu m)\\]\nBasically, the log-odds of successful repair can be predicted by calculating the result of 10.89 minus the product of 0.016 and whatever the macular hole size is in microns.\n\n4.3.1 Simulate Some Data\nWe can simulate some data from that regression equation - essentially reverse engineering fake data from the estimated best fit equation on the actual data. Why do this? Well, it gives us some sense of what the actual data may have looked like that the researchers collected empirically. We can then do all sorts of cool things like refit a model, generate model predictions, etc. Data simulation is something I am still learning more about but it opens up a world of possibilities in your analytical work.\nFor this simulation I used a normal distribution with a mean and SD for macular hole size of 486 and 142, respectively (the numbers came from the original research paper). The code to show how to reproduce this in R is included below but you can do something similar in Stata and in most other scriptable statistical software packages.\n\n\nCode\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(emmeans)\nlibrary(gtsummary)\nlibrary(ggmagnify)\nlibrary(ggpubr)\n# Simulation\nn &lt;- 1000                                          # set number of obs to simulate\nset.seed(1234)                                     # set seed for reproducibility\nmac_hole_size  &lt;-  rnorm(n, 486, 142)              # generate macular hole inner opening data with mean 486 and sd = 152\nlogodds_success  &lt;-  10.89 - 0.016 * mac_hole_size # generate variable that is linear combination of intercept = 10.89 and coefficient for macular hole -0.016 (logit scale)\nodds_success  &lt;-  exp(logodds_success)             # exponentiate logodds to get odds\nprob_success  &lt;-  odds_success/(1 + odds_success)  # generate probabilities from this\ny_success  &lt;-  rbinom(n, 1, prob_success)          # generate outcome variable as a function of those probabilities\ndf &lt;-  data.frame(cbind(mac_hole_size,             # combine into dataframe\n                        logodds_success,\n                        odds_success,\n                        prob_success, \n                        y_success))\ndf &lt;- df |&gt; \n  filter(mac_hole_size &gt; 100)                       # only include those with size &gt; 100\n\n\nEssentially, we generate some random values of macular hole size based on the specified distribution and then plug those values in to the regression equation and calculate the various outcome measures. The resultant data look like (first 20 rows shown):\n\n\nCode\nhead(df, 20) |&gt; \n  kable(align = \"c\", digits = 2) |&gt; \n  kable_styling() |&gt; \n  row_spec(1, bold = T, background = \"skyblue1\") |&gt; \n  row_spec(20, bold = T, background = \"lightgoldenrod1\")\n\n\n\n\n\nmac_hole_size\nlogodds_success\nodds_success\nprob_success\ny_success\n\n\n\n\n314.60\n5.86\n349.48\n1.00\n1\n\n\n525.39\n2.48\n11.99\n0.92\n1\n\n\n639.99\n0.65\n1.92\n0.66\n1\n\n\n152.91\n8.44\n4644.44\n1.00\n1\n\n\n546.94\n2.14\n8.49\n0.89\n1\n\n\n557.86\n1.96\n7.13\n0.88\n1\n\n\n404.39\n4.42\n83.08\n0.99\n1\n\n\n408.38\n4.36\n77.94\n0.99\n1\n\n\n405.85\n4.40\n81.16\n0.99\n1\n\n\n359.61\n5.14\n170.06\n0.99\n1\n\n\n418.24\n4.20\n66.57\n0.99\n1\n\n\n344.23\n5.38\n217.53\n1.00\n1\n\n\n375.77\n4.88\n131.32\n0.99\n1\n\n\n495.15\n2.97\n19.44\n0.95\n1\n\n\n622.25\n0.93\n2.54\n0.72\n1\n\n\n470.34\n3.36\n28.92\n0.97\n1\n\n\n413.44\n4.28\n71.88\n0.99\n1\n\n\n356.61\n5.18\n178.44\n0.99\n1\n\n\n367.12\n5.02\n150.82\n0.99\n1\n\n\n829.05\n-2.37\n0.09\n0.09\n0\n\n\n\n\n\n\n\n\nLet’s pick a couple of data points to examine more closely.\nIf we look at the first row (blue), the macular hole size is about 315 microns and this translates to a log-odds of successful repair of 5.9 (calculated straight from the regression equation), an odds of successful repair of about 350 to 1 (transformation calculation) and a probability of successful repair of 0.997 (transformation calculation). We can then generate the actual outcome status using that probability as a random variable in a binomial distribution. So in this case a relatively low macular hole size translates to a successful repair, as we might have expected.\nNow if we look at the last row of data (yellow), we see something different. Here we have a simulated size of 829 microns and this translates to a log-odds of successful repair of -2.4, an odds of 1 to 10 against and a probability of less than 0.1. Not surprisingly, the outcome status in this case is simulated as a failure.\n\n\n4.3.2 Plot the Observed Data\nWhat if we now plot the observed data, that is, plot the outcome status as a function of macular hole size?\nI’m a big advocate of always plotting your data first - before you calculate any descriptive statistics and certainly before running any statistical tests. In fact I’m not being melodramatic when I say that plotting your data can save you from looking like an analytical fool when you’re potentially ill-founded assumptions aren’t realised in the actual data. A good visualisation can reveal these kinds of problems instantly.\nAfter that big recommendation, we might be left feeling slightly deflated when we see that plotting binary data doesn’t appear that informative - I mean where’s the nice scatter plot and imaginary trend line that we’re used to thinking about?\n\n\nCode\n# Plot observed data\nggplot(df, aes(mac_hole_size, y_success)) + \n  geom_point(size = 4, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 1)) +\n  xlab(\"Macular hole size\") + ylab(\"Surgical repair success\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\nWell it’s not entirely helpless - we can see that there’s more successes skewed towards smaller macular holes and more failures skewed towards larger macular holes, so that does tell us something useful. But there are better ways to plot binary data and I’ll take you through this shortly.\n\n\n4.3.3 Fit Linear Model (Naive Approach)\nWhat if we fit a standard regression line to binary data, effectively treating Y as continuous? Well people have done this in the past, and in some parts of the data analysis world, still do. It kind of works, but it’s not the best tool for the job and we can certainly do better. The best fit line is negative suggesting that as macular hole size increases, repair success decreases, so it is in line with what we know to be true. Note that I have rescaled macular hole size so that the resulting coefficient isn’t so small (now 1 ‘unit’ = 100 microns instead of 1 micron).\n\n\nCode\n# Linear model\nmod_linear &lt;- lm(y_success ~ I(mac_hole_size/100), data = df) \nmod_linear |&gt; \n  tbl_regression(intercept = T)\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n1.5\n1.4, 1.5\n&lt;0.001\n\n\nI(mac_hole_size/100)\n-0.12\n-0.14, -0.11\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nCode\n# Plot data with regression line\nggplot(df, aes(mac_hole_size, y_success)) + \n  geom_point(size = 2, alpha = 0.1) +\n  geom_abline(slope = coef(mod_linear)[[\"I(mac_hole_size/100)\"]]/100, \n              intercept = coef(mod_linear)[[\"(Intercept)\"]], \n              color = \"cornflowerblue\", linewidth = 1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 1)) +\n  xlab(\"Macular hole size\") + ylab(\"Surgical repair success\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.4 Problem - Predictions are made Outside the Probability Domain\nThe problem with naively fitting a standard linear model is that because we are know treating Y as unbounded and continuous, the model no longer knows that the outcome has to be constrained between 0 and 1, and predictions can easily fall outside this range. And that’s exactly what we can see here - a macular hole size of 100 microns (blue) predicts outcome success at 1.35 and a size of 1200 microns (yellow) predicts outcome success at -0.02. So fitting a standard linear model to binary data is really not a good approach.\n\n\nCode\n# Create new df to predict on new values of x\nnew_dat &lt;- data.frame(mac_hole_size = seq(from = 0, to = 1200, by = 50))\n# Predict new fitted values\npred &lt;- cbind(new_dat, prediction = predict(mod_linear, newdata = new_dat))\n# Display predictions\npred |&gt; \n  kable(align = \"c\", digits = 2) |&gt; \n  kable_styling() |&gt; \n  row_spec(3, bold = T, background = \"skyblue1\") |&gt; \n  row_spec(25, bold = T, background = \"lightgoldenrod1\")\n\n\n\n\n\nmac_hole_size\nprediction\n\n\n\n\n0\n1.47\n\n\n50\n1.41\n\n\n100\n1.35\n\n\n150\n1.29\n\n\n200\n1.22\n\n\n250\n1.16\n\n\n300\n1.10\n\n\n350\n1.04\n\n\n400\n0.98\n\n\n450\n0.91\n\n\n500\n0.85\n\n\n550\n0.79\n\n\n600\n0.73\n\n\n650\n0.66\n\n\n700\n0.60\n\n\n750\n0.54\n\n\n800\n0.48\n\n\n850\n0.42\n\n\n900\n0.35\n\n\n950\n0.29\n\n\n1000\n0.23\n\n\n1050\n0.17\n\n\n1100\n0.10\n\n\n1150\n0.04\n\n\n1200\n-0.02\n\n\n\n\n\n\n\n\n\n4.3.5 Plot the ‘Binned’ Observed Data\nSo I said above that there was a better way to plot binary data in an effort to make the visualisation more information, and there is. Instead of just plotting the raw data - just the 0‘s and 1’s - you can create ’bins’ of data based on (some fairly arbitrary) thresholds of the predictor you are plotting against and then plot the proportion of successes within each bin. Here I created ‘bins’ of data for each 50 \\(\\mu m\\)’s of macular hole size and then calculated the mean success rate (i.e. the proportion of 1’s) within each bin.\n\n\nCode\n# Create bins of data for each 50 microns of macular hole size\ndf$bins &lt;- cut(df$mac_hole_size, breaks = seq(0, 1000, by = 50))\n# Calculate means (number of successful repairs divided by total number of repairs) in each bin\ndf &lt;- df |&gt; \n  group_by(bins) |&gt; \n  mutate(mean_y = mean(y_success))\n# Plot observed (averaged) data \nggplot(df, aes(as.numeric(bins)*50-25, mean_y)) + \n  geom_point(size = 2) +\n  geom_line(color = \"orange\", linewidth = 1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Proportion\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\nNow we are starting to see the classic sigmoid shape of the relationship between probability and a continuous predictor.\n\n\n4.3.6 Fit a Logistic Model to the Simulated Data\nNow, let’s finally fit a logistic model to these simulated data.\n\n\nCode\n# Logistic model\nmod_logistic &lt;- glm(y_success ~ I(mac_hole_size/100), data = df, family = \"binomial\")\nmod_logistic |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\nCharacteristic\nlog(OR)1\n95% CI1\np-value\n\n\n\n\nI(mac_hole_size/100)\n-1.5\n-1.8, -1.3\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nOn the model estimation log-odds scale, the parameter estimate for the effect of macular hole size is -1.5, meaning for every 100 micron increase in macular hole size, the log-odds of success decreases by 1.5 units. All stats software will also provide you with parameter estimates as odds ratios, because that’s how we’re accustomed to reporting effects. And the equivalent odds ratio in this case is 0.22. In other words, for every 100 micron increase in macular hole size there is about a 78% reduction in the odds of success.\n\n\nCode\nmod_logistic |&gt; \n  tbl_regression(exponentiate = T)\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nI(mac_hole_size/100)\n0.22\n0.17, 0.28\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nNote that the odds ratio coefficient is a simple transformation of the log-odds coefficient (as we described earlier.) That is:\n\\[e^{-1.5} = 0.22\\]\n\n\n4.3.7 Plot Predicted log-odds of Surgical Repair Success\nNow we can plot the predicted log-odds of surgical repair success. Note that this is a straight line effectively fitted to our binary outcome, and the reason again that we can do this is because the binary outcome has been ‘remapped’ to log-odds and the log-odds exist on an unbounded and continuous scale. So this is a valid plot, unlike that before where we plotted the predictions treating Y as if it were a continuous outcome (I’m not necessarily suggesting this is a useful plot, just that it’s valid).\n\n\nCode\n# Predictions\n# Emmeans of logodds at 600 and 700 microns\nemmeans_df &lt;- data.frame(emmeans(mod_logistic, ~ mac_hole_size, at = list(mac_hole_size = c(600, 700, 800))))\n# Create df of emmeans and corresponding odds and probs, for plotting\nemmeans_df &lt;- emmeans_df |&gt; \n  select(mac_hole_size, emmean) |&gt; \n  rename(logodds = emmean) |&gt; \n  mutate(odds = round(exp(logodds), 3),\n         probs = round(plogis(logodds), 2),\n         logodds = round(logodds, 3))\n\n# Predictions ----\n# Create new df to predict on new values of x\nnew_dat &lt;- data.frame(mac_hole_size = seq(from = 0, to = 1200, length.out = 100))\n# Predict new fitted values and SE's on logodds scale\npred_logodds &lt;- predict(mod_logistic, newdata = new_dat, type = \"link\", se = TRUE)\nnew_dat &lt;- cbind(new_dat, pred_logodds)\n# Create new df of predictions\npredictions &lt;- new_dat |&gt; \n  rename(pred_logodds_est = fit) |&gt; \n  mutate(pred_logodds_LL = pred_logodds_est - (1.96 * se.fit),\n         pred_logodds_UL = pred_logodds_est + (1.96 * se.fit)) |&gt; \n  select(-c(se.fit, residual.scale))\n# Predict new fitted values and SE's on odds scale\npredictions &lt;- predictions |&gt; \n  mutate(pred_odds_est = exp(pred_logodds_est),\n         pred_odds_LL = exp(pred_logodds_LL),\n         pred_odds_UL = exp(pred_logodds_UL))\n# Predict new fitted values and SE's on probability scale\npred_probs &lt;- predict(mod_logistic, newdata = new_dat, type = \"response\", se = TRUE)\nnew_dat &lt;- cbind(new_dat[1], pred_probs)\nnew_dat &lt;- new_dat |&gt; \n  mutate(pred_probs_LL = fit - (1.96 * se.fit),\n         pred_probs_UL = fit + (1.96 * se.fit))\n# Add predicted probs and CIs to predictions df\npredictions &lt;- cbind(predictions, \n                     pred_probs_est = new_dat$fit, \n                     pred_probs_LL = new_dat$pred_probs_LL,\n                     pred_probs_UL = new_dat$pred_probs_UL)\n# Plot predicted log-odds\nggplot(predictions, aes(x = mac_hole_size, y = pred_logodds_est)) + \n  geom_ribbon(aes(ymin = pred_logodds_LL, ymax = pred_logodds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-100, 100), breaks = seq(-100, 100, by = 2)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(-8, 8)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted log-odds\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.8 Plot Predicted Odds of Surgical Repair Success\nWe can also plot the predicted odds of surgical repair success. Note that this best-fitting line is not linear, but instead looks like an exponential plot. We established previously that odds exist on a scale from 0 to \\(+\\infty\\) and that’s what we can appeciate here. Remember that this is just another transformation of the log-odds and the same trend is seen - as hole size increases, the odds of success decline.\n\n\nCode\n# Plot predicted odds\nggplot(predictions, aes(x = mac_hole_size, y = pred_odds_est)) + \n  geom_ribbon(aes(ymin = pred_odds_LL, ymax = pred_odds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1000000), breaks = seq(0, 1000000, by = 1000)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(0, 7000)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted odds\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.9 Plot Predicted Probabilities of Surgical Repair Success\nAnd we can also plot the predicted probabilities of surgical repair success. Again this is just another transformation of the log-odds or the odds. Note the classic sigmoid shape which emulates what we saw earlier when plotting the binned raw data. To my mind this plot is more useful than either plots of predicted log-odds or odds, as it tells us the likely chance of successful repair conditional on the initial macular hole size.\n\n\nCode\n# Plot predicted probabilities\nggplot(predictions, aes(x = mac_hole_size, y = pred_probs_est)) + \n  geom_ribbon(aes(ymin = pred_probs_LL, ymax = pred_probs_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted probability\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.10 Plot Observed and Predicted Probabilities of Surgical Repair Success\nAnd in fact what you should ideally find if you plot both your observed and predicted probabilities is that they are quite similar, reflecting that the model actually fits the data quite well. Of course, in this case the convergence of observed and predicted probabilities is unsurprising given that we know the data-generating proces - i.e. we fitted a model to data that came from a model. But, in general, this is a good way to assess your model fit.\n\n\nCode\n# Plot observed and predicted data\nggplot(df, aes(as.numeric(bins)*50-25, mean_y)) + \n  geom_point(size = 2) +\n  geom_line(aes(color = \"observed\"), linewidth = 1) +\n  geom_line(data = predictions, aes(x = mac_hole_size, y = pred_probs_est, color = \"predicted\"), linewidth = 1) +\n  scale_color_manual(name = \"\", values = c(\"predicted\" = \"cornflowerblue\", \"observed\" = \"orange\")) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Probability\") +\n  theme_bw(base_size = 25) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n4.3.11 Putting It All Together\nWhat I want to try and reinforce in the figure below is how the log-odds, the odds and the probability of the outcome are all transformations of each other.\nIn logistic regression we are ultimately interested in the probability of the outcome occurring. But we use the log-odds or logit link function to map probabilities onto a linear real-number range. This means that the change in Y for each unit increase in X remains the same and we can see that in the top plot. So the log-odds coefficient that our stats software gives us is constant across all values of macular hole size. This represents the difference in the log-odds of success for each unit change in X.\nOne unit increase in log-odds:\n\n600 -&gt; 700 \\(\\mu m\\)\n= -0.181 - 1.323 = -1.504\n\n700 -&gt; 800 \\(\\mu m\\)\n= -1.686 - -0.181 = -1.505\n\n\nThe middle plot shows the equivalent odds of success across the same range of macular hole size. Here the difference in Y is no longer constant for each unit increase in X, but the commensurate ratio is constant - and this gives us the odds ratio. This means that the odds ratio is constant across all values of macular hole size.\nOne unit increase in odds:\n\n600 -&gt; 700 \\(\\mu m\\)\n= 0.834/3.756 = 0.22\n\n700 -&gt; 800 \\(\\mu m\\)\n= 0.185/0.834 = 0.22\n\n\nFinally the lower plot shows the equivalent probabilities of success and now there is no constancy in either unit differences or ratios. So, if we are interested in expressing our logistic regression model results as differences or ratios of predicted probabilities, we need to make sure that we qualify the macular hole size that a particular predicted probability corresponds to, as this will change across the range of X.\nOne unit increase in probability:\n\n600 -&gt; 700 \\(\\mu m\\)\n= 0.45 - 0.79 = -0.34\n= 0.45/0.79 = 0.57\n\n700 -&gt; 800 \\(\\mu m\\)\n= 0.16 - 0.45 = -0.29\n= 0.16/0.45 = 0.36\n\n\n\n\nCode\n# Reformat plots slightly for ggarrange ----\np1 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_logodds_est)) + \n  geom_ribbon(aes(ymin = pred_logodds_LL, ymax = pred_logodds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1150, y = 6, label = \"log-odds\", size = 10) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  annotate(\"text\", x = 1110, y = 3.5, label = \"unit differences = constant\", size = 5) +\n  scale_y_continuous(limits = c(-100, 100), breaks = seq(-100, 100, by = 2)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(-8, 8)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, logodds),\n                            label = emmeans_df$logodds, \n                            nudge_x = c(50, 50, 50), nudge_y = c(4, 4, 4),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  theme_bw(base_size = 20) +\n  ylab(\"\") +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\n\np2 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_odds_est)) + \n  geom_ribbon(aes(ymin = pred_odds_LL, ymax = pred_odds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1180, y = 6000, label = \"odds\", size = 10) +\n  annotate(\"text\", x = 1136, y = 5000, label = \"unit ratios = constant\", size = 5) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(seq(0, 1000000, by = 1000))) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(0, 7000)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ylab(\"\") +\n  theme_bw(base_size = 20) +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\np2_inset &lt;- p2 +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(1,2,3,4,5, seq(0, 1000000, by = 1000))) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, odds),\n                            label = emmeans_df$odds, \n                            nudge_x = c(50, 50, 50), nudge_y = c(0.5, 1, 1),\n                            color = \"red\", segment.size = 0.2, size = 5)\np2 &lt;- p2 + geom_magnify(from = c(xmin = 500, xmax = 1000, ymin = 0, ymax = 5), \n                          to = c(xmin = 470, xmax = 1010, ymin = 1000, ymax = 5000), \n                          shadow = T, axes = \"y\", plot = p2_inset)\n\np3 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_probs_est)) + \n  geom_ribbon(aes(ymin = pred_probs_LL, ymax = pred_probs_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1140, y = 0.8, label = \"probability\", size = 10) +\n  annotate(\"text\", x = 1075, y = 0.65, label = \"unit differences/ratios = not constant\", size = 5) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, probs),\n                            label = emmeans_df$probs, \n                            nudge_x = c(50, 50, 50), nudge_y = c(0.1, 0.1, 0.1),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  ylab(\"\") + xlab(\"Macular hole size\") +\n  theme_bw(base_size = 20)\nggarrange(p1, p2, p3, align = \"v\", ncol = 1, heights = c(1,1,1.2))\n\n\n\n\n\n\n\n\n\n\n\n4.3.12 Key points\nThis has been a long post but we are finally at the end. I hope that reading this has demystified what goes on under the hood in logistic regression as it’s really not that difficult once you understand the purpose of the link function.\nThese are the main summary points:\n\nLogistic regression is a type of generalised linear model that allows the estimation of associations of potential predictors with a binary outcome.\nThe logit link function transforms the probabilities of a binary outcome variable to a continuous, unbounded scale (log-odds). Standard linear modelling methods can then be applied to estimate the model.\nLog-odds, odds and probabilities are all simple transformations of one another.\nThe model is estimated on the log-odds scale where the association between the predictor and the log-odds of the outcome is linear, and the unit difference is constant.\nResults may be reported in terms of odds or probabilities:\n\nOn the odds scale the association between the predictor and the odds of the outcome is non-linear, but the unit ratio is constant.\nOn the probability scale the association between the predictor and the probability of the outcome is non-linear, and the unit difference/ratio is not constant.\nThis means probability estimates (risk differences of ratios) depend on the value of the predictor and this needs to be specified."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#simulate-exponential-data",
    "href": "posts/015_28Jun_2024/index.html#simulate-exponential-data",
    "title": "Logarithms and Why They’re Important in Statistics",
    "section": "2.1 Simulate Exponential Data",
    "text": "2.1 Simulate Exponential Data\nLet’s first of all visualise my statement regarding the logarithms ability to convert multiplicative effects to additive effects. I’ll create a ‘geometric’ number series of 10 numbers with base 2 - i.e. each subsequent number in the series is double the previous number. In other words, 2 is the multiplying factor in this series. The data looks like:\n\n\nCode\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(gtsummary)\nlibrary(emmeans)\nx &lt;- c(0:10)\ny &lt;- 2^(0:10)\ny2 &lt;- c(paste0(\"1 = 2\\U2070\"),\n        paste0(\"2 = 2\\U00B9\"),\n        paste0(\"2x2 = 2\\U00B2\"),\n        paste0(\"2x2x2 = 2\\U00B3\"),\n        paste0(\"2x2x2x2 = 2\\U2074\"),\n        paste0(\"2x2x2x2x2 = 2\\U2075\"),\n        paste0(\"2x2x2x2x2x2 = 2\\U2076\"),\n        paste0(\"2x2x2x2x2x2x2 = 2\\U2077\"),\n        paste0(\"2x2x2x2x2x2x2x2 = 2\\U2078\"),\n        paste0(\"2x2x2x2x2x2x2x2x2 = 2\\U2079\"),\n        paste0(\"2x2x2x2x2x2x2x2x2x2 = 2\\U00B9\\U2070\"))\ndf &lt;- data.frame(cbind(x = x, y = y, `y_in_exponential_form` = y2))\ndf$x &lt;- as.numeric(df$x); df$y &lt;- as.numeric(df$y)\ndf |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nx\ny\ny_in_exponential_form\n\n\n\n\n0\n1\n1 = 2⁰\n\n\n1\n2\n2 = 2¹\n\n\n2\n4\n2x2 = 2²\n\n\n3\n8\n2x2x2 = 2³\n\n\n4\n16\n2x2x2x2 = 2⁴\n\n\n5\n32\n2x2x2x2x2 = 2⁵\n\n\n6\n64\n2x2x2x2x2x2 = 2⁶\n\n\n7\n128\n2x2x2x2x2x2x2 = 2⁷\n\n\n8\n256\n2x2x2x2x2x2x2x2 = 2⁸\n\n\n9\n512\n2x2x2x2x2x2x2x2x2 = 2⁹\n\n\n10\n1024\n2x2x2x2x2x2x2x2x2x2 = 2¹⁰\n\n\n\n\n\nYou can see that the numbers grow large very quickly."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#plot-data-on-original-scale",
    "href": "posts/015_28Jun_2024/index.html#plot-data-on-original-scale",
    "title": "Logarithms and Why They’re Important in Statistics",
    "section": "2.3 Plot Data on Original Scale",
    "text": "2.3 Plot Data on Original Scale\nLet’s now plot this data using a normal linear scale:\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(1, 1050), breaks = c(1,100,200,300,400,500,600,700,800,900,1000)) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nIt is not hard to appreciate the exponential nature of the relationship between X and Y in this plot. As X increases, Y increases at a much faster rate, but it’s hard to tell by how much."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#plot-data-on-original-scale-modified-y-axis",
    "href": "posts/015_28Jun_2024/index.html#plot-data-on-original-scale-modified-y-axis",
    "title": "Logarithms and Why They’re Important in Statistics",
    "section": "2.4 Plot Data on Original Scale (Modified Y Axis)",
    "text": "2.4 Plot Data on Original Scale (Modified Y Axis)\nWhat does the plot look like if we use the axis tick marks to indicate the actual Y values (keeping the original scale):\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(1, 1050), breaks = c(1,2,4,8,16,32,64,128,256,512,1024)) +\n  geom_segment(aes(x = 0, y = 64, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 6, y = 1, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 128, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 7, y = 1, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 7, ymin = 64, ymax = 128), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 6, xmax = 7, ymin = 1, ymax = 64), fill = \"red\", alpha = 0.02) +\n  geom_segment(aes(x = 0, y = 256, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 8, y = 1, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 512, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 9, y = 1, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 9, ymin = 256, ymax = 512), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 8, xmax = 9, ymin = 1, ymax = 256), fill = \"red\", alpha = 0.02) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nInteresting. Obviously nothing has changed except the values on the Y axis no longer reflect evenly spaced units. In fact if you took a ruler to your screen you would see that the pixel distance between each pair of ascending tick marks is double the previous pair of tick marks. The larger numbers are nicely spread out on the axis, while the smaller numbers are all cramped together.\nWhat is certainly easier to appreciate in this plot compared to the previous one is the doubling of Y for each unit increase in X. We can see for instance that the one-unit increase in X between 6 and 7 corresponds to a doubling of Y from 64 to 128. Similarly, the one-unit increase between 8 and 9 corresponds to a doubling of Y from 256 to 512.\nSo, being good data analysts we always visualise our data before we get too far into analysing it. Although we know the data-generating mechanism for these data (because we simulated it based on what we wanted), we usually don’t know the data-generating mechanism for most real-world data that we come across. So, if we were in fact naive to the origins of these data an entirely reasonable question we might ask ourselves would be “do these come from an exponential (multiplicative) distribution?”\nA natural next step would be to see if taking logs of the data linearises (i.e straightens) the association between X and Y. Remember that I mentioned earlier that logs convert numbers that are related on a multiplicative scale to numbers that are related on an additive scale. What this means in practice is that an exponential curve flattens out and becomes linear if the data are truly multiplicative in nature."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#plot-data-on-log-scale",
    "href": "posts/015_28Jun_2024/index.html#plot-data-on-log-scale",
    "title": "Logarithms and Why They’re Important in Statistics",
    "section": "2.5 Plot Data on Log Scale",
    "text": "2.5 Plot Data on Log Scale\nThere are two ways one can plot data on a log scale using ggplot() in R. The first is to log-transform the data and plot it in the normal way; the second is to leave the data as is and use ggplot() in concert with the scales package to log-transform the axis scales. Let’s consider the second option first.\nHere we specify trans = \"log2\" within the scale_y_continuous() function to transform the Y axis to a base(2) log scale. The result is:\n\n\nCode\nlibrary(scales)\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(trans = \"log2\", breaks = c(1,2,4,8,16,32,64,128,256,512,1024)) +\n  geom_segment(aes(x = 0, y = 64, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 6, y = 1, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 128, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 7, y = 1, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 7, ymin = 64, ymax = 128), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 6, xmax = 7, ymin = 1, ymax = 64), fill = \"red\", alpha = 0.02) +\n  geom_segment(aes(x = 0, y = 256, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 8, y = 1, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 512, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 9, y = 1, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 9, ymin = 256, ymax = 512), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 8, xmax = 9, ymin = 1, ymax = 256), fill = \"red\", alpha = 0.02) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nNow that the Y axis has been rescaled we can easily see that the association between X and Y is in fact linear on this scale. We can also see that where previously the spacing of the ascending tick marks on the Y axis doubled, these now remain the same. Y is still doubling for every unit increase in X, but the Y scale is now considered additive rather than multiplicative in nature (i.e. each doubling is the now the same pixel distance along the axis in the plot).\nI can hopefully consolidate this multiplicative -&gt; additive transformation in your mind by now replacing the raw values on the Y axis with their log-transformed equivalents. If you ignore the base(2) on the ascending Y axis, each exponent is now simply 1 more than the previous value. In other words, on the base(2) log scale, the ‘effects’ are additive.\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(trans = log2_trans(),\n    breaks = c(1,2,4,8,16,32,64,128,256,512,1024),\n    labels = trans_format(\"log2\", math_format(2^.x))) +\n  geom_segment(aes(x = 0, y = 64, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 6, y = 1, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 128, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 7, y = 1, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 7, ymin = 64, ymax = 128), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 6, xmax = 7, ymin = 1, ymax = 64), fill = \"red\", alpha = 0.02) +\n  geom_segment(aes(x = 0, y = 256, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 8, y = 1, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 512, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 9, y = 1, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 9, ymin = 256, ymax = 512), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 8, xmax = 9, ymin = 1, ymax = 256), fill = \"red\", alpha = 0.02) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nThe other approach to plotting data on a log scale is to actually log-transform the data, and this is not difficult. If you knew that the data series were multiplicative by a factor of 2 you would naturally transform using a base(2) log scale as you would end up with a nice, natural interpretation of the transformed data - each unit increase in X representing a doubling in Y. Often you won’t know this, but you can still achieve the goal of linearising your data by using either natural (e) or base(10) logs.\nThe plots below show the association between X and log-transformed Y for all three of the common log transformations. Note that they all produce the same effect on the association between X and Y - just the scale differs. The numbers on each Y axis represent the powers that are raised to each base to calculate the value of Y in its original units. So, for example:\n\\[2^{5} \\approx e^{3.46} \\approx 10^{1.51} \\approx 32\\]\n\n\nCode\n# Here I have performed the log-transformation of Y on-the-fly, within the ggplot call, but you can also do this by explicitly creating a new log-transformed variable in the dataset\np1 &lt;- ggplot(df, aes(x, y = log2(y))) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_segment(aes(x = 5, y = 0, xend = 5, yend = 5), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 5, xend = 5, yend = 5), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(0, 10), breaks = c(0,2,4,6,8,10)) +\n  annotate(geom = \"text\", x = 0.8, y = 5.26, label = \"5.00\", color=\"red\", size = 8) +\n  theme_bw(base_size = 20)\np2 &lt;- ggplot(df, aes(x, y = log(y))) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_segment(aes(x = 5, y = 0, xend = 5, yend = 3.46), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 3.46, xend = 5, yend = 3.46), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  annotate(geom = \"text\", x = 0.8, y = 3.65, label = \"3.46\", color=\"red\", size = 8) +\n  theme_bw(base_size = 20)\np3 &lt;- ggplot(df, aes(x, y = log10(y))) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_segment(aes(x = 5, y = 0, xend = 5, yend = 1.51), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 1.51, xend = 5, yend = 1.51), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  annotate(geom = \"text\", x = 0.8, y = 1.6, label = \"1.51\", color=\"red\", size = 8) +\n  theme_bw(base_size = 20)\ncowplot::plot_grid(p1, p2, p3, labels = c('Base(2) log', 'Natural log', 'Base(10) log'), hjust = c(-0.9,-0.7,-0.6), vjust = 4, ncol = 3, label_size = 20)\n\n\n\n\n\n\n\n\n\nSo you might still be wondering where I am headed with all of this."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#arithmetic-vs-geometric-mean",
    "href": "posts/015_28Jun_2024/index.html#arithmetic-vs-geometric-mean",
    "title": "Logarithms and Why They’re Important in Statistics",
    "section": "2.2 Arithmetic vs Geometric Mean",
    "text": "2.2 Arithmetic vs Geometric Mean\nIf someone asked you to provide a summary statistic for these data what would you give them? The mean, median or something else? The median is always a good choice when you’re uncertain about whether your data might conform to parametric distribution assumptions. The median is just the middle value in the series and can be worked out in R as:\n\nmedian(df$y)\n\n[1] 32\n\n\nWhat about the (arithmetic) mean?\n\nmean(df$y)\n\n[1] 186.0909\n\n\nThat seems fairly highly when we see that most values are less than this. But this is symptomatic of data that are related in a multiplicative way - values tend to be condensed towards one end of the scale and skewed towards the other. The fewer, larger values ‘drag’ the average towards that end of the scale. In these cases, the conventional arithmetic mean is not the best measure of central tendency and instead we should use the geometric mean.\nRemember that the arithmetic mean is calculated as such:\n\\[\\frac{1+2+4+8+16+32+64+128+256+512+1024}{11} = 186.1\\] There are two ways to calculate the geometric mean by hand (but I will also show you how to do it in R as well):\nThe first way is to take the nth root of the product of all the terms:\n\\[\\sqrt[11]{1*2*4*8*16*32*64*128*256*512*1024} = 32\\] and the second way is to take the exponent of the mean of the logged values:\n\\[e\\ ^{\\left( \\frac{log(1)+log(2)+log(4)+log(8)+log(16)+log(32)+log(64)+log(128)+log(256)+log(512)+log(1024)}{11} \\right)} = 32\\]\nIn R:\n\n\nCode\n# nth root method - manual\n(1*2*4*8*16*32*64*128*256*512*1024)^(1/11)\n\n\n[1] 32\n\n\nCode\n# logs method - manual\nexp((log(1)+log(2)+log(4)+log(8)+log(16)+log(32)+log(64)+log(128)+log(256)+log(512)+log(1024))/11)\n\n\n[1] 32\n\n\nCode\n# logs method - quick and easy\nexp(mean(log(df$y)))\n\n\n[1] 32\n\n\nIn a perfectly geometric series the geometric mean will align with the median and is a better measure of central tendency, so keep that in the back of your mind."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#modelling-assumptions",
    "href": "posts/015_28Jun_2024/index.html#modelling-assumptions",
    "title": "Logarithms and Why They’re Important in Statistics",
    "section": "2.6 Modelling Assumptions",
    "text": "2.6 Modelling Assumptions\nWell, there are several assumptions that the linear model leans on to ensure the parameter estimates it comes up with are valid and these include that the association between a continuous predictor and the outcome is roughly linear and the distribution of the model residuals is roughly normal. Both of these assumptions can fail when you try to model an obviously curved relationship as if it were linear. Certainly if domain-knowledge dictates that the origins of your data come from an exponential distribution (many biological cell process have this basis - e.g. cell division), you need to stop and think about how you will approach your analysis.\nIf you blindly ran a standard linear regression on these data, you would get:\n\n\nCode\nmod_linear &lt;- lm(y ~ x, data = df) \nmod_linear |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nx\n75\n29, 120\n0.005\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_smooth(method = \"lm\", se = F, colour = \"black\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(1, 1050), breaks = c(1,100,200,300,400,500,600,700,800,900,1000)) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nClearly this is not ideal. The model is trying to suggest that for each unit increase in X, Y increases at a constant rate of 75. Let’s see what the model predicts for the value of Y when X = 5 (when we know the actual value is 32).\n\n\nCode\nemmeans(mod_linear, ~ x, at = (list(x = 5))) |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nx\nemmean\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\n5\n186.09\n64.04\n9\n41.22\n330.97\n\n\n\n\n\nHmmm, further evidence that this is a bad model for these data. Instead, let’s rerun the regression applying one small change - we will log-transform Y on-the-fly within the model call.\n\n\nCode\nmod_trans &lt;- lm(log(y) ~ x, data = df) \ntbl &lt;- mod_trans |&gt; \n  tbl_regression() # need work around for log transformed response for tbl_regression\ntbl |&gt;\n  # remove character version of 95% CI\n  modify_column_hide(ci) |&gt; \n  # exponentiate the regression estimates\n  modify_table_body(\n    \\(x) x |&gt; mutate(across(c(estimate, conf.low, conf.high), exp))\n  ) |&gt; \n  # merge numeric LB and UB together to display in table\n  modify_column_merge(pattern = \"{conf.low}, {conf.high}\", rows = !is.na(estimate)) |&gt; \n  modify_header(conf.low = \"**95% CI**\") |&gt; \n  as_kable() # convert to kable to display on stackoverflow\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\nx\n2.0\n2.0, 2.0\n&lt;0.001\n\n\n\n\n\nplot with all log versions\n\nlogs feature in GLM link functions - eg log odds (logistic) and log count (Poisson). Which is why we take differences in logs and ratios of exponentiated values…"
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#think-about-modelling-assumptions",
    "href": "posts/015_28Jun_2024/index.html#think-about-modelling-assumptions",
    "title": "Logarithms and Why They’re Important in Statistics",
    "section": "2.6 Think About Modelling Assumptions",
    "text": "2.6 Think About Modelling Assumptions\nWell, there are several assumptions that the linear model leans on to ensure the parameter estimates it comes up with are valid and these include that the association between a continuous predictor and the outcome is roughly linear and the distribution of the model residuals is roughly normal. Both of these assumptions can fail when you try to model an obviously curved relationship as if it were linear. Certainly if domain-knowledge dictates that the origins of your data come from an exponential distribution (many biological cell process have this basis - e.g. cell division), you need to stop and think about how you will approach your analysis.\nIf you blindly ran a standard linear regression on these data, you would get:\n\n\nCode\nmod_linear &lt;- lm(y ~ x, data = df) \nmod_linear |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nx\n75\n29, 120\n0.005\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_smooth(method = \"lm\", se = F, colour = \"black\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(1, 1050), breaks = c(1,100,200,300,400,500,600,700,800,900,1000)) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nClearly this is not ideal. The model is trying to suggest that for each unit increase in X, Y increases at a constant rate of 75. Let’s see what the model predicts for the value of Y when X = 5 (when we know the actual value is 32).\n\n\nCode\nemmeans(mod_linear, ~ x, at = (list(x = 5))) |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nx\nemmean\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\n5\n186.09\n64.04\n9\n41.22\n330.97\n\n\n\n\n\nHmmm, further evidence that this is a bad model for these data. Instead, let’s rerun the regression applying one small change - we will log-transform Y on-the-fly within the model call.\n\n\nCode\nmod_trans &lt;- lm(log(y) ~ x, data = df) \ntbl &lt;- mod_trans |&gt; \n  tbl_regression() # need work around for log transformed response for tbl_regression\ntbl |&gt;\n  # remove character version of 95% CI\n  modify_column_hide(ci) |&gt; \n  # exponentiate the regression estimates\n  modify_table_body(\n    \\(x) x |&gt; mutate(across(c(estimate, conf.low, conf.high), exp))\n  ) |&gt; \n  # merge numeric LB and UB together to display in table\n  modify_column_merge(pattern = \"{conf.low}, {conf.high}\", rows = !is.na(estimate)) |&gt; \n  modify_header(conf.low = \"**95% CI**\") |&gt; \n  as_kable()\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\nx\n2.0\n2.0, 2.0\n&lt;0.001\n\n\n\n\n\nYou will get a warning if you run this model (I have hidden it) as it’s a perfect fit, because there is no randomness in the data. That doesn’t really matter though for the sake of the illustration. The Beta value represents the exponentiated coefficient for the association between X and Y and can be considered a ‘response ratio’. This is equivalent to the ratio of each pair of successive values of Y for each unit increase in X. The response ratio of 2 implies that the outcome doubles (or increases by 100%) for each unit increase in the predictor and we know this to be true.\nWhat does this model predict the value of Y at X = 5 should be?\n\n\nCode\nemmeans(mod_trans, ~ x, at = (list(x = 5)), type = \"response\") |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nx\nresponse\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\n5\n32\n0\n9\n32\n32\n\n\n\n\n\nAnd this is what we would expect a good-fitting (perfectly-fitting in this case) model to be able to do - predict values on new data in line with our empirical observations."
  },
  {
    "objectID": "posts/016_26Jul_2024/index.html",
    "href": "posts/016_26Jul_2024/index.html",
    "title": "Logarithms and Why They’re Important in Statistics",
    "section": "",
    "text": "What do the acidity (pH - power of Hydrogen), sound intensity (dB - decibels) and earthquake intensity (measured on the Richter) scales all have in common?\nThey are all reported on a log scale.\nIn our real-world experience with these scales, I would be willing to bet that you haven’t put a lot of thought into what the numbers actually mean. Sure, we might remember from high school chemistry that something is more acidic if the pH is lower than 7. We might also know that higher numbers on the decibel scale indicate louder noises, but probably not what sources of sound specific levels relate to. We might also know from it’s reporting in the news that the most recent (thankfully infrequently occurring) earthquake wasn’t that severe based on a Richter magnitude of 4. But there’s actually much more to those numbers than meets the eye.\nLet’s take a look at each of these scales in a little more detail:\n\n\n\npH (taken from: https://www.pmel.noaa.gov/co2/file/The+pH+scale+by+numbers)\n\n\n\n\n\nSound Intensity\n\n\n\n\n\nRichter (taken from: https://en.m.wikipedia.org/wiki/File:How-the-Richter-Magnitude-Scale-is-determined.jpg)\n\n\nIf you take some time to look at those figures you will realise that there is a commonality among all three of them. In each case, two sets of number scales are presented:\n\nReporting scale\n\nAcidity (0 - 14)\nSound Intensity (0 - 150)\nEarthquake Intensity (0 - 9)\n\nMeasurement scale\n\nAcidity (\\(10^0 - 10^{-14}\\))\nSound Intensity (\\(10^{-12} - 10^{3}\\))\nEarthquake Intensity (\\(10^{-1} - 10^{9}\\))\n\n\nThe reporting scale is the one that we’re all familiar with, but in each case the actual measurements are recorded on a different scale behind the scenes.\nWhy?\nThe reason is that there is just too much variation on the measurement scale - by orders of magnitude - to make it convenient to also use to describe effects. So we convert the measurement scale to a more interpretable (but somewhat arbitrary) scale for reporting.\nWell hello, logarithms.\nWhen a physical quantity varies over a very large range, it is often convenient to take its logarithm in order to have a more manageable set of numbers (good primers on logarithms and exponents can be found here and here). And that’s exactly what is happening when we talk about acidity, sound intensity and earthquake intensity.\nThere is a key point to know about logarithms:\nLogarithms convert numbers that are related on a multiplicative (exponential) scale to numbers that are related on an additive (linear) scale.\nYou will see that in each of the above cases, the natural scale that the quantity is measured on is multiplicative in nature. Each ‘unit’ change represents an order of magnitude difference in the quantity. For example, the amplitude of seismic waves (felt as the level of ground shake) in a Richter magnitude 5 earthquake (moderate) are 10 times greater than that of a magnitude 4 earthquake (small). Similarly, a ‘major’ earthquake (Richter 7) would be considered 1000 times greater in seismic activity compared to a small earthquake.\nBut when we instead use logarithms, those multiplicative effects are now converted to additive effects. Each one-unit increase in seismic activity on the Richter scale corresponds to a 10 times greater increase in seismic activity on the natural scale.\nSo, how is this relevant in our daily data analysis endeavours?"
  },
  {
    "objectID": "posts/016_26Jul_2024/index.html#simulate-exponential-data",
    "href": "posts/016_26Jul_2024/index.html#simulate-exponential-data",
    "title": "Logarithms and Why They’re Important in Statistics",
    "section": "2.1 Simulate Exponential Data",
    "text": "2.1 Simulate Exponential Data\nLet’s first of all visualise my statement regarding the logarithms ability to convert multiplicative effects to additive effects. I’ll create a ‘geometric’ number series of 10 numbers with base 2 - i.e. each subsequent number in the series is double the previous number. In other words, 2 is the multiplying factor in this series. The data looks like:\n\n\nCode\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(gtsummary)\nlibrary(emmeans)\nx &lt;- c(0:10)\ny &lt;- 2^(0:10)\ny2 &lt;- c(paste0(\"1 = 2\\U2070\"),\n        paste0(\"2 = 2\\U00B9\"),\n        paste0(\"2x2 = 2\\U00B2\"),\n        paste0(\"2x2x2 = 2\\U00B3\"),\n        paste0(\"2x2x2x2 = 2\\U2074\"),\n        paste0(\"2x2x2x2x2 = 2\\U2075\"),\n        paste0(\"2x2x2x2x2x2 = 2\\U2076\"),\n        paste0(\"2x2x2x2x2x2x2 = 2\\U2077\"),\n        paste0(\"2x2x2x2x2x2x2x2 = 2\\U2078\"),\n        paste0(\"2x2x2x2x2x2x2x2x2 = 2\\U2079\"),\n        paste0(\"2x2x2x2x2x2x2x2x2x2 = 2\\U00B9\\U2070\"))\ndf &lt;- data.frame(cbind(x = x, y = y, `y_in_exponential_form` = y2))\ndf$x &lt;- as.numeric(df$x); df$y &lt;- as.numeric(df$y)\ndf |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nx\ny\ny_in_exponential_form\n\n\n\n\n0\n1\n1 = 2⁰\n\n\n1\n2\n2 = 2¹\n\n\n2\n4\n2x2 = 2²\n\n\n3\n8\n2x2x2 = 2³\n\n\n4\n16\n2x2x2x2 = 2⁴\n\n\n5\n32\n2x2x2x2x2 = 2⁵\n\n\n6\n64\n2x2x2x2x2x2 = 2⁶\n\n\n7\n128\n2x2x2x2x2x2x2 = 2⁷\n\n\n8\n256\n2x2x2x2x2x2x2x2 = 2⁸\n\n\n9\n512\n2x2x2x2x2x2x2x2x2 = 2⁹\n\n\n10\n1024\n2x2x2x2x2x2x2x2x2x2 = 2¹⁰\n\n\n\n\n\nYou can see that the numbers grow large very quickly."
  },
  {
    "objectID": "posts/016_26Jul_2024/index.html#arithmetic-vs-geometric-mean",
    "href": "posts/016_26Jul_2024/index.html#arithmetic-vs-geometric-mean",
    "title": "Logarithms and Why They’re Important in Statistics",
    "section": "2.2 Arithmetic vs Geometric Mean",
    "text": "2.2 Arithmetic vs Geometric Mean\nIf someone asked you to provide a summary statistic for these data what would you give them? The mean, median or something else? The median is always a good choice when you’re uncertain about whether your data might conform to parametric distribution assumptions. The median is just the middle value in the series and can be worked out in R as:\n\nmedian(df$y)\n\n[1] 32\n\n\nWhat about the (arithmetic) mean?\n\nmean(df$y)\n\n[1] 186.0909\n\n\nThat seems fairly highly when we see that most values are less than this. But this is symptomatic of data that are related in a multiplicative way - values tend to be condensed towards one end of the scale and skewed towards the other. The fewer, larger values ‘drag’ the average towards that end of the scale. In these cases, the conventional arithmetic mean is not the best measure of central tendency and instead we should use the geometric mean.\nRemember that the arithmetic mean is calculated as such:\n\\[\\frac{1+2+4+8+16+32+64+128+256+512+1024}{11} = 186.1\\] There are two ways to calculate the geometric mean by hand (but I will also show you how to do it in R as well):\nThe first way is to take the nth root of the product of all the terms:\n\\[\\sqrt[11]{1*2*4*8*16*32*64*128*256*512*1024} = 32\\] and the second way is to take the exponent of the mean of the logged values:\n\\[e\\ ^{\\left( \\frac{log(1)+log(2)+log(4)+log(8)+log(16)+log(32)+log(64)+log(128)+log(256)+log(512)+log(1024)}{11} \\right)} = 32\\]\nIn R:\n\n\nCode\n# nth root method - manual\n(1*2*4*8*16*32*64*128*256*512*1024)^(1/11)\n\n\n[1] 32\n\n\nCode\n# logs method - manual\nexp((log(1)+log(2)+log(4)+log(8)+log(16)+log(32)+log(64)+log(128)+log(256)+log(512)+log(1024))/11)\n\n\n[1] 32\n\n\nCode\n# logs method - quick and easy\nexp(mean(log(df$y)))\n\n\n[1] 32\n\n\nIn a perfectly geometric series the geometric mean will align with the median and is a better measure of central tendency, so keep that in the back of your mind."
  },
  {
    "objectID": "posts/016_26Jul_2024/index.html#plot-data-on-original-scale",
    "href": "posts/016_26Jul_2024/index.html#plot-data-on-original-scale",
    "title": "Logarithms and Why They’re Important in Statistics",
    "section": "2.3 Plot Data on Original Scale",
    "text": "2.3 Plot Data on Original Scale\nLet’s now plot this data using a normal linear scale:\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(1, 1050), breaks = c(1,100,200,300,400,500,600,700,800,900,1000)) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nIt is not hard to appreciate the exponential nature of the relationship between X and Y in this plot. As X increases, Y increases at a much faster rate, but it’s hard to tell by how much."
  },
  {
    "objectID": "posts/016_26Jul_2024/index.html#plot-data-on-original-scale-modified-y-axis",
    "href": "posts/016_26Jul_2024/index.html#plot-data-on-original-scale-modified-y-axis",
    "title": "Logarithms and Why They’re Important in Statistics",
    "section": "2.4 Plot Data on Original Scale (Modified Y Axis)",
    "text": "2.4 Plot Data on Original Scale (Modified Y Axis)\nWhat does the plot look like if we use the axis tick marks to indicate the actual Y values (keeping the original scale):\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(1, 1050), breaks = c(1,2,4,8,16,32,64,128,256,512,1024)) +\n  geom_segment(aes(x = 0, y = 64, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 6, y = 1, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 128, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 7, y = 1, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 7, ymin = 64, ymax = 128), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 6, xmax = 7, ymin = 1, ymax = 64), fill = \"red\", alpha = 0.02) +\n  geom_segment(aes(x = 0, y = 256, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 8, y = 1, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 512, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 9, y = 1, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 9, ymin = 256, ymax = 512), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 8, xmax = 9, ymin = 1, ymax = 256), fill = \"red\", alpha = 0.02) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nInteresting. Obviously nothing has changed except the values on the Y axis no longer reflect evenly spaced units. In fact if you took a ruler to your screen you would see that the pixel distance between each pair of ascending tick marks is double the previous pair of tick marks. The larger numbers are nicely spread out on the axis, while the smaller numbers are all cramped together.\nWhat is certainly easier to appreciate in this plot compared to the previous one is the doubling of Y for each unit increase in X. We can see for instance that the one-unit increase in X between 6 and 7 corresponds to a doubling of Y from 64 to 128. Similarly, the one-unit increase between 8 and 9 corresponds to a doubling of Y from 256 to 512.\nSo, being good data analysts we always visualise our data before we get too far into analysing it. Although we know the data-generating mechanism for these data (because we simulated it based on what we wanted), we usually don’t know the data-generating mechanism for most real-world data that we come across. So, if we were in fact naive to the origins of these data an entirely reasonable question we might ask ourselves would be “do these come from an exponential (multiplicative) distribution?”\nA natural next step would be to see if taking logs of the data linearises (i.e straightens) the association between X and Y. Remember that I mentioned earlier that logs convert numbers that are related on a multiplicative scale to numbers that are related on an additive scale. What this means in practice is that an exponential curve flattens out and becomes linear if the data are truly multiplicative in nature."
  },
  {
    "objectID": "posts/016_26Jul_2024/index.html#plot-data-on-log-scale",
    "href": "posts/016_26Jul_2024/index.html#plot-data-on-log-scale",
    "title": "Logarithms and Why They’re Important in Statistics",
    "section": "2.5 Plot Data on Log Scale",
    "text": "2.5 Plot Data on Log Scale\nThere are two ways one can plot data on a log scale using ggplot() in R. The first is to log-transform the data and plot it in the normal way; the second is to leave the data as is and use ggplot() in concert with the scales package to log-transform the axis scales. Let’s consider the second option first.\nHere we specify trans = \"log2\" within the scale_y_continuous() function to transform the Y axis to a base(2) log scale. The result is:\n\n\nCode\nlibrary(scales)\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(trans = \"log2\", breaks = c(1,2,4,8,16,32,64,128,256,512,1024)) +\n  geom_segment(aes(x = 0, y = 64, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 6, y = 1, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 128, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 7, y = 1, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 7, ymin = 64, ymax = 128), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 6, xmax = 7, ymin = 1, ymax = 64), fill = \"red\", alpha = 0.02) +\n  geom_segment(aes(x = 0, y = 256, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 8, y = 1, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 512, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 9, y = 1, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 9, ymin = 256, ymax = 512), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 8, xmax = 9, ymin = 1, ymax = 256), fill = \"red\", alpha = 0.02) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nNow that the Y axis has been rescaled we can easily see that the association between X and Y is in fact linear on this scale. We can also see that where previously the spacing of the ascending tick marks on the Y axis doubled, these now remain the same. Y is still doubling for every unit increase in X, but the Y scale is now considered additive rather than multiplicative in nature (i.e. each doubling is the now the same pixel distance along the axis in the plot).\nI can hopefully consolidate this multiplicative -&gt; additive transformation in your mind by now replacing the raw values on the Y axis with their log-transformed equivalents. If you ignore the base(2) on the ascending Y axis, each exponent is now simply 1 more than the previous value. In other words, on the base(2) log scale, the ‘effects’ are additive.\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(trans = log2_trans(),\n    breaks = c(1,2,4,8,16,32,64,128,256,512,1024),\n    labels = trans_format(\"log2\", math_format(2^.x))) +\n  geom_segment(aes(x = 0, y = 64, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 6, y = 1, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 128, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 7, y = 1, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 7, ymin = 64, ymax = 128), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 6, xmax = 7, ymin = 1, ymax = 64), fill = \"red\", alpha = 0.02) +\n  geom_segment(aes(x = 0, y = 256, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 8, y = 1, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 512, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 9, y = 1, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 9, ymin = 256, ymax = 512), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 8, xmax = 9, ymin = 1, ymax = 256), fill = \"red\", alpha = 0.02) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nThe other approach to plotting data on a log scale is to actually log-transform the data, and this is not difficult. If you knew that the data series were multiplicative by a factor of 2 you would naturally transform using a base(2) log scale as you would end up with a nice, natural interpretation of the transformed data - each unit increase in X representing a doubling in Y. Often you won’t know this, but you can still achieve the goal of linearising your data by using either natural (e) or base(10) logs.\nThe plots below show the association between X and log-transformed Y for all three of the common log transformations. Note that they all produce the same effect on the association between X and Y - just the scale differs. The numbers on each Y axis represent the powers that are raised to each base to calculate the value of Y in its original units. So, for example:\n\\[2^{5} \\approx e^{3.46} \\approx 10^{1.51} \\approx 32\\]\n\n\nCode\n# Here I have performed the log-transformation of Y on-the-fly, within the ggplot call, but you can also do this by explicitly creating a new log-transformed variable in the dataset\np1 &lt;- ggplot(df, aes(x, y = log2(y))) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_segment(aes(x = 5, y = 0, xend = 5, yend = 5), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 5, xend = 5, yend = 5), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(0, 10), breaks = c(0,2,4,6,8,10)) +\n  annotate(geom = \"text\", x = 0.8, y = 5.26, label = \"5.00\", color=\"red\", size = 8) +\n  theme_bw(base_size = 20)\np2 &lt;- ggplot(df, aes(x, y = log(y))) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_segment(aes(x = 5, y = 0, xend = 5, yend = 3.46), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 3.46, xend = 5, yend = 3.46), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  annotate(geom = \"text\", x = 0.8, y = 3.65, label = \"3.46\", color=\"red\", size = 8) +\n  theme_bw(base_size = 20)\np3 &lt;- ggplot(df, aes(x, y = log10(y))) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_segment(aes(x = 5, y = 0, xend = 5, yend = 1.51), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 1.51, xend = 5, yend = 1.51), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  annotate(geom = \"text\", x = 0.8, y = 1.6, label = \"1.51\", color=\"red\", size = 8) +\n  theme_bw(base_size = 20)\ncowplot::plot_grid(p1, p2, p3, labels = c('Base(2) log', 'Natural log', 'Base(10) log'), hjust = c(-0.9,-0.7,-0.6), vjust = 4, ncol = 3, label_size = 20)\n\n\n\n\n\n\n\n\n\nSo you might still be wondering where I am headed with all of this."
  },
  {
    "objectID": "posts/016_26Jul_2024/index.html#think-about-modelling-assumptions",
    "href": "posts/016_26Jul_2024/index.html#think-about-modelling-assumptions",
    "title": "Logarithms and Why They’re Important in Statistics",
    "section": "2.6 Think About Modelling Assumptions",
    "text": "2.6 Think About Modelling Assumptions\nWell, there are several assumptions that the linear model leans on to ensure the parameter estimates it comes up with are valid and these include that the association between a continuous predictor and the outcome is roughly linear and the distribution of the model residuals is roughly normal. Both of these assumptions can fail when you try to model an obviously curved relationship as if it were linear. Certainly if domain-knowledge dictates that the origins of your data come from an exponential distribution (many biological cell process have this basis - e.g. cell division), you need to stop and think about how you will approach your analysis.\nIf you blindly ran a standard linear regression on these data, you would get:\n\n\nCode\nmod_linear &lt;- lm(y ~ x, data = df) \nmod_linear |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nx\n75\n29, 120\n0.005\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_smooth(method = \"lm\", se = F, colour = \"black\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(1, 1050), breaks = c(1,100,200,300,400,500,600,700,800,900,1000)) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nClearly this is not ideal. The model is trying to suggest that for each unit increase in X, Y increases at a constant rate of 75. Let’s see what the model predicts for the value of Y when X = 5 (when we know the actual value is 32).\n\n\nCode\nemmeans(mod_linear, ~ x, at = (list(x = 5))) |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nx\nemmean\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\n5\n186.09\n64.04\n9\n41.22\n330.97\n\n\n\n\n\nHmmm, further evidence that this is a bad model for these data. Instead, let’s rerun the regression applying one small change - we will log-transform Y on-the-fly within the model call.\n\n\nCode\nmod_trans &lt;- lm(log(y) ~ x, data = df) \ntbl &lt;- mod_trans |&gt; \n  tbl_regression() # need work around for log transformed response for tbl_regression\ntbl |&gt;\n  # remove character version of 95% CI\n  modify_column_hide(ci) |&gt; \n  # exponentiate the regression estimates\n  modify_table_body(\n    \\(x) x |&gt; mutate(across(c(estimate, conf.low, conf.high), exp))\n  ) |&gt; \n  # merge numeric LB and UB together to display in table\n  modify_column_merge(pattern = \"{conf.low}, {conf.high}\", rows = !is.na(estimate)) |&gt; \n  modify_header(conf.low = \"**95% CI**\") |&gt; \n  as_kable()\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\nx\n2.0\n2.0, 2.0\n&lt;0.001\n\n\n\n\n\nYou will get a warning if you run this model (I have hidden it) as it’s a perfect fit, because there is no randomness in the data. That doesn’t really matter though for the sake of the illustration. The Beta value represents the exponentiated coefficient for the association between X and Y and can be considered a ‘response ratio’. This is equivalent to the ratio of each pair of successive values of Y for each unit increase in X. The response ratio of 2 implies that the outcome doubles (or increases by 100%) for each unit increase in the predictor and we know this to be true.\nWhat does this model predict the value of Y at X = 5 should be?\n\n\nCode\nemmeans(mod_trans, ~ x, at = (list(x = 5)), type = \"response\") |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nx\nresponse\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\n5\n32\n0\n9\n32\n32\n\n\n\n\n\nAnd this is what we would expect a good-fitting (perfectly-fitting in this case) model to be able to do - predict values on new data in line with our empirical observations."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#puns-and-plays-on-popular-culture",
    "href": "posts/015_28Jun_2024/index.html#puns-and-plays-on-popular-culture",
    "title": "Academic Research - “What’s in a Title?”",
    "section": "1 Puns and Plays on Popular Culture",
    "text": "1 Puns and Plays on Popular Culture\nFantastic yeasts and where to find them: the hidden diversity of dimorphic fungal pathogens. For the Harry Potter fans.\nMedical marijuana: can’t we all just get a bong? This was a conference poster, not a paper.\nmiR miR on the wall, who’s the most malignant medulloblastoma miR of them all? Sounds like a poisoned apple is the least of anyones worries.\nGut Microbe to Brain Signaling: What Happens in Vagus… I love this one.\nDie hard: Are cancer stem cells the Bruce Willises of tumor biology? Yippee-ki-yay…\nOne ring to multiplex them all. Well, that’s just precious.\nLeaf me alone: visual constraints on the ecology of social group formation. How I feel when my kids come up to me and ask for more money.\nHow To Train Your Oncolytic Virus: the Immunological Sequel"
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#simple-and-to-the-point",
    "href": "posts/015_28Jun_2024/index.html#simple-and-to-the-point",
    "title": "Academic Research - “What’s in a Title?”",
    "section": "3 Simple and to the Point",
    "text": "3 Simple and to the Point\nThere’s not a lot to add about any of these:\nGreat big boulders I have known"
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#offensive-or-risque",
    "href": "posts/015_28Jun_2024/index.html#offensive-or-risque",
    "title": "Academic Research - “What’s in a Title?”",
    "section": "5 Offensive or Risque",
    "text": "5 Offensive or Risque\nA couple of papers that may create offence. Click on Details at your peril.\n\nPremature Speculation Concerning Pornography’s Effects on Relationships. At least read the abstract before coming to your own conclusion.\nGet Me Off Your Fucking Mailing List. This is just awesome.\nStructural and electronic properties of chiral single-wall copper nanotubes. Surely they could have come up with a better abbreviation."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#just-clever",
    "href": "posts/015_28Jun_2024/index.html#just-clever",
    "title": "Academic Research - “What’s in a Title?”",
    "section": "2 Just Clever",
    "text": "2 Just Clever\nCan you tell your clunis from your cubitus? A benchmark for functional imaging. Or, can you tell your arse from your elbow?\nYou Probably Think this Paper’s About You: Narcissists’ Perceptions of their Personality and Reputation. So, it is about me?\nChemical processes in the deep interior of Uranus\nFactitious Diarrhea: A Case of Watery Deception"
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#but-why",
    "href": "posts/015_28Jun_2024/index.html#but-why",
    "title": "Academic Research - “What’s in a Title?”",
    "section": "3 But Why?",
    "text": "3 But Why?\nOk, perhaps these aren’t funny titles, but certainly they make for interesting, if in some cases questionable, research.\nAre full or empty beer bottles sturdier and does their fracture-threshold suffice to break the human skull? “Now let’s get ethics approval for an RCT”.\nImpact of wet underwear on thermoregulatory responses and thermal comfort in the cold. Just letting you know that wet underwear is not comfortable - tell your friends.\nSword swallowing and its side effects. It turns out that sword swallowing is a hazardous activity (please don’t distract the next sword swallower you meet).\nRole of Childhood Aerobic Fitness in Successful Street Crossing. No children were actually harmed in the conduct of this study.\nA comparison of jump performances of the dog flea, Ctenocephalides canis (Curtis, 1826) and the cat flea, Ctenocephalides felis felis (Bouché, 1835). But cats can jump?!\nChickens prefer beautiful humans. Duh - obviously!\nPigeon’s discrimination of paintings by Monet and Picasso. Clearly more cultured than me.\nEnriched environment exposure accelerates rodent driving skills. Rat designated-drivers, a market ready to exploit.\nExperimental replication shows knives manufactured from frozen human feces do not work. Science at its best.\nTermination of intractable hiccups with digital rectal massage. We should all keep this in mind at our next dinner party.\nFarting as a defence against unspeakable dread. We’ve all been there."
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#not-sure-where-to-put-this-one",
    "href": "posts/015_28Jun_2024/index.html#not-sure-where-to-put-this-one",
    "title": "Academic Research - “What’s in a Title?”",
    "section": "4 Not Sure Where to Put This One…",
    "text": "4 Not Sure Where to Put This One…\nThe effect of having Christmas dinner with in-laws on gut microbiota composition. Well now you can put some science behind your decision to abstain from visiting the in-laws during the festive season - “In participants visiting in-laws, there was a significant decrease in all Ruminococcus species, known to be associated with psychological stress and depression.”"
  },
  {
    "objectID": "posts/015_28Jun_2024/index.html#to-conclude",
    "href": "posts/015_28Jun_2024/index.html#to-conclude",
    "title": "Academic Research - “What’s in a Title?”",
    "section": "6 To Conclude",
    "text": "6 To Conclude\nI’m going to end with two papers that I think are highlights.\nThe first is really a tribute to anyone who has gone through the peer-review process and published an academic paper. At some point - if you haven’t already - you are going to have to deal with Reviewer 2. While this paper provides weak evidence that Reviewer 2 might actually be the victim of Reviewer-Identity-Theft, you can feel rest assured that you are not alone in having an obviously talentless peer-review hack underappreciate your true brilliance and fine work. We’ve all been there.\nDear Reviewer 2: Go F’ Yourself\nThe second is a classic. To the research students out there - don’t let a lack of words stop you from publishing your best work.\nThe unsuccessful self-treatment of a case of “writer’s block”\nI love the review given of it at the time:\n“I have studied this manuscript very carefully with lemon juice and X-rays and have not detected a single flaw in either design or writing style. I suggest it be published without revision. Clearly, it is the most concise manuscript I have ever seen – yet it contains sufficient detail to allow other investigators to replicate Dr. Upper’s failure. In comparison with the other manuscripts I get from you containing all that complicated detail, this one was a pleasure to examine. Surely we can find a place for this paper in the Journal – perhaps on the edge of a blank page.”\nI didn’t realise this was just the first in a series, and in fact there have been both success and failures in replication of the study. Unfortunately, the more recent meta-analysis still leaves the jury out as far as I’m concerned…\nThe Unsuccessful Self-Treatment of a Case of “Writer’s Block”: A Replication\nUnsuccessful Self-Treatment of a Case of “Writer’s Block”: A Partial Failure to Replicate\nUnsuccessful Self-Treatment of “Writer’s Block”: A Review of the Literature\nThe Unsuccessful Group-Treatment of “Writer’s Block”\nThe Unsuccessful Group Treatment of “Writer’s Block”: A Ten-Year Follow-up\nA Multisite Cross-Cultural Replication of Upper’s (1974) Unsuccessful Self-Treatment of Writer’s Block\nUnsuccessful Treatments of “Writer’s Block”: A Meta-Analysis\nUntil next time…"
  },
  {
    "objectID": "posts/017_09Aug_2024/index.html",
    "href": "posts/017_09Aug_2024/index.html",
    "title": "tidylog - Console Messaging in R",
    "section": "",
    "text": "Today’s post is really quite short (no thanks needed). It’s really to point out a super-handy little package that you should load at the beginning of every one of your R scripts (but only useful if you’re a tidyverse user).\nThe package is called tidylog and it’s designed to provide immediate feedback about what the data manipulations you make with dplyr and tidyr functions (e.g. filter, select,mutate, group_by, the various join functions, etc) are actually doing to your datasets.\nTo my mind this should be built into R, as this kind of operational feedback is taken for granted by Stata users. But I guess that’s the whole point of R being open-source and community-driven in terms of ad-hoc improvements in functionality.\nI don’t think there’s really much for me to add that the package author hasn’t already said here. So please have a look.\nBut to end I will show you a quick before and after. Let’s use the inbuilt nycflights13 dataset to illustrate what output is returned if you run a bunch of data-wrangling functions.\nWithout tidylog:\n\n\nCode\nlibrary(nycflights13)\nlibrary(tidyverse)\ndat &lt;- flights |&gt;  \n select(year:day, hour, origin, dest, tailnum, carrier) |&gt; \n mutate(month = if_else(nchar(month) == 1, paste0(\"0\",month), as.character(month)),\n day = if_else(nchar(day) == 1, paste0(\"0\",day), as.character(day))) |&gt;  \n unite(\"date\", year:day, sep = \"/\", remove = T) |&gt; \n mutate(date = lubridate::ymd(date)) |&gt; \n filter(hour &gt;= 8) |&gt; \n anti_join(planes, by = \"tailnum\") |&gt; \n count(tailnum, sort = TRUE) \n\n\nNada. Thanks for nothing R!\nWith tidylog:\n\n\nCode\nsuppressMessages(library(tidylog))\ndat &lt;- flights |&gt;  \n select(year:day, hour, origin, dest, tailnum, carrier) |&gt; \n mutate(month = if_else(nchar(month) == 1, paste0(\"0\",month), as.character(month)),\n day = if_else(nchar(day) == 1, paste0(\"0\",day), as.character(day))) |&gt;  \n unite(\"date\", year:day, sep = \"/\", remove = T) |&gt; \n mutate(date = lubridate::ymd(date)) |&gt; \n filter(hour &gt;= 8) |&gt; \n anti_join(planes, by = \"tailnum\") |&gt; \n count(tailnum, sort = TRUE) \n\n\nselect: dropped 11 variables (dep_time, sched_dep_time, dep_delay, arr_time, sched_arr_time, …)\n\n\nmutate: converted 'month' from integer to character (0 new NA)\n\n\n        converted 'day' from integer to character (0 new NA)\n\n\nmutate: converted 'date' from character to Date (0 new NA)\n\n\nfilter: removed 50,726 rows (15%), 286,050 rows remaining\n\n\nanti_join: added no columns\n\n\n           &gt; rows only in x    45,008\n\n\n           &gt; rows only in y  (     39)\n\n\n           &gt; matched rows    (241,042)\n\n\n           &gt;                 =========\n\n\n           &gt; rows total        45,008\n\n\ncount: now 716 rows and 2 columns, ungrouped\n\n\nNice!\nTill next time - Happy analysing!"
  },
  {
    "objectID": "posts/018_23Aug_2024/index.html",
    "href": "posts/018_23Aug_2024/index.html",
    "title": "Biostats Book Club",
    "section": "",
    "text": "I was playing around with AI image creation this week and asked Microsoft Bing to create an image with ‘biostatistics book club’ as a prompt - this is what it came up with:\n\n\n\n\n\nHmmm - who would have ever thought talking about biostatistics could be so interesting.\nThen, for even more fun, I asked Bing to create another image using ‘biostatistics fight club’ as a prompt and it gave me this:\n\n\n\n\n\nYep, just what I imagined a bunch of pugilistic stats-nerds to look like."
  },
  {
    "objectID": "posts/018_23Aug_2024/index.html#essential-medical-statistics",
    "href": "posts/018_23Aug_2024/index.html#essential-medical-statistics",
    "title": "Biostats Book Club",
    "section": "2.1 Essential Medical Statistics",
    "text": "2.1 Essential Medical Statistics\n\n\n\n\n\nI can’t recommend this book enough. It’s now over 20 years old but that doesn’t mean it’s dated - the ‘essentials’ of statistics, well, haven’t really changed. Kirkwood’s book explains statistical concepts in such a clear and concise manner that it makes (for me at least), understanding them much, much easier. It strikes a good balance in covering all the important ideas in enough depth while still maintaining a relative lay language style."
  },
  {
    "objectID": "posts/018_23Aug_2024/index.html#intuitive-biostatistics",
    "href": "posts/018_23Aug_2024/index.html#intuitive-biostatistics",
    "title": "Biostats Book Club",
    "section": "2.2 Intuitive Biostatistics",
    "text": "2.2 Intuitive Biostatistics\n\n\n\n\n\nThe author of Intuitive Biostatistics is also the brains behind the Prism statistical software. You’ll be pleased to know there are almost no formulae written amongst its pages and I think a reasonable summary of the authors intentions is to provide a ‘common-sense’ treatment of statistical ideas. The book is littered with teaching examples as well as sections on ‘Q & A’s’ and ‘Common Mistakes’ and their potential solutions. Get the latest edition."
  },
  {
    "objectID": "posts/018_23Aug_2024/index.html#r-for-data-science",
    "href": "posts/018_23Aug_2024/index.html#r-for-data-science",
    "title": "Biostats Book Club",
    "section": "2.3 R for Data Science",
    "text": "2.3 R for Data Science\n\n\n\n\n\nif you are one of the ‘cool kids’ and use the tidyverse approach to coding in R, then this is probably worthwhile having. There is a free online version as well. R for Data Science is predominantly aimed at data-wrangling and preparing your data for analysis - tidyverse style. I don’t consider myself a great statistical programmer, so I have found some elements of this a little difficult, but the more basic stuff is really useful (and coding should be a daily journey of self-improvement anyway)."
  },
  {
    "objectID": "posts/018_23Aug_2024/index.html#the-r-book",
    "href": "posts/018_23Aug_2024/index.html#the-r-book",
    "title": "Biostats Book Club",
    "section": "2.4 The R book",
    "text": "2.4 The R book\n\n\n\n\n\nThe R Book differs from R for Data Science in that, yes it’s a book about coding in R, but the focus isn’t just on data-wrangling. This book will give you almost any bit of code to run nearly any statistical procedure in R that you could imagine. In that sense it’s also a worthwhile reference. Mind you, as a result of the breadth of material it covers, this is a BIG book!\nI hope you find these helpful in your statistical learning."
  },
  {
    "objectID": "posts/015_26Jul_2024/index.html",
    "href": "posts/015_26Jul_2024/index.html",
    "title": "Logistic Regression - Under the Hood",
    "section": "",
    "text": "Today’s post comes from a talk that some of you may have already heard me give in lab meetings, but I thought it could be helpful to have a ‘print’ copy so I’m going to do that here. The material in the talk was loosely based on the following paper:\nOphthalmic Statistics Note 11: Logistic Regression\nI’m going back to basics today. I think too often we use statistical techniques without really understanding what is going on ‘under the hood’. While that is ok to some extent, a better appreciation of what you’re actually doing when you perform a hypothesis test, or run a regression model, lends more robustness to the validity of both your results and your interpretation of them.\nSo let’s take a more fundamental look at logistic regression - a workhorse statistical model that I’d be willing to bet most of you have run at some point in your research careers. Now, because I consider myself more an applied rather than theoretical biostatistician, I am going to try and get my main points across to you with as little maths as possible, and hopefully not bore you in the process (but this is statistics, so hey…).\nWhat I hope you can gain from reading this is to think about logistic regression in a new and different way - one that completely illuminates the technique for you."
  },
  {
    "objectID": "posts/015_26Jul_2024/index.html#primary-transformations",
    "href": "posts/015_26Jul_2024/index.html#primary-transformations",
    "title": "Logistic Regression - Under the Hood",
    "section": "3.1 Primary Transformations",
    "text": "3.1 Primary Transformations\nIf we have the odds of an event, then we can transform that to a probability - or a risk, which is another way to refer to the chance of an outcome - by dividing the odds by the odds plus 1.\n\\[\\text{p} = \\frac{\\text{odds}}{1 + \\text{odds}}\\]\nIf instead we have the probability of an event, we can transform that to the equivalent odds of the event, by dividing the probability by its complement (that is 1 minus the probability).\n\\[\\text{odds} = \\frac{\\text{p}}{1 - \\text{p}}\\]\nAnd the log-odds is simply taking the natural logarithm of the odds - there is nothing difficult about that.\n\\[\\text{log-odds} = \\text{log(odds)}\\]\nSo these three are what I consider primary transformations and we can cycle between them quite easily with a calculator."
  },
  {
    "objectID": "posts/015_26Jul_2024/index.html#secondary-transformations",
    "href": "posts/015_26Jul_2024/index.html#secondary-transformations",
    "title": "Logistic Regression - Under the Hood",
    "section": "3.2 Secondary Transformations",
    "text": "3.2 Secondary Transformations\nThere are additional transformations that you can make and while these aren’t quite as straightforward, they are even more fundamental to logistic regression, which ultimately uses the log-odds as the scale to estimate its parameters on.\nThe first is that we can get to a probability directly from the log-odds by taking the exponent of the log-odds divided by the exponent of the log-odds plus one.\n\\[\\text{p} = \\frac{e^{\\text{log-odds}}}{1 + e^{\\text{log-odds}}}\\]\nAnd as you can see, that’s really no different to the first equation above because the exponent of the log-odds is simply the odds (the reverse transformation of logging a number is to take the exponent of that logged number). i.e.\n\\[\\text{odds} = e^{\\text{log-odds}}\\]\nNow, the final transformation, and the one that’s most relevant to logistic regression shows the relationship between the log-odds and probability and is known as the logit function (red box) - and we’ll talk more about this in a minute.\n\\[\\text{log-odds} = \\text{log}\\Bigl(\\frac{\\text{p}}{1 - \\text{p}}\\Bigr)\\]"
  },
  {
    "objectID": "posts/015_26Jul_2024/index.html#probability-vs-odds-and-as-applied-to-the-research-question",
    "href": "posts/015_26Jul_2024/index.html#probability-vs-odds-and-as-applied-to-the-research-question",
    "title": "Logistic Regression - Under the Hood",
    "section": "3.3 Probability vs Odds (and as applied to the Research Question)",
    "text": "3.3 Probability vs Odds (and as applied to the Research Question)\nLet’s go back now to our original research question and specifically look at the difference between the probability and odds of an event - I want to solidify this concept in your minds. For most of us probability (or chance) is much more intuitive than the notion of odds. If I think about the likelihood of it being a wet day, I think there’s (for example) an 80% chance of it raining, rather than 4 to 1 odds in favour of it raining. However, if you like to wager on the horses than you may actually be quite comfortable with thinking in terms of odds - as those are the terms bets are placed in.\nIn the following I’ve simulated some data from the original paper (based on the summary statistics and regression model provided). Obviously macular hole size is a continuous variable, but for the sake of this illustration I am going to dichotomise it at it’s approximate mean value (480 microns). From this we can draw up a 2 x 2 contingency table of the number of successful and failed macular hole repairs as a function of the dichotomised version of macular hole size.\nWhen we talk about probabilities we focus on the proportion of successes out of the total for each row. So, the probability (or risk) of successful repair for those patients with a macular hole size less than 480 microns is therefore 479/487 or about 0.98. That’s in contrast to the probability of successful repair in those patients with a size greater than 480 microns - where the chance of success is 380/503 or 0.76 - so lower as one would expect. You can then take a ratio of those two probabilities or risks to give the classic risk ratio and that equals 1.3 which tells you that the chance of successful repair is 30% greater if you have a smaller macular hole at initial presentation.\nNow let’s look at the equivalent odds. When we talk about odds we focus on the ratio of successes to failures and don’t really worry about the totals. So the odds of successful repair for those patients with a smaller initial hole is 479/8 or 60 to 1. The odds of success with a larger hole are a lot more modest - 380/123 or about 3 to 1. We can then calculate the classic odds ratio which works out at about 20, meaning the odds of successful repair is about 20 times greater in those with thinner initial holes.\nNote that both of these effect measures are commensurate - they point in exactly the same direction - that you fare better with a smaller hole at diagnosis. They are just different ways of expressing the same thing."
  },
  {
    "objectID": "posts/015_26Jul_2024/index.html#the-logit-link-function-and-why-log-odds-are-actually-important",
    "href": "posts/015_26Jul_2024/index.html#the-logit-link-function-and-why-log-odds-are-actually-important",
    "title": "Logistic Regression - Under the Hood",
    "section": "4.1 The Logit Link Function (and why log-odds are actually important)",
    "text": "4.1 The Logit Link Function (and why log-odds are actually important)\nThe log-odds or the logit function is pivotal to how logistic regression works. The logit function is a type of link function and there’s really nothing mysterious or difficult about this. A link function is just a function of the mean of Y - in other words a transformation of the mean of Y - and we use this link function as the outcome instead of Y itself.\n\\[\\text{logit(p)} = \\text{log}\\Bigl(\\frac{\\text{p}}{1 - \\text{p}}\\Bigr)\\] The logistic model then becomes:\n\\[\\text{Y} = \\text{logit(p)} = \\beta_0 \\; + \\; \\beta_1x\\]\nThis allows the outcome to become continuous and unbounded and the association between Y and X to become linear, therefore satisfying those fundamental assumptions of the linear model that I mentioned earlier.\nThus, the logit is a type of transformation that links or maps probability values from \\((0, 1)\\) to real numbers \\({\\displaystyle (-\\infty ,+\\infty )}\\). Now, on the log-odds/logit scale the effect of a one-unit change in X on Y becomes constant once again."
  },
  {
    "objectID": "posts/015_26Jul_2024/index.html#wth-log-oddsoddsprobabilities---youre-confusing-me---pick-one-please",
    "href": "posts/015_26Jul_2024/index.html#wth-log-oddsoddsprobabilities---youre-confusing-me---pick-one-please",
    "title": "Logistic Regression - Under the Hood",
    "section": "4.2 WTH? log-odds/odds/probabilities - You’re confusing me - pick one please!!",
    "text": "4.2 WTH? log-odds/odds/probabilities - You’re confusing me - pick one please!!\nIt’s important to remember that logistic regression is really about probabilities, not odds. Probabilities are more intuitive than odds (unless you like to put a bet on the horses), so why use odds? Well, model results have historically been expressed in terms of odds and odds ratios for mathematical convenience. As it is for the log-odds scale, the ‘effect’ of a one-unit change in X on Y is also constant on the odds scale (but as a ratio of two odds, not as a difference in two log-odds). So an odds ratio describes a constant ‘effect’ of a predictor on the odds of an outcome, irrespective of the value of the predictor. This is not the case on the probability scale, where both differences and ratios vary per unit change depending on the value of the predictor (I will illustrate this later).\nThe TLDR:\n\nModel is estimated (under the hood) on the log-odds scale to satisfy modelling assumptions.\nModel results are displayed on the odds scale (as odds ratios) for their constancy.\nProbabilities are calculated post-hoc with not much extra effort."
  },
  {
    "objectID": "posts/015_26Jul_2024/index.html#back-to-the-research-question",
    "href": "posts/015_26Jul_2024/index.html#back-to-the-research-question",
    "title": "Logistic Regression - Under the Hood",
    "section": "4.3 Back to the Research Question",
    "text": "4.3 Back to the Research Question\nLet’s go back to the original research question for the final time and now illustrate these concepts in that context. The regression equation for surgical repair success as a function of macular hole size that was estimated in our paper is shown here.\n\\[\\text{logit(p)} = 10.89 - 0.016 \\; \\text{x} \\;\\text{macular hole size}\\;(\\mu m)\\]\nBasically, the log-odds of successful repair can be predicted by calculating the result of 10.89 minus the product of 0.016 and whatever the macular hole size is in microns.\n\n4.3.1 Simulate Some Data\nWe can simulate some data from that regression equation - essentially reverse engineering fake data from the estimated best fit equation on the actual data. Why do this? Well, it gives us some sense of what the actual data may have looked like that the researchers collected empirically. We can then do all sorts of cool things like refit a model, generate model predictions, etc. Data simulation is something I am still learning more about but it opens up a world of possibilities in your analytical work.\nFor this simulation I used a normal distribution with a mean and SD for macular hole size of 486 and 142, respectively (the numbers came from the original research paper). The code to show how to reproduce this in R is included below but you can do something similar in Stata and in most other scriptable statistical software packages.\n\n\nCode\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(emmeans)\nlibrary(gtsummary)\nlibrary(ggmagnify)\nlibrary(ggpubr)\n# Simulation\nn &lt;- 1000                                          # set number of obs to simulate\nset.seed(1234)                                     # set seed for reproducibility\nmac_hole_size  &lt;-  rnorm(n, 486, 142)              # generate macular hole inner opening data with mean 486 and sd = 152\nlogodds_success  &lt;-  10.89 - 0.016 * mac_hole_size # generate variable that is linear combination of intercept = 10.89 and coefficient for macular hole -0.016 (logit scale)\nodds_success  &lt;-  exp(logodds_success)             # exponentiate logodds to get odds\nprob_success  &lt;-  odds_success/(1 + odds_success)  # generate probabilities from this\ny_success  &lt;-  rbinom(n, 1, prob_success)          # generate outcome variable as a function of those probabilities\ndf &lt;-  data.frame(cbind(mac_hole_size,             # combine into dataframe\n                        logodds_success,\n                        odds_success,\n                        prob_success, \n                        y_success))\ndf &lt;- df |&gt; \n  filter(mac_hole_size &gt; 100)                       # only include those with size &gt; 100\n\n\nEssentially, we generate some random values of macular hole size based on the specified distribution and then plug those values in to the regression equation and calculate the various outcome measures. The resultant data look like (first 20 rows shown):\n\n\nCode\nhead(df, 20) |&gt; \n  kable(align = \"c\", digits = 2) |&gt; \n  kable_styling() |&gt; \n  row_spec(1, bold = T, background = \"skyblue1\") |&gt; \n  row_spec(20, bold = T, background = \"lightgoldenrod1\")\n\n\n\n\n\nmac_hole_size\nlogodds_success\nodds_success\nprob_success\ny_success\n\n\n\n\n314.60\n5.86\n349.48\n1.00\n1\n\n\n525.39\n2.48\n11.99\n0.92\n1\n\n\n639.99\n0.65\n1.92\n0.66\n1\n\n\n152.91\n8.44\n4644.44\n1.00\n1\n\n\n546.94\n2.14\n8.49\n0.89\n1\n\n\n557.86\n1.96\n7.13\n0.88\n1\n\n\n404.39\n4.42\n83.08\n0.99\n1\n\n\n408.38\n4.36\n77.94\n0.99\n1\n\n\n405.85\n4.40\n81.16\n0.99\n1\n\n\n359.61\n5.14\n170.06\n0.99\n1\n\n\n418.24\n4.20\n66.57\n0.99\n1\n\n\n344.23\n5.38\n217.53\n1.00\n1\n\n\n375.77\n4.88\n131.32\n0.99\n1\n\n\n495.15\n2.97\n19.44\n0.95\n1\n\n\n622.25\n0.93\n2.54\n0.72\n1\n\n\n470.34\n3.36\n28.92\n0.97\n1\n\n\n413.44\n4.28\n71.88\n0.99\n1\n\n\n356.61\n5.18\n178.44\n0.99\n1\n\n\n367.12\n5.02\n150.82\n0.99\n1\n\n\n829.05\n-2.37\n0.09\n0.09\n0\n\n\n\n\n\n\n\n\nLet’s pick a couple of data points to examine more closely.\nIf we look at the first row (blue), the macular hole size is about 315 microns and this translates to a log-odds of successful repair of 5.9 (calculated straight from the regression equation), an odds of successful repair of about 350 to 1 (transformation calculation) and a probability of successful repair of 0.997 (transformation calculation). We can then generate the actual outcome status using that probability as a random variable in a binomial distribution. So in this case a relatively low macular hole size translates to a successful repair, as we might have expected.\nNow if we look at the last row of data (yellow), we see something different. Here we have a simulated size of 829 microns and this translates to a log-odds of successful repair of -2.4, an odds of 1 to 10 against and a probability of less than 0.1. Not surprisingly, the outcome status in this case is simulated as a failure.\n\n\n4.3.2 Plot the Observed Data\nWhat if we now plot the observed data, that is, plot the outcome status as a function of macular hole size?\nI’m a big advocate of always plotting your data first - before you calculate any descriptive statistics and certainly before running any statistical tests. In fact I’m not being melodramatic when I say that plotting your data can save you from looking like an analytical fool when you’re potentially ill-founded assumptions aren’t realised in the actual data. A good visualisation can reveal these kinds of problems instantly.\nAfter that big recommendation, we might be left feeling slightly deflated when we see that plotting binary data doesn’t appear that informative - I mean where’s the nice scatter plot and imaginary trend line that we’re used to thinking about?\n\n\nCode\n# Plot observed data\nggplot(df, aes(mac_hole_size, y_success)) + \n  geom_point(size = 4, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 1)) +\n  xlab(\"Macular hole size\") + ylab(\"Surgical repair success\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\nWell it’s not entirely helpless - we can see that there’s more successes skewed towards smaller macular holes and more failures skewed towards larger macular holes, so that does tell us something useful. But there are better ways to plot binary data and I’ll take you through this shortly.\n\n\n4.3.3 Fit Linear Model (Naive Approach)\nWhat if we fit a standard regression line to binary data, effectively treating Y as continuous? Well people have done this in the past, and in some parts of the data analysis world, still do. It kind of works, but it’s not the best tool for the job and we can certainly do better. The best fit line is negative suggesting that as macular hole size increases, repair success decreases, so it is in line with what we know to be true. Note that I have rescaled macular hole size so that the resulting coefficient isn’t so small (now 1 ‘unit’ = 100 microns instead of 1 micron).\n\n\nCode\n# Linear model\nmod_linear &lt;- lm(y_success ~ I(mac_hole_size/100), data = df) \nmod_linear |&gt; \n  tbl_regression(intercept = T)\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n1.5\n1.4, 1.5\n&lt;0.001\n\n\nI(mac_hole_size/100)\n-0.12\n-0.14, -0.11\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nCode\n# Plot data with regression line\nggplot(df, aes(mac_hole_size, y_success)) + \n  geom_point(size = 2, alpha = 0.1) +\n  geom_abline(slope = coef(mod_linear)[[\"I(mac_hole_size/100)\"]]/100, \n              intercept = coef(mod_linear)[[\"(Intercept)\"]], \n              color = \"cornflowerblue\", linewidth = 1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 1)) +\n  xlab(\"Macular hole size\") + ylab(\"Surgical repair success\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.4 Problem - Predictions are made Outside the Probability Domain\nThe problem with naively fitting a standard linear model is that because we are know treating Y as unbounded and continuous, the model no longer knows that the outcome has to be constrained between 0 and 1, and predictions can easily fall outside this range. And that’s exactly what we can see here - a macular hole size of 100 microns (blue) predicts outcome success at 1.35 and a size of 1200 microns (yellow) predicts outcome success at -0.02. So fitting a standard linear model to binary data is really not a good approach.\n\n\nCode\n# Create new df to predict on new values of x\nnew_dat &lt;- data.frame(mac_hole_size = seq(from = 0, to = 1200, by = 50))\n# Predict new fitted values\npred &lt;- cbind(new_dat, prediction = predict(mod_linear, newdata = new_dat))\n# Display predictions\npred |&gt; \n  kable(align = \"c\", digits = 2) |&gt; \n  kable_styling() |&gt; \n  row_spec(3, bold = T, background = \"skyblue1\") |&gt; \n  row_spec(25, bold = T, background = \"lightgoldenrod1\")\n\n\n\n\n\nmac_hole_size\nprediction\n\n\n\n\n0\n1.47\n\n\n50\n1.41\n\n\n100\n1.35\n\n\n150\n1.29\n\n\n200\n1.22\n\n\n250\n1.16\n\n\n300\n1.10\n\n\n350\n1.04\n\n\n400\n0.98\n\n\n450\n0.91\n\n\n500\n0.85\n\n\n550\n0.79\n\n\n600\n0.73\n\n\n650\n0.66\n\n\n700\n0.60\n\n\n750\n0.54\n\n\n800\n0.48\n\n\n850\n0.42\n\n\n900\n0.35\n\n\n950\n0.29\n\n\n1000\n0.23\n\n\n1050\n0.17\n\n\n1100\n0.10\n\n\n1150\n0.04\n\n\n1200\n-0.02\n\n\n\n\n\n\n\n\n\n4.3.5 Plot the ‘Binned’ Observed Data\nSo I said above that there was a better way to plot binary data in an effort to make the visualisation more information, and there is. Instead of just plotting the raw data - just the 0‘s and 1’s - you can create ’bins’ of data based on (some fairly arbitrary) thresholds of the predictor you are plotting against and then plot the proportion of successes within each bin. Here I created ‘bins’ of data for each 50 \\(\\mu m\\)’s of macular hole size and then calculated the mean success rate (i.e. the proportion of 1’s) within each bin.\n\n\nCode\n# Create bins of data for each 50 microns of macular hole size\ndf$bins &lt;- cut(df$mac_hole_size, breaks = seq(0, 1000, by = 50))\n# Calculate means (number of successful repairs divided by total number of repairs) in each bin\ndf &lt;- df |&gt; \n  group_by(bins) |&gt; \n  mutate(mean_y = mean(y_success))\n# Plot observed (averaged) data \nggplot(df, aes(as.numeric(bins)*50-25, mean_y)) + \n  geom_point(size = 2) +\n  geom_line(color = \"orange\", linewidth = 1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Proportion\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\nNow we are starting to see the classic sigmoid shape of the relationship between probability and a continuous predictor.\n\n\n4.3.6 Fit a Logistic Model to the Simulated Data\nNow, let’s finally fit a logistic model to these simulated data.\n\n\nCode\n# Logistic model\nmod_logistic &lt;- glm(y_success ~ I(mac_hole_size/100), data = df, family = \"binomial\")\nmod_logistic |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\nCharacteristic\nlog(OR)1\n95% CI1\np-value\n\n\n\n\nI(mac_hole_size/100)\n-1.5\n-1.8, -1.3\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nOn the model estimation log-odds scale, the parameter estimate for the effect of macular hole size is -1.5, meaning for every 100 micron increase in macular hole size, the log-odds of success decreases by 1.5 units. All stats software will also provide you with parameter estimates as odds ratios, because that’s how we’re accustomed to reporting effects. And the equivalent odds ratio in this case is 0.22. In other words, for every 100 micron increase in macular hole size there is about a 78% reduction in the odds of success.\n\n\nCode\nmod_logistic |&gt; \n  tbl_regression(exponentiate = T)\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nI(mac_hole_size/100)\n0.22\n0.17, 0.28\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nNote that the odds ratio coefficient is a simple transformation of the log-odds coefficient (as we described earlier.) That is:\n\\[e^{-1.5} = 0.22\\]\n\n\n4.3.7 Plot Predicted log-odds of Surgical Repair Success\nNow we can plot the predicted log-odds of surgical repair success. Note that this is a straight line effectively fitted to our binary outcome, and the reason again that we can do this is because the binary outcome has been ‘remapped’ to log-odds and the log-odds exist on an unbounded and continuous scale. So this is a valid plot, unlike that before where we plotted the predictions treating Y as if it were a continuous outcome (I’m not necessarily suggesting this is a useful plot, just that it’s valid).\n\n\nCode\n# Predictions\n# Emmeans of logodds at 600 and 700 microns\nemmeans_df &lt;- data.frame(emmeans(mod_logistic, ~ mac_hole_size, at = list(mac_hole_size = c(600, 700, 800))))\n# Create df of emmeans and corresponding odds and probs, for plotting\nemmeans_df &lt;- emmeans_df |&gt; \n  select(mac_hole_size, emmean) |&gt; \n  rename(logodds = emmean) |&gt; \n  mutate(odds = round(exp(logodds), 3),\n         probs = round(plogis(logodds), 2),\n         logodds = round(logodds, 3))\n\n# Predictions ----\n# Create new df to predict on new values of x\nnew_dat &lt;- data.frame(mac_hole_size = seq(from = 0, to = 1200, length.out = 100))\n# Predict new fitted values and SE's on logodds scale\npred_logodds &lt;- predict(mod_logistic, newdata = new_dat, type = \"link\", se = TRUE)\nnew_dat &lt;- cbind(new_dat, pred_logodds)\n# Create new df of predictions\npredictions &lt;- new_dat |&gt; \n  rename(pred_logodds_est = fit) |&gt; \n  mutate(pred_logodds_LL = pred_logodds_est - (1.96 * se.fit),\n         pred_logodds_UL = pred_logodds_est + (1.96 * se.fit)) |&gt; \n  select(-c(se.fit, residual.scale))\n# Predict new fitted values and SE's on odds scale\npredictions &lt;- predictions |&gt; \n  mutate(pred_odds_est = exp(pred_logodds_est),\n         pred_odds_LL = exp(pred_logodds_LL),\n         pred_odds_UL = exp(pred_logodds_UL))\n# Predict new fitted values and SE's on probability scale\npred_probs &lt;- predict(mod_logistic, newdata = new_dat, type = \"response\", se = TRUE)\nnew_dat &lt;- cbind(new_dat[1], pred_probs)\nnew_dat &lt;- new_dat |&gt; \n  mutate(pred_probs_LL = fit - (1.96 * se.fit),\n         pred_probs_UL = fit + (1.96 * se.fit))\n# Add predicted probs and CIs to predictions df\npredictions &lt;- cbind(predictions, \n                     pred_probs_est = new_dat$fit, \n                     pred_probs_LL = new_dat$pred_probs_LL,\n                     pred_probs_UL = new_dat$pred_probs_UL)\n# Plot predicted log-odds\nggplot(predictions, aes(x = mac_hole_size, y = pred_logodds_est)) + \n  geom_ribbon(aes(ymin = pred_logodds_LL, ymax = pred_logodds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-100, 100), breaks = seq(-100, 100, by = 2)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(-8, 8)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted log-odds\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.8 Plot Predicted Odds of Surgical Repair Success\nWe can also plot the predicted odds of surgical repair success. Note that this best-fitting line is not linear, but instead looks like an exponential plot. We established previously that odds exist on a scale from 0 to \\(+\\infty\\) and that’s what we can appeciate here. Remember that this is just another transformation of the log-odds and the same trend is seen - as hole size increases, the odds of success decline.\n\n\nCode\n# Plot predicted odds\nggplot(predictions, aes(x = mac_hole_size, y = pred_odds_est)) + \n  geom_ribbon(aes(ymin = pred_odds_LL, ymax = pred_odds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1000000), breaks = seq(0, 1000000, by = 1000)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(0, 7000)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted odds\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.9 Plot Predicted Probabilities of Surgical Repair Success\nAnd we can also plot the predicted probabilities of surgical repair success. Again this is just another transformation of the log-odds or the odds. Note the classic sigmoid shape which emulates what we saw earlier when plotting the binned raw data. To my mind this plot is more useful than either plots of predicted log-odds or odds, as it tells us the likely chance of successful repair conditional on the initial macular hole size.\n\n\nCode\n# Plot predicted probabilities\nggplot(predictions, aes(x = mac_hole_size, y = pred_probs_est)) + \n  geom_ribbon(aes(ymin = pred_probs_LL, ymax = pred_probs_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Predicted probability\") +\n  theme_bw(base_size = 25)\n\n\n\n\n\n\n\n\n\n\n\n4.3.10 Plot Observed and Predicted Probabilities of Surgical Repair Success\nAnd in fact what you should ideally find if you plot both your observed and predicted probabilities is that they are quite similar, reflecting that the model actually fits the data quite well. Of course, in this case the convergence of observed and predicted probabilities is unsurprising given that we know the data-generating proces - i.e. we fitted a model to data that came from a model. But, in general, this is a good way to assess your model fit.\n\n\nCode\n# Plot observed and predicted data\nggplot(df, aes(as.numeric(bins)*50-25, mean_y)) + \n  geom_point(size = 2) +\n  geom_line(aes(color = \"observed\"), linewidth = 1) +\n  geom_line(data = predictions, aes(x = mac_hole_size, y = pred_probs_est, color = \"predicted\"), linewidth = 1) +\n  scale_color_manual(name = \"\", values = c(\"predicted\" = \"cornflowerblue\", \"observed\" = \"orange\")) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  xlab(\"Macular hole size\") + ylab(\"Probability\") +\n  theme_bw(base_size = 25) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n4.3.11 Putting It All Together\nWhat I want to try and reinforce in the figure below is how the log-odds, the odds and the probability of the outcome are all transformations of each other.\nIn logistic regression we are ultimately interested in the probability of the outcome occurring. But we use the log-odds or logit link function to map probabilities onto a linear real-number range. This means that the change in Y for each unit increase in X remains the same and we can see that in the top plot. So the log-odds coefficient that our stats software gives us is constant across all values of macular hole size. This represents the difference in the log-odds of success for each unit change in X.\nOne unit increase in log-odds:\n\n600 -&gt; 700 \\(\\mu m\\)\n= -0.181 - 1.323 = -1.504\n\n700 -&gt; 800 \\(\\mu m\\)\n= -1.686 - -0.181 = -1.505\n\n\nThe middle plot shows the equivalent odds of success across the same range of macular hole size. Here the difference in Y is no longer constant for each unit increase in X, but the commensurate ratio is constant - and this gives us the odds ratio. This means that the odds ratio is constant across all values of macular hole size.\nOne unit increase in odds:\n\n600 -&gt; 700 \\(\\mu m\\)\n= 0.834/3.756 = 0.22\n\n700 -&gt; 800 \\(\\mu m\\)\n= 0.185/0.834 = 0.22\n\n\nFinally the lower plot shows the equivalent probabilities of success and now there is no constancy in either unit differences or ratios. So, if we are interested in expressing our logistic regression model results as differences or ratios of predicted probabilities, we need to make sure that we qualify the macular hole size that a particular predicted probability corresponds to, as this will change across the range of X.\nOne unit increase in probability:\n\n600 -&gt; 700 \\(\\mu m\\)\n= 0.45 - 0.79 = -0.34\n= 0.45/0.79 = 0.57\n\n700 -&gt; 800 \\(\\mu m\\)\n= 0.16 - 0.45 = -0.29\n= 0.16/0.45 = 0.36\n\n\n\n\nCode\n# Reformat plots slightly for ggarrange ----\np1 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_logodds_est)) + \n  geom_ribbon(aes(ymin = pred_logodds_LL, ymax = pred_logodds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1150, y = 6, label = \"log-odds\", size = 10) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  annotate(\"text\", x = 1110, y = 3.5, label = \"unit differences = constant\", size = 5) +\n  scale_y_continuous(limits = c(-100, 100), breaks = seq(-100, 100, by = 2)) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(-8, 8)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, logodds),\n                            label = emmeans_df$logodds, \n                            nudge_x = c(50, 50, 50), nudge_y = c(4, 4, 4),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  theme_bw(base_size = 20) +\n  ylab(\"\") +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\n\np2 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_odds_est)) + \n  geom_ribbon(aes(ymin = pred_odds_LL, ymax = pred_odds_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1180, y = 6000, label = \"odds\", size = 10) +\n  annotate(\"text\", x = 1136, y = 5000, label = \"unit ratios = constant\", size = 5) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(seq(0, 1000000, by = 1000))) +\n  coord_cartesian(xlim = c(0, 1200), ylim = c(0, 7000)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ylab(\"\") +\n  theme_bw(base_size = 20) +\n  theme(axis.title.x = element_blank(), axis.text.x = element_blank())\np2_inset &lt;- p2 +\n  scale_y_continuous(limits = c(-20, 1000000), breaks = c(1,2,3,4,5, seq(0, 1000000, by = 1000))) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, odds),\n                            label = emmeans_df$odds, \n                            nudge_x = c(50, 50, 50), nudge_y = c(0.5, 1, 1),\n                            color = \"red\", segment.size = 0.2, size = 5)\np2 &lt;- p2 + geom_magnify(from = c(xmin = 500, xmax = 1000, ymin = 0, ymax = 5), \n                          to = c(xmin = 470, xmax = 1010, ymin = 1000, ymax = 5000), \n                          shadow = T, axes = \"y\", plot = p2_inset)\n\np3 &lt;- ggplot(predictions, aes(x = mac_hole_size, y = pred_probs_est)) + \n  geom_ribbon(aes(ymin = pred_probs_LL, ymax = pred_probs_UL), alpha = 0.2) + \n  geom_line(color = \"cornflowerblue\", linewidth = 1) +\n  geom_point(data = df, aes(x = mac_hole_size, y = y_success), size = 2, alpha = 0.1) +\n  annotate(\"text\", x = 1140, y = 0.8, label = \"probability\", size = 10) +\n  annotate(\"text\", x = 1075, y = 0.65, label = \"unit differences/ratios = not constant\", size = 5) +\n  scale_x_continuous(limits = c(0, 1200), breaks = seq(0, 1200, by = 100)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  geom_vline(xintercept = 600, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 700, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  geom_vline(xintercept = 800, color = \"red\", linetype = \"dotted\", linewidth = 0.6) +\n  ggrepel::geom_label_repel(data = emmeans_df, aes(mac_hole_size, probs),\n                            label = emmeans_df$probs, \n                            nudge_x = c(50, 50, 50), nudge_y = c(0.1, 0.1, 0.1),\n                            color = \"red\", segment.size = 0.2, size = 5) +\n  ylab(\"\") + xlab(\"Macular hole size\") +\n  theme_bw(base_size = 20)\nggarrange(p1, p2, p3, align = \"v\", ncol = 1, heights = c(1,1,1.2))\n\n\n\n\n\n\n\n\n\n\n\n4.3.12 Key points\nThis has been a long post but we are finally at the end. I hope that reading this has demystified what goes on under the hood in logistic regression as it’s really not that difficult once you understand the purpose of the link function.\nThese are the main summary points:\n\nLogistic regression is a type of generalised linear model that allows the estimation of associations of potential predictors with a binary outcome.\nThe logit link function transforms the probabilities of a binary outcome variable to a continuous, unbounded scale (log-odds). Standard linear modelling methods can then be applied to estimate the model.\nLog-odds, odds and probabilities are all simple transformations of one another.\nThe model is estimated on the log-odds scale where the association between the predictor and the log-odds of the outcome is linear, and the unit difference is constant.\nResults may be reported in terms of odds or probabilities:\n\nOn the odds scale the association between the predictor and the odds of the outcome is non-linear, but the unit ratio is constant.\nOn the probability scale the association between the predictor and the probability of the outcome is non-linear, and the unit difference/ratio is not constant.\nThis means probability estimates (risk differences of ratios) depend on the value of the predictor and this needs to be specified."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#puns-and-plays-on-popular-culture",
    "href": "posts/014_14Jun_2024/index.html#puns-and-plays-on-popular-culture",
    "title": "Academic Research - “What’s in a Title?”",
    "section": "1 Puns and Plays on Popular Culture",
    "text": "1 Puns and Plays on Popular Culture\nFantastic yeasts and where to find them: the hidden diversity of dimorphic fungal pathogens. For the Harry Potter fans.\nMedical marijuana: can’t we all just get a bong? This was a conference poster, not a paper.\nmiR miR on the wall, who’s the most malignant medulloblastoma miR of them all? Sounds like a poisoned apple is the least of anyones worries.\nGut Microbe to Brain Signaling: What Happens in Vagus… I love this one.\nDie hard: Are cancer stem cells the Bruce Willises of tumor biology? Yippee-ki-yay…\nOne ring to multiplex them all. Well, that’s just precious.\nLeaf me alone: visual constraints on the ecology of social group formation. How I feel when my kids come up to me and ask for more money.\nHow To Train Your Oncolytic Virus: the Immunological Sequel"
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#just-clever",
    "href": "posts/014_14Jun_2024/index.html#just-clever",
    "title": "Academic Research - “What’s in a Title?”",
    "section": "2 Just Clever",
    "text": "2 Just Clever\nCan you tell your clunis from your cubitus? A benchmark for functional imaging. Or, can you tell your arse from your elbow?\nYou Probably Think this Paper’s About You: Narcissists’ Perceptions of their Personality and Reputation. So, it is about me?\nChemical processes in the deep interior of Uranus\nFactitious Diarrhea: A Case of Watery Deception"
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#but-why",
    "href": "posts/014_14Jun_2024/index.html#but-why",
    "title": "Academic Research - “What’s in a Title?”",
    "section": "3 But Why?",
    "text": "3 But Why?\nOk, perhaps these aren’t funny titles, but certainly they make for interesting, if in some cases questionable, research.\nAre full or empty beer bottles sturdier and does their fracture-threshold suffice to break the human skull? “Now let’s get ethics approval for an RCT”.\nImpact of wet underwear on thermoregulatory responses and thermal comfort in the cold. Just letting you know that wet underwear is not comfortable - tell your friends.\nSword swallowing and its side effects. It turns out that sword swallowing is a hazardous activity (please don’t distract the next sword swallower you meet).\nRole of Childhood Aerobic Fitness in Successful Street Crossing. No children were actually harmed in the conduct of this study.\nA comparison of jump performances of the dog flea, Ctenocephalides canis (Curtis, 1826) and the cat flea, Ctenocephalides felis felis (Bouché, 1835). But cats can jump?!\nChickens prefer beautiful humans. Duh - obviously!\nPigeon’s discrimination of paintings by Monet and Picasso. Clearly more cultured than me.\nEnriched environment exposure accelerates rodent driving skills. Rat designated-drivers, a market ready to exploit.\nExperimental replication shows knives manufactured from frozen human feces do not work. Science at its best.\nTermination of intractable hiccups with digital rectal massage. We should all keep this in mind at our next dinner party.\nFarting as a defence against unspeakable dread. We’ve all been there."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#not-sure-where-to-put-this-one",
    "href": "posts/014_14Jun_2024/index.html#not-sure-where-to-put-this-one",
    "title": "Academic Research - “What’s in a Title?”",
    "section": "4 Not Sure Where to Put This One…",
    "text": "4 Not Sure Where to Put This One…\nThe effect of having Christmas dinner with in-laws on gut microbiota composition. Well now you can put some science behind your decision to abstain from visiting the in-laws during the festive season - “In participants visiting in-laws, there was a significant decrease in all Ruminococcus species, known to be associated with psychological stress and depression.”"
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#offensive-or-risque",
    "href": "posts/014_14Jun_2024/index.html#offensive-or-risque",
    "title": "Academic Research - “What’s in a Title?”",
    "section": "5 Offensive or Risque",
    "text": "5 Offensive or Risque\nA couple of papers that may create offence. Click on Details at your peril.\n\nPremature Speculation Concerning Pornography’s Effects on Relationships. At least read the abstract before coming to your own conclusion.\nGet Me Off Your Fucking Mailing List. This is just awesome.\nStructural and electronic properties of chiral single-wall copper nanotubes. Surely they could have come up with a better abbreviation."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#to-conclude",
    "href": "posts/014_14Jun_2024/index.html#to-conclude",
    "title": "Academic Research - “What’s in a Title?”",
    "section": "5 To Conclude",
    "text": "5 To Conclude\nI’m going to end with two papers that I think are highlights.\nThe first is really a tribute to anyone who has gone through the peer-review process and published an academic paper. At some point - if you haven’t already - you are going to have to deal with Reviewer 2. While this paper provides weak evidence that Reviewer 2 might actually be the victim of Reviewer-Identity-Theft, you can feel rest assured that you are not alone in having an obviously talentless peer-review hack underappreciate your true brilliance and fine work. We’ve all been there.\nDear Reviewer 2: Go F’ Yourself\nThe second is a classic. To the research students out there - don’t let a lack of words stop you from publishing your best work.\nThe unsuccessful self-treatment of a case of “writer’s block”\nI love the review given of it at the time:\n“I have studied this manuscript very carefully with lemon juice and X-rays and have not detected a single flaw in either design or writing style. I suggest it be published without revision. Clearly, it is the most concise manuscript I have ever seen – yet it contains sufficient detail to allow other investigators to replicate Dr. Upper’s failure. In comparison with the other manuscripts I get from you containing all that complicated detail, this one was a pleasure to examine. Surely we can find a place for this paper in the Journal – perhaps on the edge of a blank page.”\nI didn’t realise this was just the first in a series, and in fact there have been both success and failures in replication of the study. Unfortunately, the more recent meta-analysis still leaves the jury out as far as I’m concerned…\nThe Unsuccessful Self-Treatment of a Case of “Writer’s Block”: A Replication\nUnsuccessful Self-Treatment of a Case of “Writer’s Block”: A Partial Failure to Replicate\nUnsuccessful Self-Treatment of “Writer’s Block”: A Review of the Literature\nThe Unsuccessful Group-Treatment of “Writer’s Block”\nThe Unsuccessful Group Treatment of “Writer’s Block”: A Ten-Year Follow-up\nA Multisite Cross-Cultural Replication of Upper’s (1974) Unsuccessful Self-Treatment of Writer’s Block\nUnsuccessful Treatments of “Writer’s Block”: A Meta-Analysis\nUntil next time…"
  },
  {
    "objectID": "posts/016_09Aug_2024/index.html",
    "href": "posts/016_09Aug_2024/index.html",
    "title": "Logarithms and Why They’re Important in Statistics",
    "section": "",
    "text": "What do the acidity (pH - power of Hydrogen), sound intensity (dB - decibels) and earthquake intensity (measured on the Richter) scales all have in common?\nThey are all reported on a log scale.\nIn our real-world experience with these scales, I would be willing to bet that you haven’t put a lot of thought into what the numbers actually mean. Sure, we might remember from high school chemistry that something is more acidic if the pH is lower than 7. We might also know that higher numbers on the decibel scale indicate louder noises, but probably not what sources of sound specific levels relate to. We might also know from it’s reporting in the news that the most recent (thankfully infrequently occurring) earthquake wasn’t that severe based on a Richter magnitude of 4. But there’s actually much more to those numbers than meets the eye.\nLet’s take a look at each of these scales in a little more detail:\n\n\n\npH (taken from: https://www.pmel.noaa.gov/co2/file/The+pH+scale+by+numbers)\n\n\n\n\n\nSound Intensity\n\n\n\n\n\nRichter (taken from: https://en.m.wikipedia.org/wiki/File:How-the-Richter-Magnitude-Scale-is-determined.jpg)\n\n\nIf you take some time to look at those figures you will realise that there is a commonality among all three of them. In each case, two sets of number scales are presented:\n\nReporting scale\n\nAcidity (0 - 14)\nSound Intensity (0 - 150)\nEarthquake Intensity (0 - 9)\n\nMeasurement scale\n\nAcidity (\\(10^0 - 10^{-14}\\))\nSound Intensity (\\(10^{-12} - 10^{3}\\))\nEarthquake Intensity (\\(10^{-1} - 10^{9}\\))\n\n\nThe reporting scale is the one that we’re all familiar with, but in each case the actual measurements are recorded on a different scale behind the scenes.\nWhy?\nThe reason is that there is just too much variation on the measurement scale - by orders of magnitude - to make it convenient to also use to describe effects. So we convert the measurement scale to a more interpretable (but somewhat arbitrary) scale for reporting.\nWell hello, logarithms.\nWhen a physical quantity varies over a very large range, it is often convenient to take its logarithm in order to have a more manageable set of numbers (good primers on logarithms and exponents can be found here and here). And that’s exactly what is happening when we talk about acidity, sound intensity and earthquake intensity.\nThere is a key point to know about logarithms:\nLogarithms convert numbers that are related on a multiplicative (exponential) scale to numbers that are related on an additive (linear) scale.\nYou will see that in each of the above cases, the natural scale that the quantity is measured on is multiplicative in nature. Each ‘unit’ change represents an order of magnitude difference in the quantity. For example, the amplitude of seismic waves (felt as the level of ground shake) in a Richter magnitude 5 earthquake (moderate) are 10 times greater than that of a magnitude 4 earthquake (small). Similarly, a ‘major’ earthquake (Richter 7) would be considered 1000 times greater in seismic activity compared to a small earthquake.\nBut when we instead use logarithms, those multiplicative effects are now converted to additive effects. Each one-unit increase in seismic activity on the Richter scale corresponds to a 10 times greater increase in seismic activity on the natural scale.\nSo, how is this relevant in our daily data analysis endeavours?"
  },
  {
    "objectID": "posts/016_09Aug_2024/index.html#simulate-exponential-data",
    "href": "posts/016_09Aug_2024/index.html#simulate-exponential-data",
    "title": "Logarithms and Why They’re Important in Statistics",
    "section": "2.1 Simulate Exponential Data",
    "text": "2.1 Simulate Exponential Data\nLet’s first of all visualise my statement regarding the logarithms ability to convert multiplicative effects to additive effects. I’ll create a ‘geometric’ number series of 10 numbers with base 2 - i.e. each subsequent number in the series is double the previous number. In other words, 2 is the multiplying factor in this series. The data looks like:\n\n\nCode\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(gtsummary)\nlibrary(emmeans)\nx &lt;- c(0:10)\ny &lt;- 2^(0:10)\ny2 &lt;- c(paste0(\"1 = 2\\U2070\"),\n        paste0(\"2 = 2\\U00B9\"),\n        paste0(\"2x2 = 2\\U00B2\"),\n        paste0(\"2x2x2 = 2\\U00B3\"),\n        paste0(\"2x2x2x2 = 2\\U2074\"),\n        paste0(\"2x2x2x2x2 = 2\\U2075\"),\n        paste0(\"2x2x2x2x2x2 = 2\\U2076\"),\n        paste0(\"2x2x2x2x2x2x2 = 2\\U2077\"),\n        paste0(\"2x2x2x2x2x2x2x2 = 2\\U2078\"),\n        paste0(\"2x2x2x2x2x2x2x2x2 = 2\\U2079\"),\n        paste0(\"2x2x2x2x2x2x2x2x2x2 = 2\\U00B9\\U2070\"))\ndf &lt;- data.frame(cbind(x = x, y = y, `y_in_exponential_form` = y2))\ndf$x &lt;- as.numeric(df$x); df$y &lt;- as.numeric(df$y)\ndf |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nx\ny\ny_in_exponential_form\n\n\n\n\n0\n1\n1 = 2⁰\n\n\n1\n2\n2 = 2¹\n\n\n2\n4\n2x2 = 2²\n\n\n3\n8\n2x2x2 = 2³\n\n\n4\n16\n2x2x2x2 = 2⁴\n\n\n5\n32\n2x2x2x2x2 = 2⁵\n\n\n6\n64\n2x2x2x2x2x2 = 2⁶\n\n\n7\n128\n2x2x2x2x2x2x2 = 2⁷\n\n\n8\n256\n2x2x2x2x2x2x2x2 = 2⁸\n\n\n9\n512\n2x2x2x2x2x2x2x2x2 = 2⁹\n\n\n10\n1024\n2x2x2x2x2x2x2x2x2x2 = 2¹⁰\n\n\n\n\n\nYou can see that the numbers grow large very quickly."
  },
  {
    "objectID": "posts/016_09Aug_2024/index.html#arithmetic-vs-geometric-mean",
    "href": "posts/016_09Aug_2024/index.html#arithmetic-vs-geometric-mean",
    "title": "Logarithms and Why They’re Important in Statistics",
    "section": "2.2 Arithmetic vs Geometric Mean",
    "text": "2.2 Arithmetic vs Geometric Mean\nIf someone asked you to provide a summary statistic for these data what would you give them? The mean, median or something else? The median is always a good choice when you’re uncertain about whether your data might conform to parametric distribution assumptions. The median is just the middle value in the series and can be worked out in R as:\n\nmedian(df$y)\n\n[1] 32\n\n\nWhat about the (arithmetic) mean?\n\nmean(df$y)\n\n[1] 186.0909\n\n\nThat seems fairly highly when we see that most values are less than this. But this is symptomatic of data that are related in a multiplicative way - values tend to be condensed towards one end of the scale and skewed towards the other. The fewer, larger values ‘drag’ the average towards that end of the scale. In these cases, the conventional arithmetic mean is not the best measure of central tendency and instead we should use the geometric mean.\nRemember that the arithmetic mean is calculated as such:\n\\[\\frac{1+2+4+8+16+32+64+128+256+512+1024}{11} = 186.1\\] There are two ways to calculate the geometric mean by hand (but I will also show you how to do it in R as well):\nThe first way is to take the nth root of the product of all the terms:\n\\[\\sqrt[11]{1*2*4*8*16*32*64*128*256*512*1024} = 32\\] and the second way is to take the exponent of the mean of the logged values:\n\\[e\\ ^{\\left( \\frac{log(1)+log(2)+log(4)+log(8)+log(16)+log(32)+log(64)+log(128)+log(256)+log(512)+log(1024)}{11} \\right)} = 32\\]\nIn R:\n\n\nCode\n# nth root method - manual\n(1*2*4*8*16*32*64*128*256*512*1024)^(1/11)\n\n\n[1] 32\n\n\nCode\n# logs method - manual\nexp((log(1)+log(2)+log(4)+log(8)+log(16)+log(32)+log(64)+log(128)+log(256)+log(512)+log(1024))/11)\n\n\n[1] 32\n\n\nCode\n# logs method - quick and easy\nexp(mean(log(df$y)))\n\n\n[1] 32\n\n\nIn a perfectly geometric series the geometric mean will align with the median and is a better measure of central tendency, so keep that in the back of your mind."
  },
  {
    "objectID": "posts/016_09Aug_2024/index.html#plot-data-on-original-scale",
    "href": "posts/016_09Aug_2024/index.html#plot-data-on-original-scale",
    "title": "Logarithms and Why They’re Important in Statistics",
    "section": "2.3 Plot Data on Original Scale",
    "text": "2.3 Plot Data on Original Scale\nLet’s now plot this data using a normal linear scale:\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(1, 1050), breaks = c(1,100,200,300,400,500,600,700,800,900,1000)) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nIt is not hard to appreciate the exponential nature of the relationship between X and Y in this plot. As X increases, Y increases at a much faster rate, but it’s hard to tell by how much."
  },
  {
    "objectID": "posts/016_09Aug_2024/index.html#plot-data-on-original-scale-modified-y-axis",
    "href": "posts/016_09Aug_2024/index.html#plot-data-on-original-scale-modified-y-axis",
    "title": "Logarithms and Why They’re Important in Statistics",
    "section": "2.4 Plot Data on Original Scale (Modified Y Axis)",
    "text": "2.4 Plot Data on Original Scale (Modified Y Axis)\nWhat does the plot look like if we use the axis tick marks to indicate the actual Y values (keeping the original scale):\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(1, 1050), breaks = c(1,2,4,8,16,32,64,128,256,512,1024)) +\n  geom_segment(aes(x = 0, y = 64, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 6, y = 1, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 128, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 7, y = 1, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 7, ymin = 64, ymax = 128), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 6, xmax = 7, ymin = 1, ymax = 64), fill = \"red\", alpha = 0.02) +\n  geom_segment(aes(x = 0, y = 256, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 8, y = 1, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 512, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 9, y = 1, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 9, ymin = 256, ymax = 512), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 8, xmax = 9, ymin = 1, ymax = 256), fill = \"red\", alpha = 0.02) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nInteresting. Obviously nothing has changed except the values on the Y axis no longer reflect evenly spaced units. In fact if you took a ruler to your screen you would see that the pixel distance between each pair of ascending tick marks is double the previous pair of tick marks. The larger numbers are nicely spread out on the axis, while the smaller numbers are all cramped together.\nWhat is certainly easier to appreciate in this plot compared to the previous one is the doubling of Y for each unit increase in X. We can see for instance that the one-unit increase in X between 6 and 7 corresponds to a doubling of Y from 64 to 128. Similarly, the one-unit increase between 8 and 9 corresponds to a doubling of Y from 256 to 512.\nSo, being good data analysts we always visualise our data before we get too far into analysing it. Although we know the data-generating mechanism for these data (because we simulated it based on what we wanted), we usually don’t know the data-generating mechanism for most real-world data that we come across. So, if we were in fact naive to the origins of these data an entirely reasonable question we might ask ourselves would be “do these come from an exponential (multiplicative) distribution?”\nA natural next step would be to see if taking logs of the data linearises (i.e straightens) the association between X and Y. Remember that I mentioned earlier that logs convert numbers that are related on a multiplicative scale to numbers that are related on an additive scale. What this means in practice is that an exponential curve flattens out and becomes linear if the data are truly multiplicative in nature."
  },
  {
    "objectID": "posts/016_09Aug_2024/index.html#plot-data-on-log-scale",
    "href": "posts/016_09Aug_2024/index.html#plot-data-on-log-scale",
    "title": "Logarithms and Why They’re Important in Statistics",
    "section": "2.5 Plot Data on Log Scale",
    "text": "2.5 Plot Data on Log Scale\nThere are two ways one can plot data on a log scale using ggplot() in R. The first is to log-transform the data and plot it in the normal way; the second is to leave the data as is and use ggplot() in concert with the scales package to log-transform the axis scales. Let’s consider the second option first.\nHere we specify trans = \"log2\" within the scale_y_continuous() function to transform the Y axis to a base(2) log scale. The result is:\n\n\nCode\nlibrary(scales)\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(trans = \"log2\", breaks = c(1,2,4,8,16,32,64,128,256,512,1024)) +\n  geom_segment(aes(x = 0, y = 64, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 6, y = 1, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 128, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 7, y = 1, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 7, ymin = 64, ymax = 128), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 6, xmax = 7, ymin = 1, ymax = 64), fill = \"red\", alpha = 0.02) +\n  geom_segment(aes(x = 0, y = 256, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 8, y = 1, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 512, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 9, y = 1, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 9, ymin = 256, ymax = 512), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 8, xmax = 9, ymin = 1, ymax = 256), fill = \"red\", alpha = 0.02) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nNow that the Y axis has been rescaled we can easily see that the association between X and Y is in fact linear on this scale. We can also see that where previously the spacing of the ascending tick marks on the Y axis doubled, these now remain the same. Y is still doubling for every unit increase in X, but the Y scale is now considered additive rather than multiplicative in nature (i.e. each doubling is the now the same pixel distance along the axis in the plot).\nI can hopefully consolidate this multiplicative -&gt; additive transformation in your mind by now replacing the raw values on the Y axis with their log-transformed equivalents. If you ignore the base(2) on the ascending Y axis, each exponent is now simply 1 more than the previous value. In other words, on the base(2) log scale, the ‘effects’ are additive.\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(trans = log2_trans(),\n    breaks = c(1,2,4,8,16,32,64,128,256,512,1024),\n    labels = trans_format(\"log2\", math_format(2^.x))) +\n  geom_segment(aes(x = 0, y = 64, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 6, y = 1, xend = 6, yend = 64), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 128, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 7, y = 1, xend = 7, yend = 128), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 7, ymin = 64, ymax = 128), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 6, xmax = 7, ymin = 1, ymax = 64), fill = \"red\", alpha = 0.02) +\n  geom_segment(aes(x = 0, y = 256, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 8, y = 1, xend = 8, yend = 256), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 512, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 9, y = 1, xend = 9, yend = 512), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_rect(aes(xmin = 0, xmax = 9, ymin = 256, ymax = 512), fill = \"red\", alpha = 0.02) +\n  geom_rect(aes(xmin = 8, xmax = 9, ymin = 1, ymax = 256), fill = \"red\", alpha = 0.02) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nThe other approach to plotting data on a log scale is to actually log-transform the data, and this is not difficult. If you knew that the data series were multiplicative by a factor of 2 you would naturally transform using a base(2) log scale as you would end up with a nice, natural interpretation of the transformed data - each unit increase in X representing a doubling in Y. Often you won’t know this, but you can still achieve the goal of linearising your data by using either natural (e) or base(10) logs.\nThe plots below show the association between X and log-transformed Y for all three of the common log transformations. Note that they all produce the same effect on the association between X and Y - just the scale differs. The numbers on each Y axis represent the powers that are raised to each base to calculate the value of Y in its original units. So, for example:\n\\[2^{5} \\approx e^{3.46} \\approx 10^{1.51} \\approx 32\\]\n\n\nCode\n# Here I have performed the log-transformation of Y on-the-fly, within the ggplot call, but you can also do this by explicitly creating a new log-transformed variable in the dataset\np1 &lt;- ggplot(df, aes(x, y = log2(y))) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_segment(aes(x = 5, y = 0, xend = 5, yend = 5), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 5, xend = 5, yend = 5), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(0, 10), breaks = c(0,2,4,6,8,10)) +\n  annotate(geom = \"text\", x = 0.8, y = 5.26, label = \"5.00\", color=\"red\", size = 8) +\n  theme_bw(base_size = 20)\np2 &lt;- ggplot(df, aes(x, y = log(y))) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_segment(aes(x = 5, y = 0, xend = 5, yend = 3.46), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 3.46, xend = 5, yend = 3.46), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  annotate(geom = \"text\", x = 0.8, y = 3.65, label = \"3.46\", color=\"red\", size = 8) +\n  theme_bw(base_size = 20)\np3 &lt;- ggplot(df, aes(x, y = log10(y))) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_segment(aes(x = 5, y = 0, xend = 5, yend = 1.51), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = 0, y = 1.51, xend = 5, yend = 1.51), linewidth = 0.5, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  annotate(geom = \"text\", x = 0.8, y = 1.6, label = \"1.51\", color=\"red\", size = 8) +\n  theme_bw(base_size = 20)\ncowplot::plot_grid(p1, p2, p3, labels = c('Base(2) log', 'Natural log', 'Base(10) log'), hjust = c(-0.9,-0.7,-0.6), vjust = 4, ncol = 3, label_size = 20)\n\n\n\n\n\n\n\n\n\nSo you might still be wondering where I am headed with all of this."
  },
  {
    "objectID": "posts/016_09Aug_2024/index.html#think-about-modelling-assumptions",
    "href": "posts/016_09Aug_2024/index.html#think-about-modelling-assumptions",
    "title": "Logarithms and Why They’re Important in Statistics",
    "section": "2.6 Think About Modelling Assumptions",
    "text": "2.6 Think About Modelling Assumptions\nWell, there are several assumptions that the linear model leans on to ensure the parameter estimates it comes up with are valid and these include that the association between a continuous predictor and the outcome is roughly linear and the distribution of the model residuals is roughly normal. Both of these assumptions can fail when you try to model an obviously curved relationship as if it were linear. Certainly if domain-knowledge dictates that the origins of your data come from an exponential distribution (many biological cell process have this basis - e.g. cell division), you need to stop and think about how you will approach your analysis.\nIf you blindly ran a standard linear regression on these data, you would get:\n\n\nCode\nmod_linear &lt;- lm(y ~ x, data = df) \nmod_linear |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nx\n75\n29, 120\n0.005\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(x, y)) + \n  geom_line(linewidth = 1, colour = \"deepskyblue\") +\n  geom_smooth(method = \"lm\", se = F, colour = \"black\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, by = 1)) +\n  scale_y_continuous(limits = c(1, 1050), breaks = c(1,100,200,300,400,500,600,700,800,900,1000)) +\n  theme_bw(base_size = 30)\n\n\n\n\n\n\n\n\n\nClearly this is not ideal. The model is trying to suggest that for each unit increase in X, Y increases at a constant rate of 75. Let’s see what the model predicts for the value of Y when X = 5 (when we know the actual value is 32).\n\n\nCode\nemmeans(mod_linear, ~ x, at = (list(x = 5))) |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nx\nemmean\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\n5\n186.09\n64.04\n9\n41.22\n330.97\n\n\n\n\n\nHmmm, further evidence that this is a bad model for these data. Instead, let’s rerun the regression applying one small change - we will log-transform Y on-the-fly within the model call.\n\n\nCode\nmod_trans &lt;- lm(log(y) ~ x, data = df) \ntbl &lt;- mod_trans |&gt; \n  tbl_regression() # need work around for log transformed response for tbl_regression\ntbl |&gt;\n  # remove character version of 95% CI\n  modify_column_hide(ci) |&gt; \n  # exponentiate the regression estimates\n  modify_table_body(\n    \\(x) x |&gt; mutate(across(c(estimate, conf.low, conf.high), exp))\n  ) |&gt; \n  # merge numeric LB and UB together to display in table\n  modify_column_merge(pattern = \"{conf.low}, {conf.high}\", rows = !is.na(estimate)) |&gt; \n  modify_header(conf.low = \"**95% CI**\") |&gt; \n  as_kable()\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\nx\n2.0\n2.0, 2.0\n&lt;0.001\n\n\n\n\n\nYou will get a warning if you run this model (I have hidden it) as it’s a perfect fit, because there is no randomness in the data. That doesn’t really matter though for the sake of the illustration. The Beta value represents the exponentiated coefficient for the association between X and Y and can be considered a ‘response ratio’. This is equivalent to the ratio of each pair of successive values of Y for each unit increase in X. The response ratio of 2 implies that the outcome doubles (or increases by 100%) for each unit increase in the predictor and we know this to be true.\nWhat does this model predict the value of Y at X = 5 should be?\n\n\nCode\nemmeans(mod_trans, ~ x, at = (list(x = 5)), type = \"response\") |&gt; \n  kable(align = \"c\", digits = 2)\n\n\n\n\n\nx\nresponse\nSE\ndf\nlower.CL\nupper.CL\n\n\n\n\n5\n32\n0\n9\n32\n32\n\n\n\n\n\nAnd this is what we would expect a good-fitting (perfectly-fitting in this case) model to be able to do - predict values on new data in line with our empirical observations."
  },
  {
    "objectID": "posts/017_23Aug_2024/index.html",
    "href": "posts/017_23Aug_2024/index.html",
    "title": "tidylog - Console Messaging in R",
    "section": "",
    "text": "Today’s post is really quite short (no thanks needed). It’s really to point out a super-handy little package that you should load at the beginning of every one of your R scripts (but only useful if you’re a tidyverse user).\nThe package is called tidylog and it’s designed to provide immediate feedback about what the data manipulations you make with dplyr and tidyr functions (e.g. filter, select,mutate, group_by, the various join functions, etc) are actually doing to your datasets.\nTo my mind this should be built into R, as this kind of operational feedback is taken for granted by Stata users. But I guess that’s the whole point of R being open-source and community-driven in terms of ad-hoc improvements in functionality.\nI don’t think there’s really much for me to add that the package author hasn’t already said here. So please have a look.\nBut to end I will show you a quick before and after. Let’s use the inbuilt nycflights13 dataset to illustrate what output is returned if you run a bunch of data-wrangling functions.\nWithout tidylog:\n\n\nCode\nlibrary(nycflights13)\nlibrary(tidyverse)\ndat &lt;- flights |&gt;  \n select(year:day, hour, origin, dest, tailnum, carrier) |&gt; \n mutate(month = if_else(nchar(month) == 1, paste0(\"0\",month), as.character(month)),\n day = if_else(nchar(day) == 1, paste0(\"0\",day), as.character(day))) |&gt;  \n unite(\"date\", year:day, sep = \"/\", remove = T) |&gt; \n mutate(date = lubridate::ymd(date)) |&gt; \n filter(hour &gt;= 8) |&gt; \n anti_join(planes, by = \"tailnum\") |&gt; \n count(tailnum, sort = TRUE) \n\n\nNada. Thanks for nothing R!\nWith tidylog:\n\n\nCode\nsuppressMessages(library(tidylog))\ndat &lt;- flights |&gt;  \n select(year:day, hour, origin, dest, tailnum, carrier) |&gt; \n mutate(month = if_else(nchar(month) == 1, paste0(\"0\",month), as.character(month)),\n day = if_else(nchar(day) == 1, paste0(\"0\",day), as.character(day))) |&gt;  \n unite(\"date\", year:day, sep = \"/\", remove = T) |&gt; \n mutate(date = lubridate::ymd(date)) |&gt; \n filter(hour &gt;= 8) |&gt; \n anti_join(planes, by = \"tailnum\") |&gt; \n count(tailnum, sort = TRUE) \n\n\nselect: dropped 11 variables (dep_time, sched_dep_time, dep_delay, arr_time, sched_arr_time, …)\n\n\nmutate: converted 'month' from integer to character (0 new NA)\n\n\n        converted 'day' from integer to character (0 new NA)\n\n\nmutate: converted 'date' from character to Date (0 new NA)\n\n\nfilter: removed 50,726 rows (15%), 286,050 rows remaining\n\n\nanti_join: added no columns\n\n\n           &gt; rows only in x    45,008\n\n\n           &gt; rows only in y  (     39)\n\n\n           &gt; matched rows    (241,042)\n\n\n           &gt;                 =========\n\n\n           &gt; rows total        45,008\n\n\ncount: now 716 rows and 2 columns, ungrouped\n\n\nNice!\nTill next time - Happy analysing!"
  },
  {
    "objectID": "posts/018_06Sep_2024/index.html",
    "href": "posts/018_06Sep_2024/index.html",
    "title": "Biostats Book Club",
    "section": "",
    "text": "I was playing around with AI image creation this week and asked Microsoft Bing to create an image with ‘biostatistics book club’ as a prompt - this is what it came up with:\n\n\n\n\n\nHmmm - who would have ever thought talking about biostatistics could be so interesting.\nThen, for even more fun, I asked Bing to create another image using ‘biostatistics fight club’ as a prompt and it gave me this:\n\n\n\n\n\nYep, just what I imagined a bunch of pugilistic stats-nerds to look like."
  },
  {
    "objectID": "posts/018_06Sep_2024/index.html#essential-medical-statistics",
    "href": "posts/018_06Sep_2024/index.html#essential-medical-statistics",
    "title": "Biostats Book Club",
    "section": "2.1 Essential Medical Statistics",
    "text": "2.1 Essential Medical Statistics\n\n\n\n\n\nI can’t recommend this book enough. It’s now over 20 years old but that doesn’t mean it’s dated - the ‘essentials’ of statistics, well, haven’t really changed. Kirkwood’s book explains statistical concepts in such a clear and concise manner that it makes (for me at least), understanding them much, much easier. It strikes a good balance in covering all the important ideas in enough depth while still maintaining a relative lay language style."
  },
  {
    "objectID": "posts/018_06Sep_2024/index.html#intuitive-biostatistics",
    "href": "posts/018_06Sep_2024/index.html#intuitive-biostatistics",
    "title": "Biostats Book Club",
    "section": "2.2 Intuitive Biostatistics",
    "text": "2.2 Intuitive Biostatistics\n\n\n\n\n\nThe author of Intuitive Biostatistics is also the brains behind the Prism statistical software. You’ll be pleased to know there are almost no formulae written amongst its pages and I think a reasonable summary of the authors intentions is to provide a ‘common-sense’ treatment of statistical ideas. The book is littered with teaching examples as well as sections on ‘Q & A’s’ and ‘Common Mistakes’ and their potential solutions. Get the latest edition."
  },
  {
    "objectID": "posts/018_06Sep_2024/index.html#r-for-data-science",
    "href": "posts/018_06Sep_2024/index.html#r-for-data-science",
    "title": "Biostats Book Club",
    "section": "2.3 R for Data Science",
    "text": "2.3 R for Data Science\n\n\n\n\n\nif you are one of the ‘cool kids’ and use the tidyverse approach to coding in R, then this is probably worthwhile having. There is a free online version as well. R for Data Science is predominantly aimed at data-wrangling and preparing your data for analysis - tidyverse style. I don’t consider myself a great statistical programmer, so I have found some elements of this a little difficult, but the more basic stuff is really useful (and coding should be a daily journey of self-improvement anyway)."
  },
  {
    "objectID": "posts/018_06Sep_2024/index.html#the-r-book",
    "href": "posts/018_06Sep_2024/index.html#the-r-book",
    "title": "Biostats Book Club",
    "section": "2.4 The R book",
    "text": "2.4 The R book\n\n\n\n\n\nThe R Book differs from R for Data Science in that, yes it’s a book about coding in R, but the focus isn’t just on data-wrangling. This book will give you almost any bit of code to run nearly any statistical procedure in R that you could imagine. In that sense it’s also a worthwhile reference. Mind you, as a result of the breadth of material it covers, this is a BIG book!\nI hope you find these helpful in your statistical learning."
  },
  {
    "objectID": "posts/014_14Jun_2024/index.html#not-sure-where-to-put-these",
    "href": "posts/014_14Jun_2024/index.html#not-sure-where-to-put-these",
    "title": "Academic Research - “What’s in a Title?”",
    "section": "4 Not Sure Where to Put These…",
    "text": "4 Not Sure Where to Put These…\nThe effect of having Christmas dinner with in-laws on gut microbiota composition. Well now you can put some science behind your decision to abstain from visiting the in-laws during the festive season - “In participants visiting in-laws, there was a significant decrease in all Ruminococcus species, known to be associated with psychological stress and depression.”\nGet Me Off Your F’ing Mailing List. This is just awesome."
  },
  {
    "objectID": "posts/019_20Sep_2024/index.html",
    "href": "posts/019_20Sep_2024/index.html",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "",
    "text": "I know I have touched on confounding before but it’s such an important concept in statistics that I thought I would dedicate a post to it today. There is so much one could talk about on the topic, but my aim is to provide you with some intuition as to what is actually happening when one variable ‘confuses’ the relationship between two others.\nI’m going to use a fairly classic example from epidemiology that examines whether there is an association between drinking coffee (full-cream milk only thanks) and developing coronary heart disease (CHD).\n\n\n\n\n\n\nNote\n\n\n\nIn a sense this is a causal question - i.e. “Does drinking coffee cause CHD?”, but for the sake of the illustration, let’s excuse ourselves from such a question’s fairly strict assumptions for the time being.\n\n\nBeing the good researchers that we are, we give considerable thought as to how to answer our question. That means we don’t forgo our own field-expertise and knowledge at the expense of the data-driven p-value gods. First and foremost, we think about the relationships we are investigating and all the potential factors that could influence those relationships. And that comes from theory, not statistical tests.\nIn examining those knowledge-driven theoretical relationships, we draw a Directed Acyclic Graph (DAG). This is a topic in its own right, but for another post - in the meantime some good primers can be found here and here. A DAG is otherwise more simply known as a causal diagram. It is meant to provide a visual representation of the causal relationships among a set of variables and in doing this forces the researcher to think about their scientific question in a more holistic manner. Indeed, statistical tests aren’t even involved at this point.\nSo, in thinking about our coffee-CHD question, it occurs to us that smoking is known to cause CHD and we wonder whether smoking might also be related to drinking coffee - i.e. do coffee-drinkers also smoke more (or less) than people who prefer don’t drink coffee (tea perhaps?). In this case, a simple DAG might look something like:\n\n\n\n\n\nWhat this DAG helps to summarise is what we are now wondering - is smoking confusing or distorting the association between coffee drinking and the development of CHD? In other words, is smoking a confounder in the association between our exposure (coffee-drinking) and outcome (CHD) variables of interest?\nBefore we take a peek at our data, let’s refresh our memories regarding the three conditions a variable must fulfil to be considered a putative confounder:\n\nThe variable must be associated with both the exposure and the outcome.\nThe variable must be distributed unequally across the exposure (e.g. between treatment groups).\nThe variable must not lie on the causal pathway between the exposure and the outcome (this would be a mediator and requires different treatment)."
  },
  {
    "objectID": "posts/019_20Sep_2024/index.html#background",
    "href": "posts/019_20Sep_2024/index.html#background",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "",
    "text": "I know I have touched on confounding before but it’s such an important concept in statistics that I thought I would dedicate a post to it today. There is so much one could talk about on the topic, but my aim is to provide you with some intuition as to what is actually happening when one variable ‘confuses’ the relationship between two others.\nI’m going to use a fairly classic example from epidemiology that examines whether there is an association between drinking coffee (full-cream milk only thanks) and developing coronary heart disease (CHD).\n\n\n\n\n\n\nNote\n\n\n\nIn a sense this is a causal question - i.e. “Does drinking coffee cause CHD?”, but for the sake of the illustration, let’s excuse ourselves from such a question’s fairly strict assumptions for the time being.\n\n\nBeing the good researchers that we are, we give considerable thought as to how to answer our question. That means we don’t forgo our own field-expertise and knowledge at the expense of the data-driven p-value gods. First and foremost, we think about the relationships we are investigating and all the potential factors that could influence those relationships. And that comes from theory, not statistical tests.\nIn examining those knowledge-driven theoretical relationships, we draw a Directed Acyclic Graph (DAG). This is a topic in its own right, but for another post - in the meantime some good primers can be found here and here. A DAG is otherwise more simply known as a causal diagram. It is meant to provide a visual representation of the causal relationships among a set of variables and in doing this forces the researcher to think about their scientific question in a more holistic manner. Indeed, statistical tests aren’t even involved at this point.\nSo, in thinking about our coffee-CHD question, it occurs to us that smoking is known to cause CHD and we wonder whether smoking might also be related to drinking coffee - i.e. do coffee-drinkers also smoke more (or less) than people who prefer don’t drink coffee (tea perhaps?). In this case, a simple DAG might look something like:\n\n\n\n\n\nWhat this DAG helps to summarise is what we are now wondering - is smoking confusing or distorting the association between coffee drinking and the development of CHD? In other words, is smoking a confounder in the association between our exposure (coffee-drinking) and outcome (CHD) variables of interest?\nBefore we take a peek at our data, let’s refresh our memories regarding the three conditions a variable must fulfil to be considered a putative confounder:\n\nThe variable must be associated with both the exposure and the outcome.\nThe variable must be distributed unequally across the exposure (e.g. between treatment groups).\nThe variable must not lie on the causal pathway between the exposure and the outcome (this would be a mediator and requires different treatment)."
  },
  {
    "objectID": "posts/019_20Sep_2024/index.html#data",
    "href": "posts/019_20Sep_2024/index.html#data",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "2 Data",
    "text": "2 Data\nOk, let’s now take a look at the hypothetical retrospective case-control data we’ll be using today. It consists of 40 observations and three variables:\n\noutcome - did the individual have CHD (case) or not (control).\ncoffee-drinker - was the individual a coffee-drinker or not. This is our exposure variable of interest.\nsmoker - was the individual a smoker or not. This is our potential confounder.\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(ggmosaic)\nlibrary(kableExtra)\nlibrary(janitor)\n\n# Hypothetical data\ny &lt;- factor(c(rep(1, 20), rep(0, 20)), levels = c(0, 1), labels = c(\"control\", \"case\"))\nx1 &lt;- factor(c(rep(1, 10), rep(0, 10), rep(1, 5), rep(0, 15)), levels = c(0, 1), labels = c(\"no\", \"yes\"))\nx2 &lt;- factor(c(rep(1, 9), rep(0, 1), rep(1, 3), rep(0, 7), rep(1, 3), rep(0, 2), rep(1, 1), rep(0, 14)), levels = c(0, 1), labels = c(\"no\", \"yes\"))\ndf &lt;- data.frame(\"outcome\" = y, \"coffee_drinker\" = x1, \"smoker\" = x2)\ndf |&gt; \n  kable(align = \"c\")\n\n\n\n\n\noutcome\ncoffee_drinker\nsmoker\n\n\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nno\n\n\ncase\nno\nyes\n\n\ncase\nno\nyes\n\n\ncase\nno\nyes\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncontrol\nyes\nyes\n\n\ncontrol\nyes\nyes\n\n\ncontrol\nyes\nyes\n\n\ncontrol\nyes\nno\n\n\ncontrol\nyes\nno\n\n\ncontrol\nno\nyes\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno"
  },
  {
    "objectID": "posts/019_20Sep_2024/index.html#simple-epidemiological-approach",
    "href": "posts/019_20Sep_2024/index.html#simple-epidemiological-approach",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "3 Simple Epidemiological Approach",
    "text": "3 Simple Epidemiological Approach\nGiven the binary nature of all three variables we can explore the relationships nicely using a basic workhorse of epidemiological analysis - the 2x2 contingency table. In its simplest form this shows the frequency cross-tabulation of the exposure with the outcome. However, I think it’s also useful to display the conditional row percentages - in other words, the proportions (probabilities) of each outcome (categories as columns) given a particular exposure (categories as rows). In this way it is easy to eyeball whether the exposure is associated with the outcome without doing any specific test simply by looking at whether the row percentages vary greatly. As a good visual accompaniment for each cross-tabulation, I am also going to generate mosaic plots. These can give you an impression of potential associations without using any actual numbers.\n\n3.1 Crude Odds Ratio - Exposure/Outcome\nThe following cross-tabulation and mosaic plot show the potential association between coffee-drinking and CHD. If we look at the proportions of people with CHD in each exposure group we can see that these do in fact differ - 67% of coffee drinkers develop CHD, compared to 40% of non-coffee drinkers. That is a telling sign of an association before we even do anything. The mosaic plot mirrors these numbers graphically.\n\n\n\nCode\ntab &lt;- df |&gt; \n  tabyl(coffee_drinker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    coffee_drinker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    60% (15) \n    40% (10) \n    100% (25) \n  \n  \n    yes \n    33%  (5) \n    67% (10) \n    100% (15) \n  \n  \n    Total \n    50% (20) \n    50% (20) \n    100% (40) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df\ndf2$coffee_drinker &lt;- relevel(df2$coffee_drinker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, coffee_drinker), fill = coffee_drinker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nWorking out the odds ratio (OR) from a 2x2 contingency table is trivial. We want to divide the odds of having CHD given being a coffee-drinker by the odds of having CHD given being a non-coffee-drinker. That is:\n\\[\\text{OR} = \\frac{\\text{10/5}}{10/15}\\]\nOr equivalently:\n\\[\\text{OR} = \\frac{\\text{10 x 15}}{\\text{5 x 10}} = 3\\] The OR is 3 which indicates a 3-fold increase in the odds of CHD among coffee-drinkers compared to their non-coffee-drinking peers. We consider this a crude or unadjusted effect estimate.\n\n\n3.2 Adjusted Odds Ratios - Exposure/Outcome (via stratification)\nTo this point we haven’t even considered any potential confounding effect of smoking. So how could we incorporate that into the current analysis using 2x2 contingency tables? It’s actually very easy - we just stratify on smoking and generate two contingency tables - one for the association between coffee drinking and CHD in smokers and one for the association between coffee drinking and CHD in non-smokers. It then follows that within each stratum of smoking the effect of smoking is ‘held constant’ and therefore cannot confound the association between coffee drinking and CHD. In other words, this becomes an adjusted ‘effect’ of coffee drinking on CHD and is equivalent to ‘controlling’ for smoking in a multivariable statistical model (which I will demonstrate shortly).\n\n3.2.1 Smokers\nSo, the cross-tabulation for the association between coffee-drinking and CHD in smokers, becomes:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(smoker == \"yes\") |&gt; \n  tabyl(coffee_drinker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    coffee_drinker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    25% (1) \n    75%  (3) \n    100%  (4) \n  \n  \n    yes \n    25% (3) \n    75%  (9) \n    100% (12) \n  \n  \n    Total \n    25% (4) \n    75% (12) \n    100% (16) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(smoker == \"yes\")\ndf2$coffee_drinker &lt;- relevel(df2$coffee_drinker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, coffee_drinker), fill = coffee_drinker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nNote how this time the (conditional row) proportions of people with CHD in each exposure group are more similar (in this contrived example they are the same) - in this subgroup of smokers it doesn’t matter whether you’re a coffee drinker or not - 75% of people have CHD.\nNow, the OR is calculated as:\n\\[\\text{OR} = \\frac{\\text{9 x 1}}{\\text{3 x 3}} = 1\\]\nThe OR is 1 - in other words there is no longer any association between coffee drinking and CHD. We consider this an adjusted OR as we have removed any potential confounding effect of smoking by holding it at a constant value (i.e. everyone is a smoker).\n\n\n3.2.2 Non-smokers\nNow let’s consider the non-smokers. The cross-tabulation for the association between coffee-drinking and CHD in non-smokers is:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(smoker == \"no\") |&gt; \n  tabyl(coffee_drinker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    coffee_drinker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (14) \n    33% (7) \n    100% (21) \n  \n  \n    yes \n    67%  (2) \n    33% (1) \n    100%  (3) \n  \n  \n    Total \n    67% (16) \n    33% (8) \n    100% (24) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(smoker == \"no\")\ndf2$coffee_drinker &lt;- relevel(df2$coffee_drinker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, coffee_drinker), fill = coffee_drinker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nDo you see a pattern here? Again, the conditional row percentages are the same across exposure groups (33%) - so, in this subgroup of non-smokers it also doesn’t matter whether you are a coffee drnker or not, the proportions of CHD are the same.\nThe OR this time is:\n\\[\\text{OR} = \\frac{\\text{1 x 14}}{\\text{7 x 2}} = 1\\]\nThe adjusted OR is again 1 - that is, there is no association between coffee drinking and CHD when we remove any potential confounding effect of smoking by holding it at a (different) constant value (i.e. everyone is a non-smoker).\n\n\n3.2.3 Salient points\nThere are two important points to make in the comparison of the crude and adjusted estimates:\n\nThe adjusted OR is not equal to the crude OR - this suggests confounding is present.\nFurthermore, this represents a special case in which there is complete confounding - the OR reduces to the null value (i.e. 1). I will expand on these points in a moment.\n\n\n\n\n3.3 Crude Odds Ratio - Confounder/Outcome\nTo further illuminate the above ideas let’s repeat the analyses but this time ‘switch’ the exposure and confounder around (hopefully it will become clear why we are doing this as you continue reading). The cross-tabulation and mosaic plot suggest an association between smoking and CHD - 75% of smokers develop CHD, compared to 33% of non-smokers.\n\n\n\nCode\ntab &lt;- df |&gt; \n  tabyl(smoker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    smoker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (16) \n    33%  (8) \n    100% (24) \n  \n  \n    yes \n    25%  (4) \n    75% (12) \n    100% (16) \n  \n  \n    Total \n    50% (20) \n    50% (20) \n    100% (40) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df\ndf2$smoker &lt;- relevel(df2$smoker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, smoker), fill = smoker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nThe OR for the association between smoking and CHD is:\n\\[\\text{OR} = \\frac{\\text{12 x 16}}{\\text{8 x 4}} = 6\\]\nSo the crude OR is 6, which indicates a 6-fold increase in the odds of CHD in smokers compared to non-smokers (ignoring coffee-drinking status).\n\n\n3.4 Adjusted Odds Ratios - Confounder/Outcome (via stratification)\nLet us know compare the adjusted ‘effects’ in coffee-drinkers compared to non-coffee-drinkers.\n\n3.4.1 Coffee-drinkers\nSo, the cross-tabulation for the association between coffee-smoking and CHD in coffee-drinkers, becomes:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(coffee_drinker == \"yes\") |&gt; \n  tabyl(smoker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    smoker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (2) \n    33%  (1) \n    100%  (3) \n  \n  \n    yes \n    25% (3) \n    75%  (9) \n    100% (12) \n  \n  \n    Total \n    33% (5) \n    67% (10) \n    100% (15) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(coffee_drinker == \"yes\")\ndf2$smoker &lt;- relevel(df2$smoker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, smoker), fill = smoker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nAnd the adjusted OR is:\n\\[\\text{OR} = \\frac{\\text{9 x 2}}{\\text{1 x 3}} = 6\\]\nThe OR is 6 - in other words the same as the crude estimate.\n\n\n3.4.2 Non-coffee-drinkers\nNow let’s consider the non-coffee-drinkers. The cross-tabulation for the association between smokers and CHD in non-coffee-drinkers is:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(coffee_drinker == \"no\") |&gt; \n  tabyl(smoker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    smoker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (14) \n    33%  (7) \n    100% (21) \n  \n  \n    yes \n    25%  (1) \n    75%  (3) \n    100%  (4) \n  \n  \n    Total \n    60% (15) \n    40% (10) \n    100% (25) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(coffee_drinker == \"no\")\ndf2$smoker &lt;- relevel(df2$smoker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, smoker), fill = smoker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nAnd the adjusted OR is:\n\\[\\text{OR} = \\frac{\\text{3 x 14}}{\\text{7 x 1}} = 6\\]\nThe OR is again 6! Is this starting to make some sense?\n\n\n3.4.3 Salient points\nAgain, there are two important points to make in the comparison of these crude and adjusted estimates:\n\nIt doesn’t seem to matter whether we adjust for coffee-drinking or not, the association between smoking and CHD remains the same.\nIt then follows that coffee-drinking is not a confounder in the smoking-CHD relationship.\n\n\n\n\n3.5 Adjusted Odds Ratio - Overall\nIt just so happens that the adjusted OR’s from Section 3.2 are the same in each subgroup, but this is an exception rather than a rule. Normally, in the presence of confounding, effect estimates will differ in each subgroup to the crude estimate but will not be equal to each other. What do we do with separate effect estimates from each subgroup - this makes reporting somewhat painful, surely? Well, if the adjusted estimates aren’t too different from each other, they can be combined in a weighted manner to provide an overall summary estimate and this can be done using the Cochran-Mantel-Haenszel (CMH) test. I won’t illustrate this here as in the modern computing age there is really no need to be crunching this statistic anymore, and we use a statistical model instead.\nAs I alluded to above, we should only attempt to combine individual estimates when they are ‘similar’. But what does that mean? How similar should they be and how different can they be? There are no hard and fast rules but I will outline a couple of guidelines at the end of this post. The important thing is to realise that when individual estimates are different, this actually represents an effect modification/interaction effect - that is the strength of the association between the exposure and the outcome depends on the level of the third variable. Interaction effects are of interest (scientifically and clinically) and we shouldn’t try to cancel them out by averaging over them."
  },
  {
    "objectID": "posts/019_20Sep_2024/index.html#simple-epidemiological-analysis",
    "href": "posts/019_20Sep_2024/index.html#simple-epidemiological-analysis",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "3 Simple Epidemiological Analysis",
    "text": "3 Simple Epidemiological Analysis\nGiven the binary nature of all three variables …\n\n\nCode\n# Crude OR for effect of coffee drinking\nmod &lt;- glm(outcome ~ coffee_drinker, data = df, family = \"binomial\")\nexp(mod$coef) # 3\n\n\n      (Intercept) coffee_drinkeryes \n        0.6666667         3.0000000 \n\n\nCode\n# Adjusted OR for effect of coffee drinking (stratified by smoking)\nmod &lt;- glm(outcome ~ coffee_drinker, data = subset(df, smoker == \"no\"), family = \"binomial\")\nexp(mod$coef) # 1\n\n\n      (Intercept) coffee_drinkeryes \n              0.5               1.0 \n\n\nCode\nmod &lt;- glm(outcome ~ coffee_drinker, data = subset(df, smoker == \"yes\"), family = \"binomial\")\nexp(mod$coef) # 1\n\n\n      (Intercept) coffee_drinkeryes \n                3                 1 \n\n\nCode\n# Crude OR for effect of smoking\nmod &lt;- glm(outcome ~ smoker, data = df, family = \"binomial\")\nexp(mod$coef) # 6\n\n\n(Intercept)   smokeryes \n        0.5         6.0 \n\n\nCode\n# Adjusted OR for effect of smoking (stratified by coffee drinking)\nmod &lt;- glm(outcome ~ smoker, data = subset(df, coffee_drinker == \"no\"), family = \"binomial\")\nexp(mod$coef) # 6\n\n\n(Intercept)   smokeryes \n        0.5         6.0 \n\n\nCode\nmod &lt;- glm(outcome ~ smoker, data = subset(df, coffee_drinker == \"yes\"), family = \"binomial\")\nexp(mod$coef) # 6\n\n\n(Intercept)   smokeryes \n        0.5         6.0 \n\n\nCode\n# Adjusted OR's for both (statistical)\nsummary(mod &lt;- glm(outcome ~ coffee_drinker + smoker, data = df, family = \"binomial\"))\n\n\n\nCall:\nglm(formula = outcome ~ coffee_drinker + smoker, family = \"binomial\", \n    data = df)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)       -6.931e-01  4.485e-01  -1.546   0.1222  \ncoffee_drinkeryes -8.192e-16  9.342e-01   0.000   1.0000  \nsmokeryes          1.792e+00  9.283e-01   1.930   0.0536 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 55.452  on 39  degrees of freedom\nResidual deviance: 48.547  on 37  degrees of freedom\nAIC: 54.547\n\nNumber of Fisher Scoring iterations: 4\n\n\nCode\nexp(mod$coef)\n\n\n      (Intercept) coffee_drinkeryes         smokeryes \n              0.5               1.0               6.0 \n\n\nCode\ntbl_regression(mod, exponentiate = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\ncoffee_drinker\n\n\n\n\n\n\n\n\n    no\n—\n—\n\n\n\n\n    yes\n1.00\n0.13, 5.84\n&gt;0.9\n\n\nsmoker\n\n\n\n\n\n\n\n\n    no\n—\n—\n\n\n\n\n    yes\n6.00\n1.08, 48.2\n0.054\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nCode\ntable(df$coffee_drinker, df$outcome)\n\n\n     \n      control case\n  no       15   10\n  yes       5   10\n\n\nCode\nggplot(data = df) +\n geom_mosaic(aes(x = product(coffee_drinker, outcome)), fill = \"blue\") +\n  theme_bw(base_size = 15)\n\n\nWarning: The `scale_name` argument of `continuous_scale()` is deprecated as of ggplot2\n3.5.0.\n\n\nWarning: The `trans` argument of `continuous_scale()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n\n\nWarning: `unite_()` was deprecated in tidyr 1.2.0.\nℹ Please use `unite()` instead.\nℹ The deprecated feature was likely used in the ggmosaic package.\n  Please report the issue at &lt;https://github.com/haleyjeppson/ggmosaic&gt;.\n\n\n\n\n\n\n\n\n\nCode\nggplot(data = df) +\n geom_mosaic(aes(x = product(coffee_drinker, outcome), fill = smoker)) +\n  theme_bw(base_size = 15)\n\n\n\n\n\n\n\n\n\nMention that stratification stops being effective when you have too many categories (or continuous variables) to stratify on. The limitations visualisation found at the bottom of https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704-ep713_confounding-em/BS704-EP713_Confounding-EM7.html could be worthwhile including.\ngood info in: Attia, J. R., Jones, M. P., & Hure, A. (2017). Deconfounding confounding part 1: traditional explanations. Med J Aust, 206(6), 244-245. Richardson, A. M., & Joshy, G. (2020). Deconfounding confounding part 3: controlling for confounding in statistical analyses. Medical Journal of Australia.\nMaybe plan for this is: - Show contingency tables for: 1. coffee vs outcome (crude) 2. coffee vs outcome stratified by smokers (adjusted) 3. coffee vs outcome stratified by non-smokers (adjusted) Note that these are both different to the crude estimate. Also the adjusted estimates are = 1 which means complete confounding of the association between coffee and CHD by smoking 4. smoking vs outcome (crude) 5. smoking vs outcome stratified by coffee drinking (adjusted) 6. smoking vs outcome stratified by non-coffee drinking (adjusted) Note that these are all = 6 which means coffee drinking doesn’t affect the crude estimate and is not a confounder\nThen I could show the disproportionate distribution table and explain why smoking is a confounder (accompany by mosaic plot with smoking as a fill colour)\nThen replicate the 2 x 2 tables with the above models\nThen give tips on diffs between confounding and effect modification and whether to combine estimates or not from (Confounding vs Effect Modification Notes.md)\nFrom LMR Module 3 Stratification utilises the above concepts by assessing the association between the risk factor of interest and the outcome variable within homogeneous strata or cate- gories of the confounding variable. Within each stratum the confounding factor is “held constant” and therefore cannot confound the association between risk factor and outcome.\nThe key practical assessment of confounding involves comparing the “crude” and “adjusted” measures of association. When they differ by a meaningfully important amount, confounding is considered to exist. Unfortunately there is no benchmark nor statistical test for the amount of discrepancy needed to rule confounding present, and indeed its determination is necessarily context-specific—what may be an important discrepancy between crude and adjusted measures in one application may be of far less importance in another.\nHow should we combine these two unconfounded estimates of the as- sumed common gender difference in SBP? Simply take the average, or create a weighted average, based on sample size, or on precision? Each of these options can be expressed in terms of a weighted average of the two w b(1) + w b(0) estimates, in the form of b∗1 = 1 1 0 1 , where the stratum-specific w1 + w0 estimates of the gender effect on SBP are denoted by b(1) and b(0) with 11 superscripts (1) for age ≥ 40, and (0) for age &lt; 40, and similarly for the weights w1 and w0. We will see in fact that a multiple regression model produces just such a weighted average of stratum-specific effects, with weights that are sensi- ble and meaningful. We first present the general linear model with two covariates, and then examine this special case"
  },
  {
    "objectID": "posts/019_20Sep_2024/index.html#statistical-modelling-approach",
    "href": "posts/019_20Sep_2024/index.html#statistical-modelling-approach",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "4 Statistical Modelling Approach",
    "text": "4 Statistical Modelling Approach\nThe cross-tabulation approach is great for expository purposes but is limiting in the types of relationships you can practically explore - things just become unwieldy with variables that contain more than two categories and impossible with continuous variables.\nSo, we tend to use statistical models instead. These make confounder control/adjustment easy - we just need to include the potential confounder in the model along with our exposure of interest. Let’s now replicate everything we have done thus far, but in a modelling-paradigm.\n\n4.1 Crude Odds Ratio - Exposure/Outcome\n\n\nCode\nglm(outcome ~ coffee_drinker, data = df, family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n        no\n—\n—\n\n        yes\n3.00\n0.81, 12.2\n0.11\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nWe get the same OR as in Section 3.1 in addition to a 95% C.I. and p-value.\n\n\n4.2 Adjusted Odds Ratios - Exposure/Outcome (via stratification)\nWe can replicate a stratified effect in our modelling by subsetting the data to select only those people in each smoking subgroup.\n\n4.2.1 Smokers\n\n\nCode\nglm(outcome ~ coffee_drinker, data = subset(df, smoker == \"yes\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n        no\n—\n—\n\n        yes\n1.00\n0.04, 12.2\n&gt;0.9\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 1 (compare to Section 3.2.1)\n\n\n4.2.2 Non-smokers\n\n\nCode\nglm(outcome ~ coffee_drinker, data = subset(df, smoker == \"no\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n        no\n—\n—\n\n        yes\n1.00\n0.04, 12.3\n&gt;0.9\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 1 (compare to Section 3.2.2)\n\n\n\n4.3 Crude Odds Ratio - Confounder/Outcome\n\n\nCode\nglm(outcome ~ smoker, data = df, family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    smoker\n\n\n\n        no\n—\n—\n\n        yes\n6.00\n1.55, 27.5\n0.013\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 6 (compare to Section 3.3)\n\n\n4.4 Adjusted Odds Ratios - Confounder/Outcome (via stratification)\n\n4.4.1 Coffee-drinkers\n\n\nCode\nglm(outcome ~ smoker, data = subset(df, coffee_drinker == \"yes\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    smoker\n\n\n\n        no\n—\n—\n\n        yes\n6.00\n0.43, 161\n0.2\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 6 (compare to Section 3.4.1)\n\n\n4.4.2 Non-coffee-drinkers\n\n\nCode\nglm(outcome ~ smoker, data = subset(df, coffee_drinker == \"no\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    smoker\n\n\n\n        no\n—\n—\n\n        yes\n6.00\n0.64, 134\n0.15\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 6 (compare to Section 3.4.2)\n\n\n\n4.5 Adjusted Odds Ratio - Overall\nThe above models are all considered univariable in that one predictor only is specified in each model. In controlling or adjusting for a third variable we now produce a multivariable model where both the exposure and potential confounder are specified as predictors. This automatically produces an adjusted ‘effect’ of coffee drinking on CHD, controlling for smoking (and conversely an adjusted ‘effect’ of smoking on CHD, controlling for coffee drinking). We run these kinds of models all the time without thinking, but what the model is doing ‘under the hood’ is calculating a weighted average of the individual subgroup estimates in producing a single coefficient that becomes our effect/s of interest.\n\n\nCode\nglm(outcome ~ coffee_drinker + smoker, data = df, family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n        no\n—\n—\n\n        yes\n1.00\n0.13, 5.84\n&gt;0.9\n    smoker\n\n\n\n        no\n—\n—\n\n        yes\n6.00\n1.08, 48.2\n0.054\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "posts/019_20Sep_2024/index.html#some-guidelines",
    "href": "posts/019_20Sep_2024/index.html#some-guidelines",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "5 Some Guidelines…",
    "text": "5 Some Guidelines…\nTo consolidate the ideas that we have explored today I want to lay out some very broad guidelines for how to interpret and compare crude and adjusted effects.\n\n5.1 No Confounding or Effect Modification Present\nIf there is neither confounding nor effect modification, the crude estimate of association and the stratum-specific estimates will be similar (converging to being the same in the ideal context). This is reflected in Sections 3.3 and Section 3.4 above.\n\n\n5.2 Only Confounding Present\nIf there is only confounding, the stratum-specific measures of association will be similar to one another, but they will be different from the overall crude estimate (by ~ 10% or more). In this situation, one can use CMH methods to calculate a weighted estimate and p-value, or even easier is to run a statistical model including the confounder as just another covariate. i.e. (in R)\nmod1 &lt;- lm(outcome ~ exposure + confounder, data = dat)\nThis is reflected in Sections 3.1 and Section 3.2 above.\n\n\n5.3 Confounding and/or Effect Modification Present\nIn this case the stratum-specific estimates will differ from one another significantly and these will also differ from the overall crude estimate. These effects should be reported as they are and not weighted and combined (i.e. averaged over) as this is of scientific and clinical interest in its own right. In practical terms in a statistical model, an interaction term should be specified. This changes the coding from:\nmod1 &lt;- lm(outcome ~ exposure + confounder, data = dat)\nto\nmod2 &lt;- lm(outcome ~ exposure * confounder, data = dat)\nAs the two models are ‘nested’, an assessment of whether the interaction term is necessary or not can be performed using a likelihood ratio test:\nanova(mod2, mod1)\nIf the p-value is significant you can conclude that the interaction term increases the explanatory power of the model and should be retained."
  },
  {
    "objectID": "posts/019_20Sep_2024/index.html#the-end",
    "href": "posts/019_20Sep_2024/index.html#the-end",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "6 The End!",
    "text": "6 The End!\nAnother post that has gone on longer than I had anticipated - there is much more I could talk about on the topic but my goal is not to make you fall asleep on your keyboard. Hopefully this has helped to make the concept of confounding just that little bit clearer in your mind. Until next time…"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Paul Sanfilippo",
    "section": "",
    "text": "I am an applied biostatistician currently assisting researchers working in the field of multiple sclerosis research at Monash University. I like playing with data, my miniature poodle and my family - not necessarily in that order."
  },
  {
    "objectID": "posts/020_04Oct_2024/index.html",
    "href": "posts/020_04Oct_2024/index.html",
    "title": "Breaking Free From ‘The Cult of P’",
    "section": "",
    "text": "Apologies to all - I know I have already preached about this in the first WhatsApp message I sent out on ‘Weekly Stats Tips’ about 12 months ago. But given the idea is so important, there’s no permanent and easily-accessible record of that message, and also that in hindsight I think I told you what not to do, but didn’t really suggest what you could/should do - I’m going to make it a blog post here as well."
  },
  {
    "objectID": "posts/020_04Oct_2024/index.html#background",
    "href": "posts/020_04Oct_2024/index.html#background",
    "title": "Breaking Free From The Cult of P",
    "section": "",
    "text": "I know I have touched on confounding before but it’s such an important concept in statistics that I thought I would dedicate a post to it today. There is so much one could talk about on the topic, but my aim is to provide you with some intuition as to what is actually happening when one variable ‘confuses’ the relationship between two others.\nI’m going to use a fairly classic example from epidemiology that examines whether there is an association between drinking coffee (full-cream milk only thanks) and developing coronary heart disease (CHD).\n\n\n\n\n\n\nNote\n\n\n\nIn a sense this is a causal question - i.e. “Does drinking coffee cause CHD?”, but for the sake of the illustration, let’s excuse ourselves from such a question’s fairly strict assumptions for the time being.\n\n\nBeing the good researchers that we are, we give considerable thought as to how to answer our question. That means we don’t forgo our own field-expertise and knowledge at the expense of the data-driven p-value gods. First and foremost, we think about the relationships we are investigating and all the potential factors that could influence those relationships. And that comes from theory, not statistical tests.\nIn examining those knowledge-driven theoretical relationships, we draw a Directed Acyclic Graph (DAG). This is a topic in its own right, but for another post - in the meantime some good primers can be found here and here. A DAG is otherwise more simply known as a causal diagram. It is meant to provide a visual representation of the causal relationships among a set of variables and in doing this forces the researcher to think about their scientific question in a more holistic manner. Indeed, statistical tests aren’t even involved at this point.\nSo, in thinking about our coffee-CHD question, it occurs to us that smoking is known to cause CHD and we wonder whether smoking might also be related to drinking coffee - i.e. do coffee-drinkers also smoke more (or less) than people who prefer don’t drink coffee (tea perhaps?). In this case, a simple DAG might look something like:\n\n\n\n\n\nWhat this DAG helps to summarise is what we are now wondering - is smoking confusing or distorting the association between coffee drinking and the development of CHD? In other words, is smoking a confounder in the association between our exposure (coffee-drinking) and outcome (CHD) variables of interest?\nBefore we take a peek at our data, let’s refresh our memories regarding the three conditions a variable must fulfil to be considered a putative confounder:\n\nThe variable must be associated with both the exposure and the outcome.\nThe variable must be distributed unequally across the exposure (e.g. between treatment groups).\nThe variable must not lie on the causal pathway between the exposure and the outcome (this would be a mediator and requires different treatment)."
  },
  {
    "objectID": "posts/020_04Oct_2024/index.html#data",
    "href": "posts/020_04Oct_2024/index.html#data",
    "title": "Breaking Free From The Cult of P",
    "section": "2 Data",
    "text": "2 Data\nOk, let’s now take a look at the hypothetical retrospective case-control data we’ll be using today. It consists of 40 observations and three variables:\n\noutcome - did the individual have CHD (case) or not (control).\ncoffee-drinker - was the individual a coffee-drinker or not. This is our exposure variable of interest.\nsmoker - was the individual a smoker or not. This is our potential confounder.\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(ggmosaic)\nlibrary(kableExtra)\nlibrary(janitor)\n\n# Hypothetical data\ny &lt;- factor(c(rep(1, 20), rep(0, 20)), levels = c(0, 1), labels = c(\"control\", \"case\"))\nx1 &lt;- factor(c(rep(1, 10), rep(0, 10), rep(1, 5), rep(0, 15)), levels = c(0, 1), labels = c(\"no\", \"yes\"))\nx2 &lt;- factor(c(rep(1, 9), rep(0, 1), rep(1, 3), rep(0, 7), rep(1, 3), rep(0, 2), rep(1, 1), rep(0, 14)), levels = c(0, 1), labels = c(\"no\", \"yes\"))\ndf &lt;- data.frame(\"outcome\" = y, \"coffee_drinker\" = x1, \"smoker\" = x2)\ndf |&gt; \n  kable(align = \"c\")\n\n\n\n\n\noutcome\ncoffee_drinker\nsmoker\n\n\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nno\n\n\ncase\nno\nyes\n\n\ncase\nno\nyes\n\n\ncase\nno\nyes\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncontrol\nyes\nyes\n\n\ncontrol\nyes\nyes\n\n\ncontrol\nyes\nyes\n\n\ncontrol\nyes\nno\n\n\ncontrol\nyes\nno\n\n\ncontrol\nno\nyes\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno"
  },
  {
    "objectID": "posts/020_04Oct_2024/index.html#simple-epidemiological-approach",
    "href": "posts/020_04Oct_2024/index.html#simple-epidemiological-approach",
    "title": "Breaking Free From The Cult of P",
    "section": "3 Simple Epidemiological Approach",
    "text": "3 Simple Epidemiological Approach\nGiven the binary nature of all three variables we can explore the relationships nicely using a basic workhorse of epidemiological analysis - the 2x2 contingency table. In its simplest form this shows the frequency cross-tabulation of the exposure with the outcome. However, I think it’s also useful to display the conditional row percentages - in other words, the proportions (probabilities) of each outcome (categories as columns) given a particular exposure (categories as rows). In this way it is easy to eyeball whether the exposure is associated with the outcome without doing any specific test simply by looking at whether the row percentages vary greatly. As a good visual accompaniment for each cross-tabulation, I am also going to generate mosaic plots. These can give you an impression of potential associations without using any actual numbers.\n\n3.1 Crude Odds Ratio - Exposure/Outcome\nThe following cross-tabulation and mosaic plot show the potential association between coffee-drinking and CHD. If we look at the proportions of people with CHD in each exposure group we can see that these do in fact differ - 67% of coffee drinkers develop CHD, compared to 40% of non-coffee drinkers. That is a telling sign of an association before we even do anything. The mosaic plot mirrors these numbers graphically.\n\n\n\nCode\ntab &lt;- df |&gt; \n  tabyl(coffee_drinker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    coffee_drinker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    60% (15) \n    40% (10) \n    100% (25) \n  \n  \n    yes \n    33%  (5) \n    67% (10) \n    100% (15) \n  \n  \n    Total \n    50% (20) \n    50% (20) \n    100% (40) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df\ndf2$coffee_drinker &lt;- relevel(df2$coffee_drinker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, coffee_drinker), fill = coffee_drinker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nWorking out the odds ratio (OR) from a 2x2 contingency table is trivial. We want to divide the odds of having CHD given being a coffee-drinker by the odds of having CHD given being a non-coffee-drinker. That is:\n\\[\\text{OR} = \\frac{\\text{10/5}}{10/15}\\]\nOr equivalently:\n\\[\\text{OR} = \\frac{\\text{10 x 15}}{\\text{5 x 10}} = 3\\] The OR is 3 which indicates a 3-fold increase in the odds of CHD among coffee-drinkers compared to their non-coffee-drinking peers. We consider this a crude or unadjusted effect estimate.\n\n\n3.2 Adjusted Odds Ratios - Exposure/Outcome (via stratification)\nTo this point we haven’t even considered any potential confounding effect of smoking. So how could we incorporate that into the current analysis using 2x2 contingency tables? It’s actually very easy - we just stratify on smoking and generate two contingency tables - one for the association between coffee drinking and CHD in smokers and one for the association between coffee drinking and CHD in non-smokers. It then follows that within each stratum of smoking the effect of smoking is ‘held constant’ and therefore cannot confound the association between coffee drinking and CHD. In other words, this becomes an adjusted ‘effect’ of coffee drinking on CHD and is equivalent to ‘controlling’ for smoking in a multivariable statistical model (which I will demonstrate shortly).\n\n3.2.1 Smokers\nSo, the cross-tabulation for the association between coffee-drinking and CHD in smokers, becomes:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(smoker == \"yes\") |&gt; \n  tabyl(coffee_drinker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    coffee_drinker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    25% (1) \n    75%  (3) \n    100%  (4) \n  \n  \n    yes \n    25% (3) \n    75%  (9) \n    100% (12) \n  \n  \n    Total \n    25% (4) \n    75% (12) \n    100% (16) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(smoker == \"yes\")\ndf2$coffee_drinker &lt;- relevel(df2$coffee_drinker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, coffee_drinker), fill = coffee_drinker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nNote how this time the (conditional row) proportions of people with CHD in each exposure group are more similar (in this contrived example they are the same) - in this subgroup of smokers it doesn’t matter whether you’re a coffee drinker or not - 75% of people have CHD.\nNow, the OR is calculated as:\n\\[\\text{OR} = \\frac{\\text{9 x 1}}{\\text{3 x 3}} = 1\\]\nThe OR is 1 - in other words there is no longer any association between coffee drinking and CHD. We consider this an adjusted OR as we have removed any potential confounding effect of smoking by holding it at a constant value (i.e. everyone is a smoker).\n\n\n3.2.2 Non-smokers\nNow let’s consider the non-smokers. The cross-tabulation for the association between coffee-drinking and CHD in non-smokers is:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(smoker == \"no\") |&gt; \n  tabyl(coffee_drinker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    coffee_drinker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (14) \n    33% (7) \n    100% (21) \n  \n  \n    yes \n    67%  (2) \n    33% (1) \n    100%  (3) \n  \n  \n    Total \n    67% (16) \n    33% (8) \n    100% (24) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(smoker == \"no\")\ndf2$coffee_drinker &lt;- relevel(df2$coffee_drinker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, coffee_drinker), fill = coffee_drinker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nDo you see a pattern here? Again, the conditional row percentages are the same across exposure groups (33%) - so, in this subgroup of non-smokers it also doesn’t matter whether you are a coffee drnker or not, the proportions of CHD are the same.\nThe OR this time is:\n\\[\\text{OR} = \\frac{\\text{1 x 14}}{\\text{7 x 2}} = 1\\]\nThe adjusted OR is again 1 - that is, there is no association between coffee drinking and CHD when we remove any potential confounding effect of smoking by holding it at a (different) constant value (i.e. everyone is a non-smoker).\n\n\n3.2.3 Salient points\nThere are two important points to make in the comparison of the crude and adjusted estimates:\n\nThe adjusted OR is not equal to the crude OR - this suggests confounding is present.\nFurthermore, this represents a special case in which there is complete confounding - the OR reduces to the null value (i.e. 1). I will expand on these points in a moment.\n\n\n\n\n3.3 Crude Odds Ratio - Confounder/Outcome\nTo further illuminate the above ideas let’s repeat the analyses but this time ‘switch’ the exposure and confounder around (hopefully it will become clear why we are doing this as you continue reading). The cross-tabulation and mosaic plot suggest an association between smoking and CHD - 75% of smokers develop CHD, compared to 33% of non-smokers.\n\n\n\nCode\ntab &lt;- df |&gt; \n  tabyl(smoker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    smoker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (16) \n    33%  (8) \n    100% (24) \n  \n  \n    yes \n    25%  (4) \n    75% (12) \n    100% (16) \n  \n  \n    Total \n    50% (20) \n    50% (20) \n    100% (40) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df\ndf2$smoker &lt;- relevel(df2$smoker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, smoker), fill = smoker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nThe OR for the association between smoking and CHD is:\n\\[\\text{OR} = \\frac{\\text{12 x 16}}{\\text{8 x 4}} = 6\\]\nSo the crude OR is 6, which indicates a 6-fold increase in the odds of CHD in smokers compared to non-smokers (ignoring coffee-drinking status).\n\n\n3.4 Adjusted Odds Ratios - Confounder/Outcome (via stratification)\nLet us know compare the adjusted ‘effects’ in coffee-drinkers compared to non-coffee-drinkers.\n\n3.4.1 Coffee-drinkers\nSo, the cross-tabulation for the association between coffee-smoking and CHD in coffee-drinkers, becomes:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(coffee_drinker == \"yes\") |&gt; \n  tabyl(smoker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    smoker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (2) \n    33%  (1) \n    100%  (3) \n  \n  \n    yes \n    25% (3) \n    75%  (9) \n    100% (12) \n  \n  \n    Total \n    33% (5) \n    67% (10) \n    100% (15) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(coffee_drinker == \"yes\")\ndf2$smoker &lt;- relevel(df2$smoker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, smoker), fill = smoker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nAnd the adjusted OR is:\n\\[\\text{OR} = \\frac{\\text{9 x 2}}{\\text{1 x 3}} = 6\\]\nThe OR is 6 - in other words the same as the crude estimate.\n\n\n3.4.2 Non-coffee-drinkers\nNow let’s consider the non-coffee-drinkers. The cross-tabulation for the association between smokers and CHD in non-coffee-drinkers is:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(coffee_drinker == \"no\") |&gt; \n  tabyl(smoker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    smoker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (14) \n    33%  (7) \n    100% (21) \n  \n  \n    yes \n    25%  (1) \n    75%  (3) \n    100%  (4) \n  \n  \n    Total \n    60% (15) \n    40% (10) \n    100% (25) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(coffee_drinker == \"no\")\ndf2$smoker &lt;- relevel(df2$smoker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, smoker), fill = smoker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nAnd the adjusted OR is:\n\\[\\text{OR} = \\frac{\\text{3 x 14}}{\\text{7 x 1}} = 6\\]\nThe OR is again 6! Is this starting to make some sense?\n\n\n3.4.3 Salient points\nAgain, there are two important points to make in the comparison of these crude and adjusted estimates:\n\nIt doesn’t seem to matter whether we adjust for coffee-drinking or not, the association between smoking and CHD remains the same.\nIt then follows that coffee-drinking is not a confounder in the smoking-CHD relationship.\n\n\n\n\n3.5 Adjusted Odds Ratio - Overall\nIt just so happens that the adjusted OR’s from Section 3.2 are the same in each subgroup, but this is an exception rather than a rule. Normally, in the presence of confounding, effect estimates will differ in each subgroup to the crude estimate but will not be equal to each other. What do we do with separate effect estimates from each subgroup - this makes reporting somewhat painful, surely? Well, if the adjusted estimates aren’t too different from each other, they can be combined in a weighted manner to provide an overall summary estimate and this can be done using the Cochran-Mantel-Haenszel (CMH) test. I won’t illustrate this here as in the modern computing age there is really no need to be crunching this statistic anymore, and we use a statistical model instead.\nAs I alluded to above, we should only attempt to combine individual estimates when they are ‘similar’. But what does that mean? How similar should they be and how different can they be? There are no hard and fast rules but I will outline a couple of guidelines at the end of this post. The important thing is to realise that when individual estimates are different, this actually represents an effect modification/interaction effect - that is the strength of the association between the exposure and the outcome depends on the level of the third variable. Interaction effects are of interest (scientifically and clinically) and we shouldn’t try to cancel them out by averaging over them."
  },
  {
    "objectID": "posts/020_04Oct_2024/index.html#statistical-modelling-approach",
    "href": "posts/020_04Oct_2024/index.html#statistical-modelling-approach",
    "title": "Breaking Free From The Cult of P",
    "section": "4 Statistical Modelling Approach",
    "text": "4 Statistical Modelling Approach\nThe cross-tabulation approach is great for expository purposes but is limiting in the types of relationships you can practically explore - things just become unwieldy with variables that contain more than two categories and impossible with continuous variables.\nSo, we tend to use statistical models instead. These make confounder control/adjustment easy - we just need to include the potential confounder in the model along with our exposure of interest. Let’s now replicate everything we have done thus far, but in a modelling-paradigm.\n\n4.1 Crude Odds Ratio - Exposure/Outcome\n\n\nCode\nglm(outcome ~ coffee_drinker, data = df, family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n        no\n—\n—\n\n        yes\n3.00\n0.81, 12.2\n0.11\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nWe get the same OR as in Section 3.1 in addition to a 95% C.I. and p-value.\n\n\n4.2 Adjusted Odds Ratios - Exposure/Outcome (via stratification)\nWe can replicate a stratified effect in our modelling by subsetting the data to select only those people in each smoking subgroup.\n\n4.2.1 Smokers\n\n\nCode\nglm(outcome ~ coffee_drinker, data = subset(df, smoker == \"yes\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n        no\n—\n—\n\n        yes\n1.00\n0.04, 12.2\n&gt;0.9\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 1 (compare to Section 3.2.1)\n\n\n4.2.2 Non-smokers\n\n\nCode\nglm(outcome ~ coffee_drinker, data = subset(df, smoker == \"no\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n        no\n—\n—\n\n        yes\n1.00\n0.04, 12.3\n&gt;0.9\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 1 (compare to Section 3.2.2)\n\n\n\n4.3 Crude Odds Ratio - Confounder/Outcome\n\n\nCode\nglm(outcome ~ smoker, data = df, family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    smoker\n\n\n\n        no\n—\n—\n\n        yes\n6.00\n1.55, 27.5\n0.013\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 6 (compare to Section 3.3)\n\n\n4.4 Adjusted Odds Ratios - Confounder/Outcome (via stratification)\n\n4.4.1 Coffee-drinkers\n\n\nCode\nglm(outcome ~ smoker, data = subset(df, coffee_drinker == \"yes\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    smoker\n\n\n\n        no\n—\n—\n\n        yes\n6.00\n0.43, 161\n0.2\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 6 (compare to Section 3.4.1)\n\n\n4.4.2 Non-coffee-drinkers\n\n\nCode\nglm(outcome ~ smoker, data = subset(df, coffee_drinker == \"no\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    smoker\n\n\n\n        no\n—\n—\n\n        yes\n6.00\n0.64, 134\n0.15\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 6 (compare to Section 3.4.2)\n\n\n\n4.5 Adjusted Odds Ratio - Overall\nThe above models are all considered univariable in that one predictor only is specified in each model. In controlling or adjusting for a third variable we now produce a multivariable model where both the exposure and potential confounder are specified as predictors. This automatically produces an adjusted ‘effect’ of coffee drinking on CHD, controlling for smoking (and conversely an adjusted ‘effect’ of smoking on CHD, controlling for coffee drinking). We run these kinds of models all the time without thinking, but what the model is doing ‘under the hood’ is calculating a weighted average of the individual subgroup estimates in producing a single coefficient that becomes our effect/s of interest.\n\n\nCode\nglm(outcome ~ coffee_drinker + smoker, data = df, family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n        no\n—\n—\n\n        yes\n1.00\n0.13, 5.84\n&gt;0.9\n    smoker\n\n\n\n        no\n—\n—\n\n        yes\n6.00\n1.08, 48.2\n0.054\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "posts/020_04Oct_2024/index.html#some-guidelines",
    "href": "posts/020_04Oct_2024/index.html#some-guidelines",
    "title": "Breaking Free From The Cult of P",
    "section": "5 Some Guidelines…",
    "text": "5 Some Guidelines…\nTo consolidate the ideas that we have explored today I want to lay out some very broad guidelines for how to interpret and compare crude and adjusted effects.\n\n5.1 No Confounding or Effect Modification Present\nIf there is neither confounding nor effect modification, the crude estimate of association and the stratum-specific estimates will be similar (converging to being the same in the ideal context). This is reflected in Sections 3.3 and Section 3.4 above.\n\n\n5.2 Only Confounding Present\nIf there is only confounding, the stratum-specific measures of association will be similar to one another, but they will be different from the overall crude estimate (by ~ 10% or more). In this situation, one can use CMH methods to calculate a weighted estimate and p-value, or even easier is to run a statistical model including the confounder as just another covariate. i.e. (in R)\nmod1 &lt;- lm(outcome ~ exposure + confounder, data = dat)\nThis is reflected in Sections 3.1 and Section 3.2 above.\n\n\n5.3 Confounding and/or Effect Modification Present\nIn this case the stratum-specific estimates will differ from one another significantly and these will also differ from the overall crude estimate. These effects should be reported as they are and not weighted and combined (i.e. averaged over) as this is of scientific and clinical interest in its own right. In practical terms in a statistical model, an interaction term should be specified. This changes the coding from:\nmod1 &lt;- lm(outcome ~ exposure + confounder, data = dat)\nto\nmod2 &lt;- lm(outcome ~ exposure * confounder, data = dat)\nAs the two models are ‘nested’, an assessment of whether the interaction term is necessary or not can be performed using a likelihood ratio test:\nanova(mod2, mod1)\nIf the p-value is significant you can conclude that the interaction term increases the explanatory power of the model and should be retained."
  },
  {
    "objectID": "posts/020_04Oct_2024/index.html#the-end",
    "href": "posts/020_04Oct_2024/index.html#the-end",
    "title": "Breaking Free From The Cult of P",
    "section": "6 The End!",
    "text": "6 The End!\nAnother post that has gone on longer than I had anticipated - there is much more I could talk about on the topic but my goal is not to make you fall asleep on your keyboard. Hopefully this has helped to make the concept of confounding just that little bit clearer in your mind. Until next time…"
  },
  {
    "objectID": "posts/020_04Oct_2024/index.html#the-issue",
    "href": "posts/020_04Oct_2024/index.html#the-issue",
    "title": "Breaking Free From ‘The Cult of P’",
    "section": "1 The Issue",
    "text": "1 The Issue\nSo, as researchers how many of you can honestly say that you have never used language around p-values in a way reminiscent of this xkcd comic (or one of these 509 variations on the same theme):\n\n\n\n\n\nI know I can’t. Certainly in my early research career and before changing paths to become a biostatistician, I was guilty of this sort of thing. All in the name of some misguided sense of wanting to achieve research glory by getting that p-value to fall under the all-important threshold of 0.05 (my research glory did not ever materialise by the way).\nWell can I suggest, it just doesn’t matter. And can I also suggest to stop thinking about p-values in this way.\nIt’s generally accepted that Sir Ronald Fisher was the guy who at least formalised the ideas of Null Hypothesis Significance Testing (NHST) and the p-value in the 1920’s. He never intended for the p-value of 0.05 to be set in stone as an arbiter of the value of a piece of research in a statistical sense, and certainly not in a scientific sense. The intention was to use it to guide decision making, not make the actual decision. But, in the past 100 years this arbitrary threshold of p = 0.05 has not only stuck, but taken on an almost mythic status in the research community, in part it seems because humans are lazy and like to avoid decision-making where possible. The natural and silly extension to this entrenched notion of “significance” is that researchers may feel elated if their statistical test returns p = 0.049 and despondent if it returns p = 0.051. On reflection, any rational person can see that’s crazy, but it doesn’t seem to give us pause for thought when we have our results in front of us."
  },
  {
    "objectID": "posts/020_04Oct_2024/index.html#what-not-to-do",
    "href": "posts/020_04Oct_2024/index.html#what-not-to-do",
    "title": "Breaking Free From ‘The Cult of P’",
    "section": "2 What Not To Do",
    "text": "2 What Not To Do\nYou may be aware that there has been a push in recent years by the statistical community to abandon the p-value all together, set off in large part by a statement published by the American Statistical Association in 2016. There is nothing new in any of this - the same issues have been of concern for decades and a Pubmed (or even Google) search will reveal a large literature on the topic. In 2019, The American Statistician dedicated an entire special issue to statistical inference entitled “Statistical Inference in the 21st Century: A World Beyond p &lt; 0.05.” If you scan down those titles you’ll see that not too many are in favour of retaining the 0.05 dichotomy we seem to have imposed on ourselves. The Editorial in that edition doesn’t pull any punches in terms of how to use p-values in your research:\n\nDon’t base your conclusions solely on whether an association or effect was found to be “statistically significant” (i.e., the p-value passed some arbitrary threshold such as p &lt; 0.05).\nDon’t believe that an association or effect exists just because it was statistically significant.\nDon’t believe that an association or effect is absent just because it was not statistically significant.\nDon’t believe that your p-value gives the probability that chance alone produced the observed association or effect or the probability that your test hypothesis is true.\nDon’t conclude anything about scientific or practical importance based on statistical significance (or lack thereof).\n\nAnd the authors go further to say:\nThe ASA Statement on P-Values and Statistical Significance stopped just short of recommending that declarations of “statistical significance” be abandoned. We take that step here. We conclude, based on our review of the articles in this special issue and the broader literature, that it is time to stop using the term “statistically significant” entirely. Nor should variants such as “significantly different,” “p &lt; 0.05,” and “nonsignificant” survive, whether expressed in words, by asterisks in a table, or in some other way.\nPretty damning!"
  },
  {
    "objectID": "posts/020_04Oct_2024/index.html#what-you-couldshould-do",
    "href": "posts/020_04Oct_2024/index.html#what-you-couldshould-do",
    "title": "Breaking Free From ‘The Cult of P’",
    "section": "3 What You Could/Should Do",
    "text": "3 What You Could/Should Do\nIf you look at some of the papers in that special issue, you will find a wealth of suggestions about how to approach a “Post ‘p &lt; 0.05’ world”, including that we should all become Bayesians (not meaning to toot my own trumpet, but I have also written around this in one of my not-so-highly-cited papers several years ago - I’m sure it will gain traction in years to come!).\nSo I’m not going to reiterate all of those suggestions here. Instead, for what my opinion is worth, I have two practical tips that I think would make interpreting and reporting our research more robust and less reliant on the p-value.\n\n3.1 Use A Language Of ‘Evidence’\nWe can still calculate p-values as we currently do - nothing needs to change. But instead of referring to the p-value itself, let’s start discussing our results in the context of ‘evidence’ (i.e. against the null hypothesis of no effect). The following table from this paper sums it up beautifully and so I am not going to reinvent the wheel (in fact I can thoroughly recommend the other 3 papers in the series, so do give them a look if you have time).\n\n\n\n\n\nNow, I have used my own subtle variation on the terminology over the years and replaced “insufficient” with “weak”, “some” with “moderate” and “overwhelming” with “very strong”. The point is, it doesn’t really matter what words you use to describe your ‘effect’ with - this kind of language immediately frees you from the confines of an arbitrary dichotomisation that forces you to either value or devalue all of your hard work in one fell swoop. Instead of saying ‘there was a trend towards statistical significance for the association between x and y’, if your p-value turns out to be 0.051, you can now simply say there was ‘weak evidence for an association between x and y’, instead.\n\n\n3.2 Be Consistent In p-Value Reporting\nIf I had a dollar for every time a research student has sent me results that contain p-values between 0.045 and 0.05 and reported that to 3 decimal places, I’d be a little less poor than I am now. Seriously, there is no need to do this. Please adhere to the following reporting guide and don’t try to put lipstick on a pig by suggesting that your p-value of 0.049 is any different to 0.05.\n\n\n\n\n\nThese are a couple of small changes we can make in our research reporting practice that will help to break our self-imposed binds to the p-value gods. Until next time…"
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html",
    "href": "posts/021_18Oct_2024/index.html",
    "title": "Survival Analysis - Under the Hood",
    "section": "",
    "text": "Welcome to today’s post which is based on a talk that some of you may have heard me give in lab meetings. A warning in advance - it’s a bit of a lengthy one, so maybe down a couple of reds to steel yourself before you start. We’re going to discuss survival analysis, but in a similar spirit to how I presented the post on logistic regression a couple of months ago - nothing too fancy, but with an aim that you understand what is happening ‘under the hood’ when you next fire up your stats software to plot a Kaplan-Meier curve or run a Cox model.\nSurvival analysis is a BIG topic and in university statistics courses, it tends to have an entire subject dedicted just to it, so it’s not just taught as a part of regression modelling, for example. I also don’t find it particularly easy, even now - there are a lot of challenging concepts to understand, and that is in large part because time is such an integral component of everything you do in this area of biostatistics. So, what I hope I can do today is to take a broad-brush approach to survival methods, touch on the concepts that I think are most important in establishing a good base understanding, build in a little intuition for these, and also give you some practical tips along the way.\nLet’s get started."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#primary-transformations",
    "href": "posts/021_18Oct_2024/index.html#primary-transformations",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.1 Primary Transformations",
    "text": "3.1 Primary Transformations\nIf we have the odds of an event, then we can transform that to a probability - or a risk, which is another way to refer to the chance of an outcome - by dividing the odds by the odds plus 1.\n\\[\\text{p} = \\frac{\\text{odds}}{1 + \\text{odds}}\\]\nIf instead we have the probability of an event, we can transform that to the equivalent odds of the event, by dividing the probability by its complement (that is 1 minus the probability).\n\\[\\text{odds} = \\frac{\\text{p}}{1 - \\text{p}}\\]\nAnd the log-odds is simply taking the natural logarithm of the odds - there is nothing difficult about that.\n\\[\\text{log-odds} = \\text{log(odds)}\\]\nSo these three are what I consider primary transformations and we can cycle between them quite easily with a calculator."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#secondary-transformations",
    "href": "posts/021_18Oct_2024/index.html#secondary-transformations",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.2 Secondary Transformations",
    "text": "3.2 Secondary Transformations\nThere are additional transformations that you can make and while these aren’t quite as straightforward, they are even more fundamental to logistic regression, which ultimately uses the log-odds as the scale to estimate its parameters on.\nThe first is that we can get to a probability directly from the log-odds by taking the exponent of the log-odds divided by the exponent of the log-odds plus one.\n\\[\\text{p} = \\frac{e^{\\text{log-odds}}}{1 + e^{\\text{log-odds}}}\\]\nAnd as you can see, that’s really no different to the first equation above because the exponent of the log-odds is simply the odds (the reverse transformation of logging a number is to take the exponent of that logged number). i.e.\n\\[\\text{odds} = e^{\\text{log-odds}}\\]\nNow, the final transformation, and the one that’s most relevant to logistic regression shows the relationship between the log-odds and probability and is known as the logit function (red box) - and we’ll talk more about this in a minute.\n\\[\\text{log-odds} = \\text{log}\\Bigl(\\frac{\\text{p}}{1 - \\text{p}}\\Bigr)\\]"
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#section",
    "href": "posts/021_18Oct_2024/index.html#section",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.3 ",
    "text": "3.3"
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#what-is-survival-analysis",
    "href": "posts/021_18Oct_2024/index.html#what-is-survival-analysis",
    "title": "Survival Analysis - Under the Hood",
    "section": "0.1 What is survival analysis?",
    "text": "0.1 What is survival analysis?\nToday’s post comes from a talk that some of you may have already heard me give in lab meetings, but I thought it could be helpful to have a ‘print’ copy so I’m going to do that here. The material in the talk was loosely based on the following paper:\nOphthalmic Statistics Note 11: Logistic Regression\nI’m going back to basics today. I think too often we use statistical techniques without really understanding what is going on ‘under the hood’. While that is ok to some extent, a better appreciation of what you’re actually doing when you perform a hypothesis test, or run a regression model, lends more robustness to the validity of both your results and your interpretation of them.\nSo let’s take a more fundamental look at logistic regression - a workhorse statistical model that I’d be willing to bet most of you have run at some point in your research careers. Now, because I consider myself more an applied rather than theoretical biostatistician, I am going to try and get my main points across to you with as little maths as possible, and hopefully not bore you in the process (but this is statistics, so hey…).\nWhat I hope you can gain from reading this is to think about logistic regression in a new and different way - one that completely illuminates the technique for you."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#calendar-time-format",
    "href": "posts/021_18Oct_2024/index.html#calendar-time-format",
    "title": "Survival Analysis - Under the Hood",
    "section": "2.1 Calendar-time format",
    "text": "2.1 Calendar-time format\nWhat does survival data look like? Well - in the case of a prospective study, subjects are usually recruited into the study at different times. To illustrate this, let’s consider this hypothetical dataset of patients diagnosed with some disease that we follow until they either die or are right-censored. The study was planned to recruit for 4 months from Dec 2015 to Mar 2016, and during this period 5 subjects were entered into the study, all starting at various times, but importantly, the exact dates of their start were recorded. All subjects were then followed at regular intervals, and the study was then ended, as planned, in Jan 2017.\nNow, this is showing the observation times for each subject as they occurred in calendar-time format. While this reflects the reality of how survival data are collected, it doesn’t really inform you with an intuitive sense of time because you can’t easily make comparisons across subjects."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#time-on-study-format",
    "href": "posts/021_18Oct_2024/index.html#time-on-study-format",
    "title": "Survival Analysis - Under the Hood",
    "section": "2.2 Time-on-study format",
    "text": "2.2 Time-on-study format\nSo, for easier interpretation we can align the start time of all subjects like so - and we call this format, ‘time-on-study’. We can see that Subject 1 dropped out because they moved interstate and couldn’t attend clinic appointments anymore, and so they are considered censored. Their follow-up time was 6 months. Subjects 2 and 4 are also censored, but for a different reason - they were still alive when we decided to end the study and had the study gone on we may have been able to follow them for longer. In any case they were each observed for 12 months. Unfortunately, the other two subjects weren’t so lucky - Subject 3 died after 7 months and Subject 5 died after 4 months of being in the study.\n\n\n\n\n\nNow, what if we were interested in working out the one-year survival for this sample of patients?\nHow would you do that?"
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#this-bed-is-too-soft",
    "href": "posts/021_18Oct_2024/index.html#this-bed-is-too-soft",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.1 This bed is too soft",
    "text": "3.1 This bed is too soft\nThis can commonly lead us to assume that the subject survived the full year - and if I’m to borrow a metaphor from a children’s fable, it would have to in this case be Goldilocks. So what we have here is the equivalent of Goldilock’s bed being too soft.\n\n\n\n\n\nUnder this assumption we would include Subject 1 in the proportion numerator and calculate the one-year survival as 3/5 or 60% (we could also get that from a logistic regression model).\n\\[\n\\text{Survival (one-year)} = \\frac{\\text{3}}{\\text{5}} = 60\\%\n\\]\nBut there’s something not quite right about that.\nUnfortunately, Goldilocks doesn’t sleep well."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#this-bed-is-too-hard",
    "href": "posts/021_18Oct_2024/index.html#this-bed-is-too-hard",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.2 This bed is too hard",
    "text": "3.2 This bed is too hard\nWell then, let’s be conservative you might say, and assume Subject 1 has in fact died. In this case we would exclude them from the proportion numerator and calculate the one-year survival as 2/5 or 40%. Can I suggest this is now the equivalent of Goldilock’s bed being too hard, because we know Subject 1 was followed for a full 6 months - that must count for something surely?\n\n\n\n\n\n\\[\n\\text{Survival (one-year)} = \\frac{\\text{2}}{\\text{5}} = 40\\%\n\\]\nPoor Goldilocks still doesn’t sleep well."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#ahhh-just-right.",
    "href": "posts/021_18Oct_2024/index.html#ahhh-just-right.",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.3 Ahhh, just right.",
    "text": "3.3 Ahhh, just right.\nWhen you use survival methods, it in fact turns out that the probability of surviving in the entire year, taking into account censoring is (4/5)x(2/3) = 53%, somewhere in between 40 and 60% - ahhh, Goldilocks has finally found the right bed.\n\n\n\n\n\n\\[\n\\text{Survival (one-year)} = \\frac{\\text{4}}{\\text{5}} \\times \\frac{\\text{2}}{\\text{3}} = 53\\%\n\\] I will show you how to get to that number shortly."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#the-take-home-message",
    "href": "posts/021_18Oct_2024/index.html#the-take-home-message",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.4 The take-home message",
    "text": "3.4 The take-home message\nIf everybody is observed for at least the same length of time of interest (e.g. 1 year), logistic regression could be used…\n… but not if the duration of observation is less for some people."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#ahhh-just-right",
    "href": "posts/021_18Oct_2024/index.html#ahhh-just-right",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.3 Ahhh, just right",
    "text": "3.3 Ahhh, just right\nWhen you use survival methods, it in fact turns out that the probability of surviving in the entire year, taking into account censoring is (4/5)x(2/3) = 53%, somewhere in between 40 and 60% - ahhh, Goldilocks has finally found the right bed.\n\n\n\n\n\n\\[\n\\text{Survival (one-year)} = \\frac{\\text{4}}{\\text{5}} \\times \\frac{\\text{2}}{\\text{3}} = 53\\%\n\\] I will show you how to get to that number shortly."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#questions---so-many-questions",
    "href": "posts/021_18Oct_2024/index.html#questions---so-many-questions",
    "title": "Survival Analysis - Under the Hood",
    "section": "4.1 Questions - so many questions…",
    "text": "4.1 Questions - so many questions…\nSo, let me get you thinking about this by way of a thought exercise.\nIf I were to ask you about human mortality; then as a function of age, how would you draw or describe:\n\nThe probability of death?\nThe hazard of death?\nSurvival?\n\nFollowing from that:\n\nIs the probability of death greater at 80 or 100?\nIs the hazard of death greater at 80 or 100?\n\nAnd finally:\nWhat’s the difference!?"
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#some-answers",
    "href": "posts/021_18Oct_2024/index.html#some-answers",
    "title": "Survival Analysis - Under the Hood",
    "section": "4.2 Some answers",
    "text": "4.2 Some answers\n(Most figures in today’s post created with BioRender.com)."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#some-answershopefully",
    "href": "posts/021_18Oct_2024/index.html#some-answershopefully",
    "title": "Survival Analysis - Under the Hood",
    "section": "4.2 Some answers…hopefully",
    "text": "4.2 Some answers…hopefully\nOk, this is a little bit of a trick question, because it really hangs on semantics.\n\n4.2.1 Marginal probabilities of event\nWhen we talk about the probability of death we are referring to a probability density for death (as a function of age). This is where the area under the density curve (and while this involves a bit of calculus, you don’t need to concern yourself with the details) adds up to 1 - that is everyone is certain to die eventually. In other words the sums of all possible probabilities of dying across all ages equals 1 - and that’s what essentially defines a probability density function. So if you look at the following plot you will appreciate that the overall probability of death gradually increases until about 80 years than actually declines after that. So, in fact the most likely age to die at is about 80, not 100, and that’s simply because fewer people are alive at 100 in the first place. This is what is known as a ‘marginal’ probability of death.\n\n\n\n\n\n\n\n4.2.2 Conditional probabilities of event\nNow, the marginal probability of death is a different concept to the hazard of death. The hazard represents an instantaneous or conditional risk of the event happening. Looking at the commensurate hazard function, you will see that it increases exponentially. What this is saying is that the probability or risk that you die in the next year, increases the longer you remain alive. So, the risk of dying in the next year, given that you survived until 100 years is far, far, greater than than case for an 80 year old. These are conditional probabilities. In other words, the risk in the next instant of experiencing the event given you haven’t yet experienced it.\n\n\n\n\n\n\n\n4.2.3 Survival\nAnd then finally the survival. This isn’t difficult. We all die (don’t thank me for cheering you up) - so the survival function eventually reaches 0."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#definition",
    "href": "posts/021_18Oct_2024/index.html#definition",
    "title": "Survival Analysis - Under the Hood",
    "section": "5.1 Definition",
    "text": "5.1 Definition\nOk, so having some understanding of the hazard rate is really important in survival analysis. At a basic level, the hazard rate is like an incident rate - the number of new events that occur in a period of time divided by the number of people at risk of that event at the beginning of that period. But it gets a little more complicated because the hazard rate is actually the limiting case of that - the instantaneous event rate. In other words the probability that, given that a subject has survived up to some time t, that he or she has the event in the next infinitesimally small time interval, divided by the length of that interval - and we need to use calculus to be able to compute that. The hazard rate ranges from 0 (no risk of the event) to infinity (certain to experience the event) and over time, it can be constant, increase, decrease, or take some other non-monotonic pattern, which I’ll show you in a moment.\nThe non-complicated, non-calculus formula for the hazard rate is shown below: Basically the hazard rate can be thought of as the number of events between time \\(t\\) and \\(\\delta t\\), divided by the number of people at risk of the event at time \\(t\\) - that gives a conditional probability of the event - and then we divide that by the length of the interval.\n\\[\nh(t) = \\lim_{\\delta \\text{t} \\to 0} \\frac{\\left( \\cfrac{ \\text{number of events between time t and } (\\text{t} + \\delta \\text{t})}{\\text{number of people at risk at time t }} \\right) } {\\delta \\text{t}}\n\\]\n\\[\n= \\lim_{\\delta \\text{t} \\to 0} \\frac{\\text{conditional probability of event between time t and } (\\text{t} + \\delta \\text{t})} {\\delta \\text{t}}\n\\]"
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#the-hazard-rate-is-related-to-the-incident-rate",
    "href": "posts/021_18Oct_2024/index.html#the-hazard-rate-is-related-to-the-incident-rate",
    "title": "Survival Analysis - Under the Hood",
    "section": "5.2 The hazard rate is related to the incident rate",
    "text": "5.2 The hazard rate is related to the incident rate\nLet me use a silly example to illustrate this idea of how the hazard rate is related to your run-of-the-mill incident rate. Let’s think about the hazard of getting married at 20 years of age - perilous I might suggest. But jokes aside, how would you work this out? Well we could start by thinking in terms of the incident rate - in a nutshell, count up the number of people married between the ages of 20 and 21 and divide that by the number NOT already married at age 20.\nSo, let’s say we have 1000 people in a population of interest who still aren’t married at age 20 and we follow them forward and see that 100 get married in the next year. The incident rate is then 100/1000 per year or 0.1 marriages per year.\n\n\n\n\n\nWe could have actually considered a shorter time interval to work out the incident rate - let’s now say 6 months instead of one year. Assuming marriages are evenly distributed then the incident rate is now 50/1000 per 6 months or 0.05 marriages per 1/2 year.\n\n\n\n\n\nAnd we could even consider the incident rate in the forthcoming day, which would be 0.274/1000 per day or 0.000274 marriages per day.\n\n\n\n\n\nSo the incident rate differs in these three cases only because we are using different units of time. If we normalised the time period to one year in the second and third scenarios, then we would expect the same number of marriages over the year.\nThe point of all of this is really to show you that the hazard rate is a limiting case of the incident rate. As the time interval approaches zero, we essentially have an instantaneous estimate of the risk of the event at any point in time and this is what the hazard rate represents. It’s not hard when you think about it."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#some-common-hazard-function-shapes",
    "href": "posts/021_18Oct_2024/index.html#some-common-hazard-function-shapes",
    "title": "Survival Analysis - Under the Hood",
    "section": "5.3 Some common hazard function shapes",
    "text": "5.3 Some common hazard function shapes\nLet’s have a look at some of the more common hazard function shapes that we might encounter. The shape of the hazard function is determined by the underlying disease, physiological or physical process - so some of these are more realistic in biological systems and some more realistic in mechanical systems.\n\n5.3.1 Constant (exponential)\nThe first example is of a constant hazard - so the risk of failure at any point in time is the same. This could apply to a light bulb and it could also apply to a healthy individual in a study. When the hazard function is constant we say the survival model is exponential and memoryless, in that the age of the subject has no bearing on the instantaneous risk of failure. Now, this is not the most realistic distribution for biological systems, because - taking a well-known quote from a textbook - “If human lifetimes were exponential there wouldn’t be old or young people, just lucky or unlucky ones”.\n\n\n\n\n\n\n\n5.3.2 Increasing (Weibull)\nThis is a monotone increasing hazard function. This is one of the more realistic shapes that we expect in the real world - basically as things age, they wear out and are more likely to fail. In industry, cars and batteries are good examples of this. Most biological systems also follow this profile - as humans it’s why our life insurance premiums go up as we age. At 60 you’ve done well to have lived this long, but your short term prospects aren’t as optimistic as they were at 20, and the actuaries build this in to their company’s insurance premiums.\nAn example in a medical study might be the hazard of metastatic cancer patients, where the event of interest is death. As survival time increases for such a patient, and as the prognosis accordingly worsens, the patient’s potential for dying of the disease also increases.\n\n\n\n\n\n\n\n5.3.3 Decreasing (Weibull)\nBut not everything wears out over time. This is an example of a monotone decreasing hazard. Many electronic systems actually become more robust over time - believe it or not. In other words, the instantaneous risk of failure tends to be higher earlier on and then subsides with time. So manufacturers can utilise that to their advantage by running the system for a period before shipping and if it passes, it’s likely to be ok for the customer.\nIn medicine we might see this in patients recovering from major surgery, because the potential for dying after surgery usually decreases as the time after surgery increases.\n\n\n\n\n\n\n\n5.3.4 Increasing/Decreasing (lognormal)\nNow we’re getting even more complex with a hazard function that can both increase and decrease. These kinds of shapes become increasingly difficult to generalise, but a specific example might be the hazard for tuberculosis patients since their potential for dying from the disease initially increases and then subsides.\n\n\n\n\n\n\n\n5.3.5 Bathtub\nAnd then finally the ‘bathtub’ shaped hazard function, which I introduced to you before when talking about the risks of humans dying with age. In a general sense, this hazard function reflects the fact that some of the riskiest days of your life are those following birth, and then things start to settle down through and until mid-life.\nBut gradually, as you age, parts of your body start to wear out and your short-term risk begins to increase again - and if you manage to get to 80 for example, then your short-term risk of dying might be as high as it was during your first months of life."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#time-on-study",
    "href": "posts/021_18Oct_2024/index.html#time-on-study",
    "title": "Survival Analysis - Under the Hood",
    "section": "6.1 Time-on-study",
    "text": "6.1 Time-on-study\nI want to illustrate these ideas about time scales with a figure and a very simplified example. Let’s assume this is some hazard function for death related to some fictitious disease. The risk of death related to this disease initially increases and then decreases over time. \nWhen we look at the hazard, it’s a fairly safe bet that we would use time-on-study in this case, because the hazard is going to primarily be a function of time since diagnosis more than a function of age. And that’s because the subject becomes at risk of the outcome from the point of diagnosis. So in this case we can expect subjects of any age to have a similar hazard. If we were to estimate a Cox model for example, the hazards are fairly consistent in shape across subjects, regardless of age (note that the intercepts might actually be different for different ages - gives rise to proportional hazards - but the shapes are consistent). In other words, an apples-to-apples comparison of the hazard function at any point in observation time."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#age",
    "href": "posts/021_18Oct_2024/index.html#age",
    "title": "Survival Analysis - Under the Hood",
    "section": "6.2 Age",
    "text": "6.2 Age\nBut now let’s assume that the same hazard is instead a function of age instead of time-on-study - for the sake of the exercise let’s say the outcome is now a type of cancer and you become ‘at risk’ of this cancer from birth. So the risk of the outcome this time relates more to just getting older than it does to any other precursor event.\n\n\n\n\n\nIn this case, if we were to use time-on-study as the time scale, we end up trying to compare hazard functions among age groups that are not consistent in shape on that time scale. So, the hazard for a 20 year old entering the study might be different to the hazard for a 60 year old entering the study. Now we are attempting an apples-to-oranges comparison of the hazard function at any point in observation time and that is less than ideal - instead, we’re better off using age as the time scale on which to base our computations.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen using age as the time scale for the analysis, age is automatically corrected for in the analysis and does not require specific covariate adjustment."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#non-parametric-kaplan-meier",
    "href": "posts/021_18Oct_2024/index.html#non-parametric-kaplan-meier",
    "title": "Survival Analysis - Under the Hood",
    "section": "7.1 Non-parametric (Kaplan-Meier)",
    "text": "7.1 Non-parametric (Kaplan-Meier)\nSo, in statistics, what does non-parametric actually mean? Basically, we do not rely on any assumptions that the data are drawn from a given parametric family of probability distributions. So we’re less interested or able to make inferences about the population and we’re really only talking about the data that we actually have - the sample at hand.\nNow, in survival analysis, the Kaplan-Meier estimator is one such non-parametric approach to calculating the survival function. We don’t make any assumptions about the distribution of survival times in the population from which the sample was drawn. So, it’s just the empirical probability of surviving past certain times in the sample - and to that end it’s main purpose is descriptive.\nThe Kaplan-Meier approach is NOT a modelling method, and it’s really about visualising survival - and we can do this overall or commonly by splitting based on one important grouping variable, whether that be treatment or some other exposure variable (e.g. sex). Usually this a starting point in your survival analyses, and you’ll go on and do some modelling, but sometimes a simple Kaplan-Meier curve is enough to do what you want.\nLet’s revisit that first example I showed you with the 5 patients, because it’s a worthwhile exercise in understanding how the Kaplan-Meier estimator works out a probability of survival taking into account, censoring. It’s a simple exercise with these data but with more complex datasets you obviously wouldn’t do this manually.\nSo, we have our 5 patients and we want to plot the proportion surviving at every step over the study duration.\nRemember that if we’d used logistic regression, then depending on how we defined our censored subject we could have ended up with a one-year survival of either 40 or 60%, and in fact the correct one-year survival estimate is 53%. How do we get that?\n\n\n\n\n\nSo, we start at time 0 with 100% survival and with all subjects at risk of the event.\nWe know that survival remains at 100% until at least month 4 because everyone remains alive during that time.\nBut then Subject 5 dies at 4 months. So, this is the event we’re interested in and the survival probability is re-estimated each time there is an event. How do we do that? Well, we multiply the cumulative survival proportion before the event by the conditional survival proportion just after the event - conditional because a subject must have survived at least until the event, to remain in the study after it.\nAnd if we do that, this time around it’s a fairly straightforward multiplication of 1 x 4/5, which gives 80%. Note that censoring has not even entered into this calculation yet.\nSo the proportion surviving immediately after 4 months is 80% and the number of people at risk of dying at this point is 4.\n\n\n\n\n\nNow as we go along in the time line we see that Subject 1 is censored at 6 months and we indicate that with a mark at that time point. Note that the survival estimate doesn’t change after 6 months, but the number of people at risk of dying now reduces by 1 to 3.\n\n\n\n\n\nNow Subject 3 dies at 7 months. To re-estimate the survival this time we need to take just a little bit of extra care. The cumulative survival proportion immediately before 7 months is still 80% but when we work out the conditional survival immediately after 7 months, the censoring of Subject 1 means that we now only have 2 people remaining alive (that we are certain about) out of the 3 potentially at risk, just prior to the death.\nWe then multiply those two survival proportions to give us the overall survival after 7 months. And that is 4 people out of 5 (80%) alive immediately prior to and 2 people out of 3 (67%) alive immediately after. The overall estimate of survival at 7 months is then 4/5 times 2/3 which gives us the 53% we stated earlier.\nClearly this is a very simple case and you can appreciate how complicated this can get in larger datasets, but standard functions will take care of the calculations for you in whatever statistical package you’re using.\n\n\n\n\n\n(Most figures in today’s post created with BioRender.com)."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#non-parametric-kaplan-meier-estimator",
    "href": "posts/021_18Oct_2024/index.html#non-parametric-kaplan-meier-estimator",
    "title": "Survival Analysis - Under the Hood",
    "section": "7.1 Non-parametric (Kaplan-Meier estimator)",
    "text": "7.1 Non-parametric (Kaplan-Meier estimator)\nSo, in statistics, what does non-parametric actually mean? Basically, we do not rely on any assumptions that the data are drawn from a given parametric family of probability distributions. So we’re less interested or able to make inferences about the population and we’re really only talking about the data that we actually have - the sample at hand.\nNow, in survival analysis, the Kaplan-Meier estimator is one such non-parametric approach to calculating the survival function. We don’t make any assumptions about the distribution of survival times in the population from which the sample was drawn. So, it’s just the empirical probability of surviving past certain times in the sample - and to that end it’s main purpose is descriptive.\nThe Kaplan-Meier approach is NOT a modelling method, and it’s really about visualising survival - and we can do this overall or commonly by splitting based on one important grouping variable, whether that be treatment or some other exposure variable (e.g. sex). Usually this a starting point in your survival analyses, and you’ll go on and do some modelling, but sometimes a simple Kaplan-Meier curve is enough to do what you want.\nLet’s revisit that first example I showed you with the 5 patients, because it’s a worthwhile exercise in understanding how the Kaplan-Meier estimator works out a probability of survival taking into account, censoring. It’s a simple exercise with these data but with more complex datasets you obviously wouldn’t do this manually.\nSo, we have our 5 patients and we want to plot the proportion surviving at every step over the study duration.\nRemember that if we’d used logistic regression, then depending on how we defined our censored subject we could have ended up with a one-year survival of either 40 or 60%, and in fact the correct one-year survival estimate is 53%. How do we get that?\n\n\n\n\n\nSo, we start at time 0 with 100% survival and with all subjects at risk of the event.\nWe know that survival remains at 100% until at least month 4 because everyone remains alive during that time.\nBut then Subject 5 dies at 4 months. So, this is the event we’re interested in and the survival probability is re-estimated each time there is an event. How do we do that? Well, we multiply the cumulative survival proportion before the event by the conditional survival proportion just after the event - conditional because a subject must have survived at least until the event, to remain in the study after it.\nAnd if we do that, this time around it’s a fairly straightforward multiplication of 1 x 4/5, which gives 80%. Note that censoring has not even entered into this calculation yet.\nSo the proportion surviving immediately after 4 months is 80% and the number of people at risk of dying at this point is 4.\n\n\n\n\n\nNow as we go along in the time line we see that Subject 1 is censored at 6 months and we indicate that with a mark at that time point. Note that the survival estimate doesn’t change after 6 months, but the number of people at risk of dying now reduces by 1 to 3.\n\n\n\n\n\nNow Subject 3 dies at 7 months. To re-estimate the survival this time we need to take just a little bit of extra care. The cumulative survival proportion immediately before 7 months is still 80% but when we work out the conditional survival immediately after 7 months, the censoring of Subject 1 means that we now only have 2 people remaining alive (that we are certain about) out of the 3 potentially at risk, just prior to the death.\nWe then multiply those two survival proportions to give us the overall survival after 7 months. And that is 4 people out of 5 (80%) alive immediately prior to and 2 people out of 3 (67%) alive immediately after. The overall estimate of survival at 7 months is then 4/5 times 2/3 which gives us the 53% we stated earlier.\nClearly this is a very simple case and you can appreciate how complicated this can get in larger datasets, but standard functions will take care of the calculations for you in whatever statistical package you’re using."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#semi-parametric-cox-model",
    "href": "posts/021_18Oct_2024/index.html#semi-parametric-cox-model",
    "title": "Survival Analysis - Under the Hood",
    "section": "7.2 Semi-parametric (Cox model)",
    "text": "7.2 Semi-parametric (Cox model)\nIf you want to estimate survival as a function of more than just one grouping variable, then you really need to look further afield than the Kaplan-Meier estimator and this is usually where people will turn to a Cox model.\nThe Cox model is considered semi-parametric because there is both a non-parametric component and a fully parametric component. The baseline hazard function is non-parametric and is estimated directly from the sample data without any assumptions about the distribution of survival times. Because of this the hazard function in a Cox model can take on any shape you can imagine. This is both an advantage and a disadvantage - an advantage because the shape can be highly flexible, but a disadvantage because we don’t actually get any information returned in terms of parameter estimates that tell us anything about what that shape looks like.\nThe parametric part of the Cox model is to do with the effects of the predictors on the hazard function - and these are assumed to be linearly related to the log of the hazard. The coefficient estimates we get out of a Cox model are exponentiated to give a hazard ratio which is just the ratio of two hazard rates.\nBecause we don’t get any parameter estimates for the baseline hazard function, we aren’t in a position to easily predict the absolute risk of an event, and so the strength of the Cox model is really in providing information about relative rather than absolute risks of the event occurring.\nAnd finally because the hazard is empirical, you really shouldn’t try to predict from a Cox model beyond the range of observable data that you have.\nOk, before we leave the Cox model, let’s talk about The Elephant in the room - proportional hazards. This is probably the most important assumption that you need to keep in mind and one that you should really test for each time you run a Cox model.\nThe proportional hazards assumption basically requires that the hazard ratio is constant over time, or equivalently, that the hazard for one individual is proportional to the hazard for any other individual, and where the proportionality constant doesn’t depend on time.\nSo if we look at the figure below - I have plotted hypothetical hazard functions for males (in blue) and females (in red) for the risk of some event. You’ll note that the shapes are essentially the same but are scaled versions of one another. When proportional hazards hold, then we can take a ratio of the two hazard rates at any time point and that ratio should remain constant. And I’ve illustrated that here at two time points - \\(t1\\) and \\(t2\\). The difference in the hazards obviously varies but the ratio of the hazards at those two time points is the same. At time 1 a hazard rate of 3 for males and 2 for females gives a hazard ratio of 1.5 and similarly, at time 2, a hazard rate of 0.75 for males and 0.5 for females, again gives a hazard ratio of 1.5.\nIn reality, hazard rates are probably not going to be exactly proportional the entire time, but the aim is to make sure any departures from proportionality are minimal - otherwise this means the hazard ratio itself varies with time and this adds further complexity to your model."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#parametric-exponential-weibull-log-normal-etc",
    "href": "posts/021_18Oct_2024/index.html#parametric-exponential-weibull-log-normal-etc",
    "title": "Survival Analysis - Under the Hood",
    "section": "7.3 Parametric (exponential, Weibull, log-normal, etc)",
    "text": "7.3 Parametric (exponential, Weibull, log-normal, etc)\nFully parametric survival models are essentially the same in concept and interpretation to the Cox model, barring a few structural differences. The main difference to the Cox model is that the baseline hazard function is fully specified in these models. In other words, the hazard function is assumed to follow a particular statistical distribution, and we looked at some of these before - monotone increasing, decreasing, lognormal, etc. And this then becomes a modelling choice that you have to make. You can never be certain that you’ve chosen the correct distribution but there are tools that you can use to guide your choice - theory should always be at the top of the list, but you can also do things like generate empirical hazards to see what their shape might look like, compare model fits via some fitting criteria like the AIC, and so on. If you get the distribution about right, these models are more powerful than their Cox counterparts.\nLike Cox models, you get adjusted estimates of risk/survival. Unlike Cox models, you can estimate absolute risk of the event of interest, because you get a model parameter or parameters returned for that. Additionally, you’re in a better position with these models to predict beyond the maximum follow-up time, because there is a specified distribution of survival times that is expected to follow that of the population. But that comes with the usual caveat of always be careful about extrapolating beyond the range of your data."
  },
  {
    "objectID": "posts/021_18Oct_2024/index.html#extensions",
    "href": "posts/021_18Oct_2024/index.html#extensions",
    "title": "Survival Analysis - Under the Hood",
    "section": "7.4 Extensions",
    "text": "7.4 Extensions\n\nTime-dependent covariates:\n\nThe value of the covariate (e.g. treatment, weight, etc) changes over time, but it’s effect remains constant (i.e. the HR doesn’t change).\n\nTime-dependent coefficients:\n\nThe value of the covariate doesn’t change over time, but it’s effect does (i.e. the HR varies over time).\n\nFlexible parametric models:\n\nInclude splines on the log baseline hazard to allow even more flexibility in hazard function shapes.\n\nCompeting risks:\n\nExplicitly account for other events that ‘compete’ with our primary event of interest.\n\nRecurrent events/frailties (clustering)\n\nMore than one event per person/groups of people with shared characteristics\n\n\n(Most figures in today’s post created with BioRender.com)."
  },
  {
    "objectID": "posts/019_04Oct_2024/index.html",
    "href": "posts/019_04Oct_2024/index.html",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "",
    "text": "I know I have touched on confounding before but it’s such an important concept in statistics that I thought I would dedicate a post to it today. There is so much one could talk about on the topic, but my aim is to provide you with some intuition as to what is actually happening when one variable ‘confuses’ the relationship between two others.\nI’m going to use a fairly classic example from epidemiology that examines whether there is an association between drinking coffee (full-cream milk only thanks) and developing coronary heart disease (CHD).\n\n\n\n\n\n\nNote\n\n\n\nIn a sense this is a causal question - i.e. “Does drinking coffee cause CHD?”, but for the sake of the illustration, let’s excuse ourselves from such a question’s fairly strict assumptions for the time being.\n\n\nBeing the good researchers that we are, we give considerable thought as to how to answer our question. That means we don’t forgo our own field-expertise and knowledge at the expense of the data-driven p-value gods. First and foremost, we think about the relationships we are investigating and all the potential factors that could influence those relationships. And that comes from theory, not statistical tests.\nIn examining those knowledge-driven theoretical relationships, we draw a Directed Acyclic Graph (DAG). This is a topic in its own right, but for another post - in the meantime some good primers can be found here and here. A DAG is otherwise more simply known as a causal diagram. It is meant to provide a visual representation of the causal relationships among a set of variables and in doing this forces the researcher to think about their scientific question in a more holistic manner. Indeed, statistical tests aren’t even involved at this point.\nSo, in thinking about our coffee-CHD question, it occurs to us that smoking is known to cause CHD and we wonder whether smoking might also be related to drinking coffee - i.e. do coffee-drinkers also smoke more (or less) than people who prefer don’t drink coffee (tea perhaps?). In this case, a simple DAG might look something like:\n\n\n\n\n\nWhat this DAG helps to summarise is what we are now wondering - is smoking confusing or distorting the association between coffee drinking and the development of CHD? In other words, is smoking a confounder in the association between our exposure (coffee-drinking) and outcome (CHD) variables of interest?\nBefore we take a peek at our data, let’s refresh our memories regarding the three conditions a variable must fulfil to be considered a putative confounder:\n\nThe variable must be associated with both the exposure and the outcome.\nThe variable must be distributed unequally across the exposure (e.g. between treatment groups).\nThe variable must not lie on the causal pathway between the exposure and the outcome (this would be a mediator and requires different treatment)."
  },
  {
    "objectID": "posts/019_04Oct_2024/index.html#background",
    "href": "posts/019_04Oct_2024/index.html#background",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "",
    "text": "I know I have touched on confounding before but it’s such an important concept in statistics that I thought I would dedicate a post to it today. There is so much one could talk about on the topic, but my aim is to provide you with some intuition as to what is actually happening when one variable ‘confuses’ the relationship between two others.\nI’m going to use a fairly classic example from epidemiology that examines whether there is an association between drinking coffee (full-cream milk only thanks) and developing coronary heart disease (CHD).\n\n\n\n\n\n\nNote\n\n\n\nIn a sense this is a causal question - i.e. “Does drinking coffee cause CHD?”, but for the sake of the illustration, let’s excuse ourselves from such a question’s fairly strict assumptions for the time being.\n\n\nBeing the good researchers that we are, we give considerable thought as to how to answer our question. That means we don’t forgo our own field-expertise and knowledge at the expense of the data-driven p-value gods. First and foremost, we think about the relationships we are investigating and all the potential factors that could influence those relationships. And that comes from theory, not statistical tests.\nIn examining those knowledge-driven theoretical relationships, we draw a Directed Acyclic Graph (DAG). This is a topic in its own right, but for another post - in the meantime some good primers can be found here and here. A DAG is otherwise more simply known as a causal diagram. It is meant to provide a visual representation of the causal relationships among a set of variables and in doing this forces the researcher to think about their scientific question in a more holistic manner. Indeed, statistical tests aren’t even involved at this point.\nSo, in thinking about our coffee-CHD question, it occurs to us that smoking is known to cause CHD and we wonder whether smoking might also be related to drinking coffee - i.e. do coffee-drinkers also smoke more (or less) than people who prefer don’t drink coffee (tea perhaps?). In this case, a simple DAG might look something like:\n\n\n\n\n\nWhat this DAG helps to summarise is what we are now wondering - is smoking confusing or distorting the association between coffee drinking and the development of CHD? In other words, is smoking a confounder in the association between our exposure (coffee-drinking) and outcome (CHD) variables of interest?\nBefore we take a peek at our data, let’s refresh our memories regarding the three conditions a variable must fulfil to be considered a putative confounder:\n\nThe variable must be associated with both the exposure and the outcome.\nThe variable must be distributed unequally across the exposure (e.g. between treatment groups).\nThe variable must not lie on the causal pathway between the exposure and the outcome (this would be a mediator and requires different treatment)."
  },
  {
    "objectID": "posts/019_04Oct_2024/index.html#data",
    "href": "posts/019_04Oct_2024/index.html#data",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "2 Data",
    "text": "2 Data\nOk, let’s now take a look at the hypothetical retrospective case-control data we’ll be using today. It consists of 40 observations and three variables:\n\noutcome - did the individual have CHD (case) or not (control).\ncoffee-drinker - was the individual a coffee-drinker or not. This is our exposure variable of interest.\nsmoker - was the individual a smoker or not. This is our potential confounder.\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(ggmosaic)\nlibrary(kableExtra)\nlibrary(janitor)\n\n# Hypothetical data\ny &lt;- factor(c(rep(1, 20), rep(0, 20)), levels = c(0, 1), labels = c(\"control\", \"case\"))\nx1 &lt;- factor(c(rep(1, 10), rep(0, 10), rep(1, 5), rep(0, 15)), levels = c(0, 1), labels = c(\"no\", \"yes\"))\nx2 &lt;- factor(c(rep(1, 9), rep(0, 1), rep(1, 3), rep(0, 7), rep(1, 3), rep(0, 2), rep(1, 1), rep(0, 14)), levels = c(0, 1), labels = c(\"no\", \"yes\"))\ndf &lt;- data.frame(\"outcome\" = y, \"coffee_drinker\" = x1, \"smoker\" = x2)\ndf |&gt; \n  kable(align = \"c\")\n\n\n\n\n\noutcome\ncoffee_drinker\nsmoker\n\n\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nyes\n\n\ncase\nyes\nno\n\n\ncase\nno\nyes\n\n\ncase\nno\nyes\n\n\ncase\nno\nyes\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncase\nno\nno\n\n\ncontrol\nyes\nyes\n\n\ncontrol\nyes\nyes\n\n\ncontrol\nyes\nyes\n\n\ncontrol\nyes\nno\n\n\ncontrol\nyes\nno\n\n\ncontrol\nno\nyes\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno\n\n\ncontrol\nno\nno"
  },
  {
    "objectID": "posts/019_04Oct_2024/index.html#simple-epidemiological-approach",
    "href": "posts/019_04Oct_2024/index.html#simple-epidemiological-approach",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "3 Simple Epidemiological Approach",
    "text": "3 Simple Epidemiological Approach\nGiven the binary nature of all three variables we can explore the relationships nicely using a basic workhorse of epidemiological analysis - the 2x2 contingency table. In its simplest form this shows the frequency cross-tabulation of the exposure with the outcome. However, I think it’s also useful to display the conditional row percentages - in other words, the proportions (probabilities) of each outcome (categories as columns) given a particular exposure (categories as rows). In this way it is easy to eyeball whether the exposure is associated with the outcome without doing any specific test simply by looking at whether the row percentages vary greatly. As a good visual accompaniment for each cross-tabulation, I am also going to generate mosaic plots. These can give you an impression of potential associations without using any actual numbers.\n\n3.1 Crude Odds Ratio - Exposure/Outcome\nThe following cross-tabulation and mosaic plot show the potential association between coffee-drinking and CHD. If we look at the proportions of people with CHD in each exposure group we can see that these do in fact differ - 67% of coffee drinkers develop CHD, compared to 40% of non-coffee drinkers. That is a telling sign of an association before we even do anything. The mosaic plot mirrors these numbers graphically.\n\n\n\nCode\ntab &lt;- df |&gt; \n  tabyl(coffee_drinker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    coffee_drinker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    60% (15) \n    40% (10) \n    100% (25) \n  \n  \n    yes \n    33%  (5) \n    67% (10) \n    100% (15) \n  \n  \n    Total \n    50% (20) \n    50% (20) \n    100% (40) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df\ndf2$coffee_drinker &lt;- relevel(df2$coffee_drinker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, coffee_drinker), fill = coffee_drinker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nWorking out the odds ratio (OR) from a 2x2 contingency table is trivial. We want to divide the odds of having CHD given being a coffee-drinker by the odds of having CHD given being a non-coffee-drinker. That is:\n\\[\\text{OR} = \\frac{\\text{10/5}}{10/15}\\]\nOr equivalently:\n\\[\\text{OR} = \\frac{\\text{10 x 15}}{\\text{5 x 10}} = 3\\] The OR is 3 which indicates a 3-fold increase in the odds of CHD among coffee-drinkers compared to their non-coffee-drinking peers. We consider this a crude or unadjusted effect estimate.\n\n\n3.2 Adjusted Odds Ratios - Exposure/Outcome (via stratification)\nTo this point we haven’t even considered any potential confounding effect of smoking. So how could we incorporate that into the current analysis using 2x2 contingency tables? It’s actually very easy - we just stratify on smoking and generate two contingency tables - one for the association between coffee drinking and CHD in smokers and one for the association between coffee drinking and CHD in non-smokers. It then follows that within each stratum of smoking the effect of smoking is ‘held constant’ and therefore cannot confound the association between coffee drinking and CHD. In other words, this becomes an adjusted ‘effect’ of coffee drinking on CHD and is equivalent to ‘controlling’ for smoking in a multivariable statistical model (which I will demonstrate shortly).\n\n3.2.1 Smokers\nSo, the cross-tabulation for the association between coffee-drinking and CHD in smokers, becomes:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(smoker == \"yes\") |&gt; \n  tabyl(coffee_drinker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    coffee_drinker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    25% (1) \n    75%  (3) \n    100%  (4) \n  \n  \n    yes \n    25% (3) \n    75%  (9) \n    100% (12) \n  \n  \n    Total \n    25% (4) \n    75% (12) \n    100% (16) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(smoker == \"yes\")\ndf2$coffee_drinker &lt;- relevel(df2$coffee_drinker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, coffee_drinker), fill = coffee_drinker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nNote how this time the (conditional row) proportions of people with CHD in each exposure group are more similar (in this contrived example they are the same) - in this subgroup of smokers it doesn’t matter whether you’re a coffee drinker or not - 75% of people have CHD.\nNow, the OR is calculated as:\n\\[\\text{OR} = \\frac{\\text{9 x 1}}{\\text{3 x 3}} = 1\\]\nThe OR is 1 - in other words there is no longer any association between coffee drinking and CHD. We consider this an adjusted OR as we have removed any potential confounding effect of smoking by holding it at a constant value (i.e. everyone is a smoker).\n\n\n3.2.2 Non-smokers\nNow let’s consider the non-smokers. The cross-tabulation for the association between coffee-drinking and CHD in non-smokers is:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(smoker == \"no\") |&gt; \n  tabyl(coffee_drinker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    coffee_drinker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (14) \n    33% (7) \n    100% (21) \n  \n  \n    yes \n    67%  (2) \n    33% (1) \n    100%  (3) \n  \n  \n    Total \n    67% (16) \n    33% (8) \n    100% (24) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(smoker == \"no\")\ndf2$coffee_drinker &lt;- relevel(df2$coffee_drinker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, coffee_drinker), fill = coffee_drinker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nDo you see a pattern here? Again, the conditional row percentages are the same across exposure groups (33%) - so, in this subgroup of non-smokers it also doesn’t matter whether you are a coffee drnker or not, the proportions of CHD are the same.\nThe OR this time is:\n\\[\\text{OR} = \\frac{\\text{1 x 14}}{\\text{7 x 2}} = 1\\]\nThe adjusted OR is again 1 - that is, there is no association between coffee drinking and CHD when we remove any potential confounding effect of smoking by holding it at a (different) constant value (i.e. everyone is a non-smoker).\n\n\n3.2.3 Salient points\nThere are two important points to make in the comparison of the crude and adjusted estimates:\n\nThe adjusted OR is not equal to the crude OR - this suggests confounding is present.\nFurthermore, this represents a special case in which there is complete confounding - the OR reduces to the null value (i.e. 1). I will expand on these points in a moment.\n\n\n\n\n3.3 Crude Odds Ratio - Confounder/Outcome\nTo further illuminate the above ideas let’s repeat the analyses but this time ‘switch’ the exposure and confounder around (hopefully it will become clear why we are doing this as you continue reading). The cross-tabulation and mosaic plot suggest an association between smoking and CHD - 75% of smokers develop CHD, compared to 33% of non-smokers.\n\n\n\nCode\ntab &lt;- df |&gt; \n  tabyl(smoker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    smoker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (16) \n    33%  (8) \n    100% (24) \n  \n  \n    yes \n    25%  (4) \n    75% (12) \n    100% (16) \n  \n  \n    Total \n    50% (20) \n    50% (20) \n    100% (40) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df\ndf2$smoker &lt;- relevel(df2$smoker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, smoker), fill = smoker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nThe OR for the association between smoking and CHD is:\n\\[\\text{OR} = \\frac{\\text{12 x 16}}{\\text{8 x 4}} = 6\\]\nSo the crude OR is 6, which indicates a 6-fold increase in the odds of CHD in smokers compared to non-smokers (ignoring coffee-drinking status).\n\n\n3.4 Adjusted Odds Ratios - Confounder/Outcome (via stratification)\nLet us know compare the adjusted ‘effects’ in coffee-drinkers compared to non-coffee-drinkers.\n\n3.4.1 Coffee-drinkers\nSo, the cross-tabulation for the association between coffee-smoking and CHD in coffee-drinkers, becomes:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(coffee_drinker == \"yes\") |&gt; \n  tabyl(smoker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    smoker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (2) \n    33%  (1) \n    100%  (3) \n  \n  \n    yes \n    25% (3) \n    75%  (9) \n    100% (12) \n  \n  \n    Total \n    33% (5) \n    67% (10) \n    100% (15) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(coffee_drinker == \"yes\")\ndf2$smoker &lt;- relevel(df2$smoker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, smoker), fill = smoker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nAnd the adjusted OR is:\n\\[\\text{OR} = \\frac{\\text{9 x 2}}{\\text{1 x 3}} = 6\\]\nThe OR is 6 - in other words the same as the crude estimate.\n\n\n3.4.2 Non-coffee-drinkers\nNow let’s consider the non-coffee-drinkers. The cross-tabulation for the association between smokers and CHD in non-coffee-drinkers is:\n\n\n\nCode\ntab &lt;- df |&gt; \n  filter(coffee_drinker == \"no\") |&gt; \n  tabyl(smoker, outcome) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; \n  adorn_ns() |&gt; \n  adorn_title(\"combined\")\ntab[1, 2] &lt;- cell_spec(tab[1, 2], bold = T)\ntab[1, 3] &lt;- cell_spec(tab[1, 3], bold = T)\ntab[2, 2] &lt;- cell_spec(tab[2, 2], bold = T)\ntab[2, 3] &lt;- cell_spec(tab[2, 3], bold = T)\ntab |&gt; \n  kable(\"html\", escape = F) |&gt; \n  kable_paper(bootstrap_options = \"hover\", full_width = F)\n\n\n\n \n  \n    smoker/outcome \n    control \n    case \n    Total \n  \n \n\n  \n    no \n    67% (14) \n    33%  (7) \n    100% (21) \n  \n  \n    yes \n    25%  (1) \n    75%  (3) \n    100%  (4) \n  \n  \n    Total \n    60% (15) \n    40% (10) \n    100% (25) \n  \n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- df |&gt; \n  filter(coffee_drinker == \"no\")\ndf2$smoker &lt;- relevel(df2$smoker, ref = \"yes\")\nggplot(data = df2) +\n geom_mosaic(aes(x = product(outcome, smoker), fill = smoker)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_bw(base_size = 40)\n\n\n\n\n\n\n\n\n\n\nAnd the adjusted OR is:\n\\[\\text{OR} = \\frac{\\text{3 x 14}}{\\text{7 x 1}} = 6\\]\nThe OR is again 6! Is this starting to make some sense?\n\n\n3.4.3 Salient points\nAgain, there are two important points to make in the comparison of these crude and adjusted estimates:\n\nIt doesn’t seem to matter whether we adjust for coffee-drinking or not, the association between smoking and CHD remains the same.\nIt then follows that coffee-drinking is not a confounder in the smoking-CHD relationship.\n\n\n\n\n3.5 Adjusted Odds Ratio - Overall\nIt just so happens that the adjusted OR’s from Section 3.2 are the same in each subgroup, but this is an exception rather than a rule. Normally, in the presence of confounding, effect estimates will differ in each subgroup to the crude estimate but will not be equal to each other. What do we do with separate effect estimates from each subgroup - this makes reporting somewhat painful, surely? Well, if the adjusted estimates aren’t too different from each other, they can be combined in a weighted manner to provide an overall summary estimate and this can be done using the Cochran-Mantel-Haenszel (CMH) test. I won’t illustrate this here as in the modern computing age there is really no need to be crunching this statistic anymore, and we use a statistical model instead.\nAs I alluded to above, we should only attempt to combine individual estimates when they are ‘similar’. But what does that mean? How similar should they be and how different can they be? There are no hard and fast rules but I will outline a couple of guidelines at the end of this post. The important thing is to realise that when individual estimates are different, this actually represents an effect modification/interaction effect - that is the strength of the association between the exposure and the outcome depends on the level of the third variable. Interaction effects are of interest (scientifically and clinically) and we shouldn’t try to cancel them out by averaging over them."
  },
  {
    "objectID": "posts/019_04Oct_2024/index.html#statistical-modelling-approach",
    "href": "posts/019_04Oct_2024/index.html#statistical-modelling-approach",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "4 Statistical Modelling Approach",
    "text": "4 Statistical Modelling Approach\nThe cross-tabulation approach is great for expository purposes but is limiting in the types of relationships you can practically explore - things just become unwieldy with variables that contain more than two categories and impossible with continuous variables.\nSo, we tend to use statistical models instead. These make confounder control/adjustment easy - we just need to include the potential confounder in the model along with our exposure of interest. Let’s now replicate everything we have done thus far, but in a modelling-paradigm.\n\n4.1 Crude Odds Ratio - Exposure/Outcome\n\n\nCode\nglm(outcome ~ coffee_drinker, data = df, family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n        no\n—\n—\n\n        yes\n3.00\n0.81, 12.2\n0.11\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nWe get the same OR as in Section 3.1 in addition to a 95% C.I. and p-value.\n\n\n4.2 Adjusted Odds Ratios - Exposure/Outcome (via stratification)\nWe can replicate a stratified effect in our modelling by subsetting the data to select only those people in each smoking subgroup.\n\n4.2.1 Smokers\n\n\nCode\nglm(outcome ~ coffee_drinker, data = subset(df, smoker == \"yes\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n        no\n—\n—\n\n        yes\n1.00\n0.04, 12.2\n&gt;0.9\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 1 (compare to Section 3.2.1)\n\n\n4.2.2 Non-smokers\n\n\nCode\nglm(outcome ~ coffee_drinker, data = subset(df, smoker == \"no\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n        no\n—\n—\n\n        yes\n1.00\n0.04, 12.3\n&gt;0.9\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 1 (compare to Section 3.2.2)\n\n\n\n4.3 Crude Odds Ratio - Confounder/Outcome\n\n\nCode\nglm(outcome ~ smoker, data = df, family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    smoker\n\n\n\n        no\n—\n—\n\n        yes\n6.00\n1.55, 27.5\n0.013\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 6 (compare to Section 3.3)\n\n\n4.4 Adjusted Odds Ratios - Confounder/Outcome (via stratification)\n\n4.4.1 Coffee-drinkers\n\n\nCode\nglm(outcome ~ smoker, data = subset(df, coffee_drinker == \"yes\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    smoker\n\n\n\n        no\n—\n—\n\n        yes\n6.00\n0.43, 161\n0.2\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 6 (compare to Section 3.4.1)\n\n\n4.4.2 Non-coffee-drinkers\n\n\nCode\nglm(outcome ~ smoker, data = subset(df, coffee_drinker == \"no\"), family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    smoker\n\n\n\n        no\n—\n—\n\n        yes\n6.00\n0.64, 134\n0.15\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nOR = 6 (compare to Section 3.4.2)\n\n\n\n4.5 Adjusted Odds Ratio - Overall\nThe above models are all considered univariable in that one predictor only is specified in each model. In controlling or adjusting for a third variable we now produce a multivariable model where both the exposure and potential confounder are specified as predictors. This automatically produces an adjusted ‘effect’ of coffee drinking on CHD, controlling for smoking (and conversely an adjusted ‘effect’ of smoking on CHD, controlling for coffee drinking). We run these kinds of models all the time without thinking, but what the model is doing ‘under the hood’ is calculating a weighted average of the individual subgroup estimates in producing a single coefficient that becomes our effect/s of interest.\n\n\nCode\nglm(outcome ~ coffee_drinker + smoker, data = df, family = \"binomial\") |&gt;\n  tbl_regression(exponentiate = T)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    coffee_drinker\n\n\n\n        no\n—\n—\n\n        yes\n1.00\n0.13, 5.84\n&gt;0.9\n    smoker\n\n\n\n        no\n—\n—\n\n        yes\n6.00\n1.08, 48.2\n0.054\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "posts/019_04Oct_2024/index.html#some-guidelines",
    "href": "posts/019_04Oct_2024/index.html#some-guidelines",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "5 Some Guidelines…",
    "text": "5 Some Guidelines…\nTo consolidate the ideas that we have explored today I want to lay out some very broad guidelines for how to interpret and compare crude and adjusted effects.\n\n5.1 No Confounding or Effect Modification Present\nIf there is neither confounding nor effect modification, the crude estimate of association and the stratum-specific estimates will be similar (converging to being the same in the ideal context). This is reflected in Sections 3.3 and Section 3.4 above.\n\n\n5.2 Only Confounding Present\nIf there is only confounding, the stratum-specific measures of association will be similar to one another, but they will be different from the overall crude estimate (by ~ 10% or more). In this situation, one can use CMH methods to calculate a weighted estimate and p-value, or even easier is to run a statistical model including the confounder as just another covariate. i.e. (in R)\nmod1 &lt;- lm(outcome ~ exposure + confounder, data = dat)\nThis is reflected in Sections 3.1 and Section 3.2 above.\n\n\n5.3 Confounding and/or Effect Modification Present\nIn this case the stratum-specific estimates will differ from one another significantly and these will also differ from the overall crude estimate. These effects should be reported as they are and not weighted and combined (i.e. averaged over) as this is of scientific and clinical interest in its own right. In practical terms in a statistical model, an interaction term should be specified. This changes the coding from:\nmod1 &lt;- lm(outcome ~ exposure + confounder, data = dat)\nto\nmod2 &lt;- lm(outcome ~ exposure * confounder, data = dat)\nAs the two models are ‘nested’, an assessment of whether the interaction term is necessary or not can be performed using a likelihood ratio test:\nanova(mod2, mod1)\nIf the p-value is significant you can conclude that the interaction term increases the explanatory power of the model and should be retained."
  },
  {
    "objectID": "posts/019_04Oct_2024/index.html#the-end",
    "href": "posts/019_04Oct_2024/index.html#the-end",
    "title": "Epi. 101 Lesson - Confounding",
    "section": "6 The End!",
    "text": "6 The End!\nAnother post that has gone on longer than I had anticipated - there is much more I could talk about on the topic but my goal is not to make you fall asleep on your keyboard. Hopefully this has helped to make the concept of confounding just that little bit clearer in your mind. Until next time…"
  },
  {
    "objectID": "posts/020_18Oct_2024/index.html",
    "href": "posts/020_18Oct_2024/index.html",
    "title": "Breaking Free From ‘The Cult of P’",
    "section": "",
    "text": "Apologies to all - I know I have already preached about this in the first WhatsApp message I sent out on ‘Weekly Stats Tips’ about 12 months ago. But given the idea is so important, there’s no permanent and easily-accessible record of that message, and also that in hindsight I think I told you what not to do, but didn’t really suggest what you could/should do - I’m going to make it a blog post here as well."
  },
  {
    "objectID": "posts/020_18Oct_2024/index.html#the-issue",
    "href": "posts/020_18Oct_2024/index.html#the-issue",
    "title": "Breaking Free From ‘The Cult of P’",
    "section": "1 The Issue",
    "text": "1 The Issue\nSo, as researchers how many of you can honestly say that you have never used language around p-values in a way reminiscent of this xkcd comic (or one of these 509 variations on the same theme):\n\n\n\n\n\nI know I can’t. Certainly in my early research career and before changing paths to become a biostatistician, I was guilty of this sort of thing. All in the name of some misguided sense of wanting to achieve research glory by getting that p-value to fall under the all-important threshold of 0.05 (my research glory did not ever materialise by the way).\nWell can I suggest, it just doesn’t matter. And can I also suggest to stop thinking about p-values in this way.\nIt’s generally accepted that Sir Ronald Fisher was the guy who at least formalised the ideas of Null Hypothesis Significance Testing (NHST) and the p-value in the 1920’s. He never intended for the p-value of 0.05 to be set in stone as an arbiter of the value of a piece of research in a statistical sense, and certainly not in a scientific sense. The intention was to use it to guide decision making, not make the actual decision. But, in the past 100 years this arbitrary threshold of p = 0.05 has not only stuck, but taken on an almost mythic status in the research community, in part it seems because humans are lazy and like to avoid decision-making where possible. The natural and silly extension to this entrenched notion of “significance” is that researchers may feel elated if their statistical test returns p = 0.049 and despondent if it returns p = 0.051. On reflection, any rational person can see that’s crazy, but it doesn’t seem to give us pause for thought when we have our results in front of us."
  },
  {
    "objectID": "posts/020_18Oct_2024/index.html#what-not-to-do",
    "href": "posts/020_18Oct_2024/index.html#what-not-to-do",
    "title": "Breaking Free From ‘The Cult of P’",
    "section": "2 What Not To Do",
    "text": "2 What Not To Do\nYou may be aware that there has been a push in recent years by the statistical community to abandon the p-value all together, set off in large part by a statement published by the American Statistical Association in 2016. There is nothing new in any of this - the same issues have been of concern for decades and a Pubmed (or even Google) search will reveal a large literature on the topic. In 2019, The American Statistician dedicated an entire special issue to statistical inference entitled “Statistical Inference in the 21st Century: A World Beyond p &lt; 0.05.” If you scan down those titles you’ll see that not too many are in favour of retaining the 0.05 dichotomy we seem to have imposed on ourselves. The Editorial in that edition doesn’t pull any punches in terms of how to use p-values in your research:\n\nDon’t base your conclusions solely on whether an association or effect was found to be “statistically significant” (i.e., the p-value passed some arbitrary threshold such as p &lt; 0.05).\nDon’t believe that an association or effect exists just because it was statistically significant.\nDon’t believe that an association or effect is absent just because it was not statistically significant.\nDon’t believe that your p-value gives the probability that chance alone produced the observed association or effect or the probability that your test hypothesis is true.\nDon’t conclude anything about scientific or practical importance based on statistical significance (or lack thereof).\n\nAnd the authors go further to say:\nThe ASA Statement on P-Values and Statistical Significance stopped just short of recommending that declarations of “statistical significance” be abandoned. We take that step here. We conclude, based on our review of the articles in this special issue and the broader literature, that it is time to stop using the term “statistically significant” entirely. Nor should variants such as “significantly different,” “p &lt; 0.05,” and “nonsignificant” survive, whether expressed in words, by asterisks in a table, or in some other way.\nPretty damning!"
  },
  {
    "objectID": "posts/020_18Oct_2024/index.html#what-you-couldshould-do",
    "href": "posts/020_18Oct_2024/index.html#what-you-couldshould-do",
    "title": "Breaking Free From ‘The Cult of P’",
    "section": "3 What You Could/Should Do",
    "text": "3 What You Could/Should Do\nIf you look at some of the papers in that special issue, you will find a wealth of suggestions about how to approach a “Post ‘p &lt; 0.05’ world”, including that we should all become Bayesians (not meaning to toot my own trumpet, but I have also written around this in one of my not-so-highly-cited papers several years ago - I’m sure it will gain traction in years to come!).\nSo I’m not going to reiterate all of those suggestions here. Instead, for what my opinion is worth, I have two practical tips that I think would make interpreting and reporting our research more robust and less reliant on the p-value.\n\n3.1 Use A Language Of ‘Evidence’\nWe can still calculate p-values as we currently do - nothing needs to change. But instead of referring to the p-value itself, let’s start discussing our results in the context of ‘evidence’ (i.e. against the null hypothesis of no effect). The following table from this paper sums it up beautifully and so I am not going to reinvent the wheel (in fact I can thoroughly recommend the other 3 papers in the series, so do give them a look if you have time).\n\n\n\n\n\nNow, I have used my own subtle variation on the terminology over the years and replaced “insufficient” with “weak”, “some” with “moderate” and “overwhelming” with “very strong”. The point is, it doesn’t really matter what words you use to describe your ‘effect’ with - this kind of language immediately frees you from the confines of an arbitrary dichotomisation that forces you to either value or devalue all of your hard work in one fell swoop. Instead of saying ‘there was a trend towards statistical significance for the association between x and y’, if your p-value turns out to be 0.051, you can now simply say there was ‘weak evidence for an association between x and y’, instead.\n\n\n3.2 Be Consistent In p-Value Reporting\nIf I had a dollar for every time a research student has sent me results that contain p-values between 0.045 and 0.05 and reported that to 3 decimal places, I’d be a little less poor than I am now. Seriously, there is no need to do this. Please adhere to the following reporting guide and don’t try to put lipstick on a pig by suggesting that your p-value of 0.049 is any different to 0.05.\n\n\n\n\n\nThese are a couple of small changes we can make in our research reporting practice that will help to break our self-imposed binds to the p-value gods. Until next time…"
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html",
    "href": "posts/021_01Nov_2024/index.html",
    "title": "Survival Analysis - Under the Hood",
    "section": "",
    "text": "Welcome to today’s post which is based on a talk that some of you may have heard me give in lab meetings. A warning in advance - it’s a bit of a lengthy one, so maybe down a couple of reds to steel yourself before you start. We’re going to discuss survival analysis, but in a similar spirit to how I presented the post on logistic regression a couple of months ago - nothing too fancy, but with an aim that you understand what is happening ‘under the hood’ when you next fire up your stats software to plot a Kaplan-Meier curve or run a Cox model.\nSurvival analysis is a BIG topic and in university statistics courses, it tends to have an entire subject dedicted just to it, so it’s not just taught as a part of regression modelling, for example. I also don’t find it particularly easy, even now - there are a lot of challenging concepts to understand, and that is in large part because time is such an integral component of everything you do in this area of biostatistics. So, what I hope I can do today is to take a broad-brush approach to survival methods, touch on the concepts that I think are most important in establishing a good base understanding, build in a little intuition for these, and also give you some practical tips along the way.\nLet’s get started."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#calendar-time-format",
    "href": "posts/021_01Nov_2024/index.html#calendar-time-format",
    "title": "Survival Analysis - Under the Hood",
    "section": "2.1 Calendar-time format",
    "text": "2.1 Calendar-time format\nWhat does survival data look like? Well - in the case of a prospective study, subjects are usually recruited into the study at different times. To illustrate this, let’s consider this hypothetical dataset of patients diagnosed with some disease that we follow until they either die or are right-censored. The study was planned to recruit for 4 months from Dec 2015 to Mar 2016, and during this period 5 subjects were entered into the study, all starting at various times, but importantly, the exact dates of their start were recorded. All subjects were then followed at regular intervals, and the study was then ended, as planned, in Jan 2017.\nNow, this is showing the observation times for each subject as they occurred in calendar-time format. While this reflects the reality of how survival data are collected, it doesn’t really inform you with an intuitive sense of time because you can’t easily make comparisons across subjects."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#time-on-study-format",
    "href": "posts/021_01Nov_2024/index.html#time-on-study-format",
    "title": "Survival Analysis - Under the Hood",
    "section": "2.2 Time-on-study format",
    "text": "2.2 Time-on-study format\nSo, for easier interpretation we can align the start time of all subjects like so - and we call this format, ‘time-on-study’. We can see that Subject 1 dropped out because they moved interstate and couldn’t attend clinic appointments anymore, and so they are considered censored. Their follow-up time was 6 months. Subjects 2 and 4 are also censored, but for a different reason - they were still alive when we decided to end the study and had the study gone on we may have been able to follow them for longer. In any case they were each observed for 12 months. Unfortunately, the other two subjects weren’t so lucky - Subject 3 died after 7 months and Subject 5 died after 4 months of being in the study.\n\n\n\n\n\nNow, what if we were interested in working out the one-year survival for this sample of patients?\nHow would you do that?"
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#this-bed-is-too-soft",
    "href": "posts/021_01Nov_2024/index.html#this-bed-is-too-soft",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.1 This bed is too soft",
    "text": "3.1 This bed is too soft\nThis can commonly lead us to assume that the subject survived the full year - and if I’m to borrow a metaphor from a children’s fable, it would have to in this case be Goldilocks. So what we have here is the equivalent of Goldilock’s bed being too soft.\n\n\n\n\n\nUnder this assumption we would include Subject 1 in the proportion numerator and calculate the one-year survival as 3/5 or 60% (we could also get that from a logistic regression model).\n\\[\n\\text{Survival (one-year)} = \\frac{\\text{3}}{\\text{5}} = 60\\%\n\\]\nBut there’s something not quite right about that.\nUnfortunately, Goldilocks doesn’t sleep well."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#this-bed-is-too-hard",
    "href": "posts/021_01Nov_2024/index.html#this-bed-is-too-hard",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.2 This bed is too hard",
    "text": "3.2 This bed is too hard\nWell then, let’s be conservative you might say, and assume Subject 1 has in fact died. In this case we would exclude them from the proportion numerator and calculate the one-year survival as 2/5 or 40%. Can I suggest this is now the equivalent of Goldilock’s bed being too hard, because we know Subject 1 was followed for a full 6 months - that must count for something surely?\n\n\n\n\n\n\\[\n\\text{Survival (one-year)} = \\frac{\\text{2}}{\\text{5}} = 40\\%\n\\]\nPoor Goldilocks still doesn’t sleep well."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#ahhh-just-right",
    "href": "posts/021_01Nov_2024/index.html#ahhh-just-right",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.3 Ahhh, just right",
    "text": "3.3 Ahhh, just right\nWhen you use survival methods, it in fact turns out that the probability of surviving in the entire year, taking into account censoring is (4/5)x(2/3) = 53%, somewhere in between 40 and 60% - ahhh, Goldilocks has finally found the right bed.\n\n\n\n\n\n\\[\n\\text{Survival (one-year)} = \\frac{\\text{4}}{\\text{5}} \\times \\frac{\\text{2}}{\\text{3}} = 53\\%\n\\] I will show you how to get to that number shortly."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#the-take-home-message",
    "href": "posts/021_01Nov_2024/index.html#the-take-home-message",
    "title": "Survival Analysis - Under the Hood",
    "section": "3.4 The take-home message",
    "text": "3.4 The take-home message\nIf everybody is observed for at least the same length of time of interest (e.g. 1 year), logistic regression could be used…\n… but not if the duration of observation is less for some people."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#questions---so-many-questions",
    "href": "posts/021_01Nov_2024/index.html#questions---so-many-questions",
    "title": "Survival Analysis - Under the Hood",
    "section": "4.1 Questions - so many questions…",
    "text": "4.1 Questions - so many questions…\nSo, let me get you thinking about this by way of a thought exercise.\nIf I were to ask you about human mortality; then as a function of age, how would you draw or describe:\n\nThe probability of death?\nThe hazard of death?\nSurvival?\n\nFollowing from that:\n\nIs the probability of death greater at 80 or 100?\nIs the hazard of death greater at 80 or 100?\n\nAnd finally:\nWhat’s the difference!?"
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#some-answershopefully",
    "href": "posts/021_01Nov_2024/index.html#some-answershopefully",
    "title": "Survival Analysis - Under the Hood",
    "section": "4.2 Some answers…hopefully",
    "text": "4.2 Some answers…hopefully\nOk, this is a little bit of a trick question, because it really hangs on semantics.\n\n4.2.1 Marginal probabilities of event\nWhen we talk about the probability of death we are referring to a probability density for death (as a function of age). This is where the area under the density curve (and while this involves a bit of calculus, you don’t need to concern yourself with the details) adds up to 1 - that is everyone is certain to die eventually. In other words the sums of all possible probabilities of dying across all ages equals 1 - and that’s what essentially defines a probability density function. So if you look at the following plot you will appreciate that the overall probability of death gradually increases until about 80 years than actually declines after that. So, in fact the most likely age to die at is about 80, not 100, and that’s simply because fewer people are alive at 100 in the first place. This is what is known as a ‘marginal’ probability of death.\n\n\n\n\n\n\n\n4.2.2 Conditional probabilities of event\nNow, the marginal probability of death is a different concept to the hazard of death. The hazard represents an instantaneous or conditional risk of the event happening. Looking at the commensurate hazard function, you will see that it increases exponentially. What this is saying is that the probability or risk that you die in the next year, increases the longer you remain alive. So, the risk of dying in the next year, given that you survived until 100 years is far, far, greater than than case for an 80 year old. These are conditional probabilities. In other words, the risk in the next instant of experiencing the event given you haven’t yet experienced it.\n\n\n\n\n\n\n\n4.2.3 Survival\nAnd then finally the survival. This isn’t difficult. We all die (don’t thank me for cheering you up) - so the survival function eventually reaches 0."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#definition",
    "href": "posts/021_01Nov_2024/index.html#definition",
    "title": "Survival Analysis - Under the Hood",
    "section": "5.1 Definition",
    "text": "5.1 Definition\nOk, so having some understanding of the hazard rate is really important in survival analysis. At a basic level, the hazard rate is like an incident rate - the number of new events that occur in a period of time divided by the number of people at risk of that event at the beginning of that period. But it gets a little more complicated because the hazard rate is actually the limiting case of that - the instantaneous event rate. In other words the probability that, given that a subject has survived up to some time t, that he or she has the event in the next infinitesimally small time interval, divided by the length of that interval - and we need to use calculus to be able to compute that. The hazard rate ranges from 0 (no risk of the event) to infinity (certain to experience the event) and over time, it can be constant, increase, decrease, or take some other non-monotonic pattern, which I’ll show you in a moment.\nThe non-complicated, non-calculus formula for the hazard rate is shown below: Basically the hazard rate can be thought of as the number of events between time \\(t\\) and \\(\\delta t\\), divided by the number of people at risk of the event at time \\(t\\) - that gives a conditional probability of the event - and then we divide that by the length of the interval.\n\\[\nh(t) = \\lim_{\\delta \\text{t} \\to 0} \\frac{\\left( \\cfrac{ \\text{number of events between time t and } (\\text{t} + \\delta \\text{t})}{\\text{number of people at risk at time t }} \\right) } {\\delta \\text{t}}\n\\]\n\\[\n= \\lim_{\\delta \\text{t} \\to 0} \\frac{\\text{conditional probability of event between time t and } (\\text{t} + \\delta \\text{t})} {\\delta \\text{t}}\n\\]"
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#the-hazard-rate-is-related-to-the-incident-rate",
    "href": "posts/021_01Nov_2024/index.html#the-hazard-rate-is-related-to-the-incident-rate",
    "title": "Survival Analysis - Under the Hood",
    "section": "5.2 The hazard rate is related to the incident rate",
    "text": "5.2 The hazard rate is related to the incident rate\nLet me use a silly example to illustrate this idea of how the hazard rate is related to your run-of-the-mill incident rate. Let’s think about the hazard of getting married at 20 years of age - perilous I might suggest. But jokes aside, how would you work this out? Well we could start by thinking in terms of the incident rate - in a nutshell, count up the number of people married between the ages of 20 and 21 and divide that by the number NOT already married at age 20.\nSo, let’s say we have 1000 people in a population of interest who still aren’t married at age 20 and we follow them forward and see that 100 get married in the next year. The incident rate is then 100/1000 per year or 0.1 marriages per year.\n\n\n\n\n\nWe could have actually considered a shorter time interval to work out the incident rate - let’s now say 6 months instead of one year. Assuming marriages are evenly distributed then the incident rate is now 50/1000 per 6 months or 0.05 marriages per 1/2 year.\n\n\n\n\n\nAnd we could even consider the incident rate in the forthcoming day, which would be 0.274/1000 per day or 0.000274 marriages per day.\n\n\n\n\n\nSo the incident rate differs in these three cases only because we are using different units of time. If we normalised the time period to one year in the second and third scenarios, then we would expect the same number of marriages over the year.\nThe point of all of this is really to show you that the hazard rate is a limiting case of the incident rate. As the time interval approaches zero, we essentially have an instantaneous estimate of the risk of the event at any point in time and this is what the hazard rate represents. It’s not hard when you think about it."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#some-common-hazard-function-shapes",
    "href": "posts/021_01Nov_2024/index.html#some-common-hazard-function-shapes",
    "title": "Survival Analysis - Under the Hood",
    "section": "5.3 Some common hazard function shapes",
    "text": "5.3 Some common hazard function shapes\nLet’s have a look at some of the more common hazard function shapes that we might encounter. The shape of the hazard function is determined by the underlying disease, physiological or physical process - so some of these are more realistic in biological systems and some more realistic in mechanical systems.\n\n5.3.1 Constant (exponential)\nThe first example is of a constant hazard - so the risk of failure at any point in time is the same. This could apply to a light bulb and it could also apply to a healthy individual in a study. When the hazard function is constant we say the survival model is exponential and memoryless, in that the age of the subject has no bearing on the instantaneous risk of failure. Now, this is not the most realistic distribution for biological systems, because - taking a well-known quote from a textbook - “If human lifetimes were exponential there wouldn’t be old or young people, just lucky or unlucky ones”.\n\n\n\n\n\n\n\n5.3.2 Increasing (Weibull)\nThis is a monotone increasing hazard function. This is one of the more realistic shapes that we expect in the real world - basically as things age, they wear out and are more likely to fail. In industry, cars and batteries are good examples of this. Most biological systems also follow this profile - as humans it’s why our life insurance premiums go up as we age. At 60 you’ve done well to have lived this long, but your short term prospects aren’t as optimistic as they were at 20, and the actuaries build this in to their company’s insurance premiums.\nAn example in a medical study might be the hazard of metastatic cancer patients, where the event of interest is death. As survival time increases for such a patient, and as the prognosis accordingly worsens, the patient’s potential for dying of the disease also increases.\n\n\n\n\n\n\n\n5.3.3 Decreasing (Weibull)\nBut not everything wears out over time. This is an example of a monotone decreasing hazard. Many electronic systems actually become more robust over time - believe it or not. In other words, the instantaneous risk of failure tends to be higher earlier on and then subsides with time. So manufacturers can utilise that to their advantage by running the system for a period before shipping and if it passes, it’s likely to be ok for the customer.\nIn medicine we might see this in patients recovering from major surgery, because the potential for dying after surgery usually decreases as the time after surgery increases.\n\n\n\n\n\n\n\n5.3.4 Increasing/Decreasing (lognormal)\nNow we’re getting even more complex with a hazard function that can both increase and decrease. These kinds of shapes become increasingly difficult to generalise, but a specific example might be the hazard for tuberculosis patients since their potential for dying from the disease initially increases and then subsides.\n\n\n\n\n\n\n\n5.3.5 Bathtub\nAnd then finally the ‘bathtub’ shaped hazard function, which I introduced to you before when talking about the risks of humans dying with age. In a general sense, this hazard function reflects the fact that some of the riskiest days of your life are those following birth, and then things start to settle down through and until mid-life.\nBut gradually, as you age, parts of your body start to wear out and your short-term risk begins to increase again - and if you manage to get to 80 for example, then your short-term risk of dying might be as high as it was during your first months of life."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#time-on-study",
    "href": "posts/021_01Nov_2024/index.html#time-on-study",
    "title": "Survival Analysis - Under the Hood",
    "section": "6.1 Time-on-study",
    "text": "6.1 Time-on-study\nI want to illustrate these ideas about time scales with a figure and a very simplified example. Let’s assume this is some hazard function for death related to some fictitious disease. The risk of death related to this disease initially increases and then decreases over time. \nWhen we look at the hazard, it’s a fairly safe bet that we would use time-on-study in this case, because the hazard is going to primarily be a function of time since diagnosis more than a function of age. And that’s because the subject becomes at risk of the outcome from the point of diagnosis. So in this case we can expect subjects of any age to have a similar hazard. If we were to estimate a Cox model for example, the hazards are fairly consistent in shape across subjects, regardless of age (note that the intercepts might actually be different for different ages - gives rise to proportional hazards - but the shapes are consistent). In other words, an apples-to-apples comparison of the hazard function at any point in observation time."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#age",
    "href": "posts/021_01Nov_2024/index.html#age",
    "title": "Survival Analysis - Under the Hood",
    "section": "6.2 Age",
    "text": "6.2 Age\nBut now let’s assume that the same hazard is instead a function of age instead of time-on-study - for the sake of the exercise let’s say the outcome is now a type of cancer and you become ‘at risk’ of this cancer from birth. So the risk of the outcome this time relates more to just getting older than it does to any other precursor event.\n\n\n\n\n\nIn this case, if we were to use time-on-study as the time scale, we end up trying to compare hazard functions among age groups that are not consistent in shape on that time scale. So, the hazard for a 20 year old entering the study might be different to the hazard for a 60 year old entering the study. Now we are attempting an apples-to-oranges comparison of the hazard function at any point in observation time and that is less than ideal - instead, we’re better off using age as the time scale on which to base our computations.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen using age as the time scale for the analysis, age is automatically corrected for in the analysis and does not require specific covariate adjustment."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#non-parametric-kaplan-meier-estimator",
    "href": "posts/021_01Nov_2024/index.html#non-parametric-kaplan-meier-estimator",
    "title": "Survival Analysis - Under the Hood",
    "section": "7.1 Non-parametric (Kaplan-Meier estimator)",
    "text": "7.1 Non-parametric (Kaplan-Meier estimator)\nSo, in statistics, what does non-parametric actually mean? Basically, we do not rely on any assumptions that the data are drawn from a given parametric family of probability distributions. So we’re less interested or able to make inferences about the population and we’re really only talking about the data that we actually have - the sample at hand.\nNow, in survival analysis, the Kaplan-Meier estimator is one such non-parametric approach to calculating the survival function. We don’t make any assumptions about the distribution of survival times in the population from which the sample was drawn. So, it’s just the empirical probability of surviving past certain times in the sample - and to that end it’s main purpose is descriptive.\nThe Kaplan-Meier approach is NOT a modelling method, and it’s really about visualising survival - and we can do this overall or commonly by splitting based on one important grouping variable, whether that be treatment or some other exposure variable (e.g. sex). Usually this a starting point in your survival analyses, and you’ll go on and do some modelling, but sometimes a simple Kaplan-Meier curve is enough to do what you want.\nLet’s revisit that first example I showed you with the 5 patients, because it’s a worthwhile exercise in understanding how the Kaplan-Meier estimator works out a probability of survival taking into account, censoring. It’s a simple exercise with these data but with more complex datasets you obviously wouldn’t do this manually.\nSo, we have our 5 patients and we want to plot the proportion surviving at every step over the study duration.\nRemember that if we’d used logistic regression, then depending on how we defined our censored subject we could have ended up with a one-year survival of either 40 or 60%, and in fact the correct one-year survival estimate is 53%. How do we get that?\n\n\n\n\n\nSo, we start at time 0 with 100% survival and with all subjects at risk of the event.\nWe know that survival remains at 100% until at least month 4 because everyone remains alive during that time.\nBut then Subject 5 dies at 4 months. So, this is the event we’re interested in and the survival probability is re-estimated each time there is an event. How do we do that? Well, we multiply the cumulative survival proportion before the event by the conditional survival proportion just after the event - conditional because a subject must have survived at least until the event, to remain in the study after it.\nAnd if we do that, this time around it’s a fairly straightforward multiplication of 1 x 4/5, which gives 80%. Note that censoring has not even entered into this calculation yet.\nSo the proportion surviving immediately after 4 months is 80% and the number of people at risk of dying at this point is 4.\n\n\n\n\n\nNow as we go along in the time line we see that Subject 1 is censored at 6 months and we indicate that with a mark at that time point. Note that the survival estimate doesn’t change after 6 months, but the number of people at risk of dying now reduces by 1 to 3.\n\n\n\n\n\nNow Subject 3 dies at 7 months. To re-estimate the survival this time we need to take just a little bit of extra care. The cumulative survival proportion immediately before 7 months is still 80% but when we work out the conditional survival immediately after 7 months, the censoring of Subject 1 means that we now only have 2 people remaining alive (that we are certain about) out of the 3 potentially at risk, just prior to the death.\nWe then multiply those two survival proportions to give us the overall survival after 7 months. And that is 4 people out of 5 (80%) alive immediately prior to and 2 people out of 3 (67%) alive immediately after. The overall estimate of survival at 7 months is then 4/5 times 2/3 which gives us the 53% we stated earlier.\nClearly this is a very simple case and you can appreciate how complicated this can get in larger datasets, but standard functions will take care of the calculations for you in whatever statistical package you’re using."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#semi-parametric-cox-model",
    "href": "posts/021_01Nov_2024/index.html#semi-parametric-cox-model",
    "title": "Survival Analysis - Under the Hood",
    "section": "7.2 Semi-parametric (Cox model)",
    "text": "7.2 Semi-parametric (Cox model)\nIf you want to estimate survival as a function of more than just one grouping variable, then you really need to look further afield than the Kaplan-Meier estimator and this is usually where people will turn to a Cox model.\nThe Cox model is considered semi-parametric because there is both a non-parametric component and a fully parametric component. The baseline hazard function is non-parametric and is estimated directly from the sample data without any assumptions about the distribution of survival times. Because of this the hazard function in a Cox model can take on any shape you can imagine. This is both an advantage and a disadvantage - an advantage because the shape can be highly flexible, but a disadvantage because we don’t actually get any information returned in terms of parameter estimates that tell us anything about what that shape looks like.\nThe parametric part of the Cox model is to do with the effects of the predictors on the hazard function - and these are assumed to be linearly related to the log of the hazard. The coefficient estimates we get out of a Cox model are exponentiated to give a hazard ratio which is just the ratio of two hazard rates.\nBecause we don’t get any parameter estimates for the baseline hazard function, we aren’t in a position to easily predict the absolute risk of an event, and so the strength of the Cox model is really in providing information about relative rather than absolute risks of the event occurring.\nAnd finally because the hazard is empirical, you really shouldn’t try to predict from a Cox model beyond the range of observable data that you have.\nOk, before we leave the Cox model, let’s talk about The Elephant in the room - proportional hazards. This is probably the most important assumption that you need to keep in mind and one that you should really test for each time you run a Cox model.\nThe proportional hazards assumption basically requires that the hazard ratio is constant over time, or equivalently, that the hazard for one individual is proportional to the hazard for any other individual, and where the proportionality constant doesn’t depend on time.\nSo if we look at the figure below - I have plotted hypothetical hazard functions for males (in blue) and females (in red) for the risk of some event. You’ll note that the shapes are essentially the same but are scaled versions of one another. When proportional hazards hold, then we can take a ratio of the two hazard rates at any time point and that ratio should remain constant. And I’ve illustrated that here at two time points - \\(t1\\) and \\(t2\\). The difference in the hazards obviously varies but the ratio of the hazards at those two time points is the same. At time 1 a hazard rate of 3 for males and 2 for females gives a hazard ratio of 1.5 and similarly, at time 2, a hazard rate of 0.75 for males and 0.5 for females, again gives a hazard ratio of 1.5.\nIn reality, hazard rates are probably not going to be exactly proportional the entire time, but the aim is to make sure any departures from proportionality are minimal - otherwise this means the hazard ratio itself varies with time and this adds further complexity to your model."
  },
  {
    "objectID": "posts/021_01Nov_2024/index.html#parametric-exponential-weibull-log-normal-etc",
    "href": "posts/021_01Nov_2024/index.html#parametric-exponential-weibull-log-normal-etc",
    "title": "Survival Analysis - Under the Hood",
    "section": "7.3 Parametric (exponential, Weibull, log-normal, etc)",
    "text": "7.3 Parametric (exponential, Weibull, log-normal, etc)\nFully parametric survival models are essentially the same in concept and interpretation to the Cox model, barring a few structural differences. The main difference to the Cox model is that the baseline hazard function is fully specified in these models. In other words, the hazard function is assumed to follow a particular statistical distribution, and we looked at some of these before - monotone increasing, decreasing, lognormal, etc. And this then becomes a modelling choice that you have to make. You can never be certain that you’ve chosen the correct distribution but there are tools that you can use to guide your choice - theory should always be at the top of the list, but you can also do things like generate empirical hazards to see what their shape might look like, compare model fits via some fitting criteria like the AIC, and so on. If you get the distribution about right, these models are more powerful than their Cox counterparts.\nLike Cox models, you get adjusted estimates of risk/survival. Unlike Cox models, you can estimate absolute risk of the event of interest, because you get a model parameter or parameters returned for that. Additionally, you’re in a better position with these models to predict beyond the maximum follow-up time, because there is a specified distribution of survival times that is expected to follow that of the population. But that comes with the usual caveat of always be careful about extrapolating beyond the range of your data."
  }
]