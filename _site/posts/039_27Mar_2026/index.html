<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2026-03-27">
<meta name="description" content="To adjust, or not to adjust? - that is the question.">

<title>Test Site 2 - Multiple Comparisons</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Test Site 2</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">All Posts</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#the-core-problem-why-multiple-testing-matters" id="toc-the-core-problem-why-multiple-testing-matters" class="nav-link" data-scroll-target="#the-core-problem-why-multiple-testing-matters"><span class="header-section-number">2</span> The Core Problem: Why Multiple Testing Matters</a></li>
  <li><a href="#what-error-rate-are-you-trying-to-control" id="toc-what-error-rate-are-you-trying-to-control" class="nav-link" data-scroll-target="#what-error-rate-are-you-trying-to-control"><span class="header-section-number">3</span> What Error Rate Are You Trying to Control?</a>
  <ul>
  <li><a href="#family-wise-error-rate-fwer" id="toc-family-wise-error-rate-fwer" class="nav-link" data-scroll-target="#family-wise-error-rate-fwer"><span class="header-section-number">3.1</span> Family-Wise Error Rate (FWER)</a></li>
  <li><a href="#false-discovery-rate-fdr" id="toc-false-discovery-rate-fdr" class="nav-link" data-scroll-target="#false-discovery-rate-fdr"><span class="header-section-number">3.2</span> False Discovery Rate (FDR)</a></li>
  <li><a href="#no-formal-error-rate-exploratory-contexts" id="toc-no-formal-error-rate-exploratory-contexts" class="nav-link" data-scroll-target="#no-formal-error-rate-exploratory-contexts"><span class="header-section-number">3.3</span> No Formal Error Rate (Exploratory Contexts)</a></li>
  </ul></li>
  <li><a href="#what-is-a-family-of-tests" id="toc-what-is-a-family-of-tests" class="nav-link" data-scroll-target="#what-is-a-family-of-tests"><span class="header-section-number">4</span> What Is a “Family” of Tests?</a></li>
  <li><a href="#when-you-should-adjust-for-multiple-testing" id="toc-when-you-should-adjust-for-multiple-testing" class="nav-link" data-scroll-target="#when-you-should-adjust-for-multiple-testing"><span class="header-section-number">5</span> When You Should Adjust for Multiple Testing</a></li>
  <li><a href="#how-to-adjust-practical-guidance" id="toc-how-to-adjust-practical-guidance" class="nav-link" data-scroll-target="#how-to-adjust-practical-guidance"><span class="header-section-number">6</span> How to Adjust: Practical Guidance</a></li>
  <li><a href="#when-you-should-not-adjust" id="toc-when-you-should-not-adjust" class="nav-link" data-scroll-target="#when-you-should-not-adjust"><span class="header-section-number">7</span> When You Should Not Adjust</a></li>
  <li><a href="#dont-blindly-follow-rules" id="toc-dont-blindly-follow-rules" class="nav-link" data-scroll-target="#dont-blindly-follow-rules"><span class="header-section-number">8</span> Don’t Blindly Follow “Rules”</a></li>
  <li><a href="#reporting-recommendations" id="toc-reporting-recommendations" class="nav-link" data-scroll-target="#reporting-recommendations"><span class="header-section-number">9</span> Reporting Recommendations</a></li>
  <li><a href="#take-home-messages" id="toc-take-home-messages" class="nav-link" data-scroll-target="#take-home-messages"><span class="header-section-number">10</span> Take-Home Messages</a></li>
  <li><a href="#references-and-further-reading" id="toc-references-and-further-reading" class="nav-link" data-scroll-target="#references-and-further-reading"><span class="header-section-number">11</span> References and Further Reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Multiple Comparisons</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
  <div class="quarto-categories">
    <div class="quarto-category">concept</div>
  </div>
  </div>

<div>
  <div class="description">
    To adjust, or not to adjust? - that is the question.
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 27, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>A manuscript that you have worked really hard on and was very proud to submit to a high impact-factor journal comes back from review. Reviewer # 2 (why, oh why is it always Reviewer # 2) asks:</p>
<blockquote class="blockquote">
<p>“Did you adjust for multiple testing?”</p>
</blockquote>
<p>You freeze. Should you have? Would adjusting make your significant finding disappear? More importantly - should it? You try not to imagine how this one simple comment could potentially add hours and hours to a manuscript rework, potentially changing the statistical inference and therefore conclusions that can be drawn from your research. OK, that doesn’t matter you think. “I’m a good scientist, and I will follow the science, and the statistics that determine it. Just point me to the literature that helps me decide what to do.” Well, you will soon frustratingly and unsatisfyingly discover that for every paper you find that tells you it’s necessary to adjust for multiple comparisons, you will find another paper that says the opposite.</p>
<p>The TLDR for what follows in the rest of this blog piece is that there are two components to this topic that make it really difficult for students and clinicians (and statisticians for that matter) to know what to do. And that’s because the answer to the question of “Should I adjust for multiple testing?” is <strong>“It depends”</strong>. You will soon discover that this is very much a context-dependent decision process. But even considering that and perhaps even having decided that you <strong>should</strong> adjust, there still remains a lot of controversy within the statistical community as to <strong>how</strong>.</p>
<p>My aim in this blog is to have you feel more comfortable in making a decision regarding multiple testing in your own research and having the knowledge tools available to justify that decision if Reviewer # 2 once again decides to make your life difficult for their own smug satisfaction.</p>
<hr>
</section>
<section id="the-core-problem-why-multiple-testing-matters" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="the-core-problem-why-multiple-testing-matters"><span class="header-section-number">2</span> The Core Problem: Why Multiple Testing Matters</h2>
<p>The logic behind multiple testing is straightforward. If you test a single null hypothesis at the <code>5%</code> significance level, you accept a <code>5%</code> chance of a false positive (Type I error). If you test many independent null hypotheses, the chance of <em>at least one</em> false positive increases rapidly.</p>
<p>For example, if you perform <code>20</code> independent tests at (<span class="math inline">\(\alpha\)</span> = <code>0.05</code>), the probability of at least one false positive is:</p>
<p><span class="math display">\[ 1 - (1 - 0.05)^{20} \approx 0.64 \]</span></p>
<p>So even if <strong>all null hypotheses are true</strong>, there is a <code>64%</code> chance you will declare something “statistically significant”, when it clearly isn’t. This inflation of false positives is what multiple testing adjustments are designed to control.</p>
<p>But this framing already hides several important subtleties:</p>
<ul>
<li>Tests are rarely independent</li>
<li>Not all analyses have the same inferential goal</li>
<li>Not all errors are equally costly</li>
<li>Not all tests belong to the same “family”</li>
</ul>
<p>Understanding these nuances is key to making sensible decisions.</p>
<hr>
</section>
<section id="what-error-rate-are-you-trying-to-control" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="what-error-rate-are-you-trying-to-control"><span class="header-section-number">3</span> What Error Rate Are You Trying to Control?</h2>
<p>Before discussing methods, it is crucial to clarify <em>which</em> error rate you care about, because different adjustments control different quantities.</p>
<section id="family-wise-error-rate-fwer" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="family-wise-error-rate-fwer"><span class="header-section-number">3.1</span> Family-Wise Error Rate (FWER)</h3>
<p>The <strong>family-wise error rate</strong> is the probability of making <em>at least one</em> Type I error in a family of tests:</p>
<p><span class="math display">\[ \text{FWER} = P(\text{one or more false positives}) \]</span></p>
<p>It is important to keep in mind that the FWER control is strict and conservative - think of it as a “zero-tolerance” approach. The FWER is appropriate when <em>any</em> false positive across your entire set of tests would be highly problematic. In other words by controlling the FWER at 0.05, you are saying there is only a <code>5%</code> chance that any of your significant results are actually false. Common methods of FWER correction include:</p>
<ul>
<li>Bonferroni correction</li>
<li>Tukey’s procedure</li>
<li>Holm (step-down) procedure</li>
<li>Hochberg (step-up) procedure</li>
<li>Dunnett’s correction</li>
</ul>
</section>
<section id="false-discovery-rate-fdr" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="false-discovery-rate-fdr"><span class="header-section-number">3.2</span> False Discovery Rate (FDR)</h3>
<p>The <strong>false discovery rate</strong> is the expected proportion of false positives among all rejected null hypotheses:</p>
<p><span class="math display">\[ \text{FDR} = E\left( \frac{\text{false positives}}{\text{total positives}} \right) \]</span></p>
<p>In contrast to the FWER, the FDR control is less stringent and more powerful when many tests are performed and some true effects are expected. This is the expected proportion of false discoveries among all the results you declare significant. If you control the FDR at <code>0.05</code>, you are saying that, on average, <code>5%</code> of your “hits” will be false alarms, while the other <code>95%</code> are likely real. Common methods of FDR correction include:</p>
<ul>
<li>Benjamini–Hochberg (BH)</li>
<li>Benjamini–Yekutieli (BY)</li>
</ul>
<p>If the differences still aren’t clear, let me give you a ChatGPT inspired analogy:</p>
<blockquote class="blockquote">
<p>Think of FWER as a high-security checkpoint where one mistake shuts down the building, while FDR is like a spam filter that aims to keep most of your inbox clean but allows a few junk emails through so you don’t miss important messages.</p>
</blockquote>
</section>
<section id="no-formal-error-rate-exploratory-contexts" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="no-formal-error-rate-exploratory-contexts"><span class="header-section-number">3.3</span> No Formal Error Rate (Exploratory Contexts)</h3>
<p>In some analyses, the goal is hypothesis generation rather than formal inference. In these cases, strict control of FWER or FDR may not be appropriate at all.</p>
<hr>
</section>
</section>
<section id="what-is-a-family-of-tests" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="what-is-a-family-of-tests"><span class="header-section-number">4</span> What Is a “Family” of Tests?</h2>
<p>Remember when I mentioned above that there is still a lot of controversy within the statistical community regarding both the <em>when</em> and <em>how</em> of multiple comparisons adjustment. Well, I believe it’s this very question asking what constitutes a “family” of tests that remains the biggest unresolved issue, and something mathematics alone can’t answer. And it is also singly, the issue I think that creates the most confusion when one has to think about how they will adjust for multiple tests.</p>
<p>In theory, multiplicity adjustments control an error rate over a specified set of hypotheses, but in practice the challenge is deciding <em>which hypotheses meaningfully belong together</em>. This is not something that can be determined by a formula or a software option alone; it depends on the scientific question being asked and how the results will be interpreted or acted upon.</p>
<p>A sensible definition of a family usually reflects a <em>common inferential purpose</em>. Tests belong to the same family when they address the same overarching research question, are interpreted jointly, or feed into a single decision-making process. For example, all secondary endpoints in a clinical trial, all pairwise group comparisons following a single omnibus test, or all biomarkers tested for association with the same outcome can reasonably be treated as families. In each case, a false positive anywhere in the set would be interpreted as evidence against a shared null narrative.</p>
<p>Problems arise when families are defined too broadly or too mechanically. Treating every hypothesis test in a thesis, paper, or dataset as a single family typically leads to excessive conservatism and loss of power, without corresponding gains in scientific credibility. At the other extreme, defining families so narrowly that each test stands alone undermines the very purpose of multiplicity control. Striking the right balance requires judgement and transparency: researchers should be able to explain why a particular group of tests belongs together and why controlling a specific error rate over that group is scientifically meaningful. Ultimately, defining the family is not a purely statistical decision - it requires scientific judgement.</p>
<p>At this point I’m going to introduce an <a href="https://www.annalsthoracicsurgery.org/article/S0003-4975(15)01873-1/pdf">editorial</a> by a well-regarded biostatistician who does a lot of statistical research work in the area of RCT’s.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Althouse, A. D. (2016). Adjust for Multiple Comparisons? It’s Not That Simple. Ann Thorac Surg, 101(5), 1644-1645. (if anything happens to the link)</p>
</div>
</div>
<p>I think this paper is extremely useful when it comes to defending decisions relating to multiple comparisons adjustments and I’d encourage you to read it and keep a copy on hand. I believe the viewpoint is balanced and pragmatic, providing a sensible framework within which to think about and act (or not) upon multiple comparisons in one’s own work. But here, specifically relating to the topic of families of tests, the author points out that:</p>
<blockquote class="blockquote">
<p>“Choosing the number of tests that must be adjusted for is itself a spurious decision”.</p>
</blockquote>
<p>And then to support this statement points out the potential absurdity of adjusting without thinking:</p>
<blockquote class="blockquote">
<p>Suppose an investigator publishes the first paper for a study, with additional substudies and ancillary studies to follow that will use the same dataset. Would the investigator be required update the “old” papers with“updated” p values and signicance decisions as further analyses are done on thesame study data?”</p>
</blockquote>
<p>and:</p>
<blockquote class="blockquote">
<p>“If one stipulates that the researcher need not consider the above, but still must adjust for the multiple comparisons done within a single paper, that begs researchers to slice data into as many single-hypothesis papers as possible to avoid criticism and increase their chances of a “significant” finding in any individual paper.”</p>
</blockquote>
<p>These are valid criticisms against blindly following a set of statistical rules without scientific judgement to guide the decision making process.</p>
<hr>
</section>
<section id="when-you-should-adjust-for-multiple-testing" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="when-you-should-adjust-for-multiple-testing"><span class="header-section-number">5</span> When You Should Adjust for Multiple Testing</h2>
<p>In some settings, multiple testing adjustment is clearly warranted and, in certain domains, effectively unavoidable. The most straightforward case is confirmatory research with multiple endpoints, particularly in clinical trials. When a study includes multiple primary endpoints, several treatment doses compared with a common control, or a set of pre-specified secondary endpoints intended to support formal claims, adjustment for multiplicity is essential. These analyses are typically fixed in advance, tied to explicit decision-making, and often have regulatory or clinical consequences. In such contexts, controlling the FWER through methods such as Bonferroni, Holm, or structured testing strategies like gatekeeping is usually appropriate, because even a single false positive can have serious implications.</p>
<p>Another clear case for adjustment arises in large-scale screening studies, such as those encountered in genomics, proteomics, neuroimaging, or other high-dimensional data settings. Here, thousands of hypotheses may be tested simultaneously, and without adjustment, false positives would overwhelm the results. In these contexts, the scientific goal is not to avoid all false positives, but to limit their proportion among reported findings. FDR control has therefore become the dominant paradigm, with the Benjamini–Hochberg procedure widely used due to its balance between error control and statistical power. Importantly, these analyses are usually understood as part of a broader discovery pipeline, with independent validation expected downstream.</p>
<p>Multiplicity adjustment is also generally appropriate when multiple comparisons are interpreted symmetrically. If a researcher intends to interpret any statistically significant result from a set of related tests as equally meaningful - for example, when examining all pairwise group differences and highlighting those with p-values below a fixed threshold - then adjustment is necessary to avoid systematically overstating evidence. Without correction, this practice almost guarantees that at least some reported differences will be spurious, even in moderately sized studies.</p>
<hr>
</section>
<section id="how-to-adjust-practical-guidance" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="how-to-adjust-practical-guidance"><span class="header-section-number">6</span> How to Adjust: Practical Guidance</h2>
<p>The simplest and most familiar adjustment is the Bonferroni correction, which replaces the nominal significance level with the significance level divided by the number of tests. Its appeal lies in its simplicity and its robustness: it controls the family-wise error rate under any dependence structure. However, this robustness comes at a cost. Bonferroni is often extremely conservative, particularly when the number of tests is large, leading to a substantial loss of power. For this reason, it is best viewed as a safety bound rather than a default strategy.</p>
<p>Holm’s step-down procedure offers a uniformly more powerful alternative while still controlling the FWER. By adjusting p-values sequentially rather than uniformly, Holm’s method avoids some of the unnecessary conservatism of Bonferroni and is often a sensible choice when strong error control is required but power is at a premium.</p>
<p>When the goal shifts from avoiding any false positives to limiting their proportion, FDR methods become more attractive. The Benjamini–Hochberg procedure controls the expected proportion of false discoveries under independence and many forms of positive dependence, and it is both easy to implement and widely accepted. In large testing problems where some true effects are anticipated, BH often provides a pragmatic balance between credibility and sensitivity. When there is uncertainty, or when dependence structures are complex, it is frequently a reasonable starting point.</p>
<hr>
</section>
<section id="when-you-should-not-adjust" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="when-you-should-not-adjust"><span class="header-section-number">7</span> When You Should Not Adjust</h2>
<p>There are also important situations in which multiple testing adjustment is unnecessary or even counterproductive. If a study has a single, clearly defined primary hypothesis that is tested once, no adjustment is required, regardless of how much exploratory work preceded the final analysis. In this case, there is no multiplicity problem to solve, and adjusting the p-value would only dilute the intended inference.</p>
<p>Clearly labelled exploratory analyses represent another context where adjustment may do more harm than good. When the purpose of an analysis is hypothesis generation rather than confirmation, strict control of formal error rates can increase false negatives and obscure potentially interesting signals. In such settings, a transparent approach - reporting unadjusted p-values, emphasising effect sizes and uncertainty, and explicitly acknowledging the exploratory nature of the findings - is often more scientifically honest than mechanical correction.</p>
<p>Finally, in some analytic frameworks, multiplicity is already addressed implicitly through the modelling strategy. Hierarchical or multilevel models, Bayesian analyses with partial pooling, and penalised regression approaches all borrow strength across parameters and naturally shrink extreme estimates. Applying post-hoc p-value adjustments on top of these methods can distort inference rather than improve it. In these cases, careful interpretation of model-based uncertainty is usually preferable to additional multiplicity correction.</p>
<hr>
</section>
<section id="dont-blindly-follow-rules" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="dont-blindly-follow-rules"><span class="header-section-number">8</span> Don’t Blindly Follow “Rules”</h2>
<p>I hope a theme has now emerged that we shouldn’t just mechanically apply multiple testing adjustments without first thinking about the nature of the analyses we are performing within the context of the aims of our research. As much as we need to justify why we <em>haven’t</em> performed a multiplicity correction, we should also need to justify why we <em>have</em>.</p>
<p>So please don’t blindly apply a Bonferroni correction to your next set of loosely-related t-tests. Similarly, don’t spend hours reworking a manuscript to incorporate corrections requested by reviewers without first clarifying the estimand and being clear about what they think they want in your analysis. And then be prepared to push back if you don’t agree. Remember that multiplicity adjustment should be scientifically defensible, not ritualistic, otherwise you may very well end up with the worst of both worlds: analyses hamstrung by reduced power and no meaningful control of a scientifically relevant error rate.</p>
<hr>
</section>
<section id="reporting-recommendations" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="reporting-recommendations"><span class="header-section-number">9</span> Reporting Recommendations</h2>
<p>If you have made the decision to employ a multiple testing procedure in your work, you need to explain that in your methods. Good reporting should include:</p>
<ul>
<li>A clear definition of the family of tests</li>
<li>The error rate being controlled (FWER, FDR, or none)</li>
<li>The adjustment method used</li>
<li>Both adjusted and unadjusted p-values (when helpful)</li>
<li>Emphasis on effect sizes and confidence intervals</li>
</ul>
<p>If you’ve made the decision NOT to employ a multiple testing procedure - that is ok too. Remember that p-values do not exist in isolation - they are part of a broader inferential narrative. Here I will directly quote once again from the Althouse viewpoint:</p>
<blockquote class="blockquote">
<p>“In this reader’s opinion, the best approach is simply to (1) describe what was done in a study; (2) report effect sizes, conﬁdence intervals, and p values; and (3) let readers use their own judgment about the relative weight of the conclusions. Scientiﬁcally literate researchers should be able to interpret study results without a cookie-cutter statement that the p value was “signiﬁcant” or “not signiﬁcant” at an arbitrary threshold.”</p>
</blockquote>
<hr>
</section>
<section id="take-home-messages" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="take-home-messages"><span class="header-section-number">10</span> Take-Home Messages</h2>
<p>Let me summarise everything we’ve talked about today in just a few points:</p>
<ol type="1">
<li>Multiple testing is a <em>contextual</em> problem, not a universal rule.</li>
<li>The first question is <strong>what error rate matters for this scientific question</strong>.</li>
<li>FWER methods suit confirmatory settings; FDR suits large-scale screening.</li>
<li>Not all analyses require adjustment - and some are harmed by it.</li>
<li>Thoughtful justification beats automatic correction every time.</li>
</ol>
<p>If you can clearly explain <em>why</em> you did (or did not) adjust for multiple testing, you are already ahead of the curve. Don’t ever let Reviewer # 2 have the upper hand again.</p>
<hr>
</section>
<section id="references-and-further-reading" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="references-and-further-reading"><span class="header-section-number">11</span> References and Further Reading</h2>
<p>I don’t normally include references but I thought for this topic a few might be helpful if you are interested in reading more. Until next month…</p>
<ul>
<li>Benjamini, Y., &amp; Hochberg, Y. (1995). Controlling the false discovery rate: A practical and powerful approach to multiple testing. <em>Journal of the Royal Statistical Society: Series B</em>, 57(1), 289–300.</li>
<li>Rothman, K. J. (1990). No adjustments are needed for multiple comparisons. <em>Epidemiology</em>, 1(1), 43–46.</li>
<li>Bender, R., &amp; Lange, S. (2001). Adjusting for multiple testing—when and how? <em>Journal of Clinical Epidemiology</em>, 54(4), 343–349.</li>
<li>Greenland, S., Senn, S. J., Rothman, K. J., et al.&nbsp;(2016). Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations. <em>European Journal of Epidemiology</em>, 31, 337–350.</li>
<li>Hochberg, Y., &amp; Tamhane, A. C. (1987). <em>Multiple Comparison Procedures</em>. Wiley.</li>
</ul>
<hr>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="pgseye/Weekly_Stats_Tips" data-repo-id="R_kgDOKvfOfQ" data-category="General" data-category-id="DIC_kwDOKvfOfc4CbFWq" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark"><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="an">title:</span><span class="co"> "Multiple Comparisons"</span></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="an">date:</span><span class="co"> 2026-03-27</span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="an">categories:</span><span class="co"> [concept]</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="an">image:</span><span class="co"> "images/multiple.png"</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="an">description:</span><span class="co"> "To adjust, or not to adjust? - that is the question."</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co">---</span></span>
<span id="cb1-8"><a href="#cb1-8"></a></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="fu">## Introduction</span></span>
<span id="cb1-10"><a href="#cb1-10"></a></span>
<span id="cb1-11"><a href="#cb1-11"></a>A manuscript that you have worked really hard on and was very proud to submit to a high impact-factor journal comes back from review. Reviewer <span class="sc">\#</span> 2 (why, oh why is it always Reviewer <span class="sc">\#</span> 2) asks:</span>
<span id="cb1-12"><a href="#cb1-12"></a></span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="at">&gt; "Did you adjust for multiple testing?"</span></span>
<span id="cb1-14"><a href="#cb1-14"></a></span>
<span id="cb1-15"><a href="#cb1-15"></a>You freeze. Should you have? Would adjusting make your significant finding disappear? More importantly - should it? You try not to imagine how this one simple comment could potentially add hours and hours to a manuscript rework, potentially changing the statistical inference and therefore conclusions that can be drawn from your research. OK, that doesn't matter you think. "I'm a good scientist, and I will follow the science, and the statistics that determine it. Just point me to the literature that helps me decide what to do." Well, you will soon frustratingly and unsatisfyingly discover that for every paper you find that tells you it's necessary to adjust for multiple comparisons, you will find another paper that says the opposite.</span>
<span id="cb1-16"><a href="#cb1-16"></a></span>
<span id="cb1-17"><a href="#cb1-17"></a>The TLDR for what follows in the rest of this blog piece is that there are two components to this topic that make it really difficult for students and clinicians (and statisticians for that matter) to know what to do. And that's because the answer to the question of "Should I adjust for multiple testing?" is **"It depends"**. You will soon discover that this is very much a context-dependent decision process. But even considering that and perhaps even having decided that you **should** adjust, there still remains a lot of controversy within the statistical community as to **how**.</span>
<span id="cb1-18"><a href="#cb1-18"></a></span>
<span id="cb1-19"><a href="#cb1-19"></a>My aim in this blog is to have you feel more comfortable in making a decision regarding multiple testing in your own research and having the knowledge tools available to justify that decision if Reviewer <span class="sc">\#</span> 2 once again decides to make your life difficult for their own smug satisfaction.</span>
<span id="cb1-20"><a href="#cb1-20"></a></span>
<span id="cb1-21"><a href="#cb1-21"></a>------------------------------------------------------------------------</span>
<span id="cb1-22"><a href="#cb1-22"></a></span>
<span id="cb1-23"><a href="#cb1-23"></a><span class="fu">## The Core Problem: Why Multiple Testing Matters</span></span>
<span id="cb1-24"><a href="#cb1-24"></a></span>
<span id="cb1-25"><a href="#cb1-25"></a>The logic behind multiple testing is straightforward. If you test a single null hypothesis at the <span class="in">`5%`</span> significance level, you accept a <span class="in">`5%`</span> chance of a false positive (Type I error). If you test many independent null hypotheses, the chance of *at least one* false positive increases rapidly.</span>
<span id="cb1-26"><a href="#cb1-26"></a></span>
<span id="cb1-27"><a href="#cb1-27"></a>For example, if you perform <span class="in">`20`</span> independent tests at ($\alpha$ = <span class="in">`0.05`</span>), the probability of at least one false positive is:</span>
<span id="cb1-28"><a href="#cb1-28"></a></span>
<span id="cb1-29"><a href="#cb1-29"></a>$$ 1 - (1 - 0.05)^{20} \approx 0.64 $$</span>
<span id="cb1-30"><a href="#cb1-30"></a></span>
<span id="cb1-31"><a href="#cb1-31"></a>So even if **all null hypotheses are true**, there is a <span class="in">`64%`</span> chance you will declare something "statistically significant", when it clearly isn't. This inflation of false positives is what multiple testing adjustments are designed to control.</span>
<span id="cb1-32"><a href="#cb1-32"></a></span>
<span id="cb1-33"><a href="#cb1-33"></a>But this framing already hides several important subtleties:</span>
<span id="cb1-34"><a href="#cb1-34"></a></span>
<span id="cb1-35"><a href="#cb1-35"></a><span class="ss">-   </span>Tests are rarely independent</span>
<span id="cb1-36"><a href="#cb1-36"></a><span class="ss">-   </span>Not all analyses have the same inferential goal</span>
<span id="cb1-37"><a href="#cb1-37"></a><span class="ss">-   </span>Not all errors are equally costly</span>
<span id="cb1-38"><a href="#cb1-38"></a><span class="ss">-   </span>Not all tests belong to the same “family”</span>
<span id="cb1-39"><a href="#cb1-39"></a></span>
<span id="cb1-40"><a href="#cb1-40"></a>Understanding these nuances is key to making sensible decisions.</span>
<span id="cb1-41"><a href="#cb1-41"></a></span>
<span id="cb1-42"><a href="#cb1-42"></a>------------------------------------------------------------------------</span>
<span id="cb1-43"><a href="#cb1-43"></a></span>
<span id="cb1-44"><a href="#cb1-44"></a><span class="fu">## What Error Rate Are You Trying to Control?</span></span>
<span id="cb1-45"><a href="#cb1-45"></a></span>
<span id="cb1-46"><a href="#cb1-46"></a>Before discussing methods, it is crucial to clarify *which* error rate you care about, because different adjustments control different quantities.</span>
<span id="cb1-47"><a href="#cb1-47"></a></span>
<span id="cb1-48"><a href="#cb1-48"></a><span class="fu">### Family-Wise Error Rate (FWER)</span></span>
<span id="cb1-49"><a href="#cb1-49"></a></span>
<span id="cb1-50"><a href="#cb1-50"></a>The **family-wise error rate** is the probability of making *at least one* Type I error in a family of tests:</span>
<span id="cb1-51"><a href="#cb1-51"></a></span>
<span id="cb1-52"><a href="#cb1-52"></a>$$ \text{FWER} = P(\text{one or more false positives}) $$</span>
<span id="cb1-53"><a href="#cb1-53"></a></span>
<span id="cb1-54"><a href="#cb1-54"></a>It is important to keep in mind that the FWER control is strict and conservative - think of it as a "zero-tolerance" approach. The FWER is appropriate when *any* false positive across your entire set of tests would be highly problematic. In other words by controlling the FWER at 0.05, you are saying there is only a <span class="in">`5%`</span> chance that any of your significant results are actually false. Common methods of FWER correction include:</span>
<span id="cb1-55"><a href="#cb1-55"></a></span>
<span id="cb1-56"><a href="#cb1-56"></a><span class="ss">-   </span>Bonferroni correction</span>
<span id="cb1-57"><a href="#cb1-57"></a><span class="ss">-   </span>Tukey's procedure</span>
<span id="cb1-58"><a href="#cb1-58"></a><span class="ss">-   </span>Holm (step-down) procedure</span>
<span id="cb1-59"><a href="#cb1-59"></a><span class="ss">-   </span>Hochberg (step-up) procedure</span>
<span id="cb1-60"><a href="#cb1-60"></a><span class="ss">-   </span>Dunnett's correction</span>
<span id="cb1-61"><a href="#cb1-61"></a></span>
<span id="cb1-62"><a href="#cb1-62"></a><span class="fu">### False Discovery Rate (FDR)</span></span>
<span id="cb1-63"><a href="#cb1-63"></a></span>
<span id="cb1-64"><a href="#cb1-64"></a>The **false discovery rate** is the expected proportion of false positives among all rejected null hypotheses:</span>
<span id="cb1-65"><a href="#cb1-65"></a></span>
<span id="cb1-66"><a href="#cb1-66"></a>$$ \text{FDR} = E\left( \frac{\text{false positives}}{\text{total positives}} \right) $$</span>
<span id="cb1-67"><a href="#cb1-67"></a></span>
<span id="cb1-68"><a href="#cb1-68"></a>In contrast to the FWER, the FDR control is less stringent and more powerful when many tests are performed and some true effects are expected. This is the expected proportion of false discoveries among all the results you declare significant. If you control the FDR at <span class="in">`0.05`</span>, you are saying that, on average, <span class="in">`5%`</span> of your "hits" will be false alarms, while the other <span class="in">`95%`</span> are likely real. Common methods of FDR correction include:</span>
<span id="cb1-69"><a href="#cb1-69"></a></span>
<span id="cb1-70"><a href="#cb1-70"></a><span class="ss">-   </span>Benjamini–Hochberg (BH)</span>
<span id="cb1-71"><a href="#cb1-71"></a><span class="ss">-   </span>Benjamini–Yekutieli (BY)</span>
<span id="cb1-72"><a href="#cb1-72"></a></span>
<span id="cb1-73"><a href="#cb1-73"></a>If the differences still aren't clear, let me give you a ChatGPT inspired analogy:</span>
<span id="cb1-74"><a href="#cb1-74"></a></span>
<span id="cb1-75"><a href="#cb1-75"></a><span class="at">&gt; Think of FWER as a high-security checkpoint where one mistake shuts down the building, while FDR is like a spam filter that aims to keep most of your inbox clean but allows a few junk emails through so you don't miss important messages.</span></span>
<span id="cb1-76"><a href="#cb1-76"></a></span>
<span id="cb1-77"><a href="#cb1-77"></a><span class="fu">### No Formal Error Rate (Exploratory Contexts)</span></span>
<span id="cb1-78"><a href="#cb1-78"></a></span>
<span id="cb1-79"><a href="#cb1-79"></a>In some analyses, the goal is hypothesis generation rather than formal inference. In these cases, strict control of FWER or FDR may not be appropriate at all.</span>
<span id="cb1-80"><a href="#cb1-80"></a></span>
<span id="cb1-81"><a href="#cb1-81"></a>------------------------------------------------------------------------</span>
<span id="cb1-82"><a href="#cb1-82"></a></span>
<span id="cb1-83"><a href="#cb1-83"></a><span class="fu">## What Is a "Family" of Tests?</span></span>
<span id="cb1-84"><a href="#cb1-84"></a></span>
<span id="cb1-85"><a href="#cb1-85"></a>Remember when I mentioned above that there is still a lot of controversy within the statistical community regarding both the *when* and *how* of multiple comparisons adjustment. Well, I believe it's this very question asking what constitutes a "family" of tests that remains the biggest unresolved issue, and something mathematics alone can't answer. And it is also singly, the issue I think that creates the most confusion when one has to think about how they will adjust for multiple tests.</span>
<span id="cb1-86"><a href="#cb1-86"></a></span>
<span id="cb1-87"><a href="#cb1-87"></a>In theory, multiplicity adjustments control an error rate over a specified set of hypotheses, but in practice the challenge is deciding *which hypotheses meaningfully belong together*. This is not something that can be determined by a formula or a software option alone; it depends on the scientific question being asked and how the results will be interpreted or acted upon.</span>
<span id="cb1-88"><a href="#cb1-88"></a></span>
<span id="cb1-89"><a href="#cb1-89"></a>A sensible definition of a family usually reflects a *common inferential purpose*. Tests belong to the same family when they address the same overarching research question, are interpreted jointly, or feed into a single decision-making process. For example, all secondary endpoints in a clinical trial, all pairwise group comparisons following a single omnibus test, or all biomarkers tested for association with the same outcome can reasonably be treated as families. In each case, a false positive anywhere in the set would be interpreted as evidence against a shared null narrative.</span>
<span id="cb1-90"><a href="#cb1-90"></a></span>
<span id="cb1-91"><a href="#cb1-91"></a>Problems arise when families are defined too broadly or too mechanically. Treating every hypothesis test in a thesis, paper, or dataset as a single family typically leads to excessive conservatism and loss of power, without corresponding gains in scientific credibility. At the other extreme, defining families so narrowly that each test stands alone undermines the very purpose of multiplicity control. Striking the right balance requires judgement and transparency: researchers should be able to explain why a particular group of tests belongs together and why controlling a specific error rate over that group is scientifically meaningful. Ultimately, defining the family is not a purely statistical decision - it requires scientific judgement.</span>
<span id="cb1-92"><a href="#cb1-92"></a></span>
<span id="cb1-93"><a href="#cb1-93"></a>At this point I'm going to introduce an <span class="co">[</span><span class="ot">editorial</span><span class="co">]</span>(https://www.annalsthoracicsurgery.org/article/S0003-4975(15)01873-1/pdf) by a well-regarded biostatistician who does a lot of statistical research work in the area of RCT's.</span>
<span id="cb1-94"><a href="#cb1-94"></a></span>
<span id="cb1-95"><a href="#cb1-95"></a>::: callout-note</span>
<span id="cb1-96"><a href="#cb1-96"></a>Althouse, A. D. (2016). Adjust for Multiple Comparisons? It’s Not That Simple. Ann Thorac Surg, 101(5), 1644-1645. (if anything happens to the link)</span>
<span id="cb1-97"><a href="#cb1-97"></a>:::</span>
<span id="cb1-98"><a href="#cb1-98"></a></span>
<span id="cb1-99"><a href="#cb1-99"></a>I think this paper is extremely useful when it comes to defending decisions relating to multiple comparisons adjustments and I'd encourage you to read it and keep a copy on hand. I believe the viewpoint is balanced and pragmatic, providing a sensible framework within which to think about and act (or not) upon multiple comparisons in one's own work. But here, specifically relating to the topic of families of tests, the author points out that:</span>
<span id="cb1-100"><a href="#cb1-100"></a></span>
<span id="cb1-101"><a href="#cb1-101"></a><span class="at">&gt; "Choosing the number of tests that must be adjusted for is itself a spurious decision".</span></span>
<span id="cb1-102"><a href="#cb1-102"></a></span>
<span id="cb1-103"><a href="#cb1-103"></a>And then to support this statement points out the potential absurdity of adjusting without thinking:</span>
<span id="cb1-104"><a href="#cb1-104"></a></span>
<span id="cb1-105"><a href="#cb1-105"></a><span class="at">&gt; Suppose an investigator publishes the first paper for a study, with additional substudies and ancillary studies to follow that will use the same dataset. Would the investigator be required update the “old” papers with“updated” p values and signicance decisions as further analyses are done on thesame study data?"</span></span>
<span id="cb1-106"><a href="#cb1-106"></a></span>
<span id="cb1-107"><a href="#cb1-107"></a>and:</span>
<span id="cb1-108"><a href="#cb1-108"></a></span>
<span id="cb1-109"><a href="#cb1-109"></a><span class="at">&gt; "If one stipulates that the researcher need not consider the above, but still must adjust for the multiple comparisons done within a single paper, that begs researchers to slice data into as many single-hypothesis papers as possible to avoid criticism and increase their chances of a “significant” finding in any individual paper."</span></span>
<span id="cb1-110"><a href="#cb1-110"></a></span>
<span id="cb1-111"><a href="#cb1-111"></a>These are valid criticisms against blindly following a set of statistical rules without scientific judgement to guide the decision making process.</span>
<span id="cb1-112"><a href="#cb1-112"></a></span>
<span id="cb1-113"><a href="#cb1-113"></a>------------------------------------------------------------------------</span>
<span id="cb1-114"><a href="#cb1-114"></a></span>
<span id="cb1-115"><a href="#cb1-115"></a><span class="fu">## When You Should Adjust for Multiple Testing</span></span>
<span id="cb1-116"><a href="#cb1-116"></a></span>
<span id="cb1-117"><a href="#cb1-117"></a>In some settings, multiple testing adjustment is clearly warranted and, in certain domains, effectively unavoidable. The most straightforward case is confirmatory research with multiple endpoints, particularly in clinical trials. When a study includes multiple primary endpoints, several treatment doses compared with a common control, or a set of pre-specified secondary endpoints intended to support formal claims, adjustment for multiplicity is essential. These analyses are typically fixed in advance, tied to explicit decision-making, and often have regulatory or clinical consequences. In such contexts, controlling the FWER through methods such as Bonferroni, Holm, or structured testing strategies like gatekeeping is usually appropriate, because even a single false positive can have serious implications.</span>
<span id="cb1-118"><a href="#cb1-118"></a></span>
<span id="cb1-119"><a href="#cb1-119"></a>Another clear case for adjustment arises in large-scale screening studies, such as those encountered in genomics, proteomics, neuroimaging, or other high-dimensional data settings. Here, thousands of hypotheses may be tested simultaneously, and without adjustment, false positives would overwhelm the results. In these contexts, the scientific goal is not to avoid all false positives, but to limit their proportion among reported findings. FDR control has therefore become the dominant paradigm, with the Benjamini–Hochberg procedure widely used due to its balance between error control and statistical power. Importantly, these analyses are usually understood as part of a broader discovery pipeline, with independent validation expected downstream.</span>
<span id="cb1-120"><a href="#cb1-120"></a></span>
<span id="cb1-121"><a href="#cb1-121"></a>Multiplicity adjustment is also generally appropriate when multiple comparisons are interpreted symmetrically. If a researcher intends to interpret any statistically significant result from a set of related tests as equally meaningful - for example, when examining all pairwise group differences and highlighting those with p-values below a fixed threshold - then adjustment is necessary to avoid systematically overstating evidence. Without correction, this practice almost guarantees that at least some reported differences will be spurious, even in moderately sized studies.</span>
<span id="cb1-122"><a href="#cb1-122"></a></span>
<span id="cb1-123"><a href="#cb1-123"></a>------------------------------------------------------------------------</span>
<span id="cb1-124"><a href="#cb1-124"></a></span>
<span id="cb1-125"><a href="#cb1-125"></a><span class="fu">## How to Adjust: Practical Guidance</span></span>
<span id="cb1-126"><a href="#cb1-126"></a></span>
<span id="cb1-127"><a href="#cb1-127"></a>The simplest and most familiar adjustment is the Bonferroni correction, which replaces the nominal significance level with the significance level divided by the number of tests. Its appeal lies in its simplicity and its robustness: it controls the family-wise error rate under any dependence structure. However, this robustness comes at a cost. Bonferroni is often extremely conservative, particularly when the number of tests is large, leading to a substantial loss of power. For this reason, it is best viewed as a safety bound rather than a default strategy.</span>
<span id="cb1-128"><a href="#cb1-128"></a></span>
<span id="cb1-129"><a href="#cb1-129"></a>Holm’s step-down procedure offers a uniformly more powerful alternative while still controlling the FWER. By adjusting p-values sequentially rather than uniformly, Holm’s method avoids some of the unnecessary conservatism of Bonferroni and is often a sensible choice when strong error control is required but power is at a premium.</span>
<span id="cb1-130"><a href="#cb1-130"></a></span>
<span id="cb1-131"><a href="#cb1-131"></a>When the goal shifts from avoiding any false positives to limiting their proportion, FDR methods become more attractive. The Benjamini–Hochberg procedure controls the expected proportion of false discoveries under independence and many forms of positive dependence, and it is both easy to implement and widely accepted. In large testing problems where some true effects are anticipated, BH often provides a pragmatic balance between credibility and sensitivity. When there is uncertainty, or when dependence structures are complex, it is frequently a reasonable starting point.</span>
<span id="cb1-132"><a href="#cb1-132"></a></span>
<span id="cb1-133"><a href="#cb1-133"></a>------------------------------------------------------------------------</span>
<span id="cb1-134"><a href="#cb1-134"></a></span>
<span id="cb1-135"><a href="#cb1-135"></a><span class="fu">## When You Should Not Adjust</span></span>
<span id="cb1-136"><a href="#cb1-136"></a></span>
<span id="cb1-137"><a href="#cb1-137"></a>There are also important situations in which multiple testing adjustment is unnecessary or even counterproductive. If a study has a single, clearly defined primary hypothesis that is tested once, no adjustment is required, regardless of how much exploratory work preceded the final analysis. In this case, there is no multiplicity problem to solve, and adjusting the p-value would only dilute the intended inference.</span>
<span id="cb1-138"><a href="#cb1-138"></a></span>
<span id="cb1-139"><a href="#cb1-139"></a>Clearly labelled exploratory analyses represent another context where adjustment may do more harm than good. When the purpose of an analysis is hypothesis generation rather than confirmation, strict control of formal error rates can increase false negatives and obscure potentially interesting signals. In such settings, a transparent approach - reporting unadjusted p-values, emphasising effect sizes and uncertainty, and explicitly acknowledging the exploratory nature of the findings - is often more scientifically honest than mechanical correction.</span>
<span id="cb1-140"><a href="#cb1-140"></a></span>
<span id="cb1-141"><a href="#cb1-141"></a>Finally, in some analytic frameworks, multiplicity is already addressed implicitly through the modelling strategy. Hierarchical or multilevel models, Bayesian analyses with partial pooling, and penalised regression approaches all borrow strength across parameters and naturally shrink extreme estimates. Applying post-hoc p-value adjustments on top of these methods can distort inference rather than improve it. In these cases, careful interpretation of model-based uncertainty is usually preferable to additional multiplicity correction.</span>
<span id="cb1-142"><a href="#cb1-142"></a></span>
<span id="cb1-143"><a href="#cb1-143"></a>------------------------------------------------------------------------</span>
<span id="cb1-144"><a href="#cb1-144"></a></span>
<span id="cb1-145"><a href="#cb1-145"></a><span class="fu">## Don't Blindly Follow "Rules"</span></span>
<span id="cb1-146"><a href="#cb1-146"></a></span>
<span id="cb1-147"><a href="#cb1-147"></a>I hope a theme has now emerged that we shouldn't just mechanically apply multiple testing adjustments without first thinking about the nature of the analyses we are performing within the context of the aims of our research. As much as we need to justify why we *haven't* performed a multiplicity correction, we should also need to justify why we *have*.</span>
<span id="cb1-148"><a href="#cb1-148"></a></span>
<span id="cb1-149"><a href="#cb1-149"></a>So please don't blindly apply a Bonferroni correction to your next set of loosely-related t-tests. Similarly, don't spend hours reworking a manuscript to incorporate corrections requested by reviewers without first clarifying the estimand and being clear about what they think they want in your analysis. And then be prepared to push back if you don't agree. Remember that multiplicity adjustment should be scientifically defensible, not ritualistic, otherwise you may very well end up with the worst of both worlds: analyses hamstrung by reduced power and no meaningful control of a scientifically relevant error rate.</span>
<span id="cb1-150"><a href="#cb1-150"></a></span>
<span id="cb1-151"><a href="#cb1-151"></a>------------------------------------------------------------------------</span>
<span id="cb1-152"><a href="#cb1-152"></a></span>
<span id="cb1-153"><a href="#cb1-153"></a><span class="fu">## Reporting Recommendations</span></span>
<span id="cb1-154"><a href="#cb1-154"></a></span>
<span id="cb1-155"><a href="#cb1-155"></a>If you have made the decision to employ a multiple testing procedure in your work, you need to explain that in your methods. Good reporting should include:</span>
<span id="cb1-156"><a href="#cb1-156"></a></span>
<span id="cb1-157"><a href="#cb1-157"></a><span class="ss">-   </span>A clear definition of the family of tests</span>
<span id="cb1-158"><a href="#cb1-158"></a><span class="ss">-   </span>The error rate being controlled (FWER, FDR, or none)</span>
<span id="cb1-159"><a href="#cb1-159"></a><span class="ss">-   </span>The adjustment method used</span>
<span id="cb1-160"><a href="#cb1-160"></a><span class="ss">-   </span>Both adjusted and unadjusted p-values (when helpful)</span>
<span id="cb1-161"><a href="#cb1-161"></a><span class="ss">-   </span>Emphasis on effect sizes and confidence intervals</span>
<span id="cb1-162"><a href="#cb1-162"></a></span>
<span id="cb1-163"><a href="#cb1-163"></a>If you've made the decision NOT to employ a multiple testing procedure - that is ok too. Remember that p-values do not exist in isolation - they are part of a broader inferential narrative. Here I will directly quote once again from the Althouse viewpoint:</span>
<span id="cb1-164"><a href="#cb1-164"></a></span>
<span id="cb1-165"><a href="#cb1-165"></a><span class="at">&gt; "In this reader’s opinion, the best approach is simply to (1) describe what was done in a study; (2) report effect sizes, conﬁdence intervals, and p values; and (3) let readers use their own judgment about the relative weight of the conclusions. Scientiﬁcally literate researchers should be able to interpret study results without a cookie-cutter statement that the p value was “signiﬁcant” or “not signiﬁcant” at an arbitrary threshold."</span></span>
<span id="cb1-166"><a href="#cb1-166"></a></span>
<span id="cb1-167"><a href="#cb1-167"></a>------------------------------------------------------------------------</span>
<span id="cb1-168"><a href="#cb1-168"></a></span>
<span id="cb1-169"><a href="#cb1-169"></a><span class="fu">## Take-Home Messages</span></span>
<span id="cb1-170"><a href="#cb1-170"></a></span>
<span id="cb1-171"><a href="#cb1-171"></a>Let me summarise everything we've talked about today in just a few points:</span>
<span id="cb1-172"><a href="#cb1-172"></a></span>
<span id="cb1-173"><a href="#cb1-173"></a><span class="ss">1.  </span>Multiple testing is a *contextual* problem, not a universal rule.</span>
<span id="cb1-174"><a href="#cb1-174"></a><span class="ss">2.  </span>The first question is **what error rate matters for this scientific question**.</span>
<span id="cb1-175"><a href="#cb1-175"></a><span class="ss">3.  </span>FWER methods suit confirmatory settings; FDR suits large-scale screening.</span>
<span id="cb1-176"><a href="#cb1-176"></a><span class="ss">4.  </span>Not all analyses require adjustment - and some are harmed by it.</span>
<span id="cb1-177"><a href="#cb1-177"></a><span class="ss">5.  </span>Thoughtful justification beats automatic correction every time.</span>
<span id="cb1-178"><a href="#cb1-178"></a></span>
<span id="cb1-179"><a href="#cb1-179"></a>If you can clearly explain *why* you did (or did not) adjust for multiple testing, you are already ahead of the curve. Don't ever let Reviewer <span class="sc">\#</span> 2 have the upper hand again.</span>
<span id="cb1-180"><a href="#cb1-180"></a></span>
<span id="cb1-181"><a href="#cb1-181"></a>------------------------------------------------------------------------</span>
<span id="cb1-182"><a href="#cb1-182"></a></span>
<span id="cb1-183"><a href="#cb1-183"></a><span class="fu">## References and Further Reading</span></span>
<span id="cb1-184"><a href="#cb1-184"></a></span>
<span id="cb1-185"><a href="#cb1-185"></a>I don't normally include references but I thought for this topic a few might be helpful if you are interested in reading more. Until next month...</span>
<span id="cb1-186"><a href="#cb1-186"></a></span>
<span id="cb1-187"><a href="#cb1-187"></a><span class="ss">-   </span>Benjamini, Y., &amp; Hochberg, Y. (1995). Controlling the false discovery rate: A practical and powerful approach to multiple testing. *Journal of the Royal Statistical Society: Series B*, 57(1), 289–300.</span>
<span id="cb1-188"><a href="#cb1-188"></a><span class="ss">-   </span>Rothman, K. J. (1990). No adjustments are needed for multiple comparisons. *Epidemiology*, 1(1), 43–46.</span>
<span id="cb1-189"><a href="#cb1-189"></a><span class="ss">-   </span>Bender, R., &amp; Lange, S. (2001). Adjusting for multiple testing—when and how? *Journal of Clinical Epidemiology*, 54(4), 343–349.</span>
<span id="cb1-190"><a href="#cb1-190"></a><span class="ss">-   </span>Greenland, S., Senn, S. J., Rothman, K. J., et al. (2016). Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations. *European Journal of Epidemiology*, 31, 337–350.</span>
<span id="cb1-191"><a href="#cb1-191"></a><span class="ss">-   </span>Hochberg, Y., &amp; Tamhane, A. C. (1987). *Multiple Comparison Procedures*. Wiley.</span>
<span id="cb1-192"><a href="#cb1-192"></a></span>
<span id="cb1-193"><a href="#cb1-193"></a>------------------------------------------------------------------------</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>