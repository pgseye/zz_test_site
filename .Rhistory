)
library(dplyr)
library(purrr)
library(tibble)
#set.seed(123)
n <- 500
p <- 20
Sigma <- matrix(0.4, p, p)
diag(Sigma) <- 1
library(MASS)
X <- mvrnorm(n, mu = rep(0, p), Sigma = Sigma)
beta <- c(2, 2, 2, -1.5, -1.5, rep(0, p - 5))
y <- X %*% beta + rnorm(n, sd = 2)
dat <- data.frame(y, X)
colnames(dat) <- c("y", paste0("X", 1:p))
#set.seed(456)
idx <- sample(seq_len(n), size = 0.7 * n)
train <- dat[idx, ]
test  <- dat[-idx, ]
full_mod <- lm(y ~ ., data = train)
null_mod <- lm(y ~ 1, data = train)
step_mod <- step(
null_mod,
scope = list(lower = null_mod, upper = full_mod),
direction = "both",
trace = FALSE
)
summary(step_mod)
y_pred_step <- predict(step_mod, newdata = test)
rmse_step <- sqrt(mean((test$y - y_pred_step)^2))
rmse_step
knitr::opts_chunk$set(
echo = TRUE,
message = FALSE,
warning = FALSE
)
library(dplyr)
library(purrr)
library(tibble)
#set.seed(123)
n <- 500
p <- 20
Sigma <- matrix(0.4, p, p)
diag(Sigma) <- 1
library(MASS)
X <- mvrnorm(n, mu = rep(0, p), Sigma = Sigma)
beta <- c(2, 2, 2, -1.5, -1.5, rep(0, p - 5))
y <- X %*% beta + rnorm(n, sd = 2)
dat <- data.frame(y, X)
colnames(dat) <- c("y", paste0("X", 1:p))
#set.seed(456)
idx <- sample(seq_len(n), size = 0.7 * n)
train <- dat[idx, ]
test  <- dat[-idx, ]
full_mod <- lm(y ~ ., data = train)
null_mod <- lm(y ~ 1, data = train)
step_mod <- step(
null_mod,
scope = list(lower = null_mod, upper = full_mod),
direction = "both",
trace = FALSE
)
summary(step_mod)
y_pred_step <- predict(step_mod, newdata = test)
rmse_step <- sqrt(mean((test$y - y_pred_step)^2))
rmse_step
library(glmnet)
# glmnet requires predictors as a numeric matrix, not a data frame.
X_train <- as.matrix(train[, -1]) # contains all predictors but remove the outcome variable (y)
y_train <- train$y                # response
X_test  <- as.matrix(test[, -1])
y_test  <- test$y
cv_enet <- cv.glmnet(
X_train, y_train,
alpha = 0.5,                    # elastic net
nfolds = 10
)
# Extract the coefficients from the single elastic net model corresponding to the optimal λ.
coef(cv_enet, s = "lambda.min")
knitr::opts_chunk$set(
echo = TRUE,
message = FALSE,
warning = FALSE
)
library(dplyr)
library(purrr)
library(tibble)
#set.seed(123)
n <- 500
p <- 20
Sigma <- matrix(0.4, p, p)
diag(Sigma) <- 1
library(MASS)
X <- mvrnorm(n, mu = rep(0, p), Sigma = Sigma)
beta <- c(2, 2, 2, -1.5, -1.5, rep(0, p - 5))
y <- X %*% beta + rnorm(n, sd = 2)
dat <- data.frame(y, X)
colnames(dat) <- c("y", paste0("X", 1:p))
#set.seed(456)
idx <- sample(seq_len(n), size = 0.7 * n)
train <- dat[idx, ]
test  <- dat[-idx, ]
full_mod <- lm(y ~ ., data = train)
null_mod <- lm(y ~ 1, data = train)
step_mod <- step(
null_mod,
scope = list(lower = null_mod, upper = full_mod),
direction = "both",
trace = FALSE
)
summary(step_mod)
y_pred_step <- predict(step_mod, newdata = test)
rmse_step <- sqrt(mean((test$y - y_pred_step)^2))
rmse_step
library(glmnet)
# glmnet requires predictors as a numeric matrix, not a data frame.
X_train <- as.matrix(train[, -1]) # contains all predictors but remove the outcome variable (y)
y_train <- train$y                # response
X_test  <- as.matrix(test[, -1])
y_test  <- test$y
cv_enet <- cv.glmnet(
X_train, y_train,
alpha = 0.5,                    # elastic net
nfolds = 10
)
# Extract the coefficients from the single elastic net model corresponding to the optimal λ.
coef(cv_enet, s = "lambda.min")
knitr::opts_chunk$set(
echo = TRUE,
message = FALSE,
warning = FALSE
)
library(dplyr)
library(purrr)
library(tibble)
#set.seed(123)
n <- 500
p <- 20
Sigma <- matrix(0.4, p, p)
diag(Sigma) <- 1
library(MASS)
X <- mvrnorm(n, mu = rep(0, p), Sigma = Sigma)
beta <- c(2, 2, 2, -1.5, -1.5, rep(0, p - 5))
y <- X %*% beta + rnorm(n, sd = 2)
dat <- data.frame(y, X)
colnames(dat) <- c("y", paste0("X", 1:p))
#set.seed(456)
idx <- sample(seq_len(n), size = 0.7 * n)
train <- dat[idx, ]
test  <- dat[-idx, ]
full_mod <- lm(y ~ ., data = train)
null_mod <- lm(y ~ 1, data = train)
step_mod <- step(
null_mod,
scope = list(lower = null_mod, upper = full_mod),
direction = "both",
trace = FALSE
)
summary(step_mod)
y_pred_step <- predict(step_mod, newdata = test)
rmse_step <- sqrt(mean((test$y - y_pred_step)^2))
rmse_step
library(glmnet)
# glmnet requires predictors as a numeric matrix, not a data frame.
X_train <- as.matrix(train[, -1]) # contains all predictors but remove the outcome variable (y)
y_train <- train$y                # response
X_test  <- as.matrix(test[, -1])
y_test  <- test$y
cv_enet <- cv.glmnet(
X_train, y_train,
alpha = 0.5,                    # elastic net
nfolds = 10
)
# Extract the coefficients from the single elastic net model corresponding to the optimal λ.
coef(cv_enet, s = "lambda.min")
y_pred_enet <- predict(cv_enet, X_test, s = "lambda.min")
rmse_enet <- sqrt(mean((y_test - y_pred_enet)^2))
rmse_enet
c(
Stepwise_RMSE   = rmse_step,
ElasticNet_RMSE = rmse_enet
)
knitr::opts_chunk$set(
echo = TRUE,
message = FALSE,
warning = FALSE
)
library(dplyr)
library(purrr)
library(tibble)
#set.seed(123)
n <- 500
p <- 20
Sigma <- matrix(0.4, p, p)
diag(Sigma) <- 1
library(MASS)
X <- mvrnorm(n, mu = rep(0, p), Sigma = Sigma)
beta <- c(2, 2, 2, -1.5, -1.5, rep(0, p - 5))
y <- X %*% beta + rnorm(n, sd = 2)
dat <- data.frame(y, X)
colnames(dat) <- c("y", paste0("X", 1:p))
#set.seed(456)
idx <- sample(seq_len(n), size = 0.7 * n)
train <- dat[idx, ]
test  <- dat[-idx, ]
full_mod <- lm(y ~ ., data = train)
null_mod <- lm(y ~ 1, data = train)
step_mod <- step(
null_mod,
scope = list(lower = null_mod, upper = full_mod),
direction = "both",
trace = FALSE
)
summary(step_mod)
y_pred_step <- predict(step_mod, newdata = test)
rmse_step <- sqrt(mean((test$y - y_pred_step)^2))
rmse_step
library(glmnet)
# glmnet requires predictors as a numeric matrix, not a data frame.
X_train <- as.matrix(train[, -1]) # contains all predictors but remove the outcome variable (y)
y_train <- train$y                # response
X_test  <- as.matrix(test[, -1])
y_test  <- test$y
cv_enet <- cv.glmnet(
X_train, y_train,
alpha = 0.5,                    # elastic net
nfolds = 10
)
# Extract the coefficients from the single elastic net model corresponding to the optimal λ.
coef(cv_enet, s = "lambda.min")
y_pred_enet <- predict(cv_enet, X_test, s = "lambda.min")
rmse_enet <- sqrt(mean((y_test - y_pred_enet)^2))
rmse_enet
c(
Stepwise_RMSE   = rmse_step,
ElasticNet_RMSE = rmse_enet
)
knitr::opts_chunk$set(
echo = TRUE,
message = FALSE,
warning = FALSE
)
library(dplyr)
library(purrr)
library(tibble)
#set.seed(123)
n <- 500
p <- 20
Sigma <- matrix(0.4, p, p)
diag(Sigma) <- 1
library(MASS)
X <- mvrnorm(n, mu = rep(0, p), Sigma = Sigma)
beta <- c(2, 2, 2, -1.5, -1.5, rep(0, p - 5))
y <- X %*% beta + rnorm(n, sd = 2)
dat <- data.frame(y, X)
colnames(dat) <- c("y", paste0("X", 1:p))
#set.seed(456)
idx <- sample(seq_len(n), size = 0.7 * n)
train <- dat[idx, ]
test  <- dat[-idx, ]
full_mod <- lm(y ~ ., data = train)
null_mod <- lm(y ~ 1, data = train)
step_mod <- step(
null_mod,
scope = list(lower = null_mod, upper = full_mod),
direction = "both",
trace = FALSE
)
summary(step_mod)
y_pred_step <- predict(step_mod, newdata = test)
rmse_step <- sqrt(mean((test$y - y_pred_step)^2))
rmse_step
library(glmnet)
# glmnet requires predictors as a numeric matrix, not a data frame.
X_train <- as.matrix(train[, -1]) # contains all predictors but remove the outcome variable (y)
y_train <- train$y                # response
X_test  <- as.matrix(test[, -1])
y_test  <- test$y
cv_enet <- cv.glmnet(
X_train, y_train,
alpha = 0.5,                    # elastic net
nfolds = 10
)
# Extract the coefficients from the single elastic net model corresponding to the optimal λ.
coef(cv_enet, s = "lambda.min")
y_pred_enet <- predict(cv_enet, X_test, s = "lambda.min")
rmse_enet <- sqrt(mean((y_test - y_pred_enet)^2))
rmse_enet
c(
Stepwise_RMSE   = rmse_step,
ElasticNet_RMSE = rmse_enet
)
knitr::opts_chunk$set(
echo = TRUE,
message = FALSE,
warning = FALSE
)
library(dplyr)
library(purrr)
library(tibble)
#set.seed(123)
n <- 500
p <- 20
Sigma <- matrix(0.4, p, p)
diag(Sigma) <- 1
library(MASS)
X <- mvrnorm(n, mu = rep(0, p), Sigma = Sigma)
beta <- c(2, 2, 2, -1.5, -1.5, rep(0, p - 5))
y <- X %*% beta + rnorm(n, sd = 2)
dat <- data.frame(y, X)
colnames(dat) <- c("y", paste0("X", 1:p))
#set.seed(456)
idx <- sample(seq_len(n), size = 0.7 * n)
train <- dat[idx, ]
test  <- dat[-idx, ]
full_mod <- lm(y ~ ., data = train)
null_mod <- lm(y ~ 1, data = train)
step_mod <- step(
null_mod,
scope = list(lower = null_mod, upper = full_mod),
direction = "both",
trace = FALSE
)
summary(step_mod)
y_pred_step <- predict(step_mod, newdata = test)
rmse_step <- sqrt(mean((test$y - y_pred_step)^2))
rmse_step
library(glmnet)
# glmnet requires predictors as a numeric matrix, not a data frame.
X_train <- as.matrix(train[, -1]) # contains all predictors but remove the outcome variable (y)
y_train <- train$y                # response
X_test  <- as.matrix(test[, -1])
y_test  <- test$y
cv_enet <- cv.glmnet(
X_train, y_train,
alpha = 0.5,                    # elastic net
nfolds = 10
)
# Extract the coefficients from the single elastic net model corresponding to the optimal λ.
coef(cv_enet, s = "lambda.min")
y_pred_enet <- predict(cv_enet, X_test, s = "lambda.min")
rmse_enet <- sqrt(mean((y_test - y_pred_enet)^2))
rmse_enet
c(
Stepwise_RMSE   = rmse_step,
ElasticNet_RMSE = rmse_enet
)
knitr::opts_chunk$set(
echo = TRUE,
message = FALSE,
warning = FALSE
)
library(dplyr)
library(purrr)
library(tibble)
#set.seed(123)
n <- 500
p <- 20
Sigma <- matrix(0.4, p, p)
diag(Sigma) <- 1
library(MASS)
X <- mvrnorm(n, mu = rep(0, p), Sigma = Sigma)
beta <- c(2, 2, 2, -1.5, -1.5, rep(0, p - 5))
y <- X %*% beta + rnorm(n, sd = 2)
dat <- data.frame(y, X)
colnames(dat) <- c("y", paste0("X", 1:p))
#set.seed(456)
idx <- sample(seq_len(n), size = 0.7 * n)
train <- dat[idx, ]
test  <- dat[-idx, ]
full_mod <- lm(y ~ ., data = train)
null_mod <- lm(y ~ 1, data = train)
step_mod <- step(
null_mod,
scope = list(lower = null_mod, upper = full_mod),
direction = "both",
trace = FALSE
)
summary(step_mod)
y_pred_step <- predict(step_mod, newdata = test)
rmse_step <- sqrt(mean((test$y - y_pred_step)^2))
rmse_step
library(glmnet)
# glmnet requires predictors as a numeric matrix, not a data frame.
X_train <- as.matrix(train[, -1]) # contains all predictors but remove the outcome variable (y)
y_train <- train$y                # response
X_test  <- as.matrix(test[, -1])
y_test  <- test$y
cv_enet <- cv.glmnet(
X_train, y_train,
alpha = 0.5,                    # elastic net
nfolds = 10
)
# Extract the coefficients from the single elastic net model corresponding to the optimal λ.
coef(cv_enet, s = "lambda.min")
y_pred_enet <- predict(cv_enet, X_test, s = "lambda.min")
rmse_enet <- sqrt(mean((y_test - y_pred_enet)^2))
rmse_enet
c(
Stepwise_RMSE   = rmse_step,
ElasticNet_RMSE = rmse_enet
)
knitr::opts_chunk$set(
echo = TRUE,
message = FALSE,
warning = FALSE
)
library(dplyr)
library(purrr)
library(tibble)
#set.seed(123)
n <- 500
p <- 20
Sigma <- matrix(0.4, p, p)
diag(Sigma) <- 1
library(MASS)
X <- mvrnorm(n, mu = rep(0, p), Sigma = Sigma)
beta <- c(2, 2, 2, -1.5, -1.5, rep(0, p - 5))
y <- X %*% beta + rnorm(n, sd = 2)
dat <- data.frame(y, X)
colnames(dat) <- c("y", paste0("X", 1:p))
#set.seed(456)
idx <- sample(seq_len(n), size = 0.7 * n)
train <- dat[idx, ]
test  <- dat[-idx, ]
full_mod <- lm(y ~ ., data = train)
null_mod <- lm(y ~ 1, data = train)
step_mod <- step(
null_mod,
scope = list(lower = null_mod, upper = full_mod),
direction = "both",
trace = FALSE
)
summary(step_mod)
y_pred_step <- predict(step_mod, newdata = test)
rmse_step <- sqrt(mean((test$y - y_pred_step)^2))
rmse_step
library(glmnet)
# glmnet requires predictors as a numeric matrix, not a data frame.
X_train <- as.matrix(train[, -1]) # contains all predictors but remove the outcome variable (y)
y_train <- train$y                # response
X_test  <- as.matrix(test[, -1])
y_test  <- test$y
cv_enet <- cv.glmnet(
X_train, y_train,
alpha = 0.5,                    # elastic net
nfolds = 10
)
# Extract the coefficients from the single elastic net model corresponding to the optimal λ.
coef(cv_enet, s = "lambda.min")
y_pred_enet <- predict(cv_enet, X_test, s = "lambda.min")
rmse_enet <- sqrt(mean((y_test - y_pred_enet)^2))
rmse_enet
c(
Stepwise_RMSE   = rmse_step,
ElasticNet_RMSE = rmse_enet
)
knitr::opts_chunk$set(
echo = TRUE,
message = FALSE,
warning = FALSE
)
library(dplyr)
library(purrr)
library(tibble)
set.seed(123)
n <- 500
p <- 20
Sigma <- matrix(0.4, p, p)
diag(Sigma) <- 1
library(MASS)
X <- mvrnorm(n, mu = rep(0, p), Sigma = Sigma)
beta <- c(2, 2, 2, -1.5, -1.5, rep(0, p - 5))
y <- X %*% beta + rnorm(n, sd = 2)
dat <- data.frame(y, X)
colnames(dat) <- c("y", paste0("X", 1:p))
set.seed(456)
idx <- sample(seq_len(n), size = 0.7 * n)
train <- dat[idx, ]
test  <- dat[-idx, ]
full_mod <- lm(y ~ ., data = train)
null_mod <- lm(y ~ 1, data = train)
step_mod <- step(
null_mod,
scope = list(lower = null_mod, upper = full_mod),
direction = "both",
trace = FALSE
)
summary(step_mod)
y_pred_step <- predict(step_mod, newdata = test)
rmse_step <- sqrt(mean((test$y - y_pred_step)^2))
rmse_step
library(glmnet)
# glmnet requires predictors as a numeric matrix, not a data frame.
X_train <- as.matrix(train[, -1]) # contains all predictors but remove the outcome variable (y)
y_train <- train$y                # response
X_test  <- as.matrix(test[, -1])
y_test  <- test$y
cv_enet <- cv.glmnet(
X_train, y_train,
alpha = 0.5,                    # elastic net
nfolds = 10
)
# Extract the coefficients from the single elastic net model corresponding to the optimal λ.
coef(cv_enet, s = "lambda.min")
y_pred_enet <- predict(cv_enet, X_test, s = "lambda.min")
rmse_enet <- sqrt(mean((y_test - y_pred_enet)^2))
rmse_enet
c(
Stepwise_RMSE   = rmse_step,
ElasticNet_RMSE = rmse_enet
)
